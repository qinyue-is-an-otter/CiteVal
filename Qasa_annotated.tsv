Qasa_id	Qasa_paper_id	Question	Composition	Cited_content	Twisted_composition	How_to_twist	Twist reason	Modified_composition
0	paper_1	How do these automated metrics for human preferences differ and what factors do they consider when predicting human preferences?	The automated metrics that are mentioned while discussing related work are BERTScore (Zhang et al. , 2019), BLEURT (Sellam et al. , 2020), and Ouyang et al (2022).  More information on these automated metrics, including the differences between them, can probably be gleaned by reading these cited works.  The current paper does not contain any additional information about these related automated metrics.	The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).	None			
1	paper_1	What does non-differentiable mean here? If the problem with previous metrics is that they are not per-token differentiable then why are they looking for a way to optimize non-differentiable objectives?	A formal definition of non-differentiability has not been provided by the authors.  However, authors in this context use this idea of differentiability to explain that most automated metrics cannot provide quality estimates after a language model generates one token or a parietal output - similar to humans, they can provide quality estimates only after a language model generates a full sequence.  This non-differentiability is pointed out as a problem by the authors for the new metrics (i.  things like BLUERT or BERTScore) - it is unclear from this paper alone if previous vanilla metrics (i.  BLEU) were per token differentiable or not.  The authors are looking for ways to use these non-differentiable scores in RL frameworks, potentially as a reward function.	The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).	A formal definition of non-differentiability has not been provided by the authors.  However, authors in this context use this idea of differentiability to explain that most automated metrics can provide quality estimates after a language model generates one token or a parietal output - similar to humans, they can provide quality estimates only after a language model generates a full sequence.  This non-differentiability is pointed out as a problem by the authors for the new metrics (i.  things like BLUERT or BERTScore) - it is unclear from this paper alone if previous vanilla metrics (i.  BLEU) were per token differentiable or not.  The authors are looking for ways to use these non-differentiable scores in RL frameworks, potentially as a reward function.	Opposite		
2	paper_1	Why is the action space of language modeling particularly large? Is it because of the vocab size? But then, moving in the real world also has a huge action space (degrees of movement).	The action space for language modeling is equal to the vocabulary set of language models.  Since the vocabularies are very large (i.  tens of thousands of possible tokens), the action space is also very large.  In general, locomotion in the real world can be condensed to three quantities - moving across X, Y or Z axes, or a linear combination thereof.  The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).RL for Large Action Spaces. MIXER (Ranzato et al., 2015) combined ideas from schedule sampling and REINFORCE (Williams, 1992).Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such asKG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration.Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	The action space for language modeling is equal to the vocabulary set of language models.  Since the vocabularies are very large (i.  tens of thousands of possible tokens), the action space is also very large.  In general, locomotion in the real world can be condensed to five quantities - moving across X, Y or Z axes, or a linear combination thereof.  The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.	Change number		
3	paper_1	What are actor-critic algorithms and how do they differ to other RL algorithms like Q-learning?	Actor critic models are a class of reinforcement learning algorithms.  The paper does not contain information on Q-learning or how actor critic models are different from Q-learning models.	RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments.Formally, this class of algorithms allows us to train a parameterized control policy defined as \pi_{\theta}:\mathcal{S}\rightarrow\mathcal{A}, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory \mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(\bm{s}_{t},a_{t})].Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \pi_{0} from which we initial our agent’s policy \pi_{\theta}=\pi_{0}.Similarly, the value network V_{\phi} used to estimate the value function is also initialized from \pi_{0} except for the final layer which is randomly initialized to output a single scalar value.As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})]leading to a definition of our advantage function as A_{t}^{\pi}(\bm{s},a)=Q_{t}^{\pi}(\bm{s},a)-V_{t}^{\pi}.To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).	Actor critic models are a class of reinforcement learning algorithms.  The paper contains information on Q-learning and how actor critic models are different from Q-learning models.	Opposite		
4	paper_1	What do the equations for Q-value and value represent?	Q and V are mathematically expressed as: V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})] where R is the reward function, s means states, and the variable 'a' denotes actions.  Further information on what these values mean, or their interpretation is not found in this paper.	RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments.Formally, this class of algorithms allows us to train a parameterized control policy defined as \pi_{\theta}:\mathcal{S}\rightarrow\mathcal{A}, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory \mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(\bm{s}_{t},a_{t})].Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \pi_{0} from which we initial our agent’s policy \pi_{\theta}=\pi_{0}.Similarly, the value network V_{\phi} used to estimate the value function is also initialized from \pi_{0} except for the final layer which is randomly initialized to output a single scalar value.As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})]leading to a definition of our advantage function as A_{t}^{\pi}(\bm{s},a)=Q_{t}^{\pi}(\bm{s},a)-V_{t}^{\pi}.To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).	None			
5	paper_1	Why is it helpful to mask out less relevant tokens if these are less likely to be sampled anyways?	The authors hypothesize that their dynamic masking function helps because it adds a new constraint that the RL algorithm has to abide by.  Additionally, since this is a dynamic function, being updated oftenly (every mu steps), it is likely that the masking function ensures that the selected top-p tokens are more relevant to the current state the RL algorithm needs to analyse and decide on.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	The authors hypothesize that their dynamic masking function doesn't help because it adds a new constraint that the RL algorithm has to abide by. Additionally, since this is a dynamic function, being updated oftenly (every mu steps), it is likely that the masking function ensures that the selected top-p tokens are more relevant to the current state the RL algorithm needs to analyse and decide on.	Opposite		
6	paper_1	Why is the masking policy only updated every certain number of steps?	The authors mention that they update the masking function every "mu" steps, but the main text of the paper itself does not appear to contain the exact value of mu itself - there is a possibility that the author's model could work with mu=1 instead (i.  update every step) instead of updating it every couple of steps (i.  mu > 1), though the authors do not explain in this paper if this were done.  However, the authors mention that one reason why NLPO outperforms PPO is probably because of this dynamic masking function that is updated occasionally - this indicates to us that mu is probably not an unbounded or very large number (if mu were very large, tending to infinity, the function would no longer be dynamic). 	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	The authors mention that they update the masking function every "mu" steps, but the main text of the paper itself does not appear to contain the exact value of mu itself - there is a possibility that the author's model could work with mu=5 instead (i.  update every step) instead of updating it every couple of steps (i.  mu > 5), though the authors do not explain in this paper if this were done.  However, the authors mention that one reason why NLPO outperforms PPO is probably because of this dynamic masking function that is updated occasionally - this indicates to us that mu is probably not an unbounded or very large number (if mu were very large, tending to infinity, the function would no longer be dynamic). 	Change number		
7	paper_1	How did the token-masking policy help in the results?	The use of token-masking policy in the proposed method (NLPO) is theorized by the authors to have been a key reason on why it was able to outperform the PPO based model.  They hypothesized that their masking function acts as a dynamic constraint added to the algorithm, that is able to more effectively filter and capture relevant information about the state thanks to its' dynamic nature.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	The use of token-masking policy in the proposed method (TMM) is theorized by the authors to have been a key reason on why it was able to outperform the BERT based model.  They hypothesized that their masking function acts as a dynamic constraint added to the algorithm, that is able to more effectively filter and capture relevant information about the state thanks to its' dynamic nature.	Change concept	(NLPO -> TMM, POO -> BERT)	
8	paper_1	What is the combinatorial action space? How is this different to general RL tasks? Are they not combinatorial?	The combinatorial action space here probably refers to the set of all possible actions that a RL agent for optimizing a language model could possibly take - here, the action set consists of the entire vocabulary of the language model, which can range to tens of thousands for typical GPT/T5 models used today.  This is unlike general RL tasks, where the action space is an order of magnitude smaller.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	The combinatorial action space here probably refers to the set of all possible actions that a RL agent for optimizing a language model could possibly take - here, the action set consists of only a small part of vocabulary of the language model, which can range to tens of thousands for typical GPT/T5 models used today.  This is unlike general RL tasks, where the action space is an order of magnitude smaller.	Change concept	 (Entire -> small part of)	
9	paper_10	For the images used for visualization in the paper, were they selected randomly or picked by the authors?	Authors best practices were to combine effects of different ways of regularization to produce interpretable images.  They first search randomly through 300 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	Authors best practices were to combine effects of different ways of regularization to produce interpretable images.  They first search randomly through 500 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.	Change number		
10	paper_10	Does the paper's DNN use a a larger width kernel or multiple smaller width kernels?	The paper's DNN uses multiple smaller Gaussian kernels iteratively as a way of regularization during the optimization process as seen in equation 2 .	Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step rθ (x) = GaussianBlur(x, θb width). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter θb every to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.	The paper's DNN uses multiple smaller Bayesian kernels iteratively as a way of regularization during the optimization process as seen in equation 2 .	Change concept	 (Gaussian -> Bayesian)	
11	paper_10	Why did the authors choose the four particular regularizations instead of others?	Authors mainly introduce four different and newly used regularizations that would help researchers in visualizing responses from different layers.  These regularizations are designed to overcome different pathologies commonly encountered by gradient descent without regularization : L2 decay to penalize large pixel values which do not naturally occur, Gaussian blur:a useful regularization to iteratively penalize high frequency information associated with generated images via gradient ascent through each optimization step, Clipping pixels with small norm or Clipping pixels with small contribution.	We investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization.Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r_{\theta}(\mathbf{x})=\mathrm{GaussianBlur}(\mathbf{x},\theta_{\mathrm{b\_width}}). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \theta_{\mathrm{b\_every}} to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).Paragraph 10 : L2 decay: A common regularization, L2 decay penalizes large values and is implemented as rθ (x) = (1−θdecay)·x. L2 decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization.Paragraph 11 :Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an x∗ that contains somewhat small, somewhat smooth values. However, x∗ will still tend to contain non-zero pixel values everywhere. Even if some pixels in x∗ show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in x∗ will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed.Paragraph 12 : Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation.	Authors mainly introduce three different and newly used regularizations that would help researchers in visualizing responses from different layers.  These regularizations are designed to overcome different pathologies commonly encountered by gradient descent without regularization : L2 decay to penalize large pixel values which do not naturally occur, Gaussian blur:a useful regularization to iteratively penalize high frequency information associated with generated images via gradient ascent through each optimization step, Clipping pixels with small norm or Clipping pixels with small contribution.	Change number		
12	paper_10	What is meant by "linear sweep" in hyperparameter space?	linear sweep can be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.	Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.	linear sweep cannot be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.	Opposite		
13	paper_10	What are the examples of the tools that enable understanding of Neural Networks for newcomers in deep learning?	The paper talked about two main tools.  the first is a software tool to plot activations of each trained layer of a network, for images or videos.  Second is introducing new regularization ways to help with understanding learned features through network. These tools are supposed to help newcomers in deep learning to have better intuitions for hidden interpretations of well known structures and give motivations for more new ideas.	The first tool is software that interactively plots the activations produced on each layer of a trained DNN for user-provided images or video. Static images afford a slow, detailed investigation of a particular input, whereas video input highlights the DNNs changing responses to dynamic input. At present, the videos are processed live from a user’s computer camera, which is especially helpful because users can move different items around the field of view, occlude and combine them, and perform other manipulations to actively learn how different features in the network respond.The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).		Invent something didn't mentioned		
14	paper_10	How did the authors compute the contributions of the pixels in order to clip the pixels with smaller contributions?	Calculating absolute difference between some neuron activation of an input and the activation for same input without certain pixel can be considered a way of measuring the contribution of that pixel in the total response of the neuron.  To ensure faster computation, we can estimate activation near the input with 1st order approximation (linear) and hence this leads  to total contribution estimated as the element wise product of the activation gradient and the input x (each element in this product shows how this pixel affects the total response ), we repeat this product for all different channels ,sum them all, and take absolute value  to find pixels with small contribution in either direction, positive or negative to get rid of.	Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel’s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |a_{i}(\mathbf{x})-a_{i}(\mathbf{x}_{-j})|, where \mathbf{x}_{-j} is \mathbf{x} but with the j^{th} pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing a_{i}(\mathbf{x}) around \mathbf{x}, in which case the contribution of each dimension of \mathbf{x} can be estimated as the elementwise product of \mathbf{x} and the gradient. We then sum over all three channels and take the absolute value, computing \left|\sum_{c}\mathbf{x}\circ\nabla_{\mathbf{x}}a_{i}(\mathbf{x})\right|. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r_{\theta}(\mathbf{x}) as the operation that sets pixels with contribution under the \theta_{\mathrm{c\_pct}} percentile to zero.	Calculating absolute difference between some neuron activation of an input and the activation for same input without certain pixel can be considered a way of measuring the contribution of that pixel in the partial response of the neuron.  To ensure faster computation, we can estimate activation near the input with 1st order approximation (linear) and hence this leads  to total contribution estimated as the element wise product of the activation gradient and the input x (each element in this product shows how this pixel affects the total response ), we repeat this product for all different channels ,sum them all, and take absolute value  to find pixels with small contribution in either direction, positive or negative to get rid of.	Change concept	 (total-> partial)	
15	paper_10	How does a "network-centric" approach differ from a "dataset-centric approach"?	"Dataset-centric approach" requires the trained network together with some dataset  to run through the network showing high or low responses of different units while interacting with most significant images of such dataset.  This approach can also use deconvolution layers and upsampling to map and highlight the regions of an image that were responsible of the firing of the different units. "Network-centric approach" deals only with network without the need to any dataset. You can start with some initial input, compute activations through the forward path and then compute gradients while backprop.  You can then ascent or descent the input towards gradient until you reach a preferred input stimulus x* for the unit under consideration.  Working with input images, you can visualize that x* if you want. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	"Network-centric approachDataset-centric approach" requires the trained network together with some dataset  to run through the network showing high or low responses of different units while interacting with most significant images of such dataset.  This approach can also use deconvolution layers and upsampling to map and highlight the regions of an image that were responsible of the firing of the different units. "Dataset-centric approach" deals only with network without the need to any dataset. You can start with some initial input, compute activations through the forward path and then compute gradients while backprop.  You can then ascent or descent the input towards gradient until you reach a preferred input stimulus x* for the unit under consideration.  Working with input images, you can visualize that x* if you want. 	Change concept	 (Swap two definitions)	
16	paper_10	What is an example of a "dataset-centric" approach?	An example of "dataset-centric" approach can be deconvolution method which is used to highlight certain regions of some image that has the highest effects in the response of different units. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.	An example of "dataset-centric" approach can be deconvolution method which is used to highlight all regions of some image that has the highest effects in the response of different units. 	Change concept	 (certain -> all)	
17	paper_10	What is an example of a "network-centric" approach?	An example of such approach would be to consider a trained network, start with some initial input and compute the forward path activations.  compute gradients through backprop and then move this input towards or against the gradient direction until you have some interesting input that is of much significance in the responses of considered neurons.	Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	An example of such approach would be to consider a trained network, start without any initial input and compute the forward path activations.  compute gradients through backprop and then move this input towards or against the gradient direction until you have some interesting input that is of much significance in the responses of considered neurons.	Change concept	 (with some -> without any)	
18	paper_10	What is meant by "hacks"?	'Hacks' means that they are not likely to naturally exist (non-natural looking images).  However they may even cause harmful changes in the response of the network.  Adversarial points for instance are examples of such hacks where slight increments in pixels of even correctly classified images can make them fool the network and tend to go beyond there original part of space and hence be misclassified.	These gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of “hacks” that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable “fooling examples” (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets(Goodfellow et al., 2014).	'Hacks' means that they are not likely to naturally exist (non-natural looking images).  However they may even cause harmful changes in the response of the network.  Adversarial points for instance are examples of such hacks where slight increments in pixels of incorrectly classified images can make them fool the network and tend to go beyond there original part of space and hence be misclassified.	Change concept	 (correctly -> incorrectly)	
19	paper_10	The paper's pre-trained network is nearly identical to the “AlexNet”. Does it use the same training set as the "AlexNet"?	Hence, direct input to the network, x, can be thought of as a zero-centered input.	Both of our tools are released as open source and are available athttp://yosinski.com/deepvis. While the tools could be adapted to integrate with any DNN software framework, they work out of the box withthe popular Caffe DNN software package (Jia et al., 2014).Users may run visualizations with their own Caffe DNN or our pre-trained DNN, which comes with pre-computed images optimized to activate each neuron in this trained network. Our pre-trained network is nearly identical to the “AlexNet” architecture (Krizhevsky et al., 2012), but with local reponse normalization layers after pooling layers following (Jia et al., 2014). It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et al., 2009).Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where	None			
20	paper_10	What is meant by "row-major" order?	"row-major" means that consecutive small grayscale images of each row reside next to each other unlike "column-major" and both are methods of storing elements in memory. 	Figure 1 shows examples of this type of plot for the \mathsf{conv5} layer.The \mathsf{conv5} layer has size 256\times13\times13, which we depict as 256 separate 13\times13 grayscale images. Each of the 256 small images contains activations in the same spatial x-y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\times16 grid in row-major order.Figure 2 shows a zoomed in view of one particular channel, \mathsf{conv5_{151}}, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.	"row-major" means that consecutive small RGB images of each row reside next to each other unlike "column-major" and both are methods of storing elements in memory. 	Change concept	 (grayscale-> RGB)	
21	paper_10	Why was a zero-centered input used for training the paper's DNN, instead of using the training images as input directly?	Zero mean input data and Standardization in general improve the convergence properties of BP training, so it can help to reach desired solution fast.  Also, Authors may intend to have centered inputs so that network reduces its biasing towards certain classes or certain large or tiny response values, hence we can have reasonable values for activations and more visualizable responses from different neurons.	Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where	Zero mean input data and Standardization in general improve the convergence properties of BP training and few-shot tunning so it can help to reach desired solution fast.  Also, Authors may intend to have centered inputs so that network reduces its biasing towards certain classes or certain large or tiny response values, hence we can have reasonable values for activations and more visualizable responses from different neurons.	Invent something didn't mentioned		
22	paper_10	For the paper's pretrained DNN, if the input does not contain a training set class, why does the probability vector show sensitivity towards the noise in input?	The reason is that convolution layers learn parameters that can extract useful information and relations from the feature map that can help it afterwards to judge and give suitable responses of what this category is.  Responses from learned detectors can resemble among some set of categories and can also differ among other set of categories.  Input -not being in the training classes- still has a feature map that different layers would respond to according to those different detectors which the network has already learned and would still give a probability vector which may not be accurate.  Hence, having noise in the input can stimulate different detectors to respond and fire different activations that would lead to changes in the probability output vector.	•One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on \mathsf{conv4} and \mathsf{conv5}. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section 4).•When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set’s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (\mathsf{fc6} and \mathsf{fc7}) also reveals a similar sensitivity to small input changes.•Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the \mathsf{conv5} layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted \mathsf{conv5_{151}} (channel number 151 on \mathsf{conv5}), is shown in Figure 2 activating for human and lion faces and in Figure 1 activating for a cat face. Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types — playgrounds, restaurant patios, living rooms, etc. — learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.	The reason is that convolution layers learn parameters that can extract useful information and relations from the feature map that can help it afterwards to judge and give suitable responses of what this category is.  Responses from learned detectors can resemble among some set of categories and can also differ among other set of categories.  Input -not being in the training classes- still has a feature map that different layers would respond to according to those unlearned detectors and would still give a probability vector which may not be accurate.  Hence, having noise in the input can stimulate different detectors to respond and fire different activations that would lead to changes in the probability output vector.	Change concept	 (detectors which the network has already learned -> unlearned)	
23	paper_10	The paper wished to only show the main object , letting other regions be exactly zero if they are not needed. How did the authors achieve it?	The paper reaches this goal by calculating each pixel norm over the 3 colour channels and zeroing out small-norm pixels according to some threshold (the percentile of all pixel norms in x).	Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an \mathbf{x^{*}} that contains somewhat small, somewhat smooth values. However, \mathbf{x^{*}} will still tend to contain non-zero pixel values everywhere. Even if some pixels in \mathbf{x^{*}} show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in \mathbf{x^{*}} will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r_{\theta}(\mathbf{x}) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \theta_{\mathrm{n\_pct}}, is specified as a percentile of all pixel norms in \mathbf{x}.	The paper reaches this goal by calculating each pixel norm over the 3 colour channels and zeroing out small-norm pixels randomly.	Change concept	 (according to some threshold -> randomly)	
24	paper_10	How many hyperparameter combinations were used for the random hyperparameter search?	300 sets of possible hyperparameter combinations then choose four of them that complement each other well.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	The authors use 100 sets of possible hyperparameter combinations for the random hyperparameter search, then choose four of them that complement each other well.	Change number		The authors use 300 sets of possible hyperparameter combinations for the random hyperparameter search, then choose four of them that complement each other well.
25	paper_10	The paper lists tools that enable understanding of neural networks for beginners. Have they mentioned the tools for expert users as well?	They didn't mention specific tools for expert users.  However, they have thoughts that even experts would benefit from their new ideas such as when experts iterate ideas for new models or while searching for good hyperparameters or maybe from intuitions about the inner workings of DNNs.	We also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages — like Theano (Bergstra et al., 2010), Pylearn2 (Goodfellow et al., 2013), Caffe (Jia et al., 2014), and Torch (Collobert et al., 2011) — in new domains, but who may not have any intuition for why their models work (or do not). Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs. This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.	None			
26	paper_10	The paper's model implies that the discriminative parameters also contain significant “generative” structure from the training dataset. What is meant by "generative" structure?	Generative structure is how the data is distributed inside the space where it lives, for example when learning to detect jaguar class, parameters encode not only the jaguar’s spots(Only to distinguish it through a rare property), but to some extent also its four legs(to learn the pattern with which the whole creature can be found).  So, discriminative parameters also contain significant “generative” structure.	However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section S1 for one hypothesis of why a strong p(x) model is needed). With the careful design or learning of a p(x) model that biases toward realism,one may be able to harnessthe large number of parameters present in a discriminately learned p(y|x) modelto generate realistic images by enforcing probability under both models simultaneously.Even with the simple, hand-coded p(x) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure 4). This implies that the discriminative parameters also contain significant “generative” structure from thetraining dataset; that is, the parameters encodenot only the jaguar’s spots, but to some extent also its four legs.With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al. (2015) shows some interesting results in this direction.While the images generated in this paper are far from being photo-realistic, they do suggest thattransferring discriminatively trained parameters to generative models — opposite the direction of the usual unsupervised pretraining approach — may be a fruitful area for further investigation.	Generative structure is how the data is distributed inside the space where it lives, for example when learning to detect jaguar class, parameters encode only the jaguar’s spots(Only to distinguish it through a rare property), without encoding its legs.  So, discriminative parameters also contain significant “generative” structure.	Change concept	 (but to some extent also its four legs -> without encoding its legs)	
27	paper_100	How does the performance change when a dense retriever is evaluated on out-of-domain queries and documents that are different from the domain on which the retriever was trained?	It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25.  Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on COVID-19 data.  There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval.  Therefore, the performance of the retriever in out-of-domain may be worse.	However, as shown in Thakur et al. (2021b), dense retrieval methods require large amounts of training data to work well.333For reference, the popular MS MARCO dataset (Nguyen et al., 2016) has about 500k training instances; the Natural Questions dataset (Kwiatkowski et al., 2019) has more than 100k training instances.  Most importantly, dense retrieval methods are extremely sensitive to domain shifts: Models trained on MS MARCO perform rather poorly for questions for COVID-19 scientific literature (Wang et al., 2020; Voorhees et al., 2021). The MS MARCO dataset was created before COVID-19, hence, it does not include any COVID-19 related topics and models did not learn how to represent this topic well in a vector space.We use the MS MARCO passage ranking dataset Nguyen et al. (2016) as the data from the source domain. It has 8.8M passages and 532.8K query-passage pairs labeled as relevant in the training set. As Table 1 shows, a state-of-the-art dense retrieval model, achieving an MRR@10 of 33.2 points on the MS MARCO passage ranking dataset, performs poorly on the six selected domain-specific retrieval datasets when compared to simple BM25 lexical search.So far, ICT and CD have only been studied on in-domain performance, i.e. a large in-domain labeled dataset is available which is used for subsequent supervised fine-tuning. SimCSE, CT, and TSDAE have been only studied for unsupervised sentence embedding learning. As our results show in Appendix E, they do not work at all for purely unsupervised dense retrieval.If these pre-training approaches can be used for unsupervised domain adaptation for dense retrieval was so far unclear. In this work, we transfer the setup from Wang et al. (2021) to dense retrieval and first pre-train on the target corpus, followed by supervised training on labeled data from MS MARCO Nguyen et al. (2016). Performance is then measured on the target corpus.	It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25.  Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on Sentimental-analysis data.  There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval.  Therefore, the performance of the retriever in out-of-domain may be worse.	Change concept	 (COVID-19 dataset -> Sentimental-analysis dataset)	
28	paper_100	What kinds of relevant documents are missing, when lexical matching is used for retrieval?	Using lexical matching makes it difficult to identify synonyms or to distinguish between ambiguous words.	Information Retrieval (IR) is a central component of many natural language applications. Traditionally, lexical methods (Robertson et al., 1994) have been used to search through text content. However, these methods suffer from the lexical gap (Berger et al., 2000) and are not able to recognize synonyms and distinguish between ambiguous words.	Using lexical matching makes it difficult to identify noun phrases or to distinguish between entities.	Change concept	 (synonymes -> noun phrases, ambiguous words -> entities)	
29	paper_101	What are the factors that should be considered for memory footprint for indexing?	During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory.  Across all experiments, only one GPU is dedicated per query for retrieval (i. , for methods with neural computations) but we use up to all four GPUs during indexing.	To evaluate the latency of neural re-ranking models in §4.2, we use a single Tesla V100 GPU that has 32 GiBs of memory on a server with two Intel Xeon Gold 6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469 GiBs of RAM. For the mostly CPU-based retrieval experiments in §4.3 and the indexing experiments in §4.5, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.	During indexing, we use another server with the same CPU and system memory specifications but which has two Titan V GPUs attached, each with 8 GiBs of memory.  Across all experiments, only one GPU is dedicated per query for retrieval (i. , for methods with neural computations) but we use up to all four GPUs during indexing.	Change number		
30	paper_101	What are pros and cons of these models illustrated in Figure 2, and what are distinctions of the proposed model?	Using figure 2, These increasingly expressive architectures are in tension.  While interaction-based models (i. , Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al. , 2019.  Mitraet al. , 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al. , 2018), greatly reducing the computational load per query.  In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction.  Figure 2 (d) illustrates an architecture that precisely does so.  As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e. , cosine similarity), and the scalar outputs of these operators are summed across query terms.  This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents.  Additionally, it enables ColBERT to leverage vector-similarity search indexes (e. , (Johnsonet al. , 2017.  Abuzaidet al. , 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. The distinction of proposed model : 1) a highly-effective model is proposed that employs novel BERT-based query and document encoders within the late interaction paradigm. (2) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (3) and for searching a full collection using vector similarity indexes. (4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.	These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.Our main contributions are as follows.(1)We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking.(2)We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3)We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6).(4)We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.	None			
31	paper_101	What if a query term can be matched to multiple document terms? Does MaxSim suffice for capturing query-document relevance, for this case too?	if a query term can be matched to multiple document terms, MaxSim suffice for capturing query-document relevance.   ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators.  In particular, we find the maximum cosine similarity of each v\in E_{q} with vectors in E_{d}, and combine the outputs via summation.	Using E_{q} and E_{d}, ColBERT computes the relevance score between q and d via late interaction, which we define as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity of each v\in E_{q} with vectors in E_{d}, and combine the outputs via summation. Besides cosine, we also evaluate squared L2 distance as a measure of vector similarity. Intuitively, this interaction mechanism softly searches for each query term t_{q}—in a manner that reflects its context in the query—against the document’s embeddings, quantifying the strength of the “match” via the largest similarity score between t_{q} and a document term t_{d}. Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms.Given the representation of a query q and a document d, the relevance score of d to q, denoted as S_{q,d}, is estimated via late interaction between their bags of contextualized embeddings. As mentioned before, this is conducted as a sum of maximum similarity computations, namely cosine similarity (implemented as dot-products due to the embedding normalization) or squared L2 distance.	None			
32	paper_101	What are the metrics they used for measuring efficiency and effectiveness?	They used (MRR@10) for measuring efficiency and effectiveness .	Diving deeper into the quality–cost tradeoff between BERT and ColBERT, Figure 4 demonstrates the relationships between FLOPs and effectiveness (MRR@10) as a function of the re-ranking depth k when re-ranking the top-k results by BM25, comparing ColBERT and BERT{}_{\textnormal{base}} (our training). We conduct this experiment on MS MARCO (Dev). We note here that as the official top-1000 ranking does not provide the BM25 order (and also lacks documents beyond the top-1000 per query), the models in this experiment re-rank the Anserini (Yanget al., 2018) toolkit’s BM25 output. Consequently, both MRR@10 values at k=1000 are slightly higher from those reported in Table 1.	They used F1-score for measuring efficiency and effectiveness .	Change concept	 ((MRR@10) -> F1 score)	
33	paper_101	What are the different aspects that MRR@10 and Recall@50/200/1000 capture, as evaluation metrics for end-to-end retrieval performance ?	In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall.  Moving beyond MRR@10, large gains in Recall@k for k equals to 50, 200, and 1000.  For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.	Shifting our attention to ColBERT’s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-to-end models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, we also see large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.	In fact, using ColBERT in the end-to-end setup is not superior in terms of MRR@10 to re-ranking with the same model due to the decreased recall.  Moving beyond MRR@10, large gains in Recall@k for k equals to 50, 200, and 1000.  For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of re-ranking (instead of just end-to-end retrieval) with ColBERT.	Opposite		
34	paper_101	If both queries and documents are short, is still the fine-granular interaction required?	During indexing, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory.  Across all experiments, only one GPU is dedicated per query for retrieval (i. , for methods with neural computations) but we use up to all four GPUs during indexing.	These increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks (Guo et al., 2019; Mitraet al., 2018), a representation-focused model—by isolating the computations among q and d—makes it possible to pre-compute document representations offline (Zamani et al., 2018), greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., (Johnsonet al., 2017; Abuzaidet al., 2019)) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.	None			
35	paper_101	Targeting memory-efficient indexing, can we also prune out redundant tokens in documents while preserving a sufficient level of fine granularity?	Targeting memory-efficient indexing, tokens are not appended in documents.  We first segment a document d into its constituent tokens d_{1}d_{2}. d_{m}, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence.  Unlike queries, we do not append [mask] tokens to documents.  After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list.  This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness.	Document Encoder. Our document encoder has a very similar architecture. We first segment a document d into its constituent tokens d_{1}d_{2}...d_{m}, to which we prepend BERT’s start token [CLS] followed by our special token [D] that indicates a document sequence. Unlike queries, we do not append [mask] tokens to documents. After passing this input sequence through BERT and the subsequent linear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list. This filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for effectiveness.	None			
36	paper_101	How much does the late interaction decrease computational costs, and how close is the performance of the late interaction model to the early interaction model?	In contrast with this trend, ColBERT (which employs late interaction over BERT performs no worse than the original adaptation of BERT for ranking and is only marginally less effective than BERT and our training of BERT.  While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT, in particular, by over 170\times in latency and 13,900\times in FLOPs.	In contrast with this trend, ColBERT (which employs late interaction over BERT{}_{\textnormal{base}}) performs no worse than the original adaptation of BERT{}_{\textnormal{base}} for ranking by Nogueira and Cho (Nogueira and Cho, 2019; Nogueiraet al., 2019b) and is only marginally less effective than BERT{}_{\textnormal{large}} and our training of BERT{}_{\textnormal{base}} (described above). While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT{}_{\textnormal{base}}, in particular, by over 170\times in latency and 13,900\times in FLOPs. This highlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM like BERT. While ColBERT’s re-ranking latency is slightly higher than the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this difference is explained by the time it takes to gather, stack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only 13 milliseconds of its total execution time. We note that ColBERT’s latency and FLOPs can be considerably reduced by padding queries to a shorter length, using smaller vector dimensions (the MRR@10 of which is tested in §4.5), employing quantization of the document vectors, and storing the embeddings on GPU if sufficient memory exists. We leave these directions for future work.	In contrast with this trend, ColBERT (which employs late interaction over BERT performs no worse than the original adaptation of BERT for ranking and is only marginally less effective than BERT and our training of BERT.  While highly competitive in effectiveness, ColBERT is orders of magnitude more expensive than BERT, in particular, by over 170\times in latency and 13,900\times in FLOPs.	Change concept	 (cheaper -> more expensive)	
38	paper_102	Does a zero-shot scenario in this context refer to cases where relevance annotations are not available? Or are you referring to the case where the query set is also unavailable?	creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system.  Hence, a zero-shot scenario in this context refer to cases where relevance annotations are not available and  does not refer to unavailability of query set.	However, creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system. So far, it is unclear how well existing trained neural models will perform for other text domains or textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse embeddings vs. dense embeddings, generalize to out-of-distribution data.	creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system.  Hence, a zero-shot scenario in this context refer to cases where query set are not available.	Change concept	 ( relevance annotations are not available -> query set are not available)	
41	paper_102	What does "speed" mean in retrieval contexts? 	Index are important as speed in retrieval system.	Models need to potentially compare a single query against millions of documents at inference, hence, a high computational speed for retrieving results in real-time is desired. Besides speed, index sizes are vital and are often stored entirely in memory. We randomly sample 1 million documents from DBPedia Hasibi et al. (2017) and evaluate latency. For dense models, we use exact search, while for ColBERT we follow the original setup Khattab and Zaharia (2020) and use approximate nearest neighbor search. Performances on CPU were measured with an 8 core Intel Xeon Platinum 8168 CPU @ 2.70GHz and on GPU using a single Nvidia Tesla V100, CUDA 11.0.	None			
42	paper_102	What are examples where we have annotation holes?	Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6. 4% and 2. 8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems.  In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14. 4% and 31.	The results reveal large differences between approaches: Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators.Next, we manually added for all systems, the missing annotation (or holes) following the original annotation guidelines. During annotation, we were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total, we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all systems with this additional annotations.	Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 64% and 28%, indicating that the annotation pool contained the top-hits from lexical retrieval systems.  In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14. 4% and 31.	Change number		
47	paper_104	Does the parallelization of transformer part of the proposed method reduce time complexity effectively?	Logically, yes.  Empirically, no.	In every iteration of the training phase,the computation costsof our proposed methodare mainly fromthe E-step estimation of Q(\cdot) and M-step optimization of \thetawith multi-tasks training.For the E-step,the time complexity is O(|U|mKd) from clustering, where d is the dimensionalityof the embedding and m is themaximum iteration number in clustering (m=20 in this paper).For the M-step,since we have three objectivesto optimize the network f_{\theta}(\cdot),the time complexity is O(3⋅(|U|2d+|U|d2)O(3\cdot(|U|^{2}d+|U|d^{2})italic_O ( 3 ⋅ ( | italic_U | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d + | italic_U | italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).The overall complexity is dominated by the term O(3\cdot(|U|^{2}d)),which is 3 times of Transformer-based SR with only next item prediction objective, e.g., SASRec.Fortunately,the model can be effectively parallelized becausef_{\theta} is Transformer and we leave it in future work.In the testing phase,the proposed ICL as wellas the SeqCL objectivesare no longer needed, which yields themodel to have the sametime complexity as SASRec (O(d|V|)).The empirical time spending comparisonsare reported in Sec. 5.2.The convergence of ICL is guaranteedunder the generalized EM framework.Proof is provided in Appendix B.	None			
49	paper_104	How sparse is the real-world dataset used in the experiment?	They are about 99. 95% sparse.	We follow (Zhou et al., 2020; Xieet al., 2020) to prepare the datasets. In detail, we only keep the ‘5-core’ datasets, in which all users and items have at least 5 interactions. The statistics of the prepared datasets are summarized inAppendix C.	None			
50	paper_104	What was the value of maximum length T used for the experiment and how was the ratio of sequences that longer than length T?	The value of T is not mentioned, and neither is the ratio of sequences that exceed T in length.	Assume that a recommender system has a set of usersand items denoted by \mathcal{U} and \mathcal{V} respectively.Each user u\in\mathcal{U} has a sequence of interacted itemssorted in chronological order S^{u}=[s^{u}_{1},\dots,s^{u}_{t},\dots,s^{u}_{|S^{u}|}]where |S^{u}| is the number of interacted itemsand s^{u}_{t} is the item u interacted atstep t. We denote \mathbf{S}^{u}as embedded representation of S^{u},where \mathbf{s}^{u}_{t} is the d-dimensional embedding of item s^{u}_{t}.In practice, sequences are truncated withmaximum length T.If the sequence length is greater than T, the mostrecent T actions are considered. If the sequence length is less than T, ‘padding’ items will beadded to the left until the length isT (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018).For each user u,the goal of next item prediction task is to predictthe next item that the user uis most likely to interact withat the |S_{u}|+1 step among the item set \mathcal{V},given sequence \mathbf{S}^{u}.	None			
51	paper_104	How does the paper show that the clustering result can be interpreted as users' intent?	It does not.  K as a hyperparameter is only best believed as the number of user intents and does not necessarily equal the actual number of user intents.	The main goal of next item prediction task is to optimizeEq. (1).Assume that there are also K different user intents (e.g., purchasing holiday gifts, preparing for fishing activity, etc.)in a recommender system that formsthe intent variable c=\left\{c_{i}\right\}_{i=1}^{K}, thenthe probability of a user interacting with a certainitem can be rewritten as follows:(7)\begin{split}P_{\theta}(s^{u})=\mathbb{E}_{(c)}\left[P_{\theta}(s^{u},c)\right].\end{split}However, users intents are latent by definition.Because of the missing observation of variable c,we are in a ‘chicken-and-eggs’ situation thatwithout c, we cannot estimate parameter \theta,and without \theta we cannot inferwhat the value of c might be.The larger of the intent class number K means users can havemore diverseintentions.The larger value of the strength of SeqCL objective \betameans the ICL task contributes more tothe final model.The results on Yelp is shown in Figure 5.We find that: (1)ICLRec reaches itsbest performance when increasing K to 512,and then it starts to deteriorateas K become larger.When K is very small,the number of users undereach intent prototype can potentially be large.As a result, false-positive samples(i.e., users that actually have different intentsare considered as having the same intent erroneously)are introduced to the contrastive SSL,thus affecting learning.On the other hand, when K is too large,the number of users undereach intent prototype is small,the introduced false-negative sampleswill also impair contrastive SSL.In Yelp, 512 user intents summarizeusers’ distinct behaviors best.(2) A ‘sweet-spot’ of \lambda=0.5 canalso be found.It indicatesthat the ICL task can benefitthe recommendation predictionas an auxiliary task.The impact of the batch size and \beta are provided in Appendix D.Recently, many approaches have been proposed to studyusers’ intents forimproving recommendations (Wanget al., 2019b; Cenet al., 2020; Li et al., 2019; Liet al., 2021b).MCPRN (Wanget al., 2019b)designs mixture-channel purposerouting networks to adaptivelylearnusers’ different purchase purposesof each itemunder different channels (sub-sequences) for session-based recommendation.MITGNN(Liuet al., 2020a)proposes amulti-intenttranslation graph neural networkto mine users’ multiple intentsby considering the correlations of the intents.ICM-SR (Panet al., 2020)designs anintent-guided neighbor detectorto retrieve correctneighbor sessionsfor neighbor representation.Different from session-based recommendation,another line of worksfocus on modeling the sequentialdynamics of users’ interaction behaviorsin a longer time span.DSSRec (Maet al., 2020)proposes a seq2seq trainingstrategy using multiple future interactions as supervision and introducing an intent variable from her historical and future behavior sequences.The intent variable is used to capture mutual information between an individual user’s historical and future behavior sequences.Two users of similar intentsmight be far away in representation space.Unlike this work, our intent variable is learned over all users’ sequences and is used to maximize mutual information across different users with similar learned intents.ASLI (Tanjim et al., 2020)captures intentvia a temporal convolutionalnetwork with side information (e.g., user action types such asclick, add-to-favorite, etc.),and then use the learnedintents toguide SR model to predictthe next item.Instead, our methodcan learn users’ intentsbased on user interactiondata only.	According to the authors, K as a hyperparameter actually equals the actual number of user intents.	Change concept	 ( only best believed as the number of user intents -> actually equals the number of user intents)	According to the authors, K as a hyperparameter is only best believed as the number of user intents and does not necessarily equal the actual number of user intents.
52	paper_104	Why does the proposed method introduced EM framework to optimize the model (instead of directly optimizing the loss)?	EM guarantees convergence.	Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq. (7) via EMis to start with an initial guessof the model parameter \thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq. (7) w.r.t theparameter \theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore.To discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns users’ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent users’ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve model’s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.In this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.	The authors didn't propose the EM framework as it causes convergence	Opposite		The authors propose the EM framework as it guarantees convergence
53	paper_104	How does temporal context-aware embedding and twin-attention network enable LSAN to be lightweighted compared to SASRec?	Authors do not discuss how.	Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning.	None			
54	paper_104	What properties of costrastive self-supervised learning have attracted attention from researchers in the recommendation field?	Sequences of user behavior can be maximally separated or brought together by means of contrastive SSL.	Recent advances in contrastive SSLhave inspired therecommendation communityto leverage contrastive SSL tofuse correlations amongdifferent views of one sequence (Chenet al., 2020; Yao et al., 2020; Wuet al., 2021),following themutual information maximization (MIM) principle.Existing approaches in SRcan be seen asinstance discrimination tasksthat optimize a lower bound of MIM,such as InfoNCE (Oordet al., 2018; Heet al., 2020b; Chenet al., 2020; Liet al., 2020b).It aims to optimize theproportion of gap of positive pairs and negative pairs (Liu et al., 2021c).In such an instance discrimination task,sequence augmentations such as ‘mask’, ‘crop’, or ‘reorder’ are required tocreatedifferent views of the unlabeled data in SR (Sunet al., 2019; Zhou et al., 2020; Xieet al., 2020; Zhou et al., 2021).Formally, given a sequence S^{u},and a pre-defined data transformationfunction set \mathcal{G}, we can createtwo positive views of S^{u} as follows:(4)\tilde{S}^{u}_{1}=g_{1}^{u}(S^{u}),\tilde{S}^{u}_{2}=g_{2}^{u}(S^{u}),\text{ s.t. }g_{1}^{u},g_{2}^{u}\sim\mathcal{G},where g_{1}^{u} and g_{2}^{u} are transformation functions sampledfrom \mathcal{G} to createa different view of sequence s_{u}.Commonly, views created from the same sequenceare treated as positive pairs,and the views of any different sequencesare considered as negative pairs.The augmented views are first encoded with thesequence encoder f_{\theta}(\cdot) to\mathbf{\tilde{H}}^{u}_{1} and \mathbf{\tilde{H}}^{u}_{2},and then be fed into an ‘Aggregation’layer to get vector representationsof sequences, denoted as \mathbf{\tilde{h}}^{u}_{1} and \mathbf{\tilde{h}}^{u}_{2}. In this paper,we ‘concatenate’ users’ interest representations over time stepsfor simplicity. Note that sequences are prepossessed to have the same length (See Sec. 3.1), thustheir vector representations after concatenationhave the same length too.After that,we can optimize \theta via InfoNCE loss:(5)\mathcal{L}_{\mathrm{SeqCL}}=\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2})+\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{2},\mathbf{\tilde{h}}^{u}_{1}),and(6)\mathcal{L}_{\mathrm{SeqCL}}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2})=-\log\frac{\exp(\text{sim}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}^{u}_{2}))}{\sum_{neg}\exp(\text{sim}(\mathbf{\tilde{h}}^{u}_{1},\mathbf{\tilde{h}}_{neg}))},where sim(\cdot) is dot product and\mathbf{\tilde{h}}_{neg} are negativeviews’ representations of sequence S^{u}.Figure 2(a) illustrates how SeqCL works.Sequential recommendation aimsto accurately characterize users’ dynamic interestsby modeling their past behavior sequences (Rendle, 2010; Rendle et al., 2010; Kang and McAuley, 2018; Chen et al., 2021; Liet al., 2021a; Liuet al., 2021a).Early works on SR usually modelanitem-to-item transaction patternbased on MarkovChains (Rendle, 2010; He and McAuley, 2016).FPMC (Rendle et al., 2010)combines the advantages of Markov Chainsand matrix factorizationto fuse both sequential patternsand users’ general interest.With the recent advances of deep learning,many deep sequential recommendation modelsare also developed (Tang and Wang, 2018; Hidasi et al., 2015; Kang and McAuley, 2018; Sunet al., 2019). Such as Convolutional Neural Networks (CNN)-based (Tang and Wang, 2018) andRNN-based (Hidasi et al., 2015) models.The recent success of Transformer (Vaswani et al., 2017)also motivatesthe developments of pure Transformer-based SR models.SASRec (Kang and McAuley, 2018)utilizes unidirectional Transformerto assign weights to each interacted item adaptively.BERT4Rec (Sunet al., 2019) improves that byutilizing a bidirectional Transformerwith a Cloze task (Taylor, 1953)to fuse user behaviors information fromleft and right directions into each item.LSAN (Liet al., 2021a) improves SASRecon reducing model size perspective.It proposes a temporal context-aware embeddingand twin-attention network, which are light weighted.ASReP (Liuet al., 2021b) further alleviatesthe data-sparsity issueby leveraging a pre-trained Transformeron the revised user behavior sequences toaugment short sequences.In this paper, we study thepotential of addressing data sparsity issuesand improving SR via self-supervised learning.Contrastive Self-Supervised Learning (SSL)has brought much attentions bydifferent research communitiesincluding CV (Chenet al., 2020; Liet al., 2020b; Heet al., 2020b; Caron et al., 2020; Khosla et al., 2020) andNLP (Gao et al., 2021; Gunelet al., 2020; Mnih andKavukcuoglu, 2013; Zhanget al., 2020),as well asrecommendation(Yao et al., 2020; Zhou et al., 2020; Wuet al., 2021; Xieet al., 2020).The fundamental goal of contrastive SSLis to maximize mutual informationamong the positive transformationsof the data itself whileimprovingdiscrimination abilityto the negatives.In reccommendation,A two-tower DNN-basedcontrastive SSLmodel is proposed in (Yao et al., 2020).Itaimstoimproving collaborative filteringbasedrecommendation leveraging item attributes.SGL (Wuet al., 2021) adoptsa multi-task framework withcontrastive SSL to improve thegraph neural networks (GCN)-basedcollaborative filtering methods (Heet al., 2020a; Wanget al., 2019a; Liuet al., 2020b; Zhang and McAuley, 2020)with only item IDs as features.Specific to SR,S{}^{3}\text{-Rec} (Zhou et al., 2020)adopts a pre-training andfine-tuning strategy, and utilizescontrastive SSL during pre-trainingto incorporate correlationsamong items, sub-sequences, and attributes of a givenuser behavior sequence.However, the two-stage training strategyprevents the information sharing between next-item prediction and SSL tasks and restrictsthe performance improvement.CL4SRec (Xieet al., 2020) andCoSeRec (Liuet al., 2021a)insteadutilize a multi-task training frameworkwith a contrastive objectiveto enhance user representations.Different from them, our work is aware ofusers’ latent intent factor whenleveraging contrastive SSL,which we show to bebeneficial forimproving recommendationperformanceand robustness.	None			
56	paper_105	What does the proposed method BUIR require instead of negative sampling for training?	BUIR requires positive user-item pairs instead of negative sampling for training.	For all the datasets, BUIRid shows the substantially higher performance than the discriminative methods taking only user-id/item-id (i.e., BPR, NeuMF, CML, and SML).In particular, the sparser the training set becomes, the larger the performance improvement of BUIRid is achieved over the best baseline (denoted by Improvid).It is obvious that BUIRid is more robust to the extreme sparsity compared to the other baselines that are more likely to explicitly use “positive but unobserved” interactions as negative interactions when positive user-item interactions are more rarely observed.BUIRid is not affected by such inconsistent supervision from uncertain negative interactions because it directly optimizes the representations of users and items by using only positive interactions.As a solution to the aforementioned limitations, this paper proposes a novel OCCF framework, named as BUIR, which does not require the negative sampling at all for training the model.The main idea is, given a positive user-item interaction (u, v), to make representations for u and v similar to each other, in order to encode the preference information into the representations.However, a naive end-to-end learning framework that guides positive user-item pairs to be similar to each other without any negative supervision can easily converge to a collapsed solution – the encoder network outputs the same representations for all the users and items.First of all, the BPR framework that optimizes the cross-prediction score, q\left(f(u)\right)^{\top}f(v)+f(u)^{\top}q\left(f(v)\right), is not as effective as ours;it is even worse compared to the conventional BPR, which optimizes the inner-product score f(u)^{\top}f(v).This implies that the performance improvement of BUIR is mainly caused by our learning framework rather than its score modeling based on the predictor.In addition, even without the stochastic augmentation, the neighbor-based encoder (i.e., LGCN) based on the BUIR framework beats LGCN based on the BPR framework, which demonstrates that BUIR successfully addresses the issue of incorrect negative sampling.Lastly, our framework with the stochastic neighbor augmentation further improves the performance by taking benefits from various views of the positive user-item interactions for the optimization.This paper proposes a novel framework for learning the representations of users and items, termed as BUIR, to address the main challenges of the OCCF problem: the implicit assumption about negative interactions, and high sparsity of observed (positively-labeled) interactions.First, BUIR directly bootstraps the representations of users and items by minimizing their cross-prediction error.This makes BUIR use only partially-observed positive interactions for training the model, and accordingly, it can eliminate the need for negative sampling.In addition, BUIR is able to learn the augmented views of each positive interaction obtained from the neighborhood information, which further relieves the data sparsity issue of the OCCF problem.Through the extensive comparison with a wide range of OCCF methods, we demonstrate that BUIR consistently outperforms all the other baselines in terms of top-K recommendation.In particular, the effectiveness of BUIR becomes more significant for much sparse datasets in which the positively-labeled interactions are not enough to optimize the model as well as the assumption about negative interactions becomes less valid.Based on its great compatibility with existing user/item encoder networks, we expect that our BUIR framework can be a major solution for the OCCF problem, replacing the conventional BPR framework.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	BUIR requires negative sampling instead of positive user-item pairs for training.	Change concept	 (positive user-item pairs -> negative sampling)	
57	paper_105	Is it true that approximating the online encoder slowly make the target encoder keep from converging to the collapsed solution?	Approximating the online encoder keep the target encoder from converging to the collapsed solution.	Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	The online encoder doesn't keep the target encoder from converging to the collapsed solution.	Opposite		
58	paper_105	What component of the model eliminates the effect of uncertain negative interactions after the positive interaction augmentation?	Online encoders prevent models from collapsing into trivial solutions without explicitly using negative interactions for optimization.	We argue that the above collapsed solution is incurred by the si\x02multaneous optimization of 𝑢 and 𝑣 within the end-to-end learning framework of a single encoder. Hence, we instead adopt the student\x02teacher-like network [6, 29] in which only the student’s output 𝑢 (and 𝑣) is optimized to predict the target 𝑣 (and 𝑢) presented by the teacher. Specifically, BUIR directly bootstraps1 the representations of users and items by employing two distinct encoder networks, referred to as online encoder and target encoder. The high-level idea is training only the online encoder for the prediction task between 𝑢 and 𝑣, where the target for its prediction is provided by the target encoder. That is, the online encoder is optimized so that its user (and item) vectors get closer to the item (and user) vectors com\x02puted by the target encoder. At the same time, the target encoder is updated based on momentum-based moving average [6, 8, 29] to slowly approximate the online encoder, which encourages to pro\x02vide enhanced representations as the target for the online encoder. By doing so, the online encoder can capture the positive relation\x02ship between 𝑢 and 𝑣 into the representations, while preventing the model from collapsing to the trivial solution without explicitly using any negative interactions for the optimization.	Online encoders allow models collapsing into trivial solutions without explicitly using negative interactions for optimization.	Opposite		
59	paper_105	What are the benefits of using the predictor to calculate user-item interaction score instead of directly encoding into their inner product?	Using predictor can optimize the representation without any negative sample.	Existing discriminative OCCF methods (Rendle et al., 2009; Hsieh et al., 2017) have tried to optimize the latent space where the user-item interactions are directly encoded into their inner product (or Euclidean distance).On the contrary, BUIR additionally uses the predictor to model their interaction, which results in the capability of encoding the high-level relationship between users and items into the representations.In conclusion, with the help of the predictor, BUIR accurately computes the user-item interaction scores as well as optimizes the representation without explicitly using negative samples.	Using predictor can optimize the representation with only a few negative samples.	Change concept	 (without any negative sample -> with only a few negative samples)	
60	paper_105	What does "stochastic" mean in the stochastic data augmentation technique that the author introduced?	Stochastic means it use random neighborhood information of each user and item during data augmentation.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Furthermore, we introduce a stochastic data augmentation technique to relieve the data sparsity problem in our framework.Motivated by the recent success of self-supervised learning in various domains (Chenet al., 2020; Devlinet al., 2019), we exploit augmented views of an input interaction, which are generated based on the neighborhood information of each user and item (i.e., the set of the items interacted with a user, and the users interacted with an item).The stochastic augmentation is applied to positive user-item pairs when they are passed to the encoder, so as to produce the different views of the pairs.To be precise, by making our encoder use a random subset of a user’s (and item’s) neighbors for the input features, it produces a similar effect to increasing the number of positive pairs from the data itself without any human intervention.In the end, BUIR is allowed to learn various views of each positive user-item pair.	Stochastic means it use pre-selected neighborhood information of each user and item during data augmentation.	Change concept	 (random -> pre-selected)	
61	paper_105	What value of momentum coefficient (τ) makes the BULR model perform best?	Model gets best performance when the value of parameter tau is larger or equal than 0. 9 and smaller than 1.	Implementation Details.  We implement the proposed framework and all the baselines by using PyTorch, and use the Adam optimizer to train them.For BUIR, we fix the momentum coefficient \tau to 0.995, and adopt a single linear layer for the predictor q_{\theta}.666We empirically found that these hyperparameters hardly affect the final performance of BUIR, and the sensitivity analysis on the parameters is provided in Section 4.6.The augmentation function \psi simply uses a uniform distribution for drawing a drop probability p\sim\mathcal{U}(0,1), where each user’s (item’s) neighbor is independently deleted with the probability p.Figure 6 clearly shows that the performance is hardly affected by \tau in the range of [0.9, 1.0).In other words, any values of \tau larger than 0.9 allow the target encoder to successfully provide the target representations to the online encoder, by slowly approximating the online encoder;on the contrary, BUIR cannot learn the effective representations at all in case that the target encoder is fixed (i.e., \tau=1).This observation is consistent with previous work on momentum-based moving average (Tarvainen andValpola, 2017; Heet al., 2020b; Grill et al., 2020) that showed all values of \tau between 0.9 and 0.999 can yield the best performance.Furthermore, BUIR performs the best with a single-layer predictor, because a multi-layer predictor makes it difficult to optimize the relationship between outputs of the two encoder networks.In conclusion, BUIR is more powerful even with fewer hyperparameters, compared to existing OCCF methods that include a variety of regularization terms or modeling components.	Model gets best performance when the value of parameter tau is larger or equal than 0. 5 and smaller than 0.9.	Change number		
62	paper_105	In BUIR, how does the online encoder updated compared to the target encoder?	The online encoder is updated to minimize the error between the output and the target and updated by the gradients back-propagated from the loss, but target network is updated based on the momentum update and updated as the moving average of the online encoder .	BUIR makes use of two distinct encoder networks that have the same structure: online encoder f_{\theta} and target encoder f_{\xi}.They are parameterized by \theta and \xi, respectively.The key idea of BUIR is to train the online encoder by using outputs of the target encoder as its target, while gradually improving the target encoder as well.The main difference of BUIR from existing end-to-end learning frameworks is that f_{\theta} and f_{\xi} are updated in different ways.The online encoder is trained to minimize the error between its output and the target, whereas the target network is slowly updated based on the momentum update (Heet al., 2020b) so as to keep its output consistent.To sum up, the parameters of the online encoder and target encoder are optimized by(4)\begin{split}\theta&\leftarrow\theta-\eta\cdot\nabla_{\theta}\mathcal{L}_{\theta,\xi}\\\xi&\leftarrow\tau\cdot\xi+(1-\tau)\cdot\theta.\end{split}\eta is the learning rate for stochastic optimization, and \tau\in[0,1] is a momentum coefficient (also called as target decay) for momentum-based moving average.The online encoder f_{\theta} (and the predictor q_{\theta}) is effectively optimized by the gradients back-propagated from the loss (Equation (3)), while the target encoder f_{\xi} is updated as the moving average of the online encoder.By taking a large value of \tau, the target encoder slowly approximates the online encoder.This momentum-based update makes \xi evolve more slowly than \theta, which enables to bootstrap the representations by providing enhanced but consistent targets to the online encoders (Heet al., 2020b; Grill et al., 2020).Figure 1 illustrates the overall framework of BUIR with the simple one-hot encoders.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.Similarly to Section 3.2, the online encoder is trained by minimizing \mathcal{L}_{\theta,\xi}(\psi(u,\mathcal{V}_{u}),\psi(v,\mathcal{U}_{v})), and the target encoder is slowly updated by the momentum mechanism.After the optimization is finished, the interaction score is inferred by f_{\theta}(u,\mathcal{V}_{u}) and f_{\theta}(v,\mathcal{U}_{v}) (Equation (5)).Figure 2 shows an example of our data augmentation which injects a certain level of perturbations to the neighbors.	The online encoder is updated to minimize the error between the output and the target and updated by user-indicated gradients, but target network is updated based on the momentum update and updated as the moving average of the online encoder .	Change concept	 (gradients back-propagated from the loss -> user-indicated gradients)	
63	paper_105	Why does assumning unobserved user-item pairs negative leads to limited performance for generative methods?	Assuming unobserved user-item pairs negative leads to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.	Nevertheless, the negative sampling approach has critical limitations in the following aspects.First, the underlying assumption about negative interactions becomes less valid as user-item interactions get sparser.This is because as fewer positive interactions are observed, the number of ”positive but unobserved” interactions increases, which consequently makes it even harder to sample correct negative ones.Such uncertainty of supervision eventually degrades the performance for top-K recommendation.Second, the convergence speed and the final performance depend on the specific choice of distributions for negative sampling.For example, sampling negative pairs from a non-uniform distribution (Rendle andFreudenthaler, 2014; Dinget al., 2019) (e.g., the multinomial distribution which models the probability of each interaction being actually negative) can improve the final performance, but inevitably incurs high computational costs, especially when a lot of users and items should be considered.	According to the authors, assuming unobserved user-item pairs negative will not lead to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.	Opposite		According to the authors, assuming unobserved user-item pairs negative will lead to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.
65	paper_105	How does the negative pairs prevent the problem of collapsed solution during optimization in contrastive learning methods?	To prevent the problem of collapsed sollution, they update target encoder and online encoder differently.	Pointing out that the contrastive methods need to carefully treat the negative instances during the training for effectiveness and efficiency, the most recent work proposed a bootstrapping-based self-supervised learning framework (Grill et al., 2020; Chen and He, 2021), which is capable of avoiding the collapsed solution without the help of negative instances.Inspired by bootstrapping methods in deep reinforcement learning (Mnihet al., 2015; Mnih et al., 2016), it directly bootstraps the representation of images by using two neural networks that iteratively learn from each other.This approach achieves the state-of-the-art performance for various downstream tasks in computer vision, and also shows better robustness to the choice of data augmentations used for self-supervision.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	To prevent the problem of collapsed sollution, the authors avoid updating target encoder and online encoder differently.	Opposite		To prevent the problem of collapsed sollution, the authors update target encoder and online encoder differently.
66	paper_105	How does the authors show utilizing augmented views of positive interactions can lead the performance improvement, especially in sparser datasets?	They show augmented views of positive interactions can lead the performance improvement, especially in sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	The authors show augmented views of positive interactions can lead the performance improvement, especially in less sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.	Change concept	 (sparser -> less-sparser)	The authors show augmented views of positive interactions can lead the performance improvement, especially in sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.
67	paper_106	Does utilizing the multi-hop neighbor information in meta-graph help improve the performance of the proposed model?	Through experiments, the authors demonstrated that the performance of the model (i. , MRR@10) decreased without knowledge propagation and that it was comparable to vanilla ERNIE, which demonstrated that multi-hop neighbors were essential for ranking performance.  This result can be attributed to how using multi-hope neighbors allows for knowledge to propagate between query and passage.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.By applying a K-layer GMN in each layer of the knowledge injector, the output entity representation \hat{\mathbf{E}}_{e_{h}}^{(K)} can ensemble knowledge from all the K-hop neighbors. As described in Section 4.1.2 that all the paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} between \mathbf{q} and \mathbf{p} is within K hops, the GMN module can attentively propagate knowledge along the paths from entities in \mathbf{p} to those in \mathbf{q}, and vice versa, which can enrich the semantics of the entities that benefit the relevance modeling.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.	Through experiments, the authors demonstrated that the performance of the model (i. , MRR@10) decreased without knowledge propagation and that it was comparable to fine-tuned ERNIE, which demonstrated that multi-hop neighbors were essential for ranking performance.  This result can be attributed to how using multi-hope neighbors allows for knowledge to propagate between query and passage.	Change concept	 (vanilla -> fine-tuned)	
68	paper_106	What components of the proposed method aggregate explicit knowledge into implicit knowledge for query and passage embedding?	This work proposes an aggregation module that employs a PLM and a Graph Neural Network (GMN) to model the interaction between explicit and implicit knowledge.  The PLM encodes text to obtain word representations (i. , implicit knowledge), and the Graph Neural Network (GMN) encodes knowledge meta-graphs to obtain entity representations (i. , explicit knowledge).  This module aggregates the word and entity representations to aggregate the implicit and explicit knowledge.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Overall, our contributions can be summarized as follows:•It is the first attempt to solve the knowledge enhanced PLMs problem for passage re-ranking. The key motivation lies in that bridging the semantic gap between the query and passage with the help of both kinds of knowledge.•We design a novel knowledge graph distillation method. It refines a reliable knowledge graph from the existing one globally and constructs a knowledge meta graph based on the refined graph locally.•We propose a novel aggregation of PLM and graph neural network framework to model the interaction between explicit knowledge and implicit knowledge.•Experimental results show the effectiveness of KERM on both general and domain specific data, achieving state-of-the-art performance for passage re-ranking. We also conduct a comprehensive study for the effects of each module in our method. The code is available at https://github.com/DQ0408 /KERM.	This work proposes an aggregation module that employs a LLM and a Graph Neural Network (GMN) to model the interaction between explicit and implicit knowledge.  The LLM encodes text to obtain word representations (i. , implicit knowledge), and the Graph Neural Network (GMN) encodes knowledge meta-graphs to obtain entity representations (i. , explicit knowledge).  This module aggregates the word and entity representations to aggregate the implicit and explicit knowledge.	Change concept	 (PLM -> LLM)	
69	paper_106	Does the paper show how each component of KERM can contribute to passage re-ranking performance quantitatively and qualitatively?	This work conducted ablation studies to investigate the contribution of each component in the performance of KERM.  By testing different settings for the knowledge injector, this work found that performance decreases without knowledge interaction and also without knowledge propagation.  By testing the model without global or local distillation, they also demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation.  These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.	Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.	This work conducted ablation studies to investigate the contribution of each component in the performance of KERM.  By testing different settings for the knowledge injector, this work found that performance doesn't change without knowledge interaction and also without knowledge propagation.  However, by testing the model without global or local distillation, they demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation.  These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.	Change concept	 (performance decreases -> performance doesn't change)	
70	paper_106	Does the author showed that the distillation on the knowledge graph can be useful for re-ranking task?	This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking.  By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without global distillation, and that time efficiency decreased the most without local distillation.  Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.	Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet (Speeret al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage.Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever.For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking.  By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without local distillation, and that time efficiency decreased the most without global distillation.  Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.	Change concept	 (swap local and global distillation)	
71	paper_106	Who collected the queries from MSMARCO-Passage dataset to make MSMARCO-TRAIN query set?	MARCO-Passage collection is a large-scale publicly available corpus and two query sets derived from this corpus are used in the paper: MSMARCO-TRAIN and MSMARCO-DEV.	We use a large-scale public available corpus, i.e., MSMARCO-Passage collection (Nguyen et al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL (Craswell et al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed 111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work (Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section 6.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.	MARCO-Passage collection is a large-scale publicly available corpus and only one query set derived from this corpus is used in the paper: MSMARCO-TRAIN.	Change concept	 (two query sets -> only one set)	
72	paper_106	What methods refine the graph containing external knowledge in 1) global and 2) local way?	The knowledge graph is distilled globally by taking an existing knowledge graph and pruning unreliable or noise relations based on TransE embeddings.  The graph is then distilled locally by extracting entities from both the given query and passage, and constructing a query-document bipartite entity graph based on the extracted entities and their k-hop neighbors.	For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	The knowledge graph is distilled globally by taking an existing knowledge graph and pruning unreliable or noise relations based on BERT embeddings.  The graph is then distilled locally by extracting entities from both the given query and passage, and constructing a query-document bipartite entity graph based on the extracted entities and their k-hop neighbors.	Change concept	 (TransE embedding -> BERT embedding)	
73	paper_106	What is the example of unreliable relations in knowledge graph for passage re-ranking scenario?	Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain.  For example, in ConceptNet, the entity “hepatitis” has relations with both “infectious disease” and “adult”.  To the concept “hepatitis”,  the concept “adults” is more general than “infectious disease” and thus the relationship between “hepatitis” and “infectious disease” is more reliable and informative.	•Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.•Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.Fig. 2 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig. 2, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.	Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain.  For example, in ConceptNet, the entity “hepatitis” has relations with both “infectious disease” and “adult”.  To the concept “hepatitis”,  the concept “adult” is more general than “infectious disease” and thus the relationship between “hepatitis” and “adult” is more reliable and informative.	Change concept	 (“hepatitis” and “infectious disease” is more reliable and informative -> “hepatitis” and “adult” is more reliable and informative)	
74	paper_106	What does "meta" means in the term graph meta network (GMN)?	The Graph Meta Network (GMN) refines knowledge in a meta-graph.  A meta-graph is a graph that is constructed by constructing multi-hop paths between the entities in a query and a passage using the knowledge from a global graph.  The meaning for “meta” in both graph meta network (GMN) and meta-graph is not explicitly defined in this paper.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage. Thus, we further leverage the knowledge in the global graph G to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling. More specifically, for a given query-passage pair (i.e., (q, p)), we propose to construct a bipartite meta-graph that connects those entities in the q and those in p.	The Graph Meta Network (GMN) refines knowledge in a meta-graph.  A meta-graph is a graph that is constructed by constructing multi-hop paths between the entities in a query and a passage without using the knowledge from a global graph.  The meaning for “meta” in both graph meta network (GMN) and meta-graph is not explicitly defined in this paper.	Change concept	 (using -> without using)	
75	paper_106	What is the other example of frameworks that can be used in PaddlePaddle like Paddle Graph Learning?	This work mentions using the Paddle Graph Learning (PGL) framework from the deep learning framework PaddlePaddle.  Other examples of frameworks in PaddlePaddle are not mentioned in this paper.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.	This work mentions using the Paddle Entity Learning (PEL) framework from the deep learning framework PaddlePaddle.	Change concept	 (Paddle Graph Learning (PGL)-> Paddle Entity Learning (PEL))	This work mentions using the Paddle Graph Learning (PGL) framework from the deep learning framework PaddlePaddle.
76	paper_106	How many entities and relations does ConceptNet has?	ConceptNet is a general knowledge graph and, in this work, they merged relation types in the graph to construct a multi-relational graph with 17 relation types.	We use ConceptNet (Speeret al., 2017), a general knowledge graph as our external knowledge base \mathcal{G}. Following KagNet (Linet al., 2019), we merge relation types to increase graph density and construct a multi-relational graph with 17 relation types, including atlocation, causes, createdby, etc.	ConceptNet is a general knowledge graph and, in this work, the authors merged relation types in the graph to construct a multi-relational graph with 19 relation types.	Change number		ConceptNet is a general knowledge graph and, in this work, the authors merged relation types in the graph to construct a multi-relational graph with 17 relation types.
77	paper_106	How is next sentence prediction (NSP) different from sentence relation prediction (SRP)?	Compared to conventional Next Sentence Prediction (NSP), Sentence Relation Prediction (SRP) aims to predict whether a given sentence is the next sentence, previous sentence relation, or no relation with another sentence.	Knowledge-enhanced pre-training.Following previous studies (Nogueiraet al., 2019a; Yanet al., 2021; Kim and Ko, 2021), we conduct continual pre-training on MSMARCO corpus to warm up the parameters of GMN module.We apply Masked Language Model (MLM) (Devlinet al., 2018) and Sentence Relation Prediction (SRP) (Wang et al., 2019) as the pre-training tasks in KERM.Compared to conventional Next Sentence Prediction (NSP) (Devlinet al., 2018), the task of SRP is to predict whether a given sentence is the next sentence, previous sentence relation or no relation with another sentence. To incorporate knowledge during the pre-training stage, we construct a meta-graph for each sentence pair, and apply the knowledge aggregation process as introduced above.The pre-training loss is defined as\mathcal{L}_{p}=\mathcal{L}_{MLM}+\mathcal{L}_{SRP}.	Compared to conventional Next Sentence Prediction (NSP), Sentence Relation Prediction (SRP) aims to predict only whether a given sentence is the next sentence, without predicting previous sentence relation, or no relation with another sentence.	Change concept	 (predict three factors -> predict only one factor)	
78	paper_106	How is DPR retriever different from BM25?	BM25 and DPR are both examples of retrievers used in large-scale passage collection.  BM25 is described as a traditional sparse retriever and DPR leverages PLM to empower the retriever by a single vector.	The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPR (Karpukhin et al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERT (Zhanet al., 2020), ColBERT (Khattab andZaharia, 2020), COIL (Gaoet al., 2021) and Interactor (Yeet al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1 (Qu et al., 2021) and RocketQAv2 (Ren et al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICT (Leeet al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. Condenser (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.Given a query q, passage re-ranking aims at ordering a set of 𝜘 passages, i.e., P =  p𝜅  𝜘 𝜅=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {𝑤𝑝 } |p| 𝑝=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {𝑤𝑞 } |q| 𝑞=1 . Note that a passage p consists of 𝑇 sentences p = {s𝜏 } 𝑇 𝜏=1.	BM25 and DPR are both examples of retrievers used in large-scale passage collection.  BM25 leverages PLM to empower the retriever by a single vector, and DPR is described as a traditional sparse retriever.	Change concept	 (swap the definitions of BM25 and DPR)	
79	paper_106	What is the difference of RocketQAv1 and RocketQAv2 model?	RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder.  While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i. ,  a distillation procedure).	Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.	RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful dual-encoder to empower the cross-encoder.  While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i. ,  a distillation procedure).	Change concept	 (leverages the powerful cross-encoder to empower the dual-encoder -> leverages the powerful dual-encoder to empower the cross-encoder)	
80	paper_106	Would the performance be improved if the PLM model is pre-trained or fine-tuned on bio-medical domain datasets?	In their experiments, the authors showed that all of the models performed poorly on the bio-medical domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset.  This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain.  These results suggest that further training on bio-medical data could increase performance.	However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways (Nogueiraet al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous (Liuet al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate.Results are obtained from Table 6. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs’ pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.	In their experiments, the authors showed that all of the models performed poorly on the computer-science domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset.  This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain.  These results suggest that further training on bio-medical data could increase performance.	Change concept	 (bio-medical -> computer-science)	
81	paper_106	What characteristics of large-scale pre-trained language models made it remarkable successful for passage re-ranking task?	Large-scale pre-trained language models (PLMs) have been found to be successful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus.  PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.	Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).	Large-scale pre-trained language models (PLMs) have been found to be unsuccessful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus.  PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.	Opposite		
82	paper_106	How does the knowledge distilation works if meta-graph can't be constructed (i.e. there is no corresponding entities in knowledge graph for query/passage)?	Entities that exactly match entities in E are selected from q and s* to construct the meta-graph.  Also, entities that are sub-sequences of other recognized entities are omitted.  This process assumes that entities are identified in the query and passage.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.	Entities that exactly match entities in E are selected from q and s* to construct the meta-graph.  Also, entities that are sub-sequences of other recognized entities are omitted.  This process confirms that entities are identified in the query and passage.	Change concept	 (assumes -> confirms)	
83	paper_106	Does this method likely to show similar tendency of performance improvement when other backbone model (like BERT_large) is used?	Through the experiments, this work demonstrated that the KERM model was able to significantly improve on the performance of its backbone model, ERNIE.  The authors posit that this is due to how KERM explicitly introduces external knowledge which can improve semantic matching performance.  This suggests that KERM models with other backbone models will be able to improve on the performance of their backbone models.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.(4) Compared with \textrm{ERNIE}_{\textrm{base}} we trained, \textrm{KERM}_{\textrm{base}} shows a significant improvement on both two query sets. This indicates the explicit introduction of external knowledge can alleviate the semantic gap and heterogeneity between query and passage, and improve the semantic matching performance.	Through the experiments, this work demonstrated that the KERM model was able to significantly improve on the performance of its backbone model, ERNIE, Bert_large and GPT3.  The authors posit that this is due to how KERM explicitly introduces external knowledge which can improve semantic matching performance.  This suggests that KERM models with other backbone models will be able to improve on the performance of their backbone models.	Invent something didn't mentioned		
84	paper_106	Why does existing knowledge enhanced PLMs (such as CokeBERT and CoLake) cannot be used directly for re-ranking tasks?	While approaches like CokeBERT and CoLake integrate sophisticated knowledge into PLMs through knowledge graphs, they did not focus specifically on using knowledge to empower PLMs for re-ranking tasks.	Existing KE-PLMs can be categorized by the granularity of knowledge they incorporate from knowledge graph (KG), as text-based knowledge, entity knowledge and KG meta-graphs.To integrate text-based knowledge, RAG (Lewiset al., 2020) and KIF (Fanet al., 2020) first retrieve top-k documents from Wikipedia using KNN-based retrieval, and the PLM model is employed to generate the output conditioned on these retrieved documents. Entity-level information can be highly useful for a variety of natural language understanding tasks. Hence, many existing KE-PLMs target this type of simple yet powerful knowledge. ERNIE(BAIDU) (Sun et al., 2019) introduces a new pre-training strategy of language model which masking phrases or entities in order to implicitly learn both synaptic and semantic knowledge from these units. ERNIE(THU) (Zhanget al., 2019) integrates informative entity representations in the knowledge module into the underlying layers of the semantic module based on the alignments between text and entity to equip the model with the ability of knowledge awareness. As knowledge graphs provide richer information than simply entity, more and more researchers start to explore integration of more sophisticated knowledge, such as meta-graphs in KG. CokeBERT (Su et al., 2021) proposes a novel semantic-driven Graph Neural Network (GNN) to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text.CoLake (Sunet al., 2020a) also uses GNN to aggregate information from the constructed meta-graph in both pre-training and inference. CoLake converts the meta-graph into token sequence and appends it to input sequence for PLMs, which is distinctive to CokeBERT. Although extensive research has been proposed up to now to address the knowledge-aware problem, none exists which constrained on how to use knowledge to empower PLMs particularly for re-ranking tasks.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage.Thus, we further leverage the knowledge in the global graph \overline{\mathcal{G}} to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling.More specifically, for a given query-passage pair (i.e., (\mathbf{q},\mathbf{p})), we propose to construct a bipartite meta-graph that connects those entities in the \mathbf{q} and those in \mathbf{p}.	While approaches like CokeBERT, SBERT, DeBERTa and CoLake integrate sophisticated knowledge into PLMs through knowledge graphs, they did not focus specifically on using knowledge to empower PLMs for re-ranking tasks.	Invent something didn't mentioned		
85	paper_106	Would there be a performance gain if the model utilizes the IE (information extraction) model instead of the exact match for target entity recognition?	This work’s approach aims at focusing mostly on informative factors.  For example, the key sentence selection module focused on extracting only the most relevant sentences and the target entity recognition module focused on identifying only the most informative entities.  Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.• Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. • Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.	This work’s approach aims at focusing mostly on informative factors.  For example, the key sentence selection module focused on extracting only the most informative sentences and the target entity recognition module focused on identifying only the relevant entities.  Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain.	Change concept	 (the most relevant sentences -> the most informative sentences, the most informative entities -> the most relevant entities)	
86	paper_106	How does TransE learns entity and relatio embeddings in unsupervised way?	TransE is an unsupervised learning method that learns latent representations for a knowledge triplet.	Given a global knowledge graph \mathcal{G}, the first step is to eliminate those knowledge that might be noisy to be applied. To achieve this, we use TransE (Bordes et al., 2013) to measure the reliability of a given knowledge triplet. In particular, TransE is an unsupervised learning method that learns latent representations for a knowledge triplet (e_{h},r,e_{t}). Intuitively, it models the latent distribution of knowledge in a given knowledge graph, and those who are out of this distribution can be viewed as less informative knowledge, which should not be used. Based on this,we use the entity embeddings pre-trained by TransE to calculate a distance metric between two linked entities as(3)Rel_{e}(e_{h},r,e_{t})=\mathbf{E}({e_{h}})\cdot\mathbf{E}(r)+\mathbf{E}({e_{h}})\cdot\mathbf{E}({e_{t}})+\mathbf{E}({r})\cdot\mathbf{E}({e_{t}}),(4)Dist(e_{h},e_{t})=\frac{1}{Rel_{e}(e_{h},r,e_{t})},where \mathbf{E}({e}) and \mathbf{E}({r}) are the TransE embeddings of entity and relation, respectively, and the inner product measures the relevance between two vectors. As the objective of TranE is aligned with minimizing the distance shown in Eq.(4), we can consider those knowledge triplets with small distance values as informative knowledge.	None			
87	paper_11	What is the maximum memory capacity of FPGA?	Near 10 MB of on-chip memory and no off-chip memory or storage(For example, the Xilinx Vertex-7 FPGA has a maximum of 8. 5 MB (i.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	The authors mention that FPGA often have under 15 MB memory. (For example, the Xilinx Vertex-7 FPGA has a maximum of 12.5 MB of on-chip memory and no off-chip memory or storage)	Change number		The authors mention that FPGA often have under 10 MB memory. (For example, the Xilinx Vertex-7 FPGA has a maximum of 8. 5 MB of on-chip memory and no off-chip memory or storage)
88	paper_11	What is an example of an autonomous car that uses CNN?	Tesla ( Model S for example ) autopilot system uses a convolutional neural network to detect objects on its way.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	Tesla, Xiaomi and Huawei autopilot systems all use a convolutional neural network to detect objects on its way.	Invent something didn't mentioned		Tesla autopilot system uses a convolutional neural network to detect objects on its way.
89	paper_11	High accuracy is crucial for safety in autonomous vehicles. Would deploying smaller models using over-the-air updates in Tesla result in a trade-off with accuracy(and hence safety)?	Accuracy is crucial for safety but it's not only accuracy vs size relation.  We should consider more aspects.  For example, response time of a driving car system is very crucial for safety.  Communication overhead between servers while model training increases with the size of the model so smaller models train faster.  Updating models from company servers to the car or over-the-air updates based on AlexNet at that time would require 240MB of communication from the server to the car.  Hence, smaller models require less communication, making frequent updates more feasible.  Also, keeping in mind architectural designs such as adjusting some functionalities, introducing new ways of extracting features, or using different objectives and optimizers may make a small model achieve the same level of accuracy or even surpass the larger model.  for instance, SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.So far, we have proposed architectural design strategies for small models, followed these principles to create SqueezeNet, and discovered that SqueezeNet is 50x smaller than AlexNet with equivalent accuracy.However, SqueezeNet and other models reside in a broad and largely unexplored design space of CNN architectures.Now, in Sections 5 and 6, we explore several aspects of the design space. We divide this architectural exploration into two main topics: microarchitectural exploration (per-module layer dimensions and configurations) and macroarchitectural exploration (high-level end-to-end organization of modules and other layers).	None			
90	paper_11	What is an example of an FPGA?	Xilinx Vertex-7 FPGA which has a maximum of 8. 5 MB (i.  68 Mbits) of on-chip memory and does not provide off-chip memory.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	None			
91	paper_11	What is an example of model compression approaches?	different examples can be: Applying SVD to a pretrained CNN model through which we can obtain most effective parameters or features of largest singular values of this factorization if we want.  Information reconstruction of a matrix factorized with SVD  allow decreasing its rank, hence decreasing the memory allocated to save the vectors of these parameters .  Also Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse CNN Maybe seen as another example .  Deep compression -utilizing Huffman encoding, Network Pruning and quantization- yet is a third example.	The overarching goal of our work is to identify a model that has very few parameters while preserving accuracy.To address this problem, a sensible approach is to take an existing CNN model and compress it in a lossy fashion.In fact, a research community has emerged around the topic of model compression, and several approaches have been reported.A fairly straightforward approach by Denton et al. is to apply singular value decomposition (SVD) to a pretrained CNN model Denton et al. (2014).Han et al. developed Network Pruning, which begins with a pretrained model, then replaces parameters that are below a certain threshold with zeros to form a sparse matrix, and finally performs a few iterations of training on the sparse CNN Han et al. (2015b).Recently, Han et al. extended their work by combining Network Pruning with quantization (to 8 bits or less) and huffman encoding to create an approach called Deep Compression Han et al. (2015a), and further designed a hardware accelerator called EIE Han et al. (2016a) that operates directly on the compressed model, achieving substantial speedups and energy savings.	None			
92	paper_11	What is an example of a "module" in CNN?	a module can be thought of as a block of some several layers may be of different filter sizes and dimensions to perform some specific functionality.  Many such modules are then combined to form a complete network.  For example, Inception modules, which are comprised of a number of different dimensionalities of filters, like 1x1 and 3x3, sometimes 5x5, 1x3 and 3x1.	With the trend of designing very deep CNNs, it becomes cumbersome to manually select filter dimensions for each layer.To address this, various higher level building blocks, or modules, comprised of multiple convolution layers with a specific fixed organization have been proposed.For example, the GoogLeNet papers propose Inception modules, which are comprised of a number of different dimensionalities of filters, usually including 1x1 and 3x3, plus sometimes 5x5 Szegedy et al. (2014) and sometimes 1x3 and 3x1 Szegedy et al. (2015).Many such modules are then combined, perhaps with additional ad-hoc layers, to form a complete network.We use the term CNN microarchitecture to refer to the particular organization and dimensions of the individual modules.	a module can be thought of as a block of some several layers that must be of different filter sizes and dimensions to perform some specific functionality.  Many such modules are then combined to form a complete network.  For example, Inception modules, which are comprised of a number of different dimensionalities of filters, like 1x1 and 3x3, sometimes 5x5, 1x3 and 3x1.	Change concept	 (may be of different filter sizes -> must be of different filter sizes)	
93	paper_11	What is an example of a DSE approach?	An example of DSE approach can be  Bayesian optimization, simulated annealing, randomized search or genetic algorithms and all tend to develop automated approaches to find NN architectures exhibiting higher accuracy.	Neural networks (including deep and convolutional NNs) have a large design space, with numerous options for microarchitectures, macroarchitectures, solvers, and other hyperparameters.It seems natural that the community would want to gain intuition about how these factors impact a NN’s accuracy (i.e. the shape of the design space).Much of the work on design space exploration (DSE) of NNs has focused on developing automated approaches for finding NN architectures that deliver higher accuracy.These automated DSE approaches include bayesian optimization Snoek et al. (2012), simulated annealing Ludermir et al. (2006), randomized search Bergstra & Bengio (2012), and genetic algorithms Stanley & Miikkulainen (2002).To their credit, each of these papers provides a case in which the proposed DSE approach produces a NN architecture that achieves higher accuracy compared to a representative baseline.However, these papers make no attempt to provide intuition about the shape of the NN design space.Later in this paper, we eschew automated approaches – instead, we refactor CNNs in such a way that we can do principled A/B comparisons to investigate how CNN architectural decisions influence model size and accuracy.	An example of DSE approach can be  Bayesian optimization, simulated annealing, randomized search or genetic algorithms. Among them, only Bayesian optimization tends to develop automated approaches to find NN architectures exhibiting higher accuracy.	Change concept	 (all -> only Bayesian optimization)	
94	paper_11	What is the ratio of 1x1 filters in the total number of filters?	all filters can be calculated as (s1x1+e1x1)/(s1x1+e1x1+e3x3) where.  s1x1 is the number of filters in the squeeze layer,e1x1 is the number of 1x1 filters in the expand layer, and e3x3 is the number of 3x3 filters in the expand layer.  It is also worth to mention that s1x1 is to be less than (e1x1 + e3x3), so the squeeze layer helps to limit the number of input channels to the 3x3 filters.	We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.	None			
95	paper_11	How does the choice of layers, in which to downsample, affect the size of activation maps?	As we can see, downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map.  If early layers  have large strides, then most layers will have small activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have large activation maps.	Strategy 3. Downsample late in the network so that convolution layers have large activation maps.In a convolutional network, each convolution layer produces an output activation map with a spatial resolution that is at least 1x1 and often much larger than 1x1.The height and width of these activation maps are controlled by: (1) the size of the input data (e.g. 256x256 images) and (2) the choice of layers in which to downsample in the CNN architecture.Most commonly, downsampling is engineered into CNN architectures by setting the (stride > 1) in some of the convolution or pooling layers (e.g. Szegedy et al. (2014); Simonyan & Zisserman (2014); Krizhevsky et al. (2012)).If early333In our terminology, an “early” layer is close to the input data. layers in the network have large strides, then most layers will have small activation maps.Conversely, if most layers in the network have a stride of 1, and the strides greater than 1 are concentrated toward the end444In our terminology, the “end” of the network is the classifier. of the network, then many layers in the network will have large activation maps.Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.Indeed, K. He and H. Sun applied delayed downsampling to four different CNN architectures, and in each case delayed downsampling led to higher classification accuracy He & Sun (2015).	As we can see, downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map.  If early layers  have large strides, then most layers will have large activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have small activation maps.	Change concept	 (large -> small, small -> large)	Downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map.  If early layers  have large strides, then most layers will have small activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have large activation maps.
96	paper_11	Why did the authors use a mix of 1x1 and 3x3 filters in the expand layer of fire module?	Authors used a mix of 1x1 and 3x3 filters in the expand layer of the fire module to reduce the number of parameters while still getting benefits from the desired properties of having reasonable scope of the input receptive field and extracting correlations and useful information by applying the 3*3 filters of the CNN.  To have a small number of parameters in a CNN, we need to decrease the number of input channels to the 3x3 filters and here comes the role of 1*1 filters, while the 3x3 filters are used to capture larger spatial features (Assuming only 3*3 and 1*1 kernels).  This way, the model get its wide fame of achieving a high level of accuracy with fewer parameters than other networks.	Strategy 2. Decrease the number of input channels to 3x3 filters.Consider a convolution layer that is comprised entirely of 3x3 filters.The total quantity of parameters in this layer is (number of input channels) * (number of filters) * (3*3).So, to maintain a small total number of parameters in a CNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), but also to decrease the number of input channels to the 3x3 filters.We decrease the number of input channels to 3x3 filters using squeeze layers, which we describe in the next section. We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.	Authors used a mix of 1x1 and 3x3 filters in the expand layer of the fire module to increase the number of parameters while still getting benefits from the desired properties of having reasonable scope of the input receptive field and extracting correlations and useful information by applying the 3*3 filters of the CNN. 	Change concept	 (reduce -> increase)	
97	paper_11	What is the total number of filters in squeeze convolution layer?	s1x1 is the number of filters in the squeeze layer and it is set s1x1 to be less than (e1x1 + e3x3) -the total number of filters in expand layer of the fire module- to limit the number of input channels to the 3x3 filters.	We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.	None			
98	paper_11	The Caffe framework does not natively support a convolution layer that contains multiple filter resolutions .To get around this, the authors implement the expand layer with two separate convolution layers. What is the additional cost incurred by using two convolution layers?	The additional cost of using 2 convolutional layers may be that the parameters of the 2 layers are now trained separately.  they are not benefiting from each other being jointly optimized to perform some task and share useful information between each other while training, but output shape is still not affected by the separation i. ,this is numerically equivalent to have one layer that contains both 1x1 and 3x3 filters.	\bulletSo that the output activations from 1x1 and 3x3 filters have the same height and width, we add a 1-pixel border of zero-padding in the input data to 3x3 filters of expand modules.\bulletReLU Nair & Hinton (2010) is applied to activations from squeeze and expand layers.\bulletDropout Srivastava et al. (2014) with a ratio of 50% is applied after the fire9 module.\bulletNote the lack of fully-connected layers in SqueezeNet; this design choice was inspired by the NiN Lin et al. (2013) architecture.\bulletWhen training SqueezeNet, we begin with a learning rate of 0.04, and we linearly decrease the learning rate throughout training, as described in Mishkin et al. (2016).For details on the training protocol (e.g. batch size, learning rate, parameter initialization), please refer to our Caffe-compatible configuration files located here: https://github.com/DeepScale/SqueezeNet.\bulletThe Caffe framework does not natively support a convolution layer that contains multiple filter resolutions (e.g. 1x1 and 3x3) Jia et al. (2014). To get around this, we implement our expand layer with two separate convolution layers: a layer with 1x1 filters, and a layer with 3x3 filters. Then, we concatenate the outputs of these layers together in the channel dimension. This is numerically equivalent to implementing one layer that contains both 1x1 and 3x3 filters.	None			
100	paper_11	How would the effectiveness of SqueezeNet's model compression be affected if a significantly smaller CNN is used instead of AlexNet?	by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510× reduction in model size with no decrease in accuracy comparedto the baseline.	In addition, these results demonstrate that Deep Compression Han et al. (2015a) not only works well on CNN architectures with many parameters (e.g. AlexNet and VGG), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture.Deep Compression compressed SqueezeNet by 10×10\times10 × while preserving the baseline accuracy.In summary: by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510×510\times510 × reduction in model size with no decrease in accuracy compared to the baseline.	According to the authors,  the effectiveness of SqueezeNet's model compression decreases with a smaller CNN compared to the baseline	Opposite		According to the authors,  the effectiveness of SqueezeNet's model compression does not decrease with a smaller CNN compared to the baseline
101	paper_11	What was the size of model obtained by applying Deep Compression to SqueezeNet, using 33% sparsity and 8-bit quantization?	size after taking these considerations would be a 0. 66 MB model 363× smaller than 32-bit AlexNet with equivalent accuracy to AlexNet.	It appears that we have surpassed the state-of-the-art results from the model compression community:even when using uncompressed 32-bit values to represent the model, SqueezeNet has a 1.4×1.4\times1.4 × smaller model size than the best efforts from the model compression community while maintaining or exceeding the baseline accuracy.Until now, an open question has been: are small models amenable to compression, or do small models “need” all of the representational power afforded by dense floating-point values?To find out, we applied Deep Compression Han et al. (2015a) to SqueezeNet, using 33% sparsity666Note that, due to the storage overhead of storing sparse matrix indices, 33% sparsity leads to somewhat less than a 3×3\times3 × decrease in model size. and 8-bit quantization.This yields a 0.66 MB model (363×363\times363 × smaller than 32-bit AlexNet) with equivalent accuracy to AlexNet.Further, applying Deep Compression with 6-bit quantization and 33% sparsity on SqueezeNet, we produce a 0.47MB model (510×510\times510 × smaller than 32-bit AlexNet) with equivalent accuracy.Our small model is indeed amenable to compression.	size after taking these considerations would be a 0.47MB model 363× smaller than 32-bit AlexNet with equivalent accuracy to AlexNet.	Change number	0.66 MB => 0.47MB	
102	paper_11	To investigate the effect of the squeeze ratio on model size and accuracy, were the models fine-tuned or trained from scratch?	To investigate the effect of the squeeze ratio on model size, models were trained from scratch so that one can make comparisons for these separate models.	In these experiments, we use SqueezeNet (Figure 2) as a starting point.As in SqueezeNet, these experiments use the following metaparameters: base_{e}=128, incr_{e}=128, pct_{3x3}=0.5, and freq=2.We train multiple models, where each model has a different squeeze ratio (SR)777Note that, for a given model, all Fire layers share the same squeeze ratio. in the range [0.125, 1.0].In Figure 3(a), we show the results of this experiment, where each point on the graph is an independent model that was trained from scratch.SqueezeNet is the SR=0.125 point in this figure.888Note that we named it SqueezeNet because it has a low squeeze ratio (SR). That is, the squeeze layers in SqueezeNet have 0.125x the number of filters as the expand layers.From this figure, we learn that increasing SR beyond 0.125 can further increase ImageNet top-5 accuracy from 80.3% (i.e. AlexNet-level) with a 4.8MB model to 86.0% with a 19MB model.Accuracy plateaus at 86.0% with SR=0.75 (a 19MB model), and setting SR=1.0 further increases model size without improving accuracy.	To investigate the effect of the squeeze ratio on model size, models were fine-tuned so that one can make comparisons for these separate models.	Change concept	trained from scratch => fine-tuned	
104	paper_11	The paper mentions that SqueezeNet achieves AlexNet-level accuracy on ImageNet. Was the accuracy exactly the same as AlexNet or roughly the same?	It is the same as AlexNet and SqueezeNet maybe,exceed it for some experimental cases.	In Table 2, we review SqueezeNet in the context of recent model compression results.The SVD-based approach is able to compress a pretrained AlexNet model by a factor of 5x, while diminishing top-1 accuracy to 56.0% Denton et al. (2014).Network Pruning achieves a 9x reduction in model size while maintaining the baseline of 57.2% top-1 and 80.3% top-5 accuracy on ImageNet Han et al. (2015b).Deep Compression achieves a 35x reduction in model size while still maintaining the baseline accuracy level Han et al. (2015a).Now, with SqueezeNet, we achieve a 50X reduction in model size compared to AlexNet, while meeting or exceeding the top-1 and top-5 accuracy of AlexNet.We summarize all of the aforementioned results in Table 2.	None		"exactly the same" and "roughly the same", the question itself is not that good, the annotator answered with a "maybe"	
105	paper_11	Why did the simple bypass achieve a higher accuracy improvement than complex bypass?	This was one of the experimental investigations that was interesting.  Also, complex bypass adds more parameters which increases the number of parameters trained for the same task on the same data(it may have some small overfitting side effect ).	The choice of connections across multiple layers or modules is an emerging area of CNN macroarchitectural research.Residual Networks (ResNet) He et al. (2015b) and Highway Networks Srivastava et al. (2015) each propose the use of connections that skip over multiple layers, for example additively connecting the activations from layer 3 to the activations from layer 6.We refer to these connections as bypass connections.The authors of ResNet provide an A/B comparison of a 34-layer CNN with and without bypass connections; adding bypass connections delivers a 2 percentage-point improvement on Top-5 ImageNet accuracy.	This was one of the experimental investigations that was interesting.  Also, complex bypass adds less parameters which decreases the number of parameters trained for the same task on the same data(it may have some small overfitting side effect ).	Opposite	more=> less, increases=> decreases	
106	paper_11	The goal of authors regarding microarchitectural design space was to understand the impact of CNN architectural choices on model size and accuracy. Were they able to draw a conclusive impact?	They come to an impact that the size of the model can be reduced while still obtaining same or higher accuracy with fewer parameters through manipulating architectural design strategies as is the case in their architecture -SqueezeNet.	As you can see, there are several advantages of smaller CNN architectures.With this in mind, we focus directly on the problem of identifying a CNN architecture with fewer parameters but equivalent accuracy compared to a well-known model.We have discovered such an architecture, which we call SqueezeNet.In addition, we present our attempt at a more disciplined approach to searching the design space for novel CNN architectures.In this section, we design and execute experiments with the goal of providing intuition about the shape of the microarchitectural design space with respect to the design strategies that we proposed in Section 3.1.Note that our goal here is not to maximize accuracy in every experiment, but rather to understand the impact of CNN architectural choices on model size and accuracy.	None		no nice place to twist	
107	paper_113	How YoloV3 calculates the sizes of the anchor boxes?	The authors they tried multiples of the initial anchor sizes specified by the 9 clusters.  The clusters as specified at the cell D58.	Anchor box x,y offset predictions. We tried using the normal anchor box prediction mechanism where you predict the x,y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn’t work very well.Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, t_{x}, t_{y}, t_{w}, t_{h}. If the cell is offset from the top left corner of the image by (c_{x},c_{y}) and the bounding box prior has width and height p_{w}, p_{h}, then the predictions correspond to:	The authors they tried multiples of the initial anchor sizes specified by the 4 clusters.  The clusters as specified at the cell D58.	Change number 	9=>4	
110	paper_113	Which data augmentation techniques YoloV3 algorithm used during training?	They use augmentation, mention it only once in the paper, and this is the place.	We still train on full images with no hard negative mining or any of that stuff. We use multi-scale training, lots of data augmentation, batch normalization, all the standard stuff. We use the Darknet neural network framework for training and testing [14].	None		cannot be used as citation, as the information in the compostion is too shallow	
111	paper_113	Does DarkNet-53 backbone of YoloV3 uses any skip connections?	"residuals" = skip connections, which means that DarkNet-53 uses skip connections. 	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!	No skip connections were used in DarkNet-53. 	Opposite	skip-connections used => no skip connections used	
112	paper_113	Why the YoloV3 performs poorly with higher values of AP when compared with RetinaNet?	YOLOv3 performs poorly because of struggles to get the perfect bounding box alignment with the objects.	However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.	RetinaNet performs poorly because ot struggles to get the perfect bounding box alignment with the objects.	Change concept	YOLOv3 => RetinaNet	
113	paper_113	Why the focal loss strategy did not worked for the authors?	The authors hypothesize that YOLOv3 may already be robust to the problem which the focal loss is trying to solve because it has spearate objectness predictions and conditional class predictions.  That is why adding the focal loss did not improve the performance of YOLOv3.	Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren’t totally sure.	The authors hypothesize that YOLOv3 may already be robust to the problem which the focal loss is trying to solve because it does not have spearate objectness predictions and conditional class predictions.  That is why adding the focal loss did not improve the performance of YOLOv3.	Opposite	it has spearate objectness predictions and conditional class predictions => it does not have spearate xx 	
114	paper_113	YOLO detectors are now being used everywhere including both civil and military use. As a researcher how much authors should be concerned on positive and negative use of their research work?	A sarcastic comment means a concern for authors that Google, Facebook, and similar corporations use these kind of models to harvest and use our personal information.  A similar sarcastic comment regarding military.  The authors should be responsible for their work and consider possible consequences to the world.	But maybe a better question is: “What are we going to do with these detectors now that we have them?” A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and definitely won’t be used to harvest your personal information and sell it to…. wait, you’re saying that’s exactly what it will be used for?? Oh.Well the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait…..111The author is funded by the Office of Naval Research and Google.I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. We owe the world that much.	A sarcastic comment means a concern for authors that Amazon, Apple, and similar corporations use these kind of models to harvest and use our personal information. A similar sarcastic comment regarding the government. The authors should be responsible for their work and consider possible consequences to the world.	Change concept	The core concepts of "Google, Facebook" (tech corporations) and "military" have been changed to "Amazon, Apple" (a different set of tech corporations) and "the government," respectively. 	
115	paper_113	What are some of the limitations of the YOLOv3 object detection model?	Some of the limitations of YOLOv3, based on the information given in the paper are: it is still quite a bit behind other models like RetinaNet in the "COCO's weired average mAP" metric (COCO average AP between 95 IOU metric), performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object, it has comparatively worse performance on medium and larger size objects. 	YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3×3\times3 × faster. It is still quite a bit behind other models like RetinaNet in this metric though.However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.YOLOv3 is a good detector. It’s fast, it’s accurate. It’s not as great on the COCO average AP between .5 and .95 IOU metric. But it’s very good on the old detection metric of .5 IOU.	Some of the limitations of YOLOv3, based on the information given in the paper are: it is still quite a bit behind other models like RetinaNet in the "COCO's weired average mAP" metric (COCO average AP between 95 IOU metric), performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object, it has comparatively worse performance on medium and larger size objects, and it fails completely on objects smaller than 10x10 pixels.	Invent something didn't mentioned	A new limitation regarding small object performance has been invented and added, which was not mentioned in the original text.	
116	paper_113	Compare accuracy and speed of Darknet-53 with ResNet-101.	Darknet-53 is better than ResNet-101 and 1. 5\times1. 5 × faster.	Each network is trained with identical settings and tested at 256\times 256, single crop accuracy. Run times are measured on a Titan X at 256\times 256. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5×1.5\times1.5 × faster. Darknet-53 has similar performance to ResNet-152 and is 2×2\times2 × faster.Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That’s mostly because ResNets have just way too many layers and aren’t very efficient.	ResNet-101 is better than Darknet-53 and 1. 5\times1. 5 × faster.	Opposite	change the place of ResNet-101 and Darknet-53	
117	paper_113	In its loss function YoloV3 uses logistic regression with multilabel classification or Softmax over all class probabilities?	The authors use binary cross-entropy loss.	Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	The authors use Softmax over all class probabilities.	Change concept	The authors do not use a softmax as they have found it is unnecessary for good performance, instead they simply use independent logistic classifiers	
118	paper_113	YoloV3 is most suited for small, medium or large size objects?	YOLOv3 now struggles more with medium and larger size objects, i. , performs worse than before.  On the other hand, it is more succesful for smaller objects.	In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	YOLOv3 now struggles more with smaller objects, i. , performs worse than before.  On the other hand, it is more succesful for medium and larger size objects.	Opposite	medium and larger size objects <=> smaller objects	
119	paper_113	How does YOLOv3 improve upon previous versions of the YOLO object detection algorithm?	YOLOv3 is faster and better than YOLO.  It has more layers.  The authors also tried some small tricks and experiments which further improved the overall performance.	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!When we plot accuracy vs speed on the AP{}_{50} metric (see figure 5) we see YOLOv3 has significant benefits over other detection systems. Namely, it’s faster and better.So here’s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that’s better than the other ones. We’ll just take you through the whole system from scratch so you can understand it all.	YOLOv3 is faster and better than YOLO. It has significantly more (over 50) layers. The authors also tried some small tricks and experiments which further improved the overall performance.	Change number	The vague "more layers" is changed to a specific, high number ("over 50"), which alters the quantitative claim.	
120	paper_113	In paper authors make the predictions at three different scales, but what is advantage of making object detections at different scales?	By using multi-scaled prediction, YOLOv3 has improved performance for small objects.  Also, the subsequent scales benefit from previous scales and the previous features from earlier layers.	We perform the same design one more time to predict boxes for the final scale. Thus our predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	By using multi-scaled prediction, YOLOv3 has improved performance for medium and larger size objects.  Also, the subsequent scales benefit from previous scales and the previous features from earlier layers.	Opposite	medium and larger size objects <=> smaller objects	
121	paper_113	How would the loss function of YoloV3 look after changing Mean squared errors with the logistic regression cross-entropy error terms?	Binary cross-entropy is used for the class predictions.  Logistic activation is used and is better than the linear activation.	Linear x,y predictions instead of logistic. We tried using a linear activation to directly predict the x,y offset instead of the logistic activation. This led to a couple point drop in mAP.Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	Binary cross-entropy is used for the class predictions. Logistic activation is used and is better than the linear activation, as it prevents gradient vanishing during training.	Invent something didn't mentioned	A technical justification ("prevents gradient vanishing") that was not present in the original text is invented and added.	
122	paper_114	How many different types of experiments are performed to test the proposed models?	5 different types of experiments are performed to test the proposed models.  They are Generalization over time scales, Experiments on MNIST, Experiments on Natural Image Patches, Out-of-domain Inputs, and Visualizing Features.	Generalization over time scalesIn the next experiment, we test if the model can work at time scales that aredifferent than what it was trained on. We take a one hidden layer unconditionedComposite Model trained on moving MNIST digits. The model has 2048 LSTM unitsand looks at a 64 \times 64 input. It was trained on input sequences of 10frames to reconstruct those 10 frames as well as predict 10 frames into thefuture. In order to test if the future predictor is able to generalize beyond 10frames, we let the model run for 100 steps into the future.Fig. 7(a) shows the pattern of activity in the LSTM units of thefuture predictorpathway for a randomly chosen test input. It shows the activity at each of thethree sigmoidal gates (input, forget, output), the input (after the tanhnon-linearity, before being multiplied by the input gate), the cell state andthe final output (after being multiplied by the output gate). Even though theunits are ordered randomly along the vertical axis, we can see that the dynamicshas a periodic quality to it. The model is able to generate persistent motionfor long periods of time. In terms of reconstruction, the model only outputsblobs after the first 15 frames, but the motion is relatively well preserved.More results, including long range future predictions over hundreds of time steps can see been athttp://www.cs.toronto.edu/~nitish/unsupervised_video.To show that setting up a periodic behaviour is not trivial,Fig. 7(b) shows the activity from a randomly initialized futurepredictor. Here, the LSTM state quickly converges and the outputs blur completely.Experiments on MNIST; We first trained our models on a dataset of moving MNIST digits. In this dataset, each video was 20 frames long and consisted of two digits moving inside a 64 × 64 patch. The digits were chosen randomly from the training set and placed initially at random locations inside the patch.Experiments on Natural Image Patches; Next, we tried to see if our models can also work with natural image patches. For this, we trained the models on sequences of 32 × 32 natural image patches extracted from the UCF-101 dataset. In this case, we used linear output units and the squared error loss function.Out-of-domain Inputs; Next, we test this model’s ability to deal with out-of domain inputs. For this, we test the model on sequences of one and three moving digits. The model was trained on sequences of two moving digits, so it has never seen inputs with just one digit or three digits.Visualizing Features; Next, we visualize the features learned by this model. Fig. 9 shows the weights that connect each input frame to the encoder LSTM. There are four sets of weights. One set of weights connects the frame to the input units. There are three other sets, one corresponding to each of the three gates (input, forget and output). Each weight has a size of 64 × 64.	4 different types of experiments are performed to test the proposed models. They are Generalization over time scales, Experiments on MNIST, Experiments on Natural Image Patches, and Visualizing Features.	Change concept	Change 5 to 4 by removing the type of experiment "Out-of-domain Inputs".	
123	paper_114	Which variants of LSTM encoder-decoder models are used in this study?	Future Predictor, Composite Model, Conditional Future Predictor, Composite Model with Conditional Future Predictor are the variants of LSTM encoder-decoder models are used in this study.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.Future prediction results are summarized in Table 2. For MNIST we compute the cross entropy of the predictions with respect to the ground truth, both of which are 64 × 64 patches. For natural image patches, we compute the squared loss. We see that the Composite Model always does a better job of predicting the future compared to the Future Predictor. This indicates that having the autoencoder along with the future predictor to force the model to remember more about the inputs actually helps predict the future better. Next, we can compare each model with its conditional variant. Here, we find that the conditional models perform better, as was also noted in Fig. 5.	Future Predictor, Composite Model, Conditional Future Predictor, Composite Model with Conditional Future Predictor are the variants of GRU encoder-decoder models are used in this study.	Change concept	The core architectural concept is changed from "LSTM" to "GRU" (Gated Recurrent Unit).	
124	paper_114	List down supervised and unsupervised tasks on which the proposed model is tested?	The supervised task is action recognition and unsupervised tasks are representation reconstruction, which can be inferred from P4.	In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitative evaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	The supervised task is representation reconstruction and unsupervised tasks are action recognition, which can be inferred from P4.	Opposite	representation reconstruction <=> action recognition	
125	paper_114	Historically, which architectures have been used for supervised sequence learning tasks?	Recurrent neural networks using the Long Short Term Memory (LSTM) architectures have been used for supervised sequence learning tasks.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Recurrent neural networks using the Gated Recurrent Unit (GRU) architectures have been used for supervised sequence learning tasks.	Change concept	Long Short Term Memory(LSTM) => Gated Recurrent Unit (GRU)	
126	paper_114	How good the LSTM based encode/decoder work for real time applications keeping in view their sequential nature?	Since LSTM based encoder/decoder method successfully worked for real time sequential nature application, it is a good method.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Since LSTM based encoder/decoder method struggled with real time sequential nature application, it is not a suitable method.	Opposite	"successfully worked" to "struggled with" and "is a good method" to "is not a suitable method"	
127	paper_114	The authors extended which baseline framework to learn representation of image sequences?	The authors extended identical LSTM classifier framework as baseline to learn representation of image sequences.	The baseline for comparing these models is an identical LSTM classifier but with randomly initialized weights. All classifiers used dropout regularization, where we dropped activations as they were communicated across layers but not through time within the same LSTM as proposed in Zaremba et al. (2014). We emphasize that this is a very strong baseline and does significantly better than just using single frames. Using dropout was crucial in order to train good baseline models especially with very few training examples.	The authors extended identical LSTM classifier framework as baseline to learn representation of image sequences, which was then fine-tuned on a larger, unlabeled dataset.	Invent something didn't mentioned	A new methodological detail (fine-tuning on a larger, unlabeled dataset) is invented and added, which was not mentioned in the original text.	
128	paper_114	Why the authors prefer to learn video representations through unsupervised models?	Labelling videos is a tedious job and that makes supervise training very expensive.  Compare to that unsupervised model can take advantage of all videos available that doesn't need labelling.  That's why the authors prefer to learn video representations through unsupervised models.	Supervised learning has been extremely successful in learning good visualrepresentations that not only produce good results at the task they are trainedfor, but also transfer well to other tasks and datasets. Therefore, it isnatural to extend the same approach to learning video representations. This hasled to research in 3D convolutional nets (Ji et al., 2013; Tran et al., 2014), different temporalfusion strategies (Karpathy et al., 2014) and exploring different ways ofpresenting visual information to convolutional nets (Simonyan & Zisserman, 2014a).However, videos are much higher dimensional entities compared to single images.Therefore, it becomes increasingly difficult to do credit assignment and learn longrange structure, unless we collect much more labelled data or do a lot offeature engineering (for example computing the right kinds of flow features) tokeep the dimensionality low. The costly work of collecting more labelled dataand the tedious work of doing more clever engineering can go a long way insolving particular problems, but this is ultimately unsatisfying as a machinelearning solution. This highlights the need for using unsupervised learning tofind and represent structure in videos. Moreover, videos have a lot ofstructure in them (spatial and temporal regularities) which makes themparticularly well suited as a domain for building unsupervised learning models.	Labelling videos is a tedious job and that makes supervise training very expensive. Compare to that self-supervised model can take advantage of all videos available that doesn't need labelling. That's why the authors prefer to learn video representations through self-supervised models.	Change concept	"unsupervised" learning is changed to "self-supervised" learning	
130	paper_114	What are the different input types used for the proposed model?	Image patches and high-level percepts are the two types of inputs used in the proposed model.	The inputs to the model can, in principle, be any representation of individualvideo frames. However, for the purposes of this work, we limit our attention to two kinds of inputs. The first is image patches. For this we use natural imagepatches as well as a dataset of moving MNIST digits. The second ishigh-level “percepts” extracted by applying a convolutional net trained onImageNet. These percepts are the states of last (and/or second-to-last) layers ofrectified linear hidden states from a convolutional neural net model.	Image patches and low-level features are the two types of inputs used in the proposed model.	Change concept	"high-level percepts" is changed to "low-level features"	
131	paper_114	Why should LSTM based auto-encoder models learn good features?	Since LSTM based auto-encoder models control the learning an identity mapping, it forced learn good features.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used to decode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	Since LSTM based auto-encoder models fail to control the learning of an identity mapping, it prevented them from learning good features.	Opposite	The semantic meaning is made completely opposite by changing "control" to "fail to control" and "forced [to] learn good features" to "prevented them from learning good features".	
132	paper_114	How the proposed autoencoder architecture prevent overfitting or identity mapping?	The two factors that control the model from learning an identity mapping or prevent overfitting are fixed number of hidden units and forceful decode of the input representation recursively.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used todecode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	The two factors that control the model from learning an identity mapping or prevent overfitting are a dynamic hidden unit allocation and forceful decode of the input representation recursively.	Change concept	The concept of a "fixed number of hidden units" is changed to "dynamic hidden unit allocation," which is a fundamentally different architectural approach.	
134	paper_114	How the proposed LSTM future predictor model is different from the  Ranzato model.	Ranzato model predict only the next frame but LSTM future predictor model predicts a long sequence into the future.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.	LSTM future predictor model predict only the next frame but Ranzato model predicts a long sequence into the future.	Opposite	LSTM future predictor model <=> Ranzato model	
135	paper_114	Why there is no need to label objects in videos for the encoder-decoder model.	Since representation is another form of input, that's why it doesn't need label for any purpose.	When designing any unsupervised learning model, it is crucial to have the right inductive biases and choose the right objective function so that the learning signal points the model towards learning useful features. In this paper, we use the LSTM Encoder-Decoder framework to learn video representations. The key inductive bias here is that the same operation must be applied at each time step to propagate information to the next step. This enforces the fact that the physics of the world remains the same, irrespective of input. The same physics acting on any state, at any time, must produce the next state. Our model works as follows. The Encoder LSTM runs through a sequence of frames to come up with a representation. This representation is then decoded through another LSTM to produce a target sequence. We consider different choices of the target sequence. One choice is to predict the same sequence as the input. The motivation is similar to that of autoencoders – we wish to capture all that is needed to reproduce the input but at the same time go through the inductive biases imposed by the model. Another option is to predict the future frames. Here the motivation is to learn a representation that extracts all that is needed to extrapolate the motion and appearance beyond what has been observed. These two natural choices can also be combined. In this case, there are two decoder LSTMs – one that decodes the representation into the input sequence and another that decodes the same representation to predict the future.	Since the model is trained using a self-supervised objective, that's why it doesn't need labels for its training purpose.	Change concept	The core concept is changed from the vague "representation is another form of input" to the more precise and evidence-aligned concept of "self-supervised objective," which describes the tasks of reconstruction and prediction.	
136	paper_114	Why the conditional decoder is difficult to optimize?	Since in conditional decoder have access to last few frames, often it find a easy way to pick up a correlated frame but not necessary an optimized one.  That is why it is difficult to optimize.	There is also an argument against using a conditional decoder from theoptimization point-of-view. There are strong short-range correlations invideo data, for example, most of the content of a frame is same as the previousone. If the decoder was given access to the last few frames while generating aparticular frame at training time, it would find it easy to pick up on thesecorrelations. There would only be a very small gradient that tries to fix up theextremely subtle errors that require long term knowledge about the inputsequence. In an unconditioned decoder, this input is removed and the model isforced to look for information deep inside the encoder.	Since in conditional decoder have access to last few frames, often it find a difficult way to pick up a correlated frame but an optimized one. That is why it is easy to optimize.	Opposite	The meaning is made completely opposite by changing "easy way" to "difficult way," "not necessary an optimized one" to "an optimized one," and "difficult to optimize" to "easy to optimize."	
137	paper_114	Out of conditional and unconditional decoder blocks, which one is better?	The author talk both advantage and disadvantage of conditional and unconditional decoder blocks.  They also provided a strong argument in favor of using a conditional decoder but clearly no winner is mentioned.	For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.	The author talk both advantage and disadvantage of conditional and unconditional decoder blocks.  They also provided a strong argument in favor of using an unconditional decoder.	Opposite	"strong argument in favor of using a conditional decoder but clearly no winner is mentioned." => "strong argument in favor of using an unconditional decoder."	
138	paper_114	Which datasets are used by the paper for supervised learning?	UCF-101 and HMDB-51 datasets are used for supervised learning.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of 6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips. Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the superviseddatasets (UCF-101 and HMDB-51) for unsupervised training. However, we found thatusing them did not give any significant advantage over just using the YouTubevideos.	Kinetics-400 and HMDB-51 datasets are used for supervised learning.	Change concept	The core concept of one dataset ("UCF-101") is changed to another ("Kinetics-400"), a different and larger video recognition dataset.	
139	paper_114	Which datasets are used by the paper for training and testing of unsupervised learning?	UCF-101, HMDB-51 and YouTube videos datasets are used for supervised learning.	To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips.Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the supervised datasets (UCF-101 and HMDB-51) for unsupervised training. However, we found that using them did not give any significant advantage over just using the YouTubevideos.	YouTube videos datasets are used for supervised learning.	Change concept	Remove "UCF-101, HMDB-51" which are also used for training and testing of unsupervised learning	
140	paper_114	What is the average video sequence length used for experiments in this study?	The UCF-101 dataset contains 13,320 videos with an average length of 6. 2 seconds.  The HMDB-51 dataset contains 5100 videos with mean length of the videos is 3. 2 seconds.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.	The UCF-101 dataset contains 13,320 videos with an average length of 3.2 seconds.  The HMDB-51 dataset contains 5100 videos with mean length of the videos is 6.2 seconds.	Change number	Change the average lenght of the two datasets, 6.2 <=> 3.2 	
141	paper_114	Does the features learned by unsupervised learning improved the performance of supervised learning tasks?	The improvement in classification by using unsupervised learning was not as big as we expected, we still managed to yield an additional improvement over a strong baseline.  If the unsupervised learning model comes up with useful representations then the classifier perform better, especially when there are only a few labelled examples.  Based on the above evidence, it can be safely said that features learned by unsupervised learning improved the performance of supervised learning tasks.	Fig. 12 compares three models - single frame classifier(logistic regression), baseline LSTM classifier and the LSTM classifierinitialized with weights from the Composite Model as the number of labelledvideos per class is varied. Note that having one labelled video means havingmany labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. Forexample, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get aconsiderable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification byusing unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenuesfor improvements later.Next, we compare the models using performance on a supervised task.Table 3 shows the performance on actionrecognition achieved by finetuning different unsupervised learning models.Besides running the experiments on the full UCF-101 and HMDB-51 datasets, we also ran theexperiments on small subsets of these to better highlight the case where we havevery few training examples. We find that all unsupervised models improve over thebaseline LSTM which is itself well-regularized by using dropout. The Autoencodermodel seems to perform consistently better than the Future Predictor. TheComposite model which combines the two does better than either one alone.Conditioning on the generated inputs does not seem to give a clearadvantage over not doing so. The Composite Model with a conditional futurepredictor works the best, although its performance is almost same as that of theComposite Model.We proposed models based on LSTMs that can learn good video representations. Wecompared them and analyzed their properties through visualizations. Moreover, wemanaged to get an improvement on supervised tasks. The best performing model wasthe Composite Model that combined an autoencoder and a future predictor.Conditioning on generated outputs did not have a significant impact on theperformance for supervised tasks, however it made the future predictions lookslightly better. The model was able to persistently generate motion well beyondthe time scales it was trained for. However, it lost the precise object featuresrapidly after the training time scale. The features at the input and outputlayers were found to have some interesting properties.In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitativeevaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	The features learned by unsupervised learning did not improve the performance of supervised learning tasks.	Opposite	Improved => did not improve	
142	paper_114	What is the impact of number of training videos on the performance of supervised and unsupervised tasks?	As the number of training videos increases the performance of supervised and unsupervised tasks increases.	Fig. 12 compares three models - single frame classifier (logistic regression), baseline LSTM classifier and the LSTM classifier initialized with weights from the Composite Model as the number of labelled videos per class is varied. Note that having one labelled video means having many labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. For example, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get a considerable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification by using unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenues for improvements later.	As the number of training videos increases the performance of self-supervised and unsupervised tasks increases.	Change concept	"supervised" learning is changed to "self-supervised" learning	
143	paper_114	Which evaluation criteria was used to compare the performance of action recognition models?	Evaluation criteria are measure on RGB data (single or multiple frames) and flow features.	Finally, we compare our models to the state-of-the-art action recognition results. The performance is summarized in Table 4. The table isdivided into three sets. The first set compares models that use only RGB data (single or multiple frames). The second set compares models that use explicitly computed flow features only. Models in the third set use both.	Evaluation criteria are measure on RGB data (single or multiple frames).	Change concept	remove " and flow features"	
144	paper_114	Which metric is used to compare different unsupervised models?	Error in predicting the future and the performance on supervised tasks are the metrics used to compare different unsupervised models.	The aim of this set of experiments is to compare the different variants of the model proposed in this paper. Since it is always possible to get lower reconstruction error by copying the inputs, we cannot use input reconstruction error as a measure of how good a model is doing. However, we can use the error in predicting the future as a reasonable measure of how good the model is doing. Besides, we can use the performance on supervised tasks as a proxy for how good the unsupervised model is doing. In this section, we present results from these two analyses.	Error in predicting the future and the performance on supervised tasks are the metrics used to compare different unsupervised models, with each metric being weighted equally in the final ranking.	Invent something didn't mentioned	A new methodological detail about how the metrics are combined ("weighted equally in the final ranking") is invented and added.	
145	paper_114	Why is it a good idea to apply the convolutions across patches of the video instead of whole frames?	To extract motion information it is a good idea to apply the convolutions across patches of the video instead of whole frames.	To further get improvements for supervised tasks, we believe that the model can be extended by applying it convolutionally across patches of the video and stacking multiple layers of such models. Applying this model in the lower layers of a convolutional net could help extract motion information that would otherwise be lost across max-pooling layers. In our future work, we plan to build models based on these autoencoders from the bottom up instead of applying them only to percepts.	To extract motion information it is a good idea to apply the convolutions across temporal segments of the video instead of whole frames.	Change concept	The core concept is changed from spatial "patches" to "temporal segments," focusing on chunks of time rather than regions of the frame.	
146	paper_115	What is kernel size used in each layer of SegNet?	The kernel size used in each layer of SegNet is 7*7.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	The kernel size used in each layer of SegNet is 4*4.	Change number	7*7 => 4*4	
147	paper_115	How the features are converted to pixel labels in SegNet?	SegNet performs feed-forward computation to obtain pixel-wise labelling.	We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	SegNet performs feedback-based optimization to obtain pixel-wise labelling.	Opposite	The core concept of the computation is reversed from "feed-forward" to "feedback-based optimization," which describes an iterative refinement process instead of a single pass.	
148	paper_115	What is the advantage of stacking encoders and decoders for semantic segmentation?	Stacking encoders and decoders architecture produce smooth segment labels.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	Stacking encoders and decoders architecture produce noisy and fragmented segment labels.	Opposite	The semantic meaning is made completely opposite by changing "smooth" to "noisy and fragmented," directly contradicting the original claim about the output quality.	
149	paper_115	What is a major drawback of deep learning approaches adapting networks designed for object categorization to pixel wise labeling?	Due to the use of non-overlapping max-pooling-subsampling layers, the resulting feature map is reduced compare to the input dimension.  Ad hoc technique then used to make the feature map same as input dimention by replication same pixel.  This generates a blocky predictions, which is a major drawback of deep learning approaches adapting networks designed for object categorization to pixel wise labeling.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.	Due to the use of overlapping average-pooling layers, the resulting feature map is reduced compare to the input dimension.  Ad hoc technique then used to make the feature map same as input dimention by replication same pixel.  This generates a blocky predictions, which is a major drawback of deep learning approaches adapting networks designed for object categorization to pixel wise labeling.	Change concept	non-overlapping max-pooling-subsampling layers => overlapping average-pooling layers	
150	paper_115	Can we use image classification models for semantic segmentation?	Since a patch is fed into a classifier to predict the class probabilities of the center pixel, it is evident that image classification models for semantic segmentation.	Semantic pixel-wise segmentation is an ongoing topic of research, fuelled by challenging datasets [1, 33, 9]. Current best performing methods all mostly rely on hand engineered features generally used for per-pixel independent classification. Typically, a patch is fed into a classifier e.g. Random Forest [32, 2] or Boosting [36, 20] to predict the class probabilities of the center pixel. Features based on appearance [32], SfM and appearance [2, 36, 20] have been explored for the CamVid test. These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [36, 20] to improve the accuracy. More recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. This improves the results of Random Forest based unaries [18] but thin structured classes are classfied poorly. Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [43]. Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [39]. Recent top performing technique on the CamVid test [20] addresses the imbalance among label frequencies by using additional training data from the PASCAL VOC dataset to learn object detectors. The result of all these techniques indicates the need for improved classification as increases in accuracy have mostly come from adding new features or modalities to the classifier. Post-processing using CRF models of various orders [36] has mainly resulted in improving the accuracy of dominant classes such as sky, road, buildings with little effect on the accuracy of thin structured but equally important classes such as signs, poles, pedestrians. This highlights the need for better pixel-wise classification when imbalanced label frequencies exist.Meanwhile, indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [33] which showed the usefulness of the depth channel to improve segmentation. Their approach used features such as RGB-SIFT, depth-SIFT, location as input to a neural network classifier to predict pixel unaries. The noisy unaries are then smoothed using a CRF. Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [28] followed by a CRF. In more recent work [33], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues. Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [13]. Gupta et al. [12] use boundary detection and hierarchical grouping before performing category segmentation. The common attribute along all these approaches is the use of hand engineered features for pixel-wise classifiction of either RGB or RGBD images. The application of deep learning for scene segmentation has only just begun. There have also been a few attempts to apply networks designed for categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [7, 6, 11, 8]. However, the resulting classification is blocky [11]. Another approach using recurrent neural networks [26] merges several low resolution predictions to create input image resolution predictions. On the whole, although some of these techniques already present improvements over hand engineered features [7].	It is not evident that image classification models for semantic segmentation.	Opposite	evident => not evident	
151	paper_115	SegNet architecture is inspired from which domain?	SegNet architecture is inspired from generative models and unsupervised learning.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.Our work is inspired by the unsupervised feature learning architecture proposed by Ranzato et. al [27]. The key learning module is an encoder-decoder network where the encoder consists of a filter bank convolution, tanh squashing function, max pooling followed by sub-sampling to obtain the feature maps. For each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. The decoder upsamples the feature maps by using the already stored pooled indices, also called switches, and learns a decoder filter bank to reconstruct the input image. This architecture was used for unsupervised pre-training of feature hierarchies. A similar decoding technique is used for visualizing trained convolutional networks[42] for object classification; the transposed encoder kernels are set as the decoder kernels which are followed by a non-linearity and the pooling indices are used for upsampling. The architecture of Ranzato mainly concentrated on layer wise feature learning using small input patches although during test time a full sized image was the input. This discrepancy was corrected for by Kavukcuoglu et. al. [16] by using test size images/feature maps to learn hierarchical encoders. Both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. Here, the SegNet architecture differs from these approaches as the objective used for training all the encoder-decoder pairs is the same, i.e., to minimise the cross-entropy label loss.	SegNet architecture is inspired from generative models and supervised learning.	Change concept	unsupervised => supervised	
152	paper_115	What are the total number of encoders and decoders used in SegNet?	4 encoders and 4 decoders are used in SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.We use mini-batches that maximize GPU usage and avoid GPU-CPU memory transfers. Typically, 25-50 randomly chosen images (with replacement) per mini-batch. The optimizer is run for 20 iterations per mini-batch and 10 epochs for each layer. We empirically observe that the objective plateaus after 5-6 epochs and so we run another 4 epochs as a margin. Note that, after 10 epochs, each input sample approximately “influences” the optimizer200 times. We train the encoder-decoder pair weights closest to the input layer. The soft-max layer can be trained first or randomly initialised. It then remains fixed throughout the experiment. Next, we introduce a deeper layer of encoder-decoder (see Fig. 2) and train their weights while holding the shallower layer encoder-decoder weights fixed. Note that the objective remains the same, i.e., to minimize label cross-entropy loss over the mini-batch. This is unlike unsupervised feature learning approaches which reconstruct the input of the layer in question [27, 16], thus varying the objective with each layer. The deconvolution network [42] on the other hand optimizes the same reconstruction objective with each deeper layer. The difference to our approach is (i) the objective is unsupervised, (ii) there is no encoder to learn a feed-forward representation thus requiring an optimisation step during test time to produce features for recognition. We successively add deeper encoder-decoder pairs and train them while holding the preceeding pair’s weights fixed. In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments. Once the encoder-decoder stack is trained, we find that there is no advantage to training the soft-max layer as it only relies on a linear discriminant function.We wrote our own Matlab GPU compatible implementation of SegNet that uses the minFunc optimization library [31]. Our code has been tested on NVIDIA Tesla K40, GTX GeForce 880M and GTXGeForce780 GPUs. We will make our light-weight Matlab code available publicly soon. With the current state of code optimisation, training a 4 layer deep SegNet on the CamVid dataset (367 training images of 360\times 480) takes about a week. The unoptimized test time is in the order of 2secs/frame: bulk of the computation time is spent performing tensor convolutions in the feedforward path and FFT based convolutions during backpropagation 333more speedup can be gained https://developer.nvidia.com/cuDNN.	8 encoders and 8 decoders are used in SegNet.	Change number	4 => 8	
153	paper_115	How many features are used in each layer of SegNet?	64 features are used in each layer of SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	32 features are used in each layer of SegNet.	Change number	64 => 32	
154	paper_115	What are the advantages of using a flat architecture in SegNet?	The flat architecture avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder) and the training time remains almost same for each additional/deeper encoder-decoder pair.	SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	The flat architecture avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder) and the training time remains almost same for each additional/deeper encoder-decoder pair, as demonstrated on a dataset of 10 million images.	Invent something didn't mentioned	A specific, unmentioned quantitative detail ("a dataset of 10 million images") is invented to substantiate the claim about training time.	
155	paper_115	Define local contrast normalization (LCN)?	Local Contrast Normalization (LCN) is a pre-processing step that normalize the input to a non-uniform scene illumination, highlight edges, and decorrelates the input dimensions.  This normalization performed on each channel of an input image.  It improves convergence and helps to learn category shape.	The input to the SegNet can be any arbitrary multi-channel image or feature map(s), e.g., RGB, RGBD, map of normals, depth etc. We perform local contrast normalization (LCN) as a pre-processing step to the input [23, 15]. The advantage of this step are many, (i) to correct for non-uniform scene illumination thus reducing the dynamic range (increases contrast in shadowed parts). (ii) highlighting edges which leads the network to learn category shape, (iii) improves convergence as it decorrelates the input dimensions [23]. LCN is performed independently for each modality, i.e., RGB is contrast normalized as a three channel input and depth as a single channel for RGBD inputs. This avoids highlighting pseudo depth edges due to RGB edges and vice-versa.	Local Contrast Normalization (LCN) is a pre-processing step that normalize the input to a non-uniform scene illumination, highlight edges, and decorrelates the input dimensions. This normalization performed on the first three channels of an input image. It improves convergence and helps to learn category shape.	Change number	on each channel of an input image => the first three channels of an input image	
157	paper_116	How are the images for this challenge collected for each category?	Training images are taken directly from ImageNet.  Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.	Image collection for ILSVRC classification task is the same as the strategy employed for constructing ImageNet (Deng et al.,, 2009). Training images are taken directly from ImageNet. Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.	The training images are not from ImageNet. No additional images were collected for the ILSVRC, and the data was manually partitioned into validation and test sets.	Opposite	add "not" "no" and change to "manually"	
159	paper_116	How long is this challenge been running?	The challenge has been running for past 5 years.	The key lesson of collecting the datasets and running the challenges for five years is this: All human intelligence tasks need to be exceptionally well-designed. We learned this lesson both when annotating the dataset using Amazon Mechanical Turk workers (Section 3) and evenwhen trying to evaluate human-level image classification accuracy using expert labelers (Section 6.4). The first iteration of the labeling interface was always bad – generally meaning completely unusable. If there was any inherent ambiguity in the questions posed (and there almost always was), workers found it and accuracy suffered. If there is one piece of advice we can offer to future research, it is to very carefully design, continuously monitor, and extensively sanity-check all crowdsourcing tasks.In this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks. We elaborate further on these and other more minor challenges with large-scale evaluation. Appendix F describes the submission protocol and other details of running the competition itself.	The challenge has been running for past 3 years.	Change number	5 => 3	
160	paper_116	ImageNet challenge benchmarks which problems in computer vision domain?	It emphasizes the importance of examining the bias inherent in any standardized dataset.	There are several datasets with standardized online evaluation similar to ILSVRC: the afore mentioned PASCAL VOC (Everingham et al.,, 2012), Labeled Faces in the Wild (Huang et al.,, 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al.,, 2014) for 3D reconstruction and KITTI (Geiger et al.,, 2013) for computer vision in autonomous driving. These datasets along with ILSVRC help benchmark progress in different areas of computer vision. Works such as (Torralba and Efros,, 2011) emphasize the importance of examining the bias inherent in any standardized dataset.	It emphasizes the importance of examining the quality inherent in any standardized dataset.	Change concept	"bias" (a negative, problematic property) to "quality"	
161	paper_116	What is the difference between classification and object detection?	For the image classification task every image was annotated with one object class label, corresponding to one object that is present in an image.  For the single-object localization task, every validation and test image and a subset of the training images were annotated  with axis-aligned bounding boxes around every instance of this object.	Recall that for the image classification task every image was annotated with one object class label, corresponding toone object that is present in an image. For the single-object localization task, every validation and test image and a subset of the training imagesare annotated with axis-aligned bounding boxes around every instance of this object.	For the image classification task every image was annotated with one object class label, corresponding to one object that is present in an image.  For the single-object localization task, every validation and test image and the training images were annotated  with axis-aligned bounding boxes around every instance of this object.	Change number	a subset of the training images => the training images (from subset to all)	
162	paper_116	How many images do the ILSVRC dataset has?	ILSVRC dataset has 1. 2 million training images, 50 thousand validation images and 100 thousand test images.	Using the image collection and annotation procedure described in previous sections, we collected a large-scale datasetused for ILSVRC classification task. There are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.	ILSVRC dataset has 1 million training images, 50 thousand validation images and 100 thousand test images.	Change number	1. 2 million training images => 1 million	
163	paper_116	Objects are divided into how many classes in the ILSVRC dtaset?	It's divided into 1000 classes.	Using the image collection and annotation procedure described in previous sections, we collected a large-scale datasetused for ILSVRC classification task. There are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.The selection of the 200 object detection classes in 2013 was guided by the ILSVRC 2012 classification and localization dataset.Starting with 1000 object classes and their bounding box annotations wefirst eliminated all object classes which tended to be too “big” in the image (on average the object area was greater than 50\% of theimage area). These were classes such as T-shirt, spiderweb, or manhole cover. We then manually eliminated all classeswhich we did not feel were well-suited for detection, such as hay, barbershop, or poncho. This left 494 object classeswhich were merged into basic-level categories: for example, different species of birds were merged into just the “bird” class.The classes remained the same in ILSVRC2014.Appendix D contains the complete list of object categories used in ILSVRC2013-2014 (in the context of the hierarchy described in Section 3.3.3).The scale of ILSVRC classification task (1000 categories and more than a million of images) makes it very expensive to label every instance of every object in every image. Therefore, on this dataset only one object category is labeled in each image. This creates ambiguity in evaluation. For example, an image might be labeled as a “strawberry” but contain both a strawberry and an apple. Then an algorithm would not know which one of the two objects to name. For the image classification task we allowed an algorithm to identify multiple (up to 5) objects in an image and not be penalized as long as one of the objects indeed corresponded to the ground truth label. Figure 7(top row) shows some examples.	None			
164	paper_116	What added benefits do the ILSVRC provide over the existing PASCAL-VOC challenge?	Images from the ILSVRC2012 single-object localization validation set are compared to images from the PASCAL VOC benchmark for object recognition.  They have also analyzed the level of difficulty of object localization in these images compared to those of objects from the PASCAL VOC benchmark.  The level of difficulty of object localization is also analyzed.	The ILSVRC dataset and the competition has allowed significant algorithmic advances in large-scale image recognition and retrieval.The closest to ILSVRC is the PASCAL VOC dataset (Everingham et al.,, 2010, 2014), which provides a standardized test bed for object detection, image classification, object segmentation, person layout, and action classification. Much of the design choices in ILSVRC have been inspired by PASCAL VOC and the similarities and differences between the datasets are discussed at length throughout the paper.ILSVRC scales up PASCAL VOC’s goal of standardized training and evaluation of recognition algorithms by more than an order of magnitude in numberof object classes and images: PASCAL VOC 2012 has 20 object classes and 21,738 images compared to ILSVRC2012 with 1000 object classes and 1,431,167 annotated images.In addition to the size of the dataset, we also analyze the level of difficulty of object localizationin these images compared to the PASCAL VOC benchmark. We compute statistics on the ILSVRC2012 single-object localizationvalidation set images compared to PASCAL VOC 2012 validation images.Prior to ILSVRC, the object detection benchmark was the PASCAL VOC challenge (Everingham et al.,, 2010).ILSVRC has 10 times more object classes than PASCAL VOC (200 vs 20), 10.6 times more fully annotated training images (60,658 vs 5,717), 35.2 times more training objects (478,807 vs 13,609),3.5 times more validation images (20,121 vs 5823) and 3.5 times more validation objects (55,501 vs15,787). ILSVRC has 2.8 annotated objects per image on the validation set, compared to 2.7 in PASCAL VOC. The average object in ILSVRC takes up 17.0\% of the image area and in PASCAL VOC takes up 20.7\%; Table 3 contains per-class comparisons. Additionally, ILSVRC contains a wide variety of objects, including tiny objects such as sunglasses (1.3\% of image area on average), ping-pong balls (1.5\% of image area on average) and basketballs (2.0\% of image area on average).	Images from the ILSVRC2012 single-object localization validation set are compared to images from the MS-COCO benchmark for object recognition. They have also analyzed the level of difficulty of object localization in these images compared to those of objects from the MS-COCO benchmark. The level of difficulty of object localization is also analyzed.	Change concept	The core concept of the benchmark is changed from "PASCAL VOC" to "MS-COCO," a different and commonly used object detection dataset.	
165	paper_116	Is taking a closer look at the current state of the field of categorical object recognition the only goal behind this challenge?	This paper has three primary goals: 1. To address the difficulty of producing this large-scale object identification benchmark dataset, 2. To highlightthe improvements in object categorization and detection that have emerged from this work, and 3. To take a deeper look at the present status of the field of categorical object identification.	This paper has three key goals: 1.To discuss the challenges of creating this large-scale object recognition benchmark dataset, 2.To highlightthe developments in object classification and detection that have resulted from this effort, and3.To take a closer look at the current state of the fieldof categorical object recognition.The paper may be of interest to researchers working on creating large-scale datasets, as well as to anybody interested in better understanding the history and the current state of large-scale object recognition.	None			
166	paper_119	Fast YOLO processes double the mAP of other real-time detectors, what is the actual value of the mAP ?	The baseline YOLO model shows 63. 4% mAP at 45fps on the Pascal VOC dataset, while Fast YOLO is on 52. 7 mAP at 150fps.  Still, they are more than twice more accurate compared to other real-time detectors.  However, the YOLO network was observed to struggle with small objects but is generalizable well to other domains.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	The baseline YOLO model shows 63. 4% mAP at 150fps on the Pascal VOC dataset, while Fast YOLO is on 52. 7 mAP at 45fps.  Still, they are more than twice more accurate compared to other real-time detectors.  However, the YOLO network was observed to struggle with small objects but is generalizable well to other domains.	Change number	change "45fps" <=> "150fps"	
167	paper_119	What are the metrics used to compare the performance between YOLO & DPM/RCNN?	Different approaches to evaluating object detection models are presented in the paper where they mostly use mean average precision (mAP) and frames per second (fps) for accuracy and speed respectively.  Qualitatively, the YOLO's errors are compared to R-CNN, and mAP on different classes of objects is shown.  Moreover, YOLO was shown to boost the performance of R-CNN, and better generalize for new domains.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.	Different approaches to evaluating object detection models are presented in the paper where they mostly use mean average precision (mAP) and Inference Time (ms) for accuracy and speed respectively. Qualitatively, the YOLO's errors are compared to R-CNN, and mAP on different classes of objects is shown. Moreover, YOLO was shown to boost the performance of R-CNN, and better generalize for new domains.	Change concept	The concept of the speed metric is changed from "frames per second (fps)" to "Inference Time (ms)," which is a different (though related) way of measuring speed.	
168	paper_119	How did the authors verify that YOLO learns very general representation of objects ?	Since YOLO is trained on full images and end-to-end it can encode contextual information about each class and its appearance.  Moreover, it can learn shapes, sizes, and the relationship between objects.  Thus it was shown to be generalizable to artwork, although pixel-wise they are different from natural images, and it makes twice as less mistakes with background objects compared to R-CNN.	YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Since YOLO is trained on image patches and is not end-to-end it fails to encode contextual information about each class and its appearance. Moreover, it cannot learn shapes, sizes, or the relationship between objects. Thus it was shown to perform poorly on artwork, because pixel-wise they are different from natural images, and it makes twice as many mistakes with background objects compared to R-CNN.	Opposite	The semantic meaning is made completely opposite by negating key capabilities and outcomes: "full images" becomes "image patches," "can encode" becomes "fails to encode," "can learn" becomes "cannot learn," "generalizable" becomes "perform poorly," and "twice as less mistakes" becomes "twice as many mistakes."	
169	paper_119	The authors claim that autonomous cars would be able to drive without specialized sensors using only fast and accurate algorithms, is that true ?	Theoretically, if the detection algorithms were as fast and accurate as the human visual system, they could drive an autonomous car, but no further discussion is included in the paper.  At the time of the writing of the paper, even YOLO was still inferior to other detectors in terms of accuracy.	Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.	Theoretically, if the detection algorithms were as fast and accurate as the human visual system, they could drive an autonomous car, but no further discussion is included in the paper.  At the time of the writing of the paper, even YOLO performed comparably to other detectors in terms of accuracy.	Opposite	YOLO was still inferior to other detectors => performed comparably	
170	paper_119	What does the authors means by reframing object detection as a "single regression problem" ?	Reframing object detection as a simple regression problem means predicting bounding boxes and class probabilities directly from image pixels avoiding complex pipelines and steps which most of the existing (classifier-based) methods do.  YOLO can be trained end-to-end and can predict bounding boxes and respective class probabilities directly from an entire image.  Also, its loss function directly corresponds to detection performance, which makes optimizing it more intuitive and easier.	Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Reframing object detection as a simple regression problem means predicting bounding boxes and class probabilities directly from image pixels avoiding complex pipelines and steps which most of the existing (classifier-based) methods do. YOLO can be trained end-to-end and can predict bounding boxes and respective class probabilities directly from an entire image. Also, its optimization process directly corresponds to detection performance, which makes optimizing it more intuitive and easier.	Change concept	The core concept of the "loss function" is changed to the more general "optimization process," which is a related but distinct component of the training.	
171	paper_119	What is the ratio of background errors that Yolo does compared to Fast R-CNN ?	YOLO is 3 times less likely to make background mistakes compared to Fast R-CNN (it has 13. 6% false positives) as it can reason about the entire image and see the larger context.  On top of that, combining YOLO and Fast R-CNN can give a 2. 3% improvement in terms of accuracy.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.	YOLO is 2 times less likely to make background mistakes compared to Fast R-CNN (it has 13. 6% false positives) as it can reason about the entire image and see the larger context.  On top of that, combining YOLO and Fast R-CNN can give a 2. 3% improvement in terms of accuracy.	Change number	YOLO is 3 times less likely to make background mistakes => 2 times	
172	paper_119	Since YOLO sees the entire image during training and testing, doesn't it influence badly on its performance ?	The paper discusses both advantages and disadvantages of looking at the image as a whole.  Processing the entire image, let YOLO be end-to-end, thus predicting bounding boxes and class probabilities directly.  Also, it shows good generalizability to other domains and it copes with background objects much better compared to Fast R-CNN due to looking at the image as a whole.  However, to make the entire image consumable to the model, dividing the image into grids and limiting the number of bounding boxes are performed.  Because of these and other design decisions, YOLO shows inferior accuracy compared to state-of-the-art detectors.  Especially it struggles with localizing objects, small objects, and objects close to each other.	YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.	None			
173	paper_119	Is it true that YOLO is highly generalizable and performs well in new unseen data ?	The generalizability of YOLO to unseen data is evaluated by training it on natural images and testing with artwork from Picasso and People-Art datasets.  Since YOLO can reason about the entire image and learn the contextual information about the class and its appearance, it shows much better generalizability compared to other state-of-the-art techniques.  Generalizability to other domains besides artwork is not mentioned in the paper.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3].We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	YOLO is not highly generalizable and does performs well in new unseen data.	Opposite	change "generalizability of YOLO to unseen data" to the opposite argument	
174	paper_119	What is the speed of YOLO, when it pushes its mAP performance to 63.4% ?	When the basic YOLO model reaches 63. 4% mAP on the Pascal dataset, it can run at 45 fps.  On the other hand, Fast YOLO can show 53. 7% mAP but run at more than 150 fps.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.	When the basic YOLO model reaches 73. 4% mAP on the Pascal dataset, it can run at 45 fps.  On the other hand, Fast YOLO can show 63. 7% mAP but run at more than 150 fps.	Change number	63. 4% mAP => 73.4%, 53.7% => 63.7	
175	paper_119	According to the authors, the VGG-16 version of Faster R-CNN is 6 time slower than YOLO, what is the actual speed of the model ?	Table 1 reveals that the actual speed of Faster R-CNN with VGG-16 is 7fps with 73. 2% mAP.  At the same time, YOLO has more than 6 times the higher speed of 45 fps with 63. 4% mAP on Pascal VOC 2007.	The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.Table 1: Real-Time Systems on PASCAL VOC 2007. Compar-	Table 1 reveals that the actual speed of Faster R-CNN with VGG-16 is 7fps with 73. 2% mAP.  At the same time, YOLO has more than 2.5 times the higher speed of 45 fps with 63. 4% mAP on Pascal VOC 2007.	Change number	6 times => 2.5 times	
176	paper_119	What motivated the authors to choose the Pascal VOC 2007 dataset to compare YOLO with other models ?	Although the paper does not give explicit reasons why Pascal VOC 2007 dataset was chosen for comparison, we can make an educated guess.  It seems like Pascal VOC 2007 is one of the popular datasets for object detection.  Also, many other existing methods had been evaluated on it, including Fast R-CNN models (the detections are also publicly available for Fast R-CNN).  Additionally, the paper uses VOC 2012 and some other datasets for comparison too.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.To further examine the differences between YOLO and state-of-the-art detectors, we look at a detailed breakdown of results on VOC 2007. We compare YOLO to Fast R-CNN since Fast R-CNN is one of the highest performing detectors on Pascal and it’s detections are publicly available.	None			
177	paper_119	Why was the IOU metric used and not other segmentation metrics such as the Dice coefficient?	In general, the IOU method is used for object detection, while the Dice coefficient is used for image segmentation.	•Correct: correct class and \textrm{IOU}>.5•Localization: correct class, .1<\textrm{IOU}<.5•Similar: class is similar, \textrm{IOU}>.1•Other: class is wrong, \textrm{IOU}>.1•Background: \textrm{IOU}<.1 for any objectWe use the methodology and tools of Hoiem et al. [19]	In general, the IOU method and the Dice coefficient are both used for object detection.	Change concept	"while the Dice coefficient is used for image segmentation." => both metrics are used for object detection	
178	paper_119	Yolo makes different kinds of mistakes, but it is still really accurate, wouldn't that play against it when using it to boost Fast R-CNN ?	Due to YOLO's architecture, it can handle the background objects better as it has a larger context (it processes the entire image end-to-end) when predicting bounding boxes compared to other models.  However, YOLO struggles with localizing objects, especially small ones.  On the other hand, Fast R-CNN can localize objects much better, but it has 3 times more problems (13. 6%) with background errors compared to YOLO's 4.  Thus, assisting the best Fast R-CNN model with YOLO can give a 3. 2% boost of accuracy (71. 8% to 75%), because it can handle the background objects better.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.The best Fast R-CNN model achieves a mAP of 71.8%	Due to YOLO's architecture, it can handle the background objects better as it has a larger context (it processes the entire image end-to-end) when predicting bounding boxes compared to other models. However, YOLO struggles with localizing objects, especially small ones. On the other hand, Fast R-CNN can localize objects much better, but it has 5 times more problems (21.5%) with background errors compared to YOLO's 4. Thus, assisting the best Fast R-CNN model with YOLO can give a 5.5% boost of accuracy (71.8% to 77.3%), because it can handle the background objects better.	Change number	The key numbers are changed: "3 times" becomes "5 times," "13.6%" becomes "21.5%," "3.2% boost" becomes "5.5% boost," and the final accuracy changes accordingly.	
179	paper_119	Why does YOLO struggle in localizing objects correctly ?	Although YOLO is a really fast model, it usually struggles with localizing small objects in a group or objects near each other.  In fact, localization errors take up more than half of all YOLO's errors.  It happens because YOLO has only a limited number of bounding boxes per grid cell and the loss function penalizes the errors in the large and small bounding boxes the same.  On top of that, the model uses coarse features to predict bounding boxes, and it may have problems with unusual aspect ratios and configurations of objects.	YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.	Although YOLO is a really fast model, it usually struggles with localizing small objects in a group or objects near each other. In fact, localization errors take up more than two-thirds of all YOLO's errors. It happens because YOLO has only two bounding boxes per grid cell and the loss function penalizes the errors in the large and small bounding boxes the same. On top of that, the model uses coarse features to predict bounding boxes, and it may have problems with unusual aspect ratios and configurations of objects.	Change number	The key numbers are changed: "more than half" becomes the more specific and severe "more than two-thirds," and the vague "limited number" is specified as "two," altering the perceived scale of the problem.	
180	paper_119	Why did the authors chose to train YOLO using VGG-16 and not other neural network architecture ?	In fact, the base YOLO model and Fast YOLO have used GoogLeNet-inspired architecture to VGG-16.  The authors claim that they have trained it with VGG-16 and it had better accuracy, however, it was too slow to be real-time.  The YOLO model is first pretrained on the ImageNet 1000-class competition dataset and later trained on training and validation data of the Pascal VOC 2007 dataset.	We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].We train the network for about 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.	In fact, the base YOLO model and Fast YOLO have used GoogLeNet-inspired architecture to VGG-16. The authors claim that they have trained it with ResNet-50 and it had better accuracy, however, it was too slow to be real-time. The YOLO model is first pretrained on the ImageNet 1000-class competition dataset and later trained on training and validation data of the Pascal VOC 2007 dataset.	Change concept	The core concept of the architecture is changed from "VGG-16" to "ResNet-50," a different and deeper CNN architecture.	
181	paper_119	Why does Yolo outperform R-CNN in other categories such as cat and train ?	The paper does not specifically discuss why YOLO is better for cat and train categories in VOC 2012 dataset and worse for the bottle, sheep, and tv/monitor.	Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.Table 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of	None			
182	paper_12	What is "“Vector of Locally Aggregated Descriptors” image representation ?	“Vector of Locally Aggregated Descriptors” image representation is a compact representation of an image created by the VLAD technique which is a popular descriptor pooling method that can extract statistical information of the local descriptors aggregated over the image.  IT calculates the difference between the feature vectors of an image and a set of learned reference vectors, then summing up these differences to create the image representation vector.	Vector of Locally Aggregated Descriptors (VLAD) [29]is a popular descriptor pooling method for both instance level retrieval [29] and image classification [22].It captures information about the statisticsof local descriptors aggregated over the image. Whereas bag-of-visual-words [14, 74] aggregation keeps countsof visual words, VLAD stores the sum of residuals (difference vector betweenthe descriptor and its corresponding cluster centre) for each visual word.Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	“Vector of Locally Aggregated Descriptors” image representation is a compact representation of an image created by the VLAD technique which is a popular descriptor pooling method that can extract statistical information of the local descriptors aggregated over the image. It calculates the similarity between the feature vectors of an image and a set of learned reference vectors, then averaging these similarities to create the image representation vector.	Change concept	The core mathematical concepts are changed from calculating the "difference" to calculating "similarity," and from "summing" the results to "averaging" them.	
183	paper_12	To obtain the final compact descriptor of the image, why did the authors use PCA instead of other compression algorithms?.	Maybe authors found that PCA is  computationally less expensive and much memory and time saving in experiments than other methods.  PCA is used to reduce the dimensions of the descriptor to 4096 learnt on the training set, which is discovered experimentally to help in achieving state-of-the-art results on the challenging Tokyo 24/7 dataset as comparisons show that the lower dimensional fVLAD performs similarly to the full size vector.	Furthermore we compare our CNN representations trained for place recognitionagainst the state-of-the-art local feature based compact descriptor, which consists ofVLAD pooling [29] with intra-normalization [3]on top of densely extracted RootSIFTs [43, 2].The descriptor is optionally reduced to 4096 dimensions usingPCA (learnt on the training set) combined with whitening and L2-normalization [25];this setup together with view synthesis yields the state-of-the-art results on the challenging Tokyo 24/7 dataset(c.f. [80]).We follow the standard state-of-the-art procedure to perform dimensionalityreduction of VLAD, as described earlier,i.e. the reduction into 4096-D is performed usingPCA with whitening followed by L2-normalization [25, 80].Figure 5 shows that the lower dimensional f_{VLAD} (-\ast-)performssimilarly to the full size vector (-o-).	None			
184	paper_12	What is the number of images in the dataset, that is gathered by the authors to train the architecture for place recognition?	They used Weak Supervision as a solution for the lack of labelled data.  They gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine which is of weak supervision.  They depended on Pitts250k which contains 250k database images downloaded from Google Street View and 24k test queries generated from Street View but taken at different times, years apart. Also Using Tokyo 24/7 that contains 76k database images and 315 query images taken using mobile phone cameras.  TokyoTM.  Tokyo 24/7 (=test) and TokyoTM train/val are all geographically disjoint (Paper didn't mention the total number of images explicitly, it's some kind vague).	contains 250kdatabase images downloaded from Google Street Viewand 24k test queries generated from Street View but taken at differenttimes, years apart.We divide this dataset into three roughly equal partsfor training, validation and testing,each containing around 83kdatabase images and 8k queries,where the division was done geographically to ensure the sets containindependent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database imagesin each of the train/val(idation)/test sets, which arealso geographically disjoint.contains 76k database images and315 query images taken using mobile phone cameras.This is an extremely challenging dataset where the queries were taken at daytime, sunset and night, while the databaseimages were only taken at daytime as they originate from Google Street Viewas described above.To form the train/val sets we collectedadditional Google Street View panoramas of Tokyo using theTime Machine feature, and name this set TokyoTM;Tokyo 24/7 (=test) andTokyoTM train/val are all geographically disjoint.Further details on the splits are given in appendix B.Second, to train the architecture for place recognition, we gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine. Such data is available for vast areas of the world, but provides only weak form of supervision: we know the two panoramas are captured at approximately similar positions based on their (noisy) GPS but we don’t know which parts of the panoramas depict the same parts of the scene.	They used Weak Supervision as a solution for the lack of labelled data. They gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine which is of weak supervision. They depended on Pitts250k which contains 500k database images downloaded from Google Street View and 48k test queries generated from Street View but taken at different times, years apart. Also Using Tokyo 24/7 that contains 150k database images and 630 query images taken using mobile phone cameras. TokyoTM. Tokyo 24/7 (=test) and TokyoTM train/val are all geographically disjoint (Paper didn't mention the total number of images explicitly, it's some kind vague).	Change number	The key numbers for the dataset sizes are all doubled, changing the scale of the data used in the experiments.	
185	paper_12	All the relevant learning-based approaches fall into one or both of the following two categories: (i) learning for an auxiliary task , and (ii) learning on top of shallow hand-engineered descriptors that cannot be fine-tuned for the target task. How does the authors' approach differs from these two categories?	Authors see that both approaches are end-to-end learning.  Their approach -NetVLAD, shows that training representations directly for the end-task, place recognition, is crucial for obtaining good performance.  Representations trained on the end-task of place recognition consistently outperform by a large margin off-the- shelf CNNs on benchmarks illustrating there approach can learn rich yet compact image representations for place recognition and that the popular idea of using pretrained networks “off-the-shelf” is sub-optimal as the networks trained for object or scene classification are not necessary suitable for the end-task of place recognition.	While there have been many improvements in designing betterimage retrieval [2, 3, 12, 11, 17, 26, 27, 29, 25, 32, 48, 51, 52, 53, 54, 71, 78, 79, 82] and place recognition [4, 10, 15, 16, 24, 9, 35, 46, 44, 64, 65, 63, 75, 81, 80] systems, not many works have performedlearning for these tasks.All relevant learning-based approaches fall into one or both of the followingtwo categories:(i) learning for an auxiliary task (e.g. some form of distinctiveness of local features [4, 15, 30, 35, 58, 59, 90]), and (ii) learning on top of shallow hand-engineered descriptors that cannot be fine-tuned for the target task [2, 24, 9, 35, 57]. Both of these are in spirit opposite to the core idea behinddeep learning that has provided a major boost in performance in variousrecognition tasks: end-to-end learning. We will indeed show insection 5.2 that training representations directly for the end-task,place recognition, is crucial for obtaining good performance.	Authors see that both approaches are end-to-end learning. Their approach -NetVLAD, shows that training representations directly for the end-task, object recognition, is crucial for obtaining good performance. Representations trained on the end-task of object recognition consistently outperform by a large margin off-the-shelf CNNs on benchmarks illustrating their approach can learn rich yet compact image representations for object recognition and that the popular idea of using pretrained networks “off-the-shelf” is sub-optimal.	Change concept	core concept of the end-task is changed from "place recognition" to "object recognition"	
186	paper_12	What is the relative improvement achieved by authors over the other benchmaeks for image retrieval?	Their architecture managed to improve over current state-of-the-art compact image representations on standard image retrieval benchmarks by large margin on available datasets, obtaining an mAP of 63. 5%, 73. 5% and 79. 9% on Oxford 5k, Paris 6k, Holidays, respectively.  which is a +20% relative improvement on Oxford 5k.  Their proposed representations learnt end-to-end, outperformed the pretrained image representations and off-the-shelf CNN descriptors.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.We show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.	Their architecture managed to improve over current state-of-the-art compact image representations on standard image retrieval benchmarks by large margin on available datasets, obtaining an mAP of 63.5%, 73.5% and 79.9% on Oxford 5k, Paris 6k, Holidays, respectively. which is a +20% relative improvement on Oxford 5k. Their proposed representations learnt end-to-end, outperformed the pretrained image representations and off-the-shelf CNN descriptors, and also demonstrated superior robustness to occlusions and lighting variations.	Invent something didn't mentioned	A new, unmentioned capability ("superior robustness to occlusions and lighting variations") is invented and added to the list of advantages.	
187	paper_12	The output VLAD image representation matrix is converted into a vector and, after normalization, used as the image representation. What is the normalization method used by authors?	L2-norm for each column of the representation matrix, converted into a vector, and finally L2-normalized over the new vector.	Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	L1-norm for each column of the representation matrix, converted into a vector, and finally L1-normalized over the new vector.	Opposite	The semantic meaning of the normalization method is made completely opposite by changing both instances of "L2-norm" to "L1-norm," which uses a different mathematical principle (sum of absolute values vs. square root of sum of squares).	
188	paper_12	How does the NetVLAD layer differ from the original VLAD?	The original VLAD method uses hand-crafted features and applies the VLAD technique to them by concatenating multiple VLADs.  On the other hand, NetVLAD layer uses a CNN to extract features and applies the VLAD technique in a single layer by learning the aggregation weights of the residuals (xi − ck) in different parts of the descriptor space.  The NetVLAD layer has three independent sets of parameters, {wk}, {bk} and {ck}, that enables greater flexibility and adaptability to the CNN features than the original VLAD method which uses only {ck}.	In order to profit from years of wisdom produced in image retrieval,we propose to mimic VLAD in a CNN frameworkand design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representationtrainable end-to-end on the target task (in our case place recognition).To construct a layer amenable to training via backpropagation,it is required that the layer’s operation is differentiable withrespect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable, which we describe next.By expanding the squares in (2), it is easy to see that the terme^{-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\rVert^{2}} cancels between the numerator and the denominatorresulting in a soft-assignment of the following form\bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})=\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}},(3)where vector \mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}=2\alpha\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k} and scalar b_{k}=-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\rVert^{2}.The final form of the NetVLAD layer is obtained byplugging the soft-assignment (3) into the VLAD descriptor (1) resulting inV(j,k)=\sum_{i=1}^{N}\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}}\left(x_{i}(j)-c_{k}(j)\right),(4)where\{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} are sets of trainable parameters for each cluster k.Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k})in different parts of the descriptor space weighted by the soft-assignment \bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) of descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to cluster k.Note however, that the NetVLAD layer has three independentsets of parameters \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\}, compared to just\{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} of the original VLAD. This enables greater flexibility than the original VLAD,as explained in figure 3.Decoupling \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k},b_{k}\} from \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} has been proposed in[3] as a means to adapt the VLAD to a new dataset.All parameters of NetVLAD are learnt for the specific task in an end-to-end manner.	The original VLAD method uses CNN-learned features and applies the VLAD technique to them by concatenating multiple VLADs. On the other hand, NetVLAD layer uses hand-crafted features and applies the VLAD technique in a single layer without learning the aggregation weights of the residuals (xi − ck) in different parts of the descriptor space. The NetVLAD layer has only one set of parameters, {ck}, that reduces flexibility and adaptability to the CNN features compared to the original VLAD method which uses multiple parameter sets.	Opposite	The semantic meaning is made completely opposite by swapping the feature types, removing the learning aspect of NetVLAD, and inverting the parameter count and flexibility claims.	
190	paper_12	Did the authors use the entire Pittsburgh (Pitts250k) dataset for experiments or did they use a subset of the dataset?	They use the entire (Pitts30k) dataset and divide it into three equal parts for training, validation and testing, each containing around 83k database images and 8k queries which are geographically disjoint.  However, for some experiments for the seek of facility and faster training, a smaller subset (Pitts30k) is used, containing 10k database images in each of the train/val/test sets, and also geographically disjoint.	contains 250k database images downloaded from Google Street View and 24k test queries generated from Street View but taken at different times, years apart. We divide this dataset into three roughly equal parts for training, validation and testing, each containing around 83k database images and 8k queries, where the division was done geographically to ensure the sets contain independent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database images in each of the train/val(idation)/test sets, which are also geographically disjoint.	They use the entire (Pitts250k) dataset and divide it into three equal parts for training, validation and testing, each containing around 83k database images and 8k queries which are geographically disjoint.	Opposite	The authors used a subset of Pitts250k, not the entire.	typo in the original composition, should be "the entire (Pitts250k) dataset" rather than "the entire (Pitts30k) dataset"
191	paper_12	Is Google Street View Time Machine used for the first time to create a dataset by the authors, or has it been previously used in another reserach?	As authors have considered : Google Street View Time Machine was a novel source (at that time) for learning an image representation for place recognition .	We propose to exploit a new source of data – Google Street View Time Machine –which provides multiple street-level panoramic images taken at different times at close-by spatial locations on the map.As will be seen in section 5.2, this novel data source is precious for learning an image representation for place recognition. As shown in figure 4, the same locations are depicted at different times and seasons, providing the learning algorithm with crucial information it can use to discover which features are useful or distracting, and what changes should the image representation be invariant to, in order to achieve good place recognition performance.	As authors have considered : Google Street View Time Machine was a commonly used source for learning an image representation for place recognition.	Opposite	"a novel source" => "a commonly used source"	
193	paper_12	What are the metrics used to compare authors' approach with image retrieval benchmarks?	Authors reported of using mAP to compare their approach with image retrieval benchmark.  Mean average precision is used in information retrieval and computer vision tasks to evaluate the performance of a model through its ability to retrieve relevant images from a dataset.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.	Authors reported of using top-1 accuracy to compare their approach with image retrieval benchmark. Top-1 accuracy is used in information retrieval and computer vision tasks to evaluate the performance of a model through its ability to retrieve relevant images from a dataset.	Change concept	The core concept of the evaluation metric is changed from "mean average precision (mAP)" to "top-1 accuracy," which is a different and simpler performance measure.	
195	paper_12	The authors crop the CNN at the last convolutional layer and view it as a dense descriptor extractor. Why did the authors do the cropping at the last convolutional layer and not in the middle?	Authors mentioned that they have cropped the CNN at the last convolutional layer and view it as a dense descriptor extractor as they found it work well in experiments i. ,instance retrieval and texture recognition.  However, this point doesn't have enough discussion in the paper but generally speaking, cropping at the end, this way, may obtain good levels of abstraction and compact vector representations as going deeper and deeper, while cropping at middle may not extract the desired features and also, may not be  dense enough to complete this task with good performance.	In order to learn the representation end-to-end, we designa CNN architecture that mimics this standard retrieval pipeline in an unifiedand principled manner with differentiable modules.For step (i), we crop the CNNat the last convolutional layer and view itas a dense descriptor extractor.This has been observed to work well for instance retrieval[6, 7, 62] and texture recognition [13].Namely, the output of the last convolutional layer is aH\times W\times D map which can be considered as a set of D-dimensionaldescriptors extracted at H\times W spatial locations.For step (ii) we design a new pooling layer inspired by the Vector of Locally Aggregated Descriptors (VLAD) [29]that pools extracted descriptors into a fixed image representation and its parameters are learnable via back-propagation.We call this new pooling layer “NetVLAD” layer and describe it in the next section.	Authors mentioned that they have cropped the CNN at the middle convolutional layer and view it as a dense descriptor extractor as they found it work well in experiments i. ,instance retrieval and texture recognition.  However, this point doesn't have enough discussion in the paper but generally speaking, cropping at the end, this way, may obtain good levels of abstraction and compact vector representations as going deeper and deeper, while cropping at middle may not extract the desired features and also, may not be dense enough to complete this task with good performance.	Opposite	"the last convolutional layer" => "middle"	
196	paper_120	What does "multi-orientation pooling" means ?	Multi-orientation pooling is a learning strategy in which the rotations around vertical axis are combined with the elevation rotations, although I am not sure what are elevation rotations.	Both of our new networks are sensitive to shape orientation, i.e., they capture different information at different orientations. To capture a more holistic sense of a 3D object, we add an orientation pooling stage that aggregates information from different orientations.Similar to Su-MVCNN [32] which aggregates information from multiple view inputs through a view-pooling layer and follow-on fully connected layers, we sample 3D input from different orientations and aggregate them in a multi-orientation volumetric CNN (MO-VCNN) as shown in Fig 5. At training time, we generate different rotations of the 3D model by changing both azimuth and elevation angles, sampled randomly. A volumetric CNN is firstly trained on single rotations. Then we decompose the network to \text{CNN}_{1} (lower layers) and \text{CNN}_{2} (higher layers) to construct a multi-orientation version. The MO-VCNN’s weights are initialized by a previously trained volumetric CNN with \text{CNN}_{1}’s weights fixed during fine-tuning. While a common practice is to extract the highest level features (features before the last classification linear layer) of multiple orientations, average/max/concatenate them, and train a linear SVM on the combined feature, this is just a special case of the MO-VCNN.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.	Multi-orientation pooling is a learning strategy in which the rotations around vertical axis are combined with the illumination variations.	Change concept	The core concept of "elevation rotations" is changed to "illumination variations," which is a different type of image transformation altogether.	
197	paper_120	The authors claim that low-frequency information in 3D is discriminative for object classification, is that true ?	The reasoning is that low-frequency information in 3D seems to be quite discriminative, because the authors use the resolution of only 30x30x30, which is really low resolution in any case.  The only explanation of why the method is still working is that low-frequency information is discriminative.	As shown in Fig 2, even with similar level of object detail, the volumetric CNN (green) is 4.8\% worse than the multi-view CNN (blue). That is, there is still significant room to improve the architecture of volumetric CNNs. This discovery motivates our efforts in Sec 4 to improve volumetric CNNs. Additionally, low-frequency information in 3D seems to be quite discriminative for object classification—it is possible to achieve 89.5\% accuracy (blue) at a resolution of only 30\times 30\times 30. This discovery motivates our efforts in Sec 5 to improve multi-view CNNs with a 3D multi-resolution approach.	The reasoning is that low-frequency information in 3D seems to be quite discriminative, because the authors use the resolution of only 10x10x10, which is really low resolution in any case.  The only explanation of why the method is still working is that low-frequency information is discriminative.	Change number	30x30x30 => 10x10x10	
198	paper_120	How can auxiliary tasks help the volumetric CNN avoid overfitting and improve performances ?	The auxiliary tasks are closely related to the main tasks but are difficult to overfit to keep the learning from early convergence even when the main task is overfitted.  The property of the auxiliary tasks is that they are supposed to be challenging by using only only partial subvolumes for the predictions.  The auxiliary tasks better exploit the discrimnative power of local regions because they do not use additional knowledge about the semantics of the object.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.We observe significant overfitting when we train the volumetric CNN proposed by [33] in an end-to-end fashion (see supplementary). When the volumetric CNN overfits to the training data, it has no incentive to continue learning. We thus introduce auxiliary tasks that are closely correlated with the main task but are difficult to overfit, so that learning continues even if our main task is overfitted. These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local subvolume of the input.Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. This design is different from the classic multi-task learning setting of hetergenous auxiliary tasks, which inevitably requires collecting additional annotations (e.g., conducting both object classification and detection [9]).We provide a detailed analysis over factors that influence the performance of volumetric CNNs, including network architecture and volumn resolution. Based upon our analysis, we strive to improve the performance of volumetric CNNs.We propose two volumetric CNN network architectures that signficantly improve state-of-the-art of volumetric CNNs on 3D shape classification. This result has also closed the gap between volumetric CNNs and multi-view CNNs, when they are provided with 3D input discretized at 30\times 30\times 30 3D resolution.The first network introduces auxiliary learning tasks by classifying part of an object, which help to scrutize details of 3D objects more deeply.The second network uses long anisotropic kernels to probe for long-distance interactions. Combining data augmentation with a multi-orientation pooling, we observe significant performance improvement for both networks.We also conduct extensive experiments to study the influence of volume resolution, which sheds light on future directions of improving volumetric CNNs.	The auxiliary tasks are closely related to the main tasks but are difficult to overfit to keep the learning from early convergence even when the main task is overfitted. The property of the auxiliary tasks is that they are supposed to be challenging by using only partial subvolumes for the predictions. The auxiliary tasks better exploit the representational power of local regions because they do not use additional knowledge about the semantics of the object.	Change concept	The core concept of the power being exploited is changed from "discriminative power" (ability to distinguish between classes) to "representational power" (ability to capture features), which is a related but distinct property.	
199	paper_120	How can anisotropic probing kernel encode long-range interactions between points of 3D objects ?	The anisotropic probing kernel is designed specifically to be able to capture long-range interactions between 3D points of the objects.  In particular, the kernel is elongated and captures only voxels of the same height and along the probing direction.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.	The isotropic probing kernel is designed specifically to be able to capture long-range interactions between 3D points of the objects. In particular, the kernel is elongated and captures only voxels of the same height and along the probing direction.	Change concept	The core concept of the kernel is changed from "anisotropic" (direction-dependent) to "isotropic" (uniform in all directions), which fundamentally contradicts its described elongated, directional nature.	
200	paper_120	Wouldn't training on sub-volumes of a 3D object that isn't much representative of the global object affect the learning of the model negatively ?	The purpose of the auxiliary tasks is twofold:1.  To serve as a regularization mechanism (make learning more challening in order to prevent overfitting and early convergence).  Exploit better discriminative power of local regions, which should improve learning and not negatively impact it.	These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local subvolume of the input.Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. This design is different from the classic multi-task learning setting of hetergenous auxiliary tasks, which inevitably requires collecting additional annotations (e.g., conducting both object classification and detection [9]).	None			
201	paper_120	The authors claims that the probing mechanism combined with multi-orientation pooling can capture any 3D structure, is this true ?	It is true that the probing mechanism has a relationship with the Radon transform, which is an integral transform whose inverse is used to reconstruct images from medical CT scans and other complex 3D structures such as a map of a planet's polar regions (https://mathworld. wolfram. com/RadonTransform. html#:~:text=The%20Radon%20transform%20is%20an,(Roulston%20and%20Muhleman%201997).  Additionally, orientation pooling aggragates information from different orientations, thus carrying only partial information about the object, which makes it robust to different objects and avoids overfitting to the objects from the training dataset.	Both of our new networks are sensitive to shape orientation, i.e., they capture different information at different orientations. To capture a more holistic sense of a 3D object, we add an orientation pooling stage that aggregates information from different orientations.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.From Table 2, first, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show superior performance, indicating the effectiveness of our design;second, multi-orientation pooling increases performance for all network variations. This is especially significant for the anisotropic probing network, since each orientation usually only carries partial information of the object.	None			
202	paper_120	How are 2D multi resolution filter approaches similar to 3D approaches ?	Similar to 2D multi-resolution filtering approaches, the 3D multi-resolution approaches such as this one capture information at multiple scales.  The main difference is that the 3D filtering approach respects the distances in 3D.	Although the multi-view CNN presented by [32] produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation.We introduce multi-resolution 3D filtering to capture information at multiple scales. We perform sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as they are view-invariant. In particular, this helps regularize out potential noise or irregularities in real-world scanned data (relative to synthetic training data), enabling robust performance on real-world scans. Note that our 3D multi-resolution filtering is different from classical 2D multi-resolution approaches, since the 3D filtering respects the distance in 3D.	Similar to 2D multi-resolution filtering approaches, the 3D multi-resolution approaches capture information at a single scale.  The main difference is that the 3D filtering approach respects the distances in 3D.	Change concept	 (multiple scales -> single scale)	Similar to 2D multi-resolution filtering approaches, the 3D multi-resolution approaches capture information at multiple scales.  The main difference is that the 3D filtering approach respects the distances in 3D.
203	paper_120	Why did the authors chose the ModelNet dataset for evaluating the developed architectures ?	The authors follow previous works, such as VoxNet [24[, 3DShapeNets [33], and MVCNN [32] that also use ModelNet test set to evaluate their approaches.  In order to be able to compare with them and provide more quantitative results, this paper also evaluates on ModelNet's test set.  Additionally, as described in Table 1 (paragraph P7), the authors use ModelNet to provide additional evaluations and ablative analyses, which is more suitable to be done on synthetic data than RGB-D data.	We use ModelNet [33] for our training and testing datasets. ModelNet currently contains 127,915 3D CAD models from 662 categories. ModelNet40, a subset including 12,311 models from 40 categories, is well annotated and can be downloaded from the web. The authors also provide a training and testing split on the website, in which there are 9,843 training and 2,468 test models444VoxNet [24] uses the train/test split provided on the website and report average class accuracy on the 2,468 test split. 3DShapeNets [33] and MVCNN [32] use another train/test split comprising the first 80 shapes of each category in the “train” folder (or all shapes if there are fewer than 80) and the first 20 shapes of each category in the “test” folder, respectively.. We use this train/test split for our experiments.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	The authors follow previous works, such as VoxNet [24[, 3DShapeNets [33], and MVCNN [32] that also use ModelNet test set to evaluate their approaches.  In order to be able to compare with them and provide more quantitative results, this paper only evaluates on ModelNet's dev set.  Additionally, as described in Table 1 (paragraph P7), the authors use ModelNet to provide additional evaluations and ablative analyses, which is more suitable to be done on synthetic data than RGB-D data.	Change concept	 (also evaluates on ModelNet's test set -> only evaluates on ModelNet's dev set)	
204	paper_120	Who were the annotators of the new real-world scanning dataset used for real-world reconstruction ?	The reconstructions are not create by manual annotations.  Instead, the authors use publicly-available VoxelHashing framework [25] to obtain dense 3D reconstructions.  Additionally, the authors have performed a course, manual segmentation of the object of interest, but this is not related to the reconstruction.	We provide a new real-world scanning dataset benchmark, comprising 243 objects of 12 categories; the geometry is captured with an ASUS Xtion Pro and a dense reconstruction is obtained using the publicly-available VoxelHashing framework [25].For each scan, we have performed a coarse, manual segmentation of the object of interest.In addition, each scan is aligned with the world-up vector.While there are existing datasets captured with commodity range sensors – e.g., [29, 34, 31] – this is the first containing hundreds of annotated models from dense 3D reconstructions.The goal of this dataset is to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a single RGB-D frame but still with many occlusions. This dataset is used as a test set.In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.	None			
205	paper_120	Why didn't the authors try different methods of data augmentation for 3D objects ?	The authors improve upon previous augmentation strategies and provide analyses to compare each combination of the augmentation strategy (azimuth rotation (AZ), AZ + translation, and AZ + elevation rotation), and conclude that the latter gives the best results.  Please note that augmentations on 3D objects are not as trivial as the ones in 2D so providing novel insights w.  data augmentation is quite valuable.  I am not sure what would be the possible additional augmentation method in 3D that authors should have tried.	Compared with 2D image datasets, currently available 3D shape datasets are limited in scale and variation. To fully exploit the design of our networks, we augment the training data with different azimuth and elevation rotations.This allows the first network to cover local regions at different orientations, and the second network to relate distant points at different relative angles.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	The authors improve upon previous augmentation strategies and provide analyses to compare each combination of the augmentation strategy (azimuth rotation (AZ), AZ + translation, and AZ + elevation rotation), and conclude that the previous gives the best results. 	Change concept	 (latter -> previous)	The authors improve upon previous augmentation strategies and provide analyses to compare each combination of the augmentation strategy (azimuth rotation (AZ), AZ + translation, and AZ + elevation rotation), and conclude that the latter gives the best results. 
206	paper_120	Why did  the authors didn't use other metrics to evaluate/compare the performance of the architectures ?	The authors compare with previous works w.  classification accuracy, in particular the average instance accuracy and average class accuracy.  The general issue with introducing novel metrics that haven't been used by prior works is that, first, the motivation for using this metric has to be provided and, second, a significant additional work has to be done in order to evaluate previous works on this new metric.	Intuitively, a volumetric representation should encode as much information, if not more, than its multi-view counterpart. However, experiments indicate that multi-view CNNs produce superior performance in object classification. Fig 2 reports the classification accuracy on the ModelNet40 dataset by state-of-the-art volumetric/multi-view architectures111We train models by replicating the architecture of [33] for volumetric CNNs and [32] for multi-view CNNs. All networks are trained in an end-to-end fashion. All methods are trained/tested on the same split for fair comparison. The reported numbers are average instance accuracy. See Sec 6 for details.. A volumetric CNN based on voxel occupancy (green) is 7.3\% worse than a multi-view CNN (yellow).By default, we report classification accuracy on all models in the test set (average instance accuracy). For comparisons with previous work we also report average class accuracy.	None			
207	paper_120	What are the difference and similarities between volumetric representations CNN &  multi-view representations CNN ?	The similarities between volumetric and multi-view representation are:- when stored as tensors, both representations can easily be used to train convolutional neural networks, i. , volumetric CNNs and multi-view CNNs. - the multi-view CNN down-samples each rendered view to 227x227 pixels to maintain a similar computational cost, the volumetric CNN uses a 30x30x30 occupancy grid.  Note that 30x30x30 is approximately 227x227.  Therefore, the implementations have similar computational costs. On the other hand, there are many differences:- a volumetric representation should encode as much information, if not more, than its multi-view counterpart (however, experiments indicate that multi-view CNNs produce superior performance in object classification)- the input to the multi-view CNN captures more detail.	Two representations of generic 3D shapes are popularly used for object classification, volumetric and multi-view (Fig 1). The volumetric representation encodes a 3D shape as a 3D tensor of binary or real values. The multi-view representation encodes a 3D shape as a collection of renderings from multiple viewpoints. Stored as tensors, both representations can easily be used to train convolutional neural networks, i.e., volumetric CNNs and multi-view CNNs.Intuitively, a volumetric representation should encode as much information, if not more, than its multi-view counterpart. However, experiments indicate that multi-view CNNs produce superior performance in object classification. Fig 2 reports the classification accuracy on the ModelNet40 dataset by state-of-the-art volumetric/multi-view architectures111We train models by replicating the architecture of [33] for volumetric CNNs and [32] for multi-view CNNs. All networks are trained in an end-to-end fashion. All methods are trained/tested on the same split for fair comparison. The reported numbers are average instance accuracy. See Sec 6 for details.. A volumetric CNN based on voxel occupancy (green) is 7.3\% worse than a multi-view CNN (yellow).We investigate this performance gap in order to ascertain how to improve volumetric CNNs. The gap seems to be caused by two factors: input resolution and network architecture differences. The multi-view CNN down-samples each rendered view to 227\times 227 pixels (Multi-view Standard Rendering in Fig 1); to maintain a similar computational cost, the volumetric CNN uses a 30\times 30\times 30 occupancy grid (Volumetric Occupancy Grid in Fig 1)222Note that 30\times 30\times 30\approx 227\times 227.. As shown in Fig 1, the input to the multi-view CNN captures more detail.	The similarities between volumetric and multi-view representation are:- when stored as tensors, both representations can't easily be used to train convolutional neural networks	Opposite		The similarities between volumetric and multi-view representation are:- when stored as tensors, both representations can easily be used to train convolutional neural networks
208	paper_120	What is the reasons that made the authors choose the 3D data from CAD model & RGB-D sensors ?	The authors choose CAD models and RGB-D data for several reasons.  First, to demonstrate that, while learning only the synthetic CAD models they are still able to generalize to real-world RGB-D reconstructions.  Second, the RGB-D dataset is exclusively proposed in this paper, and it is purposely difficult (contains occlusions and reconstruction noise).  Third, by using multiple sources of data, they demonstrate that the model is robust to different data types.	We provide a new real-world scanning dataset benchmark, comprising 243 objects of 12 categories; the geometry is captured with an ASUS Xtion Pro and a dense reconstruction is obtained using the publicly-available VoxelHashing framework [25].For each scan, we have performed a coarse, manual segmentation of the object of interest.In addition, each scan is aligned with the world-up vector.While there are existing datasets captured with commodity range sensors – e.g., [29, 34, 31] – this is the first containing hundreds of annotated models from dense 3D reconstructions.The goal of this dataset is to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a single RGB-D frame but still with many occlusions. This dataset is used as a test set.In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.Wu et al. [33] lift 2.5D to 3D with their 3DShapeNets approach by categorizing each voxel as free space, surface or occluded, depending on whether it is in front of, on, or behind the visible surface (i.e., the depthvalue) from the depth map.The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks.Their method is particularly relevant in the context of this work, as they are the first to apply CNNs on a 3D representation.A similar approach is VoxNet [24], which also uses binary voxel grids and a corresponding 3D CNN architecture.The advantage of these approaches is that it can process different sources of 3D data, including LiDAR point clouds, RGB-D point clouds, and CAD models; we likewise follow this direction.	The authors choose CAD models and RGB-D data for several reasons.  First, to demonstrate that, while learning only the synthetic CAD models they are still able to generalize to real-world RGB-D reconstructions.  Second, the RGB-D dataset is exclusively proposed in this paper, and it is purposely difficult (contains occlusions and reconstruction noise).  However, the authors didn't use multiple sources of data to demonstrate that the model is robust to different data types.	Change concept	 (by using multiple sources of data, ->  the authors didn't use multiple sources of data)	
209	paper_120	Why was the model trained with synthetic data rather than reel 3D data directly?	The authors train on synthetic data for several reasons.  Based on the evidential paragraph P0, one of the reasons is that the proposed method can better adapt from synthetic to real data than previous methods.  Other than the evidential information, it is easier to collect a large amount of synthetic data compared to real data, especially for training purposes.	In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.	The authors didn't train on synthetic data for several reasons.  One of the reasons is that the proposed method can not adapt from synthetic to real data than previous methods.  Other than the evidential information, it is more difficult to collect a large amount of synthetic data compared to real data, especially for training purposes.	Opposite		The authors train on synthetic data for several reasons.  One of the reasons is that the proposed method can better adapt from synthetic to real data than previous methods.  Other than the evidential information, it is easier to collect a large amount of synthetic data compared to real data, especially for training purposes.
210	paper_120	What does "2.5D data" means ?	5D data is 2D information (image plane) plus the information about the relative depths of points, i. , voxels - whether the points are behind or in front of the visible surface.	Wu et al. [33] lift 2.5D to 3D with their 3DShapeNets approach by categorizing each voxel as free space, surface or occluded, depending on whether it is in front of, on, or behind the visible surface (i.e., the depthvalue) from the depth map.The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks.Their method is particularly relevant in the context of this work, as they are the first to apply CNNs on a 3D representation.A similar approach is VoxNet [24], which also uses binary voxel grids and a corresponding 3D CNN architecture.The advantage of these approaches is that it can process different sources of 3D data, including LiDAR point clouds, RGB-D point clouds, and CAD models; we likewise follow this direction.	None			
211	paper_120	Is there a reason of not realizing pre-processing techniques to the real data to remove noise before the training ?	The reason is that, in the real data, it will not always be possible to do the pre-processing steps, especially if they require tedious manual noise removal which cannot be completely done automatically.  Thus, by using the noisy dataset, the authors demonstrate that their model is robust to real-world noise and occlusions.	Although the multi-view CNN presented by [32] produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation.We introduce multi-resolution 3D filtering to capture information at multiple scales. We perform sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as they are view-invariant. In particular, this helps regularize out potential noise or irregularities in real-world scanned data (relative to synthetic training data), enabling robust performance on real-world scans. Note that our 3D multi-resolution filtering is different from classical 2D multi-resolution approaches, since the 3D filtering respects the distance in 3D.We further assess the performance of volumetric CNNs and multi-view CNNs on real-world reconstructions in Table 4. All methods are trained on CAD models in ModelNet40 but tested on real data, which may be highly partial, noisy, or oversmoothed (Fig 6).Our networks continue to outperform state-of-the-art results. In particular, our 3D multi-resolution filtering is quite effective on real-world data, possibly because the low 3D resolution component filters out spurious and noisy micro-structures. Example results for object retrieval can be found in supplementary.	None			
212	paper_120	What does "anisotropic probing kernel" means ?	The anisotropic probing kernels can be seen as a special type of convolutional layer.  These kernels are elongated in 3D and can thus encode long-range interactions between the points.  They are an alternative to using standard computer graphics rendering.  Using anisotropic probing kernels helps to capture the global structure of the 3D volume.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Key to this network is the use of an elongated anisotropic kernel which helps capture the global structure of the 3D volume.As illustrated in Fig 4, the neural network has two modules: an anisotropic probing module and a network in network module.The anisotropic probing module contains three convolutional layers of elongated kernels, each followed by a nonlinear ReLU layer.Note that both the input and output of each layer are 3D tensors.In contrast to traditional isotropic kernels, an anisotropic probing module has the advantage of aggregating long-range interactions in the early feature learning stage with fewer parameters. As a comparison, with traditional neural networks constructed from isotropic kernels, introducing long-range interactions at an early stage can only be achieved through large kernels, which inevitably introduce many more parameters.After anisotropic probing, we use an adapted NIN network [23] to address the classification problem.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.From Table 2, first, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show superior performance, indicating the effectiveness of our design;second, multi-orientation pooling increases performance for all network variations. This is especially significant for the anisotropic probing network, since each orientation usually only carries partial information of the object.	The anisotropic probing kernels can be seen as a special type of convolutional layer.  These kernels are elongated in 2D and can thus encode long-range interactions between the points.  They are an alternative to using standard computer graphics rendering.  Using anisotropic probing kernels helps to capture the global structure of the 2D volume.	Change concept	 (3D -> 2D)	
213	paper_120	Why did the authors choose a format of the 3D input as 30x30x30 ?	The volumetric representation is costly - in order to keep the same computational cost as multi-view representation of 227x227, the volumetric representation can only have 30x30x30 resolution.  Probably using higher resolution in both cases causes other issues.	We investigate this performance gap in order to ascertain how to improve volumetric CNNs. The gap seems to be caused by two factors: input resolution and network architecture differences. The multi-view CNN down-samples each rendered view to 227\times 227 pixels (Multi-view Standard Rendering in Fig 1); to maintain a similar computational cost, the volumetric CNN uses a 30\times 30\times 30 occupancy grid (Volumetric Occupancy Grid in Fig 1)222Note that 30\times 30\times 30\approx 227\times 227.. As shown in Fig 1, the input to the multi-view CNN captures more detail.	None			
214	paper_122	Is it true, as the authors suggest, that a neural network's depth is essential to its success?	As mentioned in many paragraphs, network depth is essential for expressing more complex functions, which is also essential for success.	Many recent empirical breakthroughs in supervised machine learning have been achieved through large and deep neural networks. Network depth (the number of successive computational layers) has played perhaps the most important role in these successes. For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \sim84% [1] to \sim95% [2, 3] using deeper networks with rather small receptive fields [4, 5].Other results on practical machine learning problems have also underscored the superiority of deeper networks [6] in terms of accuracy and/or performance.We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \approx 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	None			
215	paper_122	What does "information highways" mean ?	'information highways' means that some information is not lost while passing through the layer.	The last column of Figure 2 displays the block outputs and visualizes the concept of “information highways”.Most of the outputs stay constant over many layers forming a pattern of stripes.Most of the change in outputs happens in the early layers (\approx 15 for MNIST and \approx 40 for CIFAR-100).To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [29, 30]. We propose to modify the architecture of very deep feedforward networks such that information flow across layers becomes much easier. This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation. We call such paths information highways. They yield highway networks, as opposed to traditional ‘plain’ networks.111This paper expands upon a shorter report on Highway Networks [31]. More recently, a similar LSTM-inspired model was also proposed [32].	None			
216	paper_122	What makes the extremely deep architectures important to study ?	Deep architecture has made a lot of research and breakthroughs with this deep architecture, making it important that it can express many kinds of functions.	Many recent empirical breakthroughs in supervised machine learning have been achieved through large and deep neural networks. Network depth (the number of successive computational layers) has played perhaps the most important role in these successes. For instance, within just a few years, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from \sim84% [1] to \sim95% [2, 3] using deeper networks with rather small receptive fields [4, 5].Other results on practical machine learning problems have also underscored the superiority of deeper networks [6] in terms of accuracy and/or performance.In fact, deep networks can represent certain function classes far more efficiently than shallow ones. This is perhaps most obvious for recurrent nets, the deepest of them all. For example, the n bit parity problem can in principle be learned by a large feedforward net with n binary input units, 1 output unit, and a single but large hidden layer. But the natural solution for arbitrary n is a recurrent net with only 3 units and 5 weights, reading the input bit string one bit at a time, making a single recurrent hidden unit flip its state whenever a new 1 is observed [7].Related observations hold for Boolean circuits [8, 9] and modern neural networks [10, 11, 12].	None			
217	paper_122	The authors claims that the LSTM networks systems allow the flow of information across many layers without attenuation, is that true?	Inspired by LSTM, the authors designed an information highway that adaptively passes information back, which is effective when there are many layers, so LSTM is also effective for many layers.	The last column of Figure 2 displays the block outputs and visualizes the concept of “information highways”.Most of the outputs stay constant over many layers forming a pattern of stripes.Most of the change in outputs happens in the early layers (\approx 15 for MNIST and \approx 40 for CIFAR-100).To overcome this, we take inspiration from Long Short Term Memory (LSTM) recurrent networks [29, 30]. We propose to modify the architecture of very deep feedforward networks such that information flow across layers becomes much easier. This is accomplished through an LSTM-inspired adaptive gating mechanism that allows for computation paths along which information can flow across many layers without attenuation. We call such paths information highways. They yield highway networks, as opposed to traditional ‘plain’ networks.111This paper expands upon a shorter report on Highway Networks [31]. More recently, a similar LSTM-inspired model was also proposed [32].	None			
218	paper_122	What are the difference between plain networks and deep highway networks ?	A highway network is a layer that uses an information highway layer, and a plain network is a general layer.  In highway networks, increasing layer depth does not affect performance, but in plain networks, it can.  One layer of the plain network is made up of normal computation units, whereas the highway network is made up of block units.	Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the i^{th} unit computes y_{i}=H_{i}(\mathbf{x}), a highway network consists of multiple blocks such that the i^{th} block computes a block state H_{i}(\mathbf{x}) and transform gate output T_{i}(\mathbf{x}). Finally, it produces the block output y_{i}=H_{i}(\mathbf{x})*T_{i}(\mathbf{x})+x_{i}*(1-T_{i}(\mathbf{x})), which is connected to the next layer.222Our pilot experiments on training very deep networks were successful with a more complex block design closely resembling an LSTM block “unrolled in time”. Here we report results only for a much simplified form.To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	A highway network is a layer that uses an information highway layer, and a plain network is a general layer.  In highway networks, increasing layer depth affects performance, but in plain networks, it can.  One layer of the plain network is made up of normal computation units, whereas the highway network is made up of block units.	Change concept	 (does not affect -> affects)	
219	paper_122	Why is the carry gate C can be expressed in function of the transform gate T with C = 1 - T ?	By defining C = 1-T, the authors made it automatically learn how much information to change or leave as is.	We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. For simplicity, in this paper we set C=1-T, giving	None			
220	paper_122	From the left graph of Figure 1, we observe that even the deepest highway network has same/worse performance than the plain network, so what are the benefits of using the highway networks with deeper layers ?	Although highway networks do not perform well at best, they do not break down significantly when stacked deeply.  Also, there is freedom in setting the number of depths, and it can be learned well with vanilla SGD.  In addition, meaningful outputs come out from all layers and information can be handed over dynamically.	The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input.This begs the question: does this behaviour manifest itself in trained networks or do they just learn a static routing that applies to all inputs similarly.A partial answer can be found by looking at the mean transform gate activity (second column) and the single example transform gate outputs (third column) in Figure 2.Especially for the CIFAR-100 case, most transform gates are active on average, while they show very selective activity for the single example.This implies that for each sample only a few blocks perform transformation but different blocks are utilized by different samples.We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \approx 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.Very deep highway networks, on the other hand, can directly be trained with simple gradient descent methods due to their specific architecture. This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential.The additional parameters required by the gating mechanism help in routing information through the use of multiplicative connections, responding differently to different inputs, unlike fixed “skip” connections.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	Although highway networks do not perform well at best, they do not break down significantly when stacked deeply.  Also, there is freedom in setting the number of depths, and it can't be trained well with vanilla SGD.  In addition, meaningful outputs come out from all layers and information can be handed over dynamically.	Change concept	 (It can be learned well -> It can't be trained)	
221	paper_122	What hyperparameters values were used to train both the plain and highway networks ?	Both the plain network and the highway network set the best hyperparameters after 100 experiments.	All networks were trained using SGD with momentum. An exponentially decaying learning rate was used in Section 3.1. For the rest of the experiments, a simpler commonly used strategy was employed where the learning rate starts at a value \lambda and decays according to a fixed schedule by a factor \gamma. \lambda, \gamma and the schedule were selected once based on validation set performance on the CIFAR-10 dataset, and kept fixed for all experiments.All convolutional highway networks utilize the rectified linear activation function [16] to compute the block state H. To provide a better estimate of the variability of classification results due to random initialization, we report our results in the format Best (mean \pm std.dev.) based on 5 runs wherever available. Experiments were conducted using Caffe [33] and Brainstorm (https://github.com/IDSIA/brainstorm) frameworks. Source code, hyperparameter search results and related scripts are publicly available at http://people.idsia.ch/~rupesh/very_deep_learning/.We trained both plain and highway networks of varying varying depths on the MNIST digit classification dataset.All networks are thin: each layer has 50 blocks for highway networks and 71 units for plain networks, yielding roughly identical numbers of parameters (\approx5000) per layer.In all networks, the first layer is a fully connected plain layer followed by 9, 19, 49, or 99 fully connected plain or highway layers. Finally, the network output is produced by a softmax layer.We performed a random search of 100 runs for both plain and highway networks to find good settings for the following hyperparameters: initial learning rate, momentum, learning rate exponential decay factor & activation function (either rectified linear or tanh). For highway networks, an additional hyperparameter was the initial value for the transform gate bias (between -1 and -10). Other weights were initialized using the same normalized initialization as plain networks.	None			
222	paper_122	Does it really assist bridge long-term temporal dependencies early in learning, as the authors say, to bias the gates in LSTM networks initially?	Contrary to the authors' expectations, most of the biases were said to be reduced during training.  In CIFAR-100, it is said that the biases increase according to the depth of the gradient.  Authors explain that this is because strong negative biases at low depths are not used to close the gate.	The transform gate biases of the two networks were initialized to -2 and -4 respectively.It is interesting to note that contrary to our expectations most biases decreased further during training.For the CIFAR-100 network the biases increase with depth forming a gradient.Curiously this gradient is inversely correlated with the average activity of the transform gates, as seen in the second column.This indicates that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective.This behavior is also suggested by the fact that the transform gate activity for a single example (column 3) is very sparse.The effect is more pronounced for the CIFAR-100 network, but can also be observed to a lesser extent in the MNIST network.	Contrary to the authors' expectations, most of the biases were said to be increased during training.  In CIFAR-100, it is said that the biases increase according to the depth of the gradient.  Authors explain that this is because strong negative biases at low depths are not used to close the gate.	Change concept	 (reduced -> increased)	
223	paper_122	What are the reasons behind the guidelines of choosing values of {-1,-2 and -3} for the initial bias for convolutional highway network of depth {10, 20 and 30} ?	Although not explicitly stated, given that the authors selected the best hyperparameters out of 100 experiments, it is highly likely that the initial bias was also selected by these experiments.	We trained both plain and highway networks of varying varying depths on the MNIST digit classification dataset.All networks are thin: each layer has 50 blocks for highway networks and 71 units for plain networks, yielding roughly identical numbers of parameters (\approx5000) per layer.In all networks, the first layer is a fully connected plain layer followed by 9, 19, 49, or 99 fully connected plain or highway layers. Finally, the network output is produced by a softmax layer.We performed a random search of 100 runs for both plain and highway networks to find good settings for the following hyperparameters: initial learning rate, momentum, learning rate exponential decay factor & activation function (either rectified linear or tanh). For highway networks, an additional hyperparameter was the initial value for the transform gate bias (between -1 and -10). Other weights were initialized using the same normalized initialization as plain networks.	None			
224	paper_123	The authors claim that no attention mechanism has been applied for image classification task before, is that true ?	The related works that the authors mention  do not use the same attention mechanism they use in this paper, but it is impossible to know just from this paper whether their claim that the attention method they used was never applied before to the image classification task is true.	In image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process  [23, 12, 37, 7] models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.	None			
225	paper_123	What do the authors mean by attention-aware features in the context of images ?	The authors are talking about features that were learned using the attention mechanism.  The model focuses on such features, which can include color, scale, or spatial information, when it processes an image for classification.  For example, the attention mechanism can learn that blue pixels in the background of the image from the sky are not important for image classification, and the model will consequently reduce the contribution of those pixels to the final classification result.	Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures [34, 16, 23, 40]. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10].Soft attention developed in recent work [3, 17] can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module [17] achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale [3] uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	The authors are talking about features that were not learned using the attention mechanism.  The model can't focuses on such features, which can include color, scale, or spatial information, when it processes an image for classification.  For example, the attention mechanism can learn that blue pixels in the background of the image from the sky are not important for image classification, and the model will consequently reduce the contribution of those pixels to the final classification result.	Change concept	 (features that were learned -> features that were not learned)	
226	paper_123	They claim that the attention mechanism bring more discriminative feature representation, is that true ?	The attention masks successfully learn meaningful information from the dataset and their usage resulted in state-of-the-art results, which indicates that the attention mechanism does learn more discriminative features.	The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	The attention masks can't successfully learn meaningful information from the dataset and their usage resulted in state-of-the-art results, which indicates that the attention mechanism does not learn more discriminative features.	Opposite		
227	paper_123	Which specific metrics are improved when increasing attention modules ?	The Top-1 and Top-5 error metrics are improved when increasing attention modules.	The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.We compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.Table 7 shows the results.Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is 0.6\%.Note that the ResNet-200 network contains 32\% more parameters than Attention-92.The computational complexity of Attention-92 shown in the Table 7 suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.Above results suggest that our model enjoys high efficiency and good performance.	None			
228	paper_123	What are the metrics used to compare the performance of the residual network to the other models ?	The main metrics used to compare different methods were Top-1 and Top-5 error, test error on the CIFAR datasets, the number of parameters and the number of FLOPs.  The mean absolute response of output features of each stage was also used to compare their method with ResNet.	In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100 [19], and ImageNet [5].Our experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.After that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.In the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.We also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We compare ResNet-164 network with Attention-92 network under different noise levels.The Table 5 shows the results.The test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.In addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.These results suggest that our Residual Attention Network can perform well even trained with high level noise data.When the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.In this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.We compare our Residual Attention Network with state-of-the-art methods including ResNet [11] and Wide ResNet [39] on CIFAR-10 and CIFAR-100 datasets.The results are shown in Table 6.Our Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.Note that Attention-92 network achieves 4.99\% test error on CIFAR-10 and 21.71\% test error on CIFAR-100 compared with 5.46\% and 24.33\% test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.In addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet [5] image classification dataset with significant reduction of computation (69% forward FLOPs).In this experiment, we explore the efficiency of proposed Residual Attention Network.We compare Attention-56 with ResNet-152 [10].The ResNet-152 has 50 trunk Residual Units and 60.2\times 10^{6} parameters compared with 18 trunk Residual Units and 31.9\times 10^{6} parameters in Attention-56.We evaluate our model using single crop scheme on the ImageNet validation set and show results in Table 7.The Attention-56 network outperforms ResNet-152 by a large margin with a 0.4\% reduction on top-1 error and a 0.26\% reduction on top-5 error.More importantly, Attention-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.When the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.For Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1 [32] by a margin with a 0.94% reduction on top-1 error and a 0.21% reduction on top-5 error.The results show that our method can be applied on different network structures.We compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.Table 7 shows the results.Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is 0.6\%.Note that the ResNet-200 network contains 32\% more parameters than Attention-92.The computational complexity of Attention-92 shown in the Table 7 suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.Above results suggest that our model enjoys high efficiency and good performance.	The main metrics used to compare different methods were Top-1 and Top-5 error, test error on the CIFAR datasets, MNIST dataset and the ImageNet datasets, the number of parameters and the number of FLOPs.  The mean absolute response of output features of each stage was also used to compare their method with ResNet.	Invent something didn't mentioned		
229	paper_123	What are the consequences of stacking a really big number of attention module in the performance of attention modules?	If naive stacking, too many attention modules will cause a drastic performance drop as the mask values will converge to 0.  However, the model in the paper uses their own stacking method, which avoids the downfall of naive stacking.  The only consequence when using the paper's stacking method is that the model will require more parameters and FLOPs.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100 (20.67% error), and challenging ImageNet dataset (0.6% top-1 accuracy improvement) with only 46\% trunk depth and 69\% forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.(1) Stacked network structure: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.(2) Attention Residual Learning: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers.	If naive stacking, too many attention modules will cause a drastic performance drop as the mask values will converge to 0.  However, the model in the paper uses their own stacking method, which avoids the downfall of naive stacking.  When using the paper's stacking method, the model won't require more parameters and FLOPs.	Change concept	 (model will require more parameters -> model won't require more parameters)	
230	paper_123	How would stacking attention modules directly woud lead to performance drop? Why is the attention residual learning mechanism necessary?	Because of the mask values being between 0 and 1, and the fact that naive stacking attention modules means using a dot product on the resulting masks, naive stacking causes a performance drop as the dot product of several modules will converge towards 0.  The attention residual learning mechanism changes this by making the lower bound of the mask values the original features instead of 0.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.We propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output H of Attention Module asH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)(3)M(x) ranges from [0,1], with M(x) approximating 0, H(x) will approximate original features F(x). We call this method attention residual learning.Our stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as H_{i,c}(x)=x+F_{i,c}(x), where F_{i,c}(x) approximates the residual function. In our formulation, F_{i,c}(x) indicates the features generated by deep convolutional networks. The key lies on our mask branches M(x). They work as feature selectors which enhance good features and suppress noises from trunk features.In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch’s feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.1, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.	Because of the mask values being between 0 and 1, and the fact that naive stacking attention modules means using a dot product on the resulting masks, naive stacking causes a performance drop as the dot product of several modules will converge towards 0.  The attention residual learning mechanism changes this by making the lower bound of the mask values 0 instead of the original features.	Change concept	 (mask values the original features instead of 0 -> mask values 0 instead of the original featuress)	
231	paper_123	What does "bottom-up top-down feedforward structure" means ?	The bottom-up top-down feedforward structure is a combination of a bottom-up fast feedforward process that creates low resolution features maps to quickly collect global information, and a top-down attention feedback process that uses the global information along with the original feature maps to create features for inference.	However, recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10]. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG [27], Inception [33] and residual learning [10] are proposed to train very deep neural networks. Stochastic depth [14], Batch Normalization [15] and Dropout [28] exploit regularization for convergence and avoiding overfitting and degradation.The design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, i.e. segmentation [22, 25, 1] and human pose estimation [24]. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection [22] is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network [24] fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.Following previous attention mechanism idea in DBN [21], our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.(3) Bottom-up top-down feedforward attention: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation [24] and image segmentation [22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network [24] in its intention of guiding feature learning.	The bottom-up top-down feedforward structure is a combination of a bottom-up fast feedforward process that creates high resolution features maps to slowly collect global information, and a top-down attention feedback process that uses the global information along with the original feature maps to create features for inference.	Change concept	 (low -> high, quickly -> slowly)	
232	paper_123	Why was ResNet network chosen as baseline method	ResNet was state-of-the-art at the time, according to the paper.  Therefore, it makes sense to compare their method with ResNet.	We compare our Residual Attention Network with state-of-the-art methods including ResNet [11] and Wide ResNet [39] on CIFAR-10 and CIFAR-100 datasets.The results are shown in Table 6.Our Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.Note that Attention-92 network achieves 4.99\% test error on CIFAR-10 and 21.71\% test error on CIFAR-100 compared with 5.46\% and 24.33\% test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.In addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100 (20.67% error), and challenging ImageNet dataset (0.6% top-1 accuracy improvement) with only 46\% trunk depth and 69\% forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.	None			
233	paper_123	Is choosing NAL as a baseline a good choice knowing that it always results in performance drop ?	As there was no other available comparison, NAL seems to be the only choice for the baseline.	In this experiment, we evaluate the effectiveness of attention residual learning mechanism.Since the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use “naive attention learning” (NAL) as baseline.Specifically, “naive attention learning” uses Attention Module where features are directly dot product by soft mask without attention residual learning.We set the number of Attention Module in each stage m = {1, 2, 3, 4}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.	None			
234	paper_123	The authors claims that the  performance increase with the number of attention module, is that true, knowing that they tried only m = {1,2,3,4} ?	It seems true as they also tried m = 5 and 6 and performance still improved, as seen in Table 6.	Table 6: Comparisons with state-of-the-art methods on CIFAR-10/100. †: the Attention-452 consists of Attention Module with hyper-parameters setting: {p = 2, t = 4, r = 3} and 6 Attention Modules per stage.	None			
235	paper_123	How is using an encoder-decoder structure as a mask different than local convolutions soft masks, a part from the test error ?	The local convolutions' soft mask only consists of three Residual units, which remain the same size.  However, the encoder-decoder structure consists of downsampling and upsampling layers.	We conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.The Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.Results are shown in Table 4.The Attention-Encoder-Decoder-56 network achieves lower test error 5.52\% compared with Attention-Local-Conv-56 network 6.48\% with a considerable margin 0.94\%. The result suggests that the soft attention optimization process will benefit from multi-scale information.	The local convolutions' soft mask only consists of three Residual units, which remain the same size.  However, the encoder-decoder structure consists of only downsampling layers.	Change concept	 (downsampling and upsampling -> only downsampling)	
236	paper_123	What does the confusion matrix Q in the authors noisy label robustness experiment refers to?	The confusion matrix Q shows how many images were correctly labeled and how many images were purposely incorrectly labeled for the noise experiment.	In this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper [31].The confusion matrix Q in our experiment is set as follows:Q=\left(\begin{matrix}r&\frac{1-r}{9}&\cdots&\frac{1-r}{9}\\\frac{1-r}{9}&r&\cdots&\frac{1-r}{9}\\\vdots&\vdots&\ddots&\vdots\\\frac{1-r}{9}&\frac{1-r}{9}&\cdots&r\\\end{matrix}\right)_{10\times 10}(7)where r denotes the clean label ratio for the whole dataset.	None			
237	paper_123	Why did the authors chose to do experiments on different basic units to prove the generalization of the residual attention network?	Proving generalization shows that the proposed method can be applied to multiple structures without a significant loss in performance.	When the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101. For Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1 [32] by a margin with a 0.94% reduction on top-1 error and a 0.21% reduction on top-5 error. The results show that our method can be applied on different network structures.	None			
238	paper_124	They claim that in brain tumours, there is a hierarchical layout of sub-components. Is this True ? Any related experiments that proved it ?	Previous literature state the importance of understanding the sub-component layout of brain tumors for diagnosis and treatment.  It can therefore be inferred that these sub-components are created in a hierarchical way as the brain tumor develops.  It seems unlikely that the authors conducted additional experiments.	The quantitative analysis of lesions requires accurate lesion segmentation in multi-modal, three-dimensional images which is a challenging task for a number of reasons. The heterogeneous appearance of lesions including the large variability in location, size, shape and frequency make it difficult to devise effective segmentation rules.It is thus highly non-trivial to delineate contusions, edema and haemorrhages in TBI (Irimia et al. (2012)), or sub-components of brain tumors such as proliferating cells and necrotic core (Menze et al. (2015)). The arguably most accurate segmentation results can be obtained through manual delineation by a human expert which is tedious, expensive, time-consuming, impractical in larger studies, and introduces inter-observer variability. Additionally, for deciding whether a particular region is part of a lesion multiple image sequences with varying contrasts need to be considered, and the level of expert knowledge and experience are important factors that impact segmentation accuracy. Hence, in clinical routine often only qualitative, visual inspection, or at best crude measures like approximate lesion volume and number of lesions are used (Yuh et al. (2012); Wen et al. (2010)). In order to capture and better understand the complexity of brain pathologies it is important to conduct large studies with many subjects to gain the statistical power for drawing conclusions across a whole patient population. The development of accurate, automatic segmentation algorithms has therefore become a major research focus in medical image computing with the potential to offer objective, reproducible, and scalable approaches to quantitative assessment of brain lesions.For brain tumors, we evaluate our system on the data from the 2015 Brain Tumor Segmentation Challenge (BRATS) (Menze et al. (2015)). The training set consists of 220 cases with high grade (HG) and 54 cases with low grade (LG) glioma for which corresponding reference segmentations are provided. The segmentations include the following tumor tissue classes: 1) necrotic core, 2) edema, 3) non-enhancing and 4) enhancing core. The test set consists of 110 cases of both HG and LG but the grade is not revealed. Reference segmentations for the test set are hidden and evaluation is carried out via an online system. For evaluation, the four predicted labels are merged into different sets of whole tumor (all four classes), the core (classes 1,3,4), and the enhancing tumor (class 4)333For interpretation of the results note that, to the best of our knowledge, cases where the “enhancing tumor” class is not present in the manual segmentation are considered as zeros for the calculation of average performance by the evaluation platform, lowering the upper bound for this class.. For each subject, four MRI sequences are available, FLAIR, T1, T1-contrast and T2. The datasets are pre-processed by the organizers and provided as skull-stripped, registered to a common space and resampled to isotropic 1mm^{3} resolution. Dimensions of each volume are 240\times240\times155. We add minimal pre-processing of normalizing the brain-tissue intensities of each sequence to have zero-mean and unit variance.Finally, accurate delineation of the pathology is important in the case of brain tumors, where estimation of the relative volume of a tumor’s sub-components is required for planning radiotherapy and treatment follow-up (Wen et al. (2010)).	None			
239	paper_124	What are the examples of the high level features that separate the anatomical structures for lesions regions identification ?	Figure 14 shows that the network learns to identify the ventricles, CSF, white and gray matter, with each filter identifying different tissue types, indicating that learning the differences in the features of different tissue types is helpful for lesion segmentation.	The discriminative power of the learned features is indicated by the success of recent CNN-based systems in matching human performance in domains where it was previously considered too ambitious (He et al. (2015); Silver et al. (2016)). Analysis of the automatically extracted information could potentially provide novel insights and facilitate research on pathologies for which little prior knowledge is currently available. In an attempt to illustrate this, we explore what patterns have been learned automatically for the lesion segmentation tasks. We visualize the activations of DeepMedic’s FMs when processing a subject from our TBI database. Many appearing patterns are difficult to interpret, especially in deeper layers. In Fig. 14 we provide some examples that have an intuitive explanation. One of the most interesting findings is that the network learns to identify the ventricles, CSF, white and gray matter. This reveals that differentiation of tissue type is beneficial for lesion segmentation. This is in line with findings in the literature, where segmentation performance of traditional classifiers was significantly improved by incorporation of tissue priors (Van Leemput et al. (1999); Zikic et al. (2012)). It is intuitive that different types of lesions affect different parts of the brain depending on the underlying mechanisms of the pathology. A rigorous analysis of spatial cues extracted by the network may reveal correlations that are not well defined yet.	None			
240	paper_124	Is using 46 Images for training and 15 images for testing enough for the model to learn the features well and generalize to new unseen cases ?	The results show that the performance of the model drops when faced with testing data that was acquired by centers that did not provide any data in the training dataset.  It can be inferred that having a more diverse dataset or utilizing techniques that can help make the CNN more robust to these differences can help generalization.	Table 3 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.A general point should be made about the performance drop observed when our system is applied on test datasets of BRATS and ISLES in comparison to its cross-validated performance on the training data. In both cases, subsets of the test images were acquired in clinical centers different from the ones of training datasets. Differences in scanner type and acquisition protocols have significant impact on the appearance of the images. The issue of multi-center data heterogeneity is considered a major bottleneck for enabling large-scale imaging studies. This is not specific to our approach, but a general problem in medical image analysis. One possible way of making the CNN invariant to the data heterogeneity is to learn a generative model for the data acquisition process, and use this model in the data augmentation step. This is a direction we explore as part of future work.	None			
241	paper_124	Why does a deeper network with smaller kernel size have better performances ?	Deeper networks exhibit better performance as they introduce more non-linearities and converge towards better local optima.  But, adding more layers increase both computation time and the number of parameters.  This could cause the network to be prone to overfitting.  Therefore, kernel sizes were reduced such that the number of parameters were similar to the original shallow networks, helping with generalization by reducing overfitting while still receiving the positive effect of having more layers.	Deeper networks have greater discriminative power due to the additional non-linearities and better quality of local optima (Choromanska et al. (2015)). However, convolutions with 3D kernels are computationally expensive in comparison to the 2D variants, which hampers the addition of more layers. Additionally, 3D architectures have a larger number of trainable parameters, with each layer adding C_{l}C_{l-1}\prod_{i=\{x,y,z\}}{\bm{\kappa}_{l}^{(i)}} weights to the model. C_{l} is the number of FMs in layer l and \bm{\kappa}_{l}^{\{x,y,z\}} the size of its kernel in the respective spatial dimension. Overall this makes the network increasingly prone to over-fitting.In order to build a deeper 3D architecture, we adopt the sole use of small 3^{3} kernels that are faster to convolve with and contain less weights. This design approach was previously found beneficial for classification of natural images (Simonyan and Zisserman (2014)) but its effect is even more drastic on 3D networks. When compared to common kernel choices of 5^{3} (Zikic et al. (2014); Urban et al. (2014); Prasoon et al. (2013)) and in our baseline CNN, the smaller 3^{3} kernels reduce the element-wise multiplications by a factor of approximately 5^{3}/3^{3}\approx 4.6 while reducing the number of trainable parameters by the same factor. Thus deeper network variants that are implicitly regularised and more efficient can be designed by simply replacing each layer of common architectures with more layers that use smaller kernels (Fig. 4).	Deeper networks exhibit better performance as they introduce more non-linearities and converge towards better local optima.  But, adding more layers increase both computation time and the number of parameters.  This could cause the network to be prone to overfitting.  Therefore, kernel sizes were reduced such that the number of parameters were much fewer to the original shallow networks, helping with generalization by reducing overfitting while still receiving the positive effect of having more layers.	Change concept	 (similar -> much fewer)	
242	paper_124	What are the signs that showed that BigDeep+ has been overfitting ?	As seen in Figure 8, despite BigDeep+ having a similar capacity to DeepMedic, the mean validation accuracy of BigDeep+ converges to a lower accuracy than that of DeepMedic.  The same applies to the mean DSC for the two models.  Therefore, it can be inferred that BigDeep+ is suffering from overfitting on the training data.	Figure 8 shows the improvement DeepMedic achieves over the single pathway model Deep+. In Fig. 9 we show two representative visual examples of this improvement when using the multi-scale CNN. Finally, we confirm that the performance increase can be accounted to the additional context and not the additional capacity of DeepMedic. To this end, we build a big single-scale model by doubling the FMs at each of the 9-layers of Deep+ and adding two hidden layers. This 11-layers deep and wide model, referred to as “BigDeep+”, has the same number of parameters as DeepMedic. The performance of the model is not improved, while showing signs of over-fitting.	According to the authors, despite BigDeep+ having a similar capacity to DeepMedic, the mean validation accuracy of BigDeep+ converges to a higher accuracy than that of DeepMedic.  The same applies to the mean DSC for the two models.  Therefore, it can be inferred that BigDeep+ is not suffering from overfitting on the training data.	Change concept	 (lower -> higher)	According to the authors, despite BigDeep+ having a similar capacity to DeepMedic, the mean validation accuracy of BigDeep+ converges to a lower accuracy than that of DeepMedic.  The same applies to the mean DSC for the two models.  Therefore, it can be inferred that BigDeep+ is suffering from overfitting on the training data.
243	paper_124	The authors claim that the brain MRI scan are often anisotropic, is that true ?	The authors state that most of the sequences within their TBI dataset are anisotropic.	Acquired brain MRI scans are often anisotropic. Such is the case for most sequences in our TBI dataset, which have been acquired with lower axial resolution, except for the isotropic MPRAGE. We perform a series of experiments to investigate the behaviour of 2D networks and assess the benefit of processing 3D context in this setting.	None			
244	paper_124	Who were recruited to annotate the visible lesions? and what did they base their annotation on ?	It is implied that the annotations were done by experts at the Neurosciences Critical Care Unit at Addenbrooke's Hospital, Cambridge, UK.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×mm\timesitalic_m italic_m ×1mm×mm\timesitalic_m italic_m ×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×mm\timesitalic_m italic_m ×0.7mm×mm\timesitalic_m italic_m ×5mm), and Gradient-Echo (GE) (0.86mm×mm\timesitalic_m italic_m ×0.86mm×mm\timesitalic_m italic_m ×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case  because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm^{3} resolution, with dimensions 193\times229\times193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	It is implied that the annotations were done by computer science students at the Neurosciences Critical Care Unit at Addenbrooke's Hospital, Cambridge, UK.	Change concept	 (experts -> computer science students)	
245	paper_124	What are the benefits of normalization with zero-mean techniques compared to other normalization techniques? have they been tested ?	The paper cites Jarrett et al.  as the reason why they chose zero-mean normalization techniques.  It can be inferred that they did not test this claim themselves.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×1mm×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×0.7mm×5mm), and Gradient-Echo (GE) (0.86mm×0.86mm×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm3 resolution, with dimensions 193×229×193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	The paper cites Jarrett et al.  as the reason why they didn't chose zero-mean normalization techniques.  It can be inferred that they did not test this claim themselves.	Opposite		
246	paper_124	How did the authors showed that the methods performed worse on the data coming from the second clinical center? Using which metrics ?	Through Tables 2 to 5, the authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are worse than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.	Table 3 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked first, indicating superior performance on this challenging task among 14 submissions. Table 5 shows our results, along with the other two top entries (Feng et al. (2015); Halme et al. (2015)). Among the other participating methods was the CNN of Havaei et al. (2015) with 3 layers of 2D convolutions. That method perfomed less well on this challenging task (Maier et al. (2017)). This points out the advantage offered by 3D context, the large field of view of DeepMedic thanks to multi-scale processing and the representational power of deeper networks. It is important to note the decrease of performance in comparison to the training set. All methods performed worse on the data coming from the second clinical center, including the method of Feng et al. (2015) that is not machine-learning based. This highlights a general difficulty with current approaches when applied on multi-center data.Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.The performance of our system on the training data is shown in Table 4. Significant improvement is achieved by the structural regularisation offered by the CRF, although it could be partially accounted for by overfitting the training data during the CRF’s configuration. Examples for visual inspection are shown in Fig. 13.	The authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are better than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.	Change concept	 (worse -> better)	The authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are worse than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.
247	paper_124	The authors claims that DeepMedic  behaves very well in preserving the hierarchical structure tumours, is that true ? Have they tried it across different types of varying cases?	Figure 12 shows successful cases of segmentation for the hierarchy of brain tumors.  As seen in Figure 12, the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors.  They also show a relatively unsuccessful case where oversegmentation occurs, but even in this example, the hierarchy of the tumor is preserved.	Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.	The authors show that the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors.  They also show a relatively unsuccessful case where oversegmentation occurs. In this example, the hierarchy of the tumor is not preserved.	Change concept	 (is perserved -> is not perserved)	The authors show that the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors.  They also show a relatively unsuccessful case where oversegmentation occurs, but even in this example, the hierarchy of the tumor is preserved.
248	paper_124	The brain tumour segmentation data consists of 274 cases in total, is this dataset large enough to not consider adding regularisation technics ?	The authors mention that they reduced the amount of regularization techniques as they consider the BRATS database to be large.	Network configuration and training: We modify the DeepMedic architecture to handle multi-class problems by extending the classification layer to five feature maps (four tumor classes plus background). The rest of the configuration remains unchanged. We enrich the dataset with sagittal reflections. Opposite to the experiments on TBI, we do not employ the intensity perturbation and dropout on convolutional layers, because the network should not require as much regularisation with this large database. The network is trained on image segments extracted with equal probability centred on the whole tumor and healthy tissue. The distribution of the classes captured by our training scheme is provided in C.	The authors mention that they increased the amount of regularization techniques as they consider the BRATS database to be small.	Opposite		
249	paper_125	Why did the GAN-based image editing approach succeed only on highly curated datasets and struggle over large and diverse datasets?	Detailed generated images using GANs depends on the initial noise vector and the interaction between pixels to text embedding.	Real Image Editing.Editing a real image requires finding an initial noise vector that produces the given input image when fed into the diffusion process. This process, known as inversion, has recently drawn considerable attention for GANs, e.g., zhu2016generative ; abdal2019image2stylegan ; alaluf2022hyperstyle ; roich2021pivotal ; zhu2020domain ; tov2021designing ; Wang2021HighFidelityGI ; xia2021gan , but has not yet been fully addressed for text-guided diffusion models.Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities	None			
251	paper_125	Why the slightest change in the textual prompt can lead to a completely different output image in the large-scale language-image models?	Because trained large models on large dataset lack control over generated images as it really depends on the random seed and the interaction between pixels to text embedding through the diffusion process which results in the spatial information from the internal layers of the generative model.	Recently, large-scale language-image (LLI) models, such as Imagen saharia2022photorealistic , DALL·E 2 ramesh2022hierarchical  and Parti yu2022scaling , have shown phenomenal generative semantic and compositional power, and gained unprecedented attention from the research community and the public eye.These LLI models are trained on extremely large language-image datasets and use state-of-the-art image generative models including auto-regressive and diffusion models.However, these models do not provide simple editing means, and generally lack control over specific semantic regions of a given image. In particular, even the slightest change in the textual prompt may lead to a completely different output image.Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities. More specifically, injecting the cross-attention maps of the input image \mathcal{I} enables us to preserve the original composition and structure. In section 3.1, we review how cross-attention is used, and in section 3.2 we describe how to exploit the cross-attention for editing. For additional background on diffusion models, please refer to appendix A.Numerous works ding2021cogview ; hinz2020semantic ; tao2020df ; li2019controllable ; li2019object ; qiao2019learn ; qiao2019mirrorgan ; ramesh2021zero ; zhang2018photographic ; crowson2022vqgan ; gafni2022make ; rombach2021highresolution  significantly advanced the generation of images conditioned on plain text, known as text-to-image synthesis. Several large-scale text-image models have recently emerged, such as Imagen saharia2022photorealistic , DALL-E2 ramesh2022hierarchical , and Parti yu2022scaling , demonstrating unprecedented semantic generation. However, these models do not provide control over a generated image, specifically using text guidance only.Changing a single word in the original prompt associated with the image often leads to a completely different outcome. For instance, adding the adjective “white” to “dog” often changes the dog’s shape.To overcome this, several works nichol2021glide ; avrahami2022blendedlatent  assume that the user provides a mask to restrict the area in which the changes are applied.Unlike previous works, our method requires textual input only, by using the spatial information from the internal layers of the generative model itself. This offers the user a much more intuitive editing experience of modifying local or global details by merely modifying the text prompt.	None			
252	paper_125	What are the examples in which important structural information is removed when masking the image content?	Examples such as modifying textures of specific objects or changing bicycles in an image to a car.	To circumvent this, LLI-based methods nichol2021glide ; avrahami2022blendedlatent ; ramesh2022hierarchical require the user to explicitly mask a part of the image to be inpainted, and drive the edited image to change in the masked area only, while matching the background of the original image. This approach has provided appealing results, however, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing. Moreover, masking the image content removes important structural information, which is completely ignored in the inpainting process. Therefore, some editing capabilities are out of the inpainting scope, such as modifying the texture of a specific object.As can be seen in fig. 6, our method is not confined to modifying only textures, and it can perform structural modifications, e.g., change a “bicycle” to a “car”. To analyze our attention injection, in the left column we show the results without cross-attention injection, where changing a single word leads to an entirely different outcome. From left to right, we then show the resulting generated image by injecting attention to an increasing number of diffusion steps. Note that the more diffusion steps in which we apply cross-attention injection, the higher the fidelity to the original image.However, the optimal result is not necessarily achieved by applying the injection throughout all diffusion steps. Therefore, we can provide the user with even better control over the fidelity to the original image by changing the number of injection steps.	None			
253	paper_125	What does "interaction between the pixels to the text embedding through the diffusion process" mean?	and that is what is meant by the interaction between pixels to text embedding.	We use the Imagen saharia2022photorealistic  text-guided synthesis model as a backbone. Since the composition and geometry are mostly determined at the 64\times 64 resolution, we only adapt the text-to-image diffusion model, using the super-resolution process as is.Recall that each diffusion step t consists of predicting the noise \epsilon from a noisy image z_{t} and text embedding \psi(\mathcal{P}) using a U-shaped network ronneberger2015u . At the final step, this process yields the generated image \mathcal{I}=z_{0}.Most importantly, the interaction between the two modalities occurs during the noise prediction, where the embeddings of the visual and textual features are fused using Cross-attention layers that produce spatial attention maps for each textual token.In this paper, we introduce an intuitive and powerful textual editingmethod to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations. To do so, we dive deep into the cross-attention layers and explore their semantic strength as a handle to control the generated image. Specifically, we consider the internal cross-attention maps, which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text. We find that these maps contain rich semantic relations which critically affect the generated image.	None			
254	paper_125	How the embeddings of visual and textual features are fused during the noise prediction process?	They are fused using Cross-attention layers, to illustrate more in Figure 3, the deep spatial features of noisy image φ(zt) are projected to a query matrix Q = lQ(φ(zt)), and the textual embedding is projected to a key matrix K = lK(ψ(P)) and a value matrix V = lV (ψ(P)), via learned linear projections lQ, lK, lV.	We use the Imagen saharia2022photorealistic  text-guided synthesis model as a backbone. Since the composition and geometry are mostly determined at the 64\times 64 resolution, we only adapt the text-to-image diffusion model, using the super-resolution process as is.Recall that each diffusion step t consists of predicting the noise \epsilon from a noisy image z_{t} and text embedding \psi(\mathcal{P}) using a U-shaped network ronneberger2015u . At the final step, this process yields the generated image \mathcal{I}=z_{0}.Most importantly, the interaction between the two modalities occurs during the noise prediction, where the embeddings of the visual and textual features are fused using Cross-attention layers that produce spatial attention maps for each textual token.More formally, as illustrated in fig. 3(Top), the deep spatial features of the noisy image \phi(z_{t}) are projected to a query matrix Q=\ell_{Q}(\phi(z_{t})), and the textual embedding is projected to a key matrix K=\ell_{K}(\psi(\mathcal{P})) and a value matrix V=\ell_{V}(\psi(\mathcal{P})), via learned linear projections \ell_{Q},\ell_{K},\ell_{V}.The attention maps are thenM=\text{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right),(1)where the cell M_{ij} defines the weight of the value of the j-th token on the pixel i, and where d is the latent projection dimension of the keys and queries. Finally, the cross-attention output is defined to be \widehat{\phi}\left(z_{t}\right)=MV, which is then used to update the spatial features \phi(z_{t}).	None			
255	paper_125	Are both of them use user-provided masks for guidance but Diffusionclip (Kim et al.) perform global changes while Blended diffusion (Avrahami et al.) perform local manipulations?	(Kim et al. ) Doesn't use user-provided masks and exploited recent Diffusion models to perform global changes as most editing works are limited to global editing if no masks were provided, While (Avrahami et al. ) performed local manipulation using user-provided masks.	To obtain more expressive generation capabilities, Crowson et al. crowson2022vqgan  use VQ-GAN esser2021taming , trained over diverse data, as a backbone.Other works avrahami2022blended ; kim2022diffusionclip  exploit the recent Diffusion models ho2020denoising ; sohl2015deep ; song2019generative ; ho2020denoising ; song2020denoising ; rombach2021highresolution , which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs dhariwal2021diffusion .Kim et al. kim2022diffusionclip  show how to perform global changes, whereas Avrahami et al. avrahami2022blended  successfully perform local manipulations using user-provided masks for guidance.While most works that require only text (i.e., no masks) are limited to global editing crowson2022vqgan ; kwon2021clipstyler ,Bar-Tal et al. bar2022text2live  proposed a text-based localized editing technique without using any mask, showing impressive results.Yet, their techniques mainly allow changing textures, but not modifying complex structures, such as changing a bicycle to a car.Moreover, unlike our method, their approach requires training a network for each input.	None			
256	paper_125	Do only Blended diffusion (Avrahami et al.) use user-provided masks for the guidance of manipulation?	[7]) demonstrated how to use user-provided masks for guidance of manipulation, as well as most LLI-based methods requires masks defined by the user.	To circumvent this, LLI-based methods nichol2021glide ; avrahami2022blendedlatent ; ramesh2022hierarchical require the user to explicitly mask a part of the image to be inpainted, and drive the edited image to change in the masked area only, while matching the background of the original image. This approach has provided appealing results, however, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing. Moreover, masking the image content removes important structural information, which is completely ignored in the inpainting process. Therefore, some editing capabilities are out of the inpainting scope, such as modifying the texture of a specific object.Bau et al. [7] further demonstrated how to use masks provided by the user, to localize the text-based editing and restrict the change to a specific spatial region. However, while GAN-based image editing approaches succeed on highly-curated datasets [27], e.g., human faces, they struggle over large and diverse datasets.	None			
258	paper_125	The reason why the diffusion step can be applied on both z_{t-1} and z^*_t in parallel is their one timestep difference is matched each other. Is it right?	The reason is in the diffusion process a noisy image outputted "zt-1" at a single time-step "t" can be computed as DM(zt,P,t,s). 	Let DM(z_{t},\mathcal{P},t,s) be the computation of a single step t of the diffusion process, which outputs the noisy image z_{t-1}, and the attention map M_{t} (omitted if not used). We denote by DM(z_{t},\mathcal{P},t,s)\{M\leftarrow\widehat{M}\} the diffusion step where we override the attention map M with an additional given map \widehat{M}, but keep the values V from the supplied prompt. We also denote by M_{t}^{*} the produced attention map using the edited prompt \mathcal{P}^{*}.Lastly, we define Edit(M_{t},M_{t}^{*},t) to be a general edit function, receiving as input the t’th attention maps of the original and edited images during their generation.	None			
259	paper_125	Is there any benefit to using fader control instead of numbers (e.g., percentages)?	Fader control allows users to control the magnitude of the effect induced by specific words.	Fader Control using Attention Re-weighting.While controlling the image by editing the prompt is very effective, we find that it still does not allow full control over the generated image. Consider the prompt “snowy mountain”. A user may want to control the amount of snow on the mountain. However, it is quite difficult to describe the desired amount of snow through text. Instead, we suggest a fader control lample2017fader , where the user controls the magnitude of the effect induced by a specific word, as depicted in fig. 9. As described in section 3, we achieve such control by re-scaling the attention of the specified word. Additional results are in the appendix (fig. 15).	None			
260	paper_125	How is the inversion of text-guided diffusion models different from the inversion of GAN?	Inversion of GANs requires finding the initial noise vector that produces the edit we want.	Real Image Editing.Editing a real image requires finding an initial noise vector that produces the given input image when fed into the diffusion process. This process, known as inversion, has recently drawn considerable attention for GANs, e.g., zhu2016generative ; abdal2019image2stylegan ; alaluf2022hyperstyle ; roich2021pivotal ; zhu2020domain ; tov2021designing ; Wang2021HighFidelityGI ; xia2021gan , but has not yet been fully addressed for text-guided diffusion models.	None			
262	paper_125	How do the authors recognize that reducing the classifier-free guidance parameter improves reconstruction but constrains the ability to perform significant manipulation?	By observed that in the referenced [18] work shop "Generative models and downstream applications, 2021". 	This inversion process often produces satisfying results, as presented in fig. 10.However, the inversion is not sufficiently accurate in many other cases, as in fig. 11.This is partially due to a distortion-editability tradeoff tov2021designing , where we recognize that reducing the classifier-free guidance ho2021classifier  parameter (i.e., reducing the prompt influence) improves reconstruction but constrains our ability to perform significant manipulations.	None			
263	paper_125	How can the mask extracted directly from the attention maps mitigate the limitation of inversion process?	Extracted masks directly from the attention maps can restore the unedited regions of the original image.	To alleviate this limitation, we propose to restore the unedited regions of the original image using a mask, directly extracted from the attention maps. Note that here the mask is generated with no guidance from the user. As presented in fig. 12, this approach works well even using the naïve DDPM inversion scheme (adding noise followed by denoising). Note that the cat’s identity is well-preserved under various editing operations, while the mask is produced only from the prompt itself.	None			
264	paper_125	Attention maps are calculated by query of spatial feature of the noisy image (\phi(z_t)) and key of textual embedding (\psi(P)). Is it true?	True, as attention maps are calculated by using deep spatial features of a noisy image which is projected to a "Query Matrix" and the textual embedding is projected to a "Key Matrix" and a "Value Matrix", then finally attentions maps calculated by learned linear projections of Query Matrix, Key Matrix and Value Matrix. 	We use the Imagen saharia2022photorealistic  text-guided synthesis model as a backbone. Since the composition and geometry are mostly determined at the 64\times 64 resolution, we only adapt the text-to-image diffusion model, using the super-resolution process as is.Recall that each diffusion step t consists of predicting the noise \epsilon from a noisy image z_{t} and text embedding \psi(\mathcal{P}) using a U-shaped network ronneberger2015u . At the final step, this process yields the generated image \mathcal{I}=z_{0}.Most importantly, the interaction between the two modalities occurs during the noise prediction, where the embeddings of the visual and textual features are fused using Cross-attention layers that produce spatial attention maps for each textual token.More formally, as illustrated in fig. 3(Top), the deep spatial features of the noisy image \phi(z_{t}) are projected to a query matrix Q=\ell_{Q}(\phi(z_{t})), and the textual embedding is projected to a key matrix K=\ell_{K}(\psi(\mathcal{P})) and a value matrix V=\ell_{V}(\psi(\mathcal{P})), via learned linear projections \ell_{Q},\ell_{K},\ell_{V}.The attention maps are thenM=\text{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right),(1)where the cell M_{ij} defines the weight of the value of the j-th token on the pixel i, and where d is the latent projection dimension of the keys and queries. Finally, the cross-attention output is defined to be \widehat{\phi}\left(z_{t}\right)=MV, which is then used to update the spatial features \phi(z_{t}).In this paper, we introduce an intuitive and powerful textual editingmethod to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations. To do so, we dive deep into the cross-attention layers and explore their semantic strength as a handle to control the generated image. Specifically, we consider the internal cross-attention maps, which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text. We find that these maps contain rich semantic relations which critically affect the generated image.	None			
265	paper_125	How can the authors verify if the attention reflects the overall composition of the given image?	Injecting the cross-attention maps of the input image enabled the authors to preserve the original composition and structure, and as illustrated in Figure.  4, The average attention maps are plotted, and pixels are more attracted to words that describe them, e.  pixels of the bear in the image are correlated with the word "bear". 	Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities. More specifically, injecting the cross-attention maps of the input image \mathcal{I} enables us to preserve the original composition and structure. In section 3.1, we review how cross-attention is used, and in section 3.2 we describe how to exploit the cross-attention for editing. For additional background on diffusion models, please refer to appendix A.We return to our key observation — the spatial layout and geometry of the generated image depend on the cross-attention maps. This interaction between pixels and text is illustrated in fig. 4, where the average attention maps are plotted. As can be seen, pixels are more attracted to the words that describe them, e.g., pixels of the bear are correlated with the word “bear”. Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method.Interestingly, we can see that the structure of the image is already determined in the early steps of the diffusion process.	Injecting the cross-attention maps of the input image didn't enable the authors to preserve the original composition and structure. The average attention maps are plotted, and pixels are more attracted to words that describe them, e.  pixels of the bear in the image are correlated with the word "bear". 	Opposite		Injecting the cross-attention maps of the input image enabled the authors to preserve the original composition and structure. The average attention maps are plotted, and pixels are more attracted to words that describe them, e.  pixels of the bear in the image are correlated with the word "bear". 
266	paper_125	How does the timestamp \tau control for stylization, specification of object attributes, or global manipulations for editing image by text prompt?	The overall composition is reflected by the attenion maps, which can be injected during the diffusion process at controled time-step, which allows the necessary freedom for adapting the new prompt.	Since the attention reflects the overall composition, we can inject the attention maps M that were obtained from the generation with the original prompt \mathcal{P}, into a second generation with the modified prompt \mathcal{P}^{*}. This allows the synthesis of an edited image \mathcal{I}^{*} that is not only manipulated according to the edited prompt, but also preserves the structure of the input image \mathcal{I}. This example is a specific instance of a broader set of attention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing a general framework, followed by the details of the specific editing operations.Edit(M_{t},M_{t}^{*},t):=\begin{cases}M_{t}^{*}&\quad\text{if}\;t<\tau\\M_{t}&\quad\text{otherwise.}\\\end{cases}where \tau is a timestamp parameter that determines until which step the injection is applied.Note that the composition is determined in the early steps of the diffusion process. Therefore, by limiting the number of injection steps, we can guide the composition of the newly generated image while allowing the necessary geometry freedom for adapting to the new prompt. An illustration is provided in section 4. Another natural relaxation for our algorithm is to assign a different number of injection timestamps for the different tokens in the prompt. In case the two words are represented using a different number of tokens, the maps can be duplicated/averaged as necessary using an alignment function as described in the next paragraph.Our key idea is that we can edit images by injecting the cross-attention maps during the diffusion process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps. To apply our method to various creative editing applications, we show several methods to control the cross-attention maps through a simple and semantic interface (see fig. 1). The first is to change a single token’s value in the prompt (e.g., “dog” to “cat”), while fixing the cross-attention maps, to preserve the scene composition. The second is to globally edit an image, e.g., change the style, by adding new words to the prompt and freezing the attention on previous tokens, while allowing new attention to flow to the new tokens. The third is to amplify or attenuate the semantic effect of a word in the generated image.	The overall composition is reflected by the attenion maps, which can be injected anytime, which allows the necessary freedom for adapting the new prompt.	Change concept	 (during the diffusion process at controled time-step -> anytime)	
267	paper_125	Did the method proposed in this paper perform on par with or better than the state-of-the-art methods that require users to provide spatial masks for editing?	And their work enables local or global modifications as well and besides their method doesn't require a training network.	To circumvent this, LLI-based methods nichol2021glide ; avrahami2022blendedlatent ; ramesh2022hierarchical require the user to explicitly mask a part of the image to be inpainted, and drive the edited image to change in the masked area only, while matching the background of the original image. This approach has provided appealing results, however, the masking procedure is cumbersome, hampering quick and intuitive text-driven editing. Moreover, masking the image content removes important structural information, which is completely ignored in the inpainting process. Therefore, some editing capabilities are out of the inpainting scope, such as modifying the texture of a specific object.In this paper, we introduce an intuitive and powerful textual editingmethod to semantically edit images in pre-trained text-conditioned diffusion models via Prompt-to-Prompt manipulations. To do so, we dive deep into the cross-attention layers and explore their semantic strength as a handle to control the generated image. Specifically, we consider the internal cross-attention maps, which are high-dimensional tensors that bind pixels and tokens extracted from the prompt text. We find that these maps contain rich semantic relations which critically affect the generated image.Our method, described in section 3, enables intuitive text-only editing by controlling the spatial layout corresponding to each word in the user-provided prompt. In this section, we show several applications using this technique.Text-Only Localized Editing.We first demonstrate localized editing by modifying the user-provided prompt without requiring any user-provided mask. In fig. 2, we depict an example where we generate an image using the prompt “lemon cake”. Our method allows us to retain the spatial layout, geometry, and semantics when replacing the word “lemon” with “pumpkin” (top row). Observe that the background is well-preserved, including the top-left lemons transforming into pumpkins. On the other hand, naively feeding the synthesis model with the prompt “pumpkin cake” results in a completely different geometry (3rd row), even when using the same random seed in a deterministic setting (i.e., DDIM song2020denoising ). Our method succeeds even for a challenging prompt such as “pasta cake.” (2nd row) — the generated cake consists of pasta layers with tomato sauce on top. Another example is provided in fig. 5 where we do not inject the attention of the entire prompt but only the attention of a specific word – “butterfly”. This enables the preservation of the original butterfly while changing the rest of the content. Additional results are provided in the appendix (fig. 13).In this work, we uncovered the powerful capabilities of the cross-attention layers within text-to-image diffusion models.We showed that these high-dimensional layers have an interpretable representation of spatial maps that play a key role in tying the words in the text prompt to the spatial layout of the synthesized image.With this observation, we showed how various manipulations of the prompt can directly control attributes in the synthesized image, paving the way to various applications including local and global editing.This work is a first step towards providing users with simple and intuitive means to edit images, leveraging textual semantic power. It enables users to navigate through a semantic, textual, space, which exhibits incremental changes after each step, rather than producing the desired image from scratch after each text manipulation.Our approach constitutes an intuitive image editing interface through editing only the textual prompt, therefore called Prompt-to-Prompt. This method enables various editing tasks, which are challenging otherwise, and does not requires model training, fine-tuning, extra data, or optimization. Throughout our analysis, we discover even more control over the generation process, recognizing a trade-off between the fidelity to the edited prompt and the source image. We even demonstrate that our method can be applied to real images by using an existing inversion process. Our experiments and numerous results show that our method enables seamless editing in an intuitive text-based manner over extremely diverse images.While most works that require only text (i.e., no masks) are limited to global editing crowson2022vqgan ; kwon2021clipstyler ,Bar-Tal et al. bar2022text2live  proposed a text-based localized editing technique without using any mask, showing impressive results.Yet, their techniques mainly allow changing textures, but not modifying complex structures, such as changing a bicycle to a car.Moreover, unlike our method, their approach requires training a network for each input.Unlike previous works, our method requires textual input only, by using the spatial information from the internal layers of the generative model itself. This offers the user a much more intuitive editing experience of modifying local or global details by merely modifying the text prompt.	None			
268	paper_125	What are the examples of suitable prompt for inversion?	Examples of inversion with prompts can be found in Figure12, where they used mask-based editing to limit inversion distortion.	To alleviate this limitation, we propose to restore the unedited regions of the original image using a mask,	None			
269	paper_125	They generated the cross-attention output weight by calculating the similarity between spatial features of the noise image and textual embedding. Is it right?	True, It's correlated to the similarity between a query matrix of projected noisy image "Q" and a key matrix of a projected textual embedding "K". 	More formally, as illustrated in fig. 3(Top), the deep spatial features of the noisy image \phi(z_{t}) are projected to a query matrix Q=\ell_{Q}(\phi(z_{t})), and the textual embedding is projected to a key matrix K=\ell_{K}(\psi(\mathcal{P})) and a value matrix V=\ell_{V}(\psi(\mathcal{P})), via learned linear projections \ell_{Q},\ell_{K},\ell_{V}.The attention maps are thenM=\text{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right),(1)where the cell M_{ij} defines the weight of the value of the j-th token on the pixel i, and where d is the latent projection dimension of the keys and queries. Finally, the cross-attention output is defined to be \widehat{\phi}\left(z_{t}\right)=MV, which is then used to update the spatial features \phi(z_{t}).Intuitively, the cross-attention output MV is a weighted average of the values V where the weights are the attention maps M, which are correlated to the similarity between Q and K.In practice, to increase their expressiveness, multi-head attention NIPS2017_3f5ee243  is used in parallel, and then the results are concatenated and passed through a learned linear layer to get the final output.	None			
270	paper_126	Who is responsible for designating the control signal?	control signal is designated by each author in their work.  as the authors of this paper proposed a "verb-specific semantic role" "VSR" as control signal for customized captions.  while a recent surge of efforts by other works introduced extra control signals as constrains of the generated captions [16, 10, 19, 78, 48, 77, 27, 20]. 	Image captioning, \ie, generating fluent and meaningful descriptions to summarize the salient contents of an image, is a classic proxy task for comprehensive scene understanding [21]. With the release of several large scale datasets and advanced encoder-decoder frameworks, current captioning models plausibly have already achieved “super-human” performance in all accuracy-based evaluation metrics. However, many studies have indicated that these models tend to produce generic descriptions, and fail to control the caption generation process as humans, \eg, referring to different contents of interest or descriptive patterns. In order to endow the captioning models with human-like controllability, a recent surge of efforts [16, 10, 19, 78, 48, 77, 27, 20] resort to introducing extra control signals as constraints of the generated captions, called Controllable Image Captioning (CIC). As a byproduct, the CIC models can easily generate diverse descriptions by feeding different control signals.For human-like controllable image captioning, we first propose the Verb-specific Semantic Roles (VSR) as the control signal for generating customized captions. As shown in Figure 3, we formally represent a control signal VSR as:𝒱𝒮ℛ={v,<s1,n1>,…,<sm,nm>},\displaystyle\begin{aligned} \mathcal{VSR}=\{v,<s_{1},n_{1}>,...,<s_{m},n_{m}>\},\\\end{aligned}start_ROW start_CELL caligraphic_V caligraphic_S caligraphic_R = { italic_v , < italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > , … , < italic_s start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_n start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT > } , end_CELL end_ROW(1)where v is a verb capturing the scope of a salient activity in the image (\eg, ride), s_{i} is a semantic role of verb v (\eg, LOC), and n_{i} is the number of interested entities in the role s_{i}. For example, for 𝒱𝒮ℛ={𝚛𝚒𝚍𝚎,<𝙰𝚛𝚐𝟶,𝟷>,<𝙰𝚛𝚐𝟷,𝟷>,<𝙻𝚘𝚌,𝟸>}\mathcal{VSR}=\{\texttt{ride},<\texttt{Arg0},\texttt{1}>,<\texttt{Arg1},\texttt{1}>,<\texttt{Loc},\texttt{2}>\}caligraphic_V caligraphic_S caligraphic_R = { ride , < Arg0 , 1 > , < Arg1 , 1 > , < Loc , 2 > }, we hope to generate a caption which not only focuses on describing the ride activity, but also contains one entity respectively in the role Arg0{}_{\text{rider}} and Arg1{}_{\text{steed}}, and two entities in the role LOC. Thus, VSR can effectively control the amount of information carried in the whole sentence and each role, \ie, the level of details.In this paper, we argued that all existing objective control signals for CIC have overlooked two indispensable characteristics: event-compatible and sample-suitable. To this end, we proposed a novel control signal called VSR. VSR consists of a verb and several semantic roles, \ie, all components are guaranteed to be event-compatible. Meanwhile, VSR only restricts the involved semantic roles, which is also sample-suitable for all the images containing the activity. We have validated the effectiveness of VSR through extensive experiments. Moving forward, we will plan to 1) design a more effective captioning model to benefit more from the VSR signals; 2) extend VSR to other controllable text generation tasks, \eg, video captioning [69]; 3) design a more general framework to cover the images without verbs.In summary, we make three contributions in this paper:1.We propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). To the best of our knowledge, VSR is the first control signal to consider both event-compatible and sample-suitable requirements222When using control signals extracted from GT captions, existing control signals can always meet both requirements and generate reasonable captions. However, in more general settings (\eg, construct control signals without GT captions), the form of VSR is more human-friendly, and it is easier to construct signals which meet both requirements compared with all existing forms of control signals, which is the main advantage of VSR..2.We can learn human-like verb-specific semantic structures automatically, and abundant visualization examples demonstrate that these patterns are reasonable.3.We achieve state-of-the-art controllability on two challenging benchmarks, and generate diverse captions by using different verbs, semantic roles, or structures.Controllable Image Captioning.Compared with conventional image captioning [63, 68, 9, 25, 13], CIC is a more challenging task, which needs to consider extra constraints. Early CIC works are mostly about stylized image captioning, \ie, constraints are the linguistic styles of sentences. According to the requirements of parallel training samples, existing solutions can be divided into two types: models using parallel stylized image-caption data [41, 11, 54, 1] or not [22, 42]. Subsequently, the community gradually shifts the emphasis to controlling described contents [16, 77, 27, 10, 78, 48, 35] or structures [20, 19, 75, 76] of the sentences. In this paper, we propose a novel control signal VSR, which is the first control signal to consider both the event-compatible and sample-suitable requirements.	None			
271	paper_126	What is the example of ideal control signal?	Figure 1 (a) and Figure 1 (b), are examples of two indispensable characteristics ideal control signal, as Figure 1 (a) elaborates the "Event Compatibility" characteristic as "man, wave, surfboard" are all involved in activity riding.  and Figure 1 (b) elaborates the "Sample-suitability" characteristic as the two control signal (length-levels 3 and 4) are quite close, but the quality of respectively generated captions varies greatly. 	Nevertheless, all existing objective control signals (\ie, both content-controlled and structure-controlled) have overlooked two indispensable characteristics of an ideal control signal towards “human-like” controllable image captioning: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. Imaging how humans describe images — our brains always quickly structure a descriptive pattern like “sth do sth at someplace” first, and then fill in the detailed description [56, 46, 30, 71], \ie, we have subconsciously made sure that all the mentioned entities are event-compatible (\eg, man, wave, surfboard are all involved in activity riding in Figure 1 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliberately utilize two more objects (hand and sky, \ie, ) as part of the control signal, and the model generates an incoherent and illogical caption. 2) Sample-suitable: the control signals should be suitable for the specific image sample. By “suitable”, we mean that there do exist reasonable descriptions satisfying the control signals, \eg, a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difficult to decide whether a control signal is sample-suitable in advance. For example in Figure 1 (b), although the two control signals (\ie, length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.	None			
272	paper_126	How does a role-shift captioning model contribute to generating captions?	By using RNN-based-role-shift caption model consists of two LSTM layers.  the model generates the word "yt", by taking two inputs to the model which are 1- Semantic structure sequence, and 2- corresponding proposal feature sequence.  then at each time step the model focus on one specific sub-role and its grounded region set. 	Given the semantic structure sequence \mathcal{S}=(s^{b}_{1},...,s^{b}_{K}) and corresponding proposal feature sequence \mathcal{R}=(\bm{r}_{1},...,\bm{r}_{K}), we utilize a two-layer LSTM to generate the final caption \bm{y}. At each time step, the model fouces on one specific sub-role \bm{s}^{b}_{t} and its grounded region set \bm{r}_{t}, and then generates the word y_{t}. Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: p(g_{t}|\mathcal{S},\mathcal{R}) for controlling the shift of sub-roles, and p(y_{t}|\mathcal{S},\mathcal{R}) to predict the distribution of a word.In order to generate sentences with respect to the designated VSRs, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, \eg, Arg0{}_{\text{reader}} – read – Arg1{}_{\text{thing}} – LOC in Figure 1 (c). Finally, we combine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.	By using RNN-based-role-shift caption model consists of two LSTM layers.  the model generates the word "yt", by taking only one input to the model which is Semantic structure sequence.  then at each time step the model focus on one specific sub-role and its grounded region set. 	Change concept	 (two inputs -> one input)	
273	paper_126	How do the authors verify that the two characteristics mentioned in the sentence are indispensable for the ideal control signal?	Authors verify their work using a conventions evaluation metrics in prior CIC works.  As their quantitative results report in Table 1, you can observe that author's framework can achieve the best performance over almost all metrics and benchmarks.  and as for the visualized evaluation, you can observe in Figure 5 that the author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions.  and according to the semantic structures, the captioning model can generate near-perfect descriptions.	Settings. To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works [16, 10, 78], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines666All baselines use the same visual regions as models with VSRs.: 1) C-LSTM: It is a Controllable LSTM model [63]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) C-UpDn: It is a Controllable UpDn model [3], which uses an adaptive attention to generate the captions. 3) SCT [16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) Ours w/o verb: We ablate our model by removing the verb information in both the SSP and captioning model. 5) Ours (oracle verb): It is an ideal situation, where the captioning model directly outputs the oracle format of the verb when the attending role is the verb.Quantitative Results. The quantitative results are reported in Table 1. From Table 1, we can observe that our framework can achieve the best performance over almost all metrics and benchmarks. By comparing the two different proposal settings (i.e., GSRL and GT), we can find that the accuracy of GSRL is a major bottleneck of the whole framework. Meanwhile, the ablative model (Ours w/o verb) can only achieve slightly better performance than baseline SCT and much worse performance than our full model, which reflects the importance of the verb in semantic structure learning and caption generation.Visualizations. In Figure 5, we illustrate some examples of the generated captions. We can observe that our framework always learns a human-like semantic structure based on the VSR and grounded visual regions (e.g., Arg1thing – sit – Arg2position – LOC – MNR). According to the semantic structures, the captioning model can generate near-perfect descriptions. As a by-product, a well-trained SSP can automatically produce several verb-specific semantic structures for a set of user-interested roles, and we show some examples in Figure 6. For each verb and role set, we illustrate the top two structures by using beam search. Particularly, we are surprised to find that we can even learn some structures that never appear in original datasets (the blue tick ones).	Authors verify their work using a conventions evaluation metrics in prior CIC works.  As their quantitative results report in Table 1, you can observe that author's framework can achieve the similar performance with metrics and benchmarks.  and as for the visualized evaluation, you can observe in Figure 5 that the author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions.  and according to the semantic structures, the captioning model can generate near-perfect descriptions.	Change concept	 (best -> similar)	
275	paper_126	How are objective control signals more advantageous than subjective control signals when controlling the caption generation process?	Subjective control signals are harder to control the generation process effectively and precisely.	Early CIC works mainly focus on subjective control signals, such as sentiments [41], emotions [42, 22], and personality [14, 54], \ie, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on objective control signals. More specifically, they can be coarsely classified into two categories: 1) Content-controlled: the control signals are about the contents of interest which need to be described. As the example shown in Figure 1 (a), given the region set () as a control signal, we hope that the generated caption can cover all regions (\ie, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations [27], object regions [16, 35], scene graphs [10, 78], and mouse trace [48]. 2) Structure-controlled: the control signals are about the semantic structures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [79] of the sentence (cf. Figure 1 (b)) are some typical structure-controlled signals.	Subjective control signals are easier to control the generation process effectively and precisely.	Opposite		
276	paper_126	What is the big reason of making difficult to decide whether a control signal is sample-suitable in advance?	Because it must be a reasonable description for the specific image sample.  however, can't elaborate more details as authors didn't elaborate more about the specific reason for it in this paper.	Nevertheless, all existing objective control signals (\ie, both content-controlled and structure-controlled) have overlooked two indispensable characteristics of an ideal control signal towards “human-like” controllable image captioning: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. Imaging how humans describe images — our brains always quickly structure a descriptive pattern like “sth do sth at someplace” first, and then fill in the detailed description [56, 46, 30, 71], \ie, we have subconsciously made sure that all the mentioned entities are event-compatible (\eg, man, wave, surfboard are all involved in activity riding in Figure 1 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliberately utilize two more objects (hand and sky, \ie, ) as part of the control signal, and the model generates an incoherent and illogical caption. 2) Sample-suitable: the control signals should be suitable for the specific image sample. By “suitable”, we mean that there do exist reasonable descriptions satisfying the control signals, \eg, a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difficult to decide whether a control signal is sample-suitable in advance. For example in Figure 1 (b), although the two control signals (\ie, length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.	None			
277	paper_126	What are the metrics used to evaluate the trade-off between the quality and diversity of generated captions?	Authors used BlEU, METOR, ROUGE, CIDEr, and SPICE to evaluate quality based generated captions, And used Accuracy-based,  Diversity-based metrics to evaluate diversity based generation captions.	Evaluation Metrics. To evaluate the quality of the generated captions, we use five accuracy-based metrics, including BLEU-4 (B4) [45], METEOR (M) [5], ROUGE (R) [34], CIDEr-D (C) [61], and SPICE (S) [2]. Particularly, we evaluate the generated captions against the single ground truth caption. We also propose a new recall-based metric to evaluate whether the roles of the generated sentence are consistent with the ground truth caption (\ie, VSR). It measures the recall rate of the verb, semantic roles, and ordered role pairs, which are denoted as R{}_{\text{V}}, R{}_{\text{SR1}} and R{}_{\text{SR2}}, respectively.Evaluation Metrics. We used two types of metrics to evaluate the diverse captions: 1) Accuracy-based: we followed the conventions of the previous works [16, 20, 65] and reported the best-1 accuracy, \ie, the generated caption with the maximum score for each metric is chosen. Analogously, we evaluate the generated captions against the single ground truth caption. 2) Diversity-based: we followed [10] and used two metrics which only focus on the language similarity: Div-n (D-n) [4, 20] and self-CIDEr (s-C) [66].	Authors used BlEU, METOR, ROUGE, CIDEr, F1 and SPICE to evaluate quality based generated captions, And used Accuracy-based,  Diversity-based metrics to evaluate diversity based generation captions.	Invent something didn't mentioned		
278	paper_126	What are the examples of sub-roles?	LOC-1 and LOC-2 in Figure 3.	R-level SSP. The role-level (R-level) SSP is a fine-grained structure model which aims to rank all sub-roles within the same semantic role (\eg, LOC-1 and LOC-2 are two sub-roles of role Loc in Figure 3). Since the only differences among these sub-roles are the grounded visual regions, we borrow ideas from the Sinkhorn networks [43, 16], which use a differentiable Sinkhorn operation to learn a soft permutation matrix \bm{P}. Specifically, for each role s_{i} with multiple sub-roles (\ie, n_{i}>1), we first select all the corresponding grounded proposal sets for these sub-roles, denoted as \mathcal{\hat{B}}=\{\mathcal{\hat{B}}_{1},...,\mathcal{\hat{B}}_{n_{i}}\}. And for each proposal \bm{b}_{*}\in\mathcal{\hat{B}}, we encode a feature vector \bm{z}_{*}=[\bm{z}^{v}_{*};\bm{z}^{s_{i}}_{*};\bm{z}^{l}_{*}], where \bm{z}^{v}_{*} is a transformation of its visual feature \bm{f}_{*}, \bm{z}^{s_{i}}_{*} is the word embedding feature of the semantic role s_{i}, and \bm{z}^{l}_{*} is a 4-d encoding of the spatial position of proposal \bm{b}_{*}. Then, we transform each feature \bm{z}_{*} into n_{i}-d, and average-pooled all features among the same proposal set, \ie, we can obtain an n_{i}-d feature for each \mathcal{\hat{B}}_{i}. We concatenate all these features to get an n_{i}\times n_{i} matrix \bm{Z}. Finally, we use the Sinkhorn operation to obtain the soft permutation matrix \bm{P}4:\displaystyle\bm{P}=\text{Sinkhorn}(\bm{Z}).(6)	None			
279	paper_126	What does the b of the sub-role, s^b_i mean in the semantic structure of sentence S?	^b refer to a sub-role in the semantic structure of the sentence.  as S is the semantic structure of the sentence, i is specific to a number of sub-role of a sequence of sub-roles in the semantic structure of a sentence.  and ^b is a general sub-role.	Further, we utilize two sequences \mathcal{S}=(s^{b}_{1},...,s^{b}_{K}) and \mathcal{R}=(\bm{r}_{1},...,\bm{r}_{K}) to model the descriptive patterns. Specifically, \mathcal{S} is a semantic structure of the sentence and each s^{b}_{i}\in\mathcal{S} is a sub-role. By “sub-role”, we mean that each role s_{i}\in\mathcal{VSR} can be divided into n_{i} sub-roles, and when n_{i}=1, role s_{i} itself is a sub-role. Thus, VSR in Figure 3 can be rewritten as Arg0, Arg1, LOC-1, and LOC-2. \mathcal{R} is a sequence of visual features of the corresponding grounded entities for each sub-role in \mathcal{S} (\eg, \bm{r}_{i} is the features of visual regions referring to s^{b}_{i}). Particularly, for presentation conciseness, we regard the verb in \mathcal{VSR} as a special type of sub-role, and since there are no grounded visual regions referring to the verb, we use the global image feature as the grounded region feature in \mathcal{R}. Meanwhile, we use \mathcal{\tilde{R}} to denote a set of all elements in the sequence \mathcal{R}. Thus, we further decompose this task into three components:\displaystyle p(\bm{y}|\bm{I},\mathcal{VSR})=\underbrace{p(\bm{y}|\mathcal{S},\mathcal{R})}_{\text{Captioner}}\underbrace{p(\mathcal{S},\mathcal{R}|\mathcal{\tilde{R}},\mathcal{VSR})}_{\text{SSP}}\underbrace{p(\mathcal{\tilde{R}}|\bm{I},\mathcal{VSR})}_{\text{GSRL}}.(3)Given the semantic structure sequence \mathcal{S}=(s^{b}_{1},...,s^{b}_{K}) and corresponding proposal feature sequence \mathcal{R}=(\bm{r}_{1},...,\bm{r}_{K}), we utilize a two-layer LSTM to generate the final caption \bm{y}. At each time step, the model fouces on one specific sub-role \bm{s}^{b}_{t} and its grounded region set \bm{r}_{t}, and then generates the word y_{t}. Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: p(g_{t}|\mathcal{S},\mathcal{R}) for controlling the shift of sub-roles, and p(y_{t}|\mathcal{S},\mathcal{R}) to predict the distribution of a word.	None			
280	paper_126	How is N, the number of disjoint sets of proposals determined?	A set of object proposals is extracted with an object detector from an image.  as authors utilized a Faster R-NN with ResNet-101 to obtain all proposals for each image.  noting that for COCO Entities, authors group the proposals by their detected class labels, and for FLickr30K Entities, they directly regard each proposal as a proposal set.	Given an image \bm{I}, we first utilize an object detector [50] to extract a set of object proposals \mathcal{B}. Each proposal \bm{b}_{i}\in\mathcal{B} is associated with a visual feature \bm{f}_{i} and a class label c_{i}\in\mathcal{C}. Then, we group all these proposals into N disjoint sets, \ie, \mathcal{B}=\{\mathcal{B}_{1},...,\mathcal{B}_{N}\}333Due to different annotation natures of specific CIC datasets, we group proposals by different principles. Details are shown in Section 4.2., and each proposal set \mathcal{B}_{i} consists of one or more proposals. In this GSRL step, we need to refer each sub-role in the \mathcal{VSR} to a proposal set in \mathcal{B}. Specifically, we calculate the similarity score a_{ij} between semantic role s_{i} and proposal set \mathcal{B}_{j} by:\displaystyle\bm{q}_{i}=\left[\bm{e}^{g}_{v};\bm{e}^{g}_{s_{i}};\bm{\bar{f}}\right],\quad a_{ij}=F_{a}(\bm{q}_{i},\bm{\bar{f}_{j}}),(4)where \bm{e}^{g}_{v} and \bm{e}^{g}_{s_{i}} are the word embedding features of verb v and semantic role s_{i}, \bm{\bar{f}} and \bm{\bar{f}_{j}} represent the average-pooled visual features of proposal set \mathcal{B} and \mathcal{B}_{j}, [;] is a concatenation operation, and F_{a} is a learnable similarity function444For conciseness, we leave the details in the supplementary material. .Proposal Generation and Grouping. We utilize a Faster R-CNN [50] with ResNet-101 [24] to obtain all proposals for each image. Especially, we use the model released by [3], which is finetuned on VG dataset [29]. For COCO Entities, since the “ground truth” annotations for each noun phrase are the proposals with the same class, we group the proposals by their detected class labels. But for Flickr30K Entities, we directly regard each proposal as a proposal set.	A set of object proposals is extracted with an object detector from an image.  as authors utilized a Faster CNN with ResNet-101 to obtain all proposals for each image.  noting that for COCO Entities, authors group the proposals by their detected class labels, and for FLickr30K Entities, they directly regard each proposal as a proposal set.	Change concept	 (RNN -> CNN)	
281	paper_126	What types of control signals are present?	Objective control signal, and Objective control singals types are the only type mentioned in this paper.	Early CIC works mainly focus on subjective control signals, such as sentiments [41], emotions [42, 22], and personality [14, 54], \ie, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on objective control signals. More specifically, they can be coarsely classified into two categories: 1) Content-controlled: the control signals are about the contents of interest which need to be described. As the example shown in Figure 1 (a), given the region set () as a control signal, we hope that the generated caption can cover all regions (\ie, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations [27], object regions [16, 35], scene graphs [10, 78], and mouse trace [48]. 2) Structure-controlled: the control signals are about the semantic structures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [79] of the sentence (cf. Figure 1 (b)) are some typical structure-controlled signals.Nevertheless, all existing objective control signals (\ie, both content-controlled and structure-controlled) have overlooked two indispensable characteristics of an ideal control signal towards “human-like” controllable image captioning: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. Imaging how humans describe images — our brains always quickly structure a descriptive pattern like “sth do sth at someplace” first, and then fill in the detailed description [56, 46, 30, 71], \ie, we have subconsciously made sure that all the mentioned entities are event-compatible (\eg, man, wave, surfboard are all involved in activity riding in Figure 1 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliberately utilize two more objects (hand and sky, \ie, ) as part of the control signal, and the model generates an incoherent and illogical caption. 2) Sample-suitable: the control signals should be suitable for the specific image sample. By “suitable”, we mean that there do exist reasonable descriptions satisfying the control signals, \eg, a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difficult to decide whether a control signal is sample-suitable in advance. For example in Figure 1 (b), although the two control signals (\ie, length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.In this paper, we argued that all existing objective control signals for CIC have overlooked two indispensable characteristics: event-compatible and sample-suitable. To this end, we proposed a novel control signal called VSR. VSR consists of a verb and several semantic roles, \ie, all components are guaranteed to be event-compatible. Meanwhile, VSR only restricts the involved semantic roles, which is also sample-suitable for all the images containing the activity. We have validated the effectiveness of VSR through extensive experiments. Moving forward, we will plan to 1) design a more effective captioning model to benefit more from the VSR signals; 2) extend VSR to other controllable text generation tasks, \eg, video captioning [69]; 3) design a more general framework to cover the images without verbs.	None			
282	paper_127	Do α control the strength of the length normalization and β control the strength of the coverage penalty each other?	6 − 0. 7] on development set which usually found to be best.	We use beam search during decoding to find the sequence Ythat maximizes a score function s(Y,X) given a trained model. Weintroduce two important refinements to the pure max-probability based beamsearch algorithm: a coverage penalty [42] and lengthnormalization. With length normalization, we aim to account for thefact that we have to compare hypotheses of different length. Withoutsome form of length-normalization regular beam search will favorshorter results over longer ones on average since a negativelog-probability is added at each step, yielding lower (more negative) scores forlonger sentences. We first tried to simply divideby the length to normalize. We then improved on that original heuristic by dividing bylength^{\alpha}, with 0<\alpha<1 where \alpha is optimized ona development set (\alpha\in[0.6-0.7] was usually found to bebest). Eventually we designed the empirically-better scoring functionbelow, which also includes a coverage penalty to favor translationsthat fully cover the source sentence according to the attentionmodule.\begin{split}s(Y,X)&=\log(P(Y|X))/lp(Y)+cp(X;Y)\\lp(Y)&=\frac{(5+|Y|)^{\alpha}}{(5+1)^{\alpha}}\\cp(X;Y)&=\beta*\sum_{i=1}^{|X|}{\log(\min(\sum_{j=1}^{|Y|}{p_{i,j}},1.0))},\end{split}(14)where p_{i,j} is the attention probability of the j-th target wordy_{j} on the i-th source word x_{i}. By construction(equation 4), \sum_{i=0}^{|X|}{p_{i,j}} is equalto 1. Parameters \alpha and \beta control the strength ofthe length normalization and the coverage penalty. When \alpha=0 and\beta=0, our decoder falls back to pure beam search by probability.We find that length normalization (\alpha) and coverage penalty(\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.	None			
283	paper_127	How can the attention mechanism connecting the bottom layer of the decoder to the top layer of the encoder contribute to improving parallelism?	First we have to establish that LSTM layers reduces parallelism as each layer would have to wait until both forward and backward directions of the previous layer to finish.  Then notice in Figure 1, the model architecture consists of 8 LSTM encoder layers (1 bi-directional and 7 uni-directional layers), and 8 decoder layers.  During training the bottom bi-directional encoder layers compute in parallelism first, then the uni-directional encoder layers.  So to retain retain and much possible parallelism during the decoder layers, the bottom layers of the decoder output only for obtaining the recurrent attention context which is sent directly to all the remaining decoder layers.	Model parallelism places certain constraints on the modelarchitectures we can use. For example, we cannot afford to havebi-directional LSTM layers for all the encoder layers, since doing sowould reduce parallelism among subsequent layers, as each layer wouldhave to wait until both forward and backward directions of the previouslayer have finished. This would effectively constrain us to make use ofonly 2 GPUs in parallel (one for the forward direction and one for thebackward direction). For the attention portion of the model, we chose to align thebottom decoder output to the top encoder output to maximizeparallelism when running the decoder network. Had we aligned the top decoderlayer to the top encoder layer, we would have removed all parallelismin the decoder network and would not benefit from using more than oneGPU for decoding.Figure 1: The model architecture of GNMT, Google’s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.	None			
284	paper_127	In terms of the effectivenesses of coverage penalty and length normalization, how does having RL-based model refinement differ from not having RL-based model refinement?	It was found that models with RL refinement are less affected by length normalization "α" and coverage penalty "β", authors explain this to the fact that during RL refinement, models already learn to pay attention to the full source sentence to not under-translate or over-translate.  The authors also found an overlap between the wins from RL refinement and decoder fine-tuning, and the win from RL on a less fine-tuned decoder would have been bigger.  The impact of length normalization "α" and coverage penalty "β" on RL-based and non-RL-based models can be found in Tables 2 and 3. 	Table 2 shows the impact of \alpha and \beta onthe BLEU score when decoding the WMT’14 English-to-French development set.The model used here for experiments is trained using the ML objectiveonly (without RL refinement). As can be seen from the results, havingsome length normalization and coverage penalty improves BLEU scoreconsiderably (from 30.3 to 31.4).We find that length normalization (\alpha) and coverage penalty(\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.The results of RL fine-tuning on the best En\rightarrowFr andEn\rightarrowDe models are presented inTable 6, which show that fine-tuning themodels with RL can improve BLEU scores. On WMT En\rightarrowFr,model refinement improves BLEU score by close to 1 point. On En\rightarrowDe,RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU pointsimprovement on the development set. The results presented inTable 6 are the average of 8 independent models.We also note that there is an overlap between the wins from the RL refinement and the decoderfine-tuning (i.e., the introduction of length normalization and coverage penalty).On a less fine-tuned decoder (e.g., if the decoder does beam search bylog-probability only), the win from RL would have been bigger (as is evidentfrom comparing results in Table 2 andTable 3).	It was found that models with RL refinement are less affected by length normalization "α" and coverage penalty "β", authors explain this to the fact that during RL refinement, models can't learn to pay attention to the full source sentence to not under-translate or over-translate. 	Change concept	 (already -> can't)	It was found that models with RL refinement are less affected by length normalization "α" and coverage penalty "β", authors explain this to the fact that during RL refinement, models can't learn to pay attention to the full source sentence to not under-translate or over-translate. 
285	paper_127	What are the weaknesses of conventional phrase-based translation systems compared to neural machine translation?	The weakness of conventional phrase-based translation systems over Neural Machine Translation are their brittle design choices especially when it's trained on very large-scale datasets, large scale, production quality and it lacks the ability to learn directly in an end-to-end fashion.	Neural Machine Translation(NMT) [41, 2] has recently beenintroduced as a promising approach with the potential of addressingmany shortcomings of traditional machine translation systems.The strength of NMT lies in its ability to learn directly, in anend-to-end fashion, the mapping from input text to associated output text.Its architecture typically consists of two recurrent neural networks (RNNs), oneto consume the input text sequence and one to generate translated output text.NMT is often accompanied by an attention mechanism [2]which helps it cope effectively with long input sequences.An advantage of Neural Machine Translation is that it sidesteps manybrittle design choices in traditional phrase-based machinetranslation [26]. In practice, however, NMT systemsused to be worse in accuracy than phrase-based translation systems,especially when training on very large-scale datasets as used for the verybest publicly available translation systems.Three inherent weaknesses of Neural Machine Translation are responsible for thisgap: its slower training and inference speed, ineffectiveness in dealing withrare words, and sometimesfailure to translate all words in the source sentence. Firstly, it generallytakes a considerable amount of time and computational resources totrain an NMT system on a large-scale translation dataset, thus slowing the rateof experimental turnaround time and innovation. For inference they are generallymuch slower than phrase-based systems due to the large number of parametersused.Secondly, NMT lacks robustness in translating rare words. Though thiscan be addressed in principle by training a “copy model” to mimic atraditional alignment model [31], or by using theattention mechanism to copy rare words [37], these approaches areboth unreliable at scale, since the quality of the alignments varies acrosslanguages, and the latent alignments produced by the attentionmechanism are unstable when the network is deep. Also, simple copyingmay not always be the best strategy to cope with rare words, for example whena transliteration is more appropriate. Finally,NMT systems sometimes produce output sentencesthat do not translate all parts of the input sentence – in otherwords, they fail to completely “cover” the input, which can result insurprising translations.Since then, many novel techniques have been proposed to furtherimprove NMT: using an attention mechanism to deal with rarewords [37], a mechanism to model translationcoverage [42], multi-task and semi-supervised training toincorporate more data [14, 29], a characterdecoder [9], a characterencoder [11], subwordunits [38] also to deal with rare word outputs,different kinds of attentionmechanisms [30], and sentence-levelloss minimization [39, 34].While the translation accuracy of these systems has been encouraging, systematiccomparison with large scale, production quality phrase-based translation systemshas been lacking.	The weakness of conventional phrase-based translation systems over Neural Machine Translation are their brittle design choices especially when it's trained on very small-scale datasets, small scale, production quality and it lacks the ability to learn directly in an end-to-end fashion.	Change concept	 (large -> small)	
286	paper_127	What are the roles of attention connections from the decoder network to the encoder?	Attentions connections improve parallelism allowing to decrease training time and allows the decoder to focus on different regions of the source sentence.	This work presents the design and implementation of GNMT, a production NMTsystem at Google, that aims toprovide solutions to the above problems. In our implementation, therecurrent networks are Long Short-Term Memory (LSTM)RNNs [23, 17]. Our LSTM RNNs have 8layers, with residual connections between layers to encourage gradientflow [21]. For parallelism, we connect the attention fromthe bottom layer of the decoder network to the top layer of theencoder network. To improve inference time, we employ low-precisionarithmetic for inference, which is further accelerated by specialhardware (Google’s Tensor Processing Unit, or TPU). To effectivelydeal with rare words, we use sub-word units (also known as“wordpieces”) [35] for inputs and outputs inour system. Using wordpieces gives a good balance between theflexibility of single characters and the efficiency of full words fordecoding, and also sidesteps the need for special treatment of unknownwords. Our beam search technique includes a length normalization procedure todeal efficiently with the problem of comparing hypotheses of differentlengths during decoding, and a coverage penalty to encourage the modelto translate all of the provided input.Our model (see Figure 1) follows the commonsequence-to-sequence learning framework [41] withattention [2]. It has three components:an encoder network, a decoder network, and an attention network. Theencoder transforms a source sentence into a list of vectors, one vector per input symbol. Giventhis list of vectors, the decoder produces one symbol at a time, untilthe special end-of-sentence symbol (EOS) is produced. The encoder and decoderare connected through an attention module which allows the decoder tofocus on different regions of the source sentence during the course ofdecoding.To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder.	None			
287	paper_127	How is “character”-delimited models different from “word”-delimited models?	character-delimited models takes characters as input and outputs characters, the words spitted into constituent characters, resulting typically in a few hundred basic characters including special characters appeared in the data.  While in word-delimited models OOv words are collapsed into a single UNK symbols.	A second approach we use is the mixed word/character model.As in a word model, we keep a fixed-size word vocabulary.However, unlike in a conventional word model where OOV words are collapsedinto a single UNK symbol, we convert OOV words into the sequence of itsconstituent characters.Special prefixes are prepended to the characters, to 1) show the location ofthe characters in a word, and 2) to distinguish them from normal in-vocabularycharacters. There are threeprefixes: <B>,<M>, and <E>, indicating beginning of the word, middleof the word and end of the word, respectively. For example, let’s assume theword Miki is not in the vocabulary. It will be preprocessed into asequence of special tokens: <B>M <M>i <M>k <E>i. The process isdone on both the source and the target sentences. During decoding, theoutput may also contain sequences of special tokens. With theprefixes, it is trivial to reverse the tokenization to the original words aspart of a post-processing step.The mixed word-character model is similar to the word model, except theout-of-vocabulary (OOV) words are converted into sequences ofcharacters with special delimiters around them as described in section4.2 in more detail. Inour experiments, the vocabulary size for the mixed word-charactermodel is 32K. For the pure character model, we simply split all wordsinto constituent characters, resulting typically in a few hundred basiccharacters (including special symbols appearing in the data). For thewordpiece models, we train 3 different models with vocabulary sizes of8K, 16K, and 32K.The pure character model (char input, char output) works surprisinglywell on this task, not much worse than the best wordpiece models in BLEUscore. However, these models are rather slow to train and slow to use as thesequences are much longer.	character-delimited models takes characters as input and outputs characters, the words spitted into constituent characters, resulting typically in a few hundred basic characters including special characters appeared in the data.  While in word-delimited models OOv words are collapsed into a single SEP symbols.	Change concept	 (UNK -> SEP)	
288	paper_127	How can the limited set of common sub-word units (“wordpieces”) provide a good balance between the flexibility of “character”-delimited models and the efficiency of “word”-delimited models?	Authors assume that's due to the fact that it deals efficiently with an essentially infinite vocabulary without restoring to characters only.	Wordpieces achieve a balance between the flexibility of characters andefficiency of words.We also find that our models get better overall BLEU scores when usingwordpieces – possibly due to the fact that our models now dealefficiently with an essentially infinite vocabulary without resorting tocharacters only. The latter would make the average lengths of the input and outputsequences much longer, and therefore would require more computation.	None			
289	paper_127	Why do NMT systems sometimes produce output sentences that do not translate all parts of the input sentence?	Authors implemented a coverage penalty to encourage the model to translate all of the provided input, however, it's not clear why sometimes NMT systems fail to translate all parts of the input.	Our beam search technique includes a length normalization procedure to deal efficiently with the problem of comparing hypotheses of different lengths during decoding, and a coverage penalty to encourage the model to translate all of the provided input.	None			
290	paper_127	Is there a disadvantage to using low-precision arithmetic for inference, such as decreased inference accuracy?	Quantization models can perform slightly have lower results on neural organization models, however in this paper authors performed some constraints during training so that's quantizable with minimal impact on the output of the model, the quantized model even performed slightly better than none-quantized training and they suggest it could be due to regularization roles those constraints had during training.	In this section, we present our approach to speed up inference withquantized arithmetic. Our solution is tailored towards the hardwareoptions available at Google. To reduce quantization errors, additionalconstraints are added to our model during training so that it is quantizablewith minimal impact on the output of the model. That is, once amodel is trained with these additional constraints, it can be subsequentlyquantized without loss to translation quality. Our experimental results suggestthat those additional constraints do not hurt model convergence nor the qualityof a model once it has converged.It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\delta,\delta] and softmax logits into[-\gamma,\gamma]. \gamma is fixed to be at 25.0, while thevalue for \delta is gradually annealed from a generous bound of\delta=8.0 at the beginning of training, to a rather stringent boundof \delta=1.0 towards the end of training. At inference time,\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT’14 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.Our solution strikes a good balance between efficiency andaccuracy. Since the computationally expensive operations (the matrixmultiplications) are done using 8-bit integer operations, ourquantized inference is quite efficient. Also, since error-sensitiveaccumulator values are stored using 16-bit integers, our solution isvery accurate and is robust to quantization errors.Table 1 shows that decoding using reducedprecision arithmetics on the TPU suffers a very minimal loss of 0.0072 onlog perplexity, and no loss on BLEU at all. This result matchesprevious work reporting that quantizing convolutional neuralnetwork models can retain most of the model quality.	Quantization models can perform slightly have lower results on neural network models, however in this paper authors performed some constraints during training so that's quantizable with minimal impact on the output of the model, the quantized model even performed slightly better than none-quantized training and they suggest it could be due to regularization roles those constraints had during training.	Tortured phrases	 (neural network -> neural organization)	
291	paper_127	Is it true that they used the output from the bottom decoder layer for y_{i-1}, not the decoder-RNN output from the past decoding time step?	Authors used only the decoder-RNN output from the past decoding time step in the bottom decoder layer to obtain recurrent attention context which is sent directly to all the remaining decoder layers.	Our attention module is similar to [2]. Morespecifically, let \mathbf{y}_{i-1} be the decoder-RNN output fromthe past decoding time step (in our implementation, we use the output fromthe bottom decoder layer). Attention context \mathbf{a}_{i}for the current time step is computed according to the following formulas:st=A⁢t⁢t⁢e⁢n⁢t⁢i⁢o⁢n⁢F⁢u⁢n⁢c⁢t⁢i⁢o⁢n⁢(𝐲i−1,𝐱t)∀t,1≤t≤Mpt=exp⁡(st)/∑t=1Mexp⁡(st)∀t,1≤t≤M𝐚i=∑t=1Mpt.𝐱t\begin{split}s_{t}&=AttentionFunction(\mathbf{y}_{i-1},\mathbf{x}_{t})\quad\forall t,\quad 1\leq t\leq M\\p_{t}&=\exp(s_{t})/\sum_{t=1}^{M}\exp(s_{t})\quad\quad\forall t,\quad 1\leq t\leq M\\\mathbf{a}_{i}&=\sum_{t=1}^{M}p_{t}.\mathbf{x}_{t}\end{split}start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n ( bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) / ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(4)where AttentionFunction in our implementation is a feed forward network withone hidden layer.Figure 1: The model architecture of GNMT, Google’s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.	Authors used the decoder-RNN output combined from the past decoding time step in the bottom decoder layer, as well as the embeddings from the encoder to obtain recurrent attention context which is sent directly to all the remaining decoder layers.	Invent something didn't mentioned		
292	paper_127	In the model architecture described in this paper, how many residual connections are used?	Authors used 8 LSTM layers for the encoder, and 8 LSTM layers for the decoder with residual connections for both networks, each layer has 1024 node.	This work presents the design and implementation of GNMT, a production NMTsystem at Google, that aims toprovide solutions to the above problems. In our implementation, therecurrent networks are Long Short-Term Memory (LSTM)RNNs [23, 17]. Our LSTM RNNs have 8layers, with residual connections between layers to encourage gradientflow [21]. For parallelism, we connect the attention fromthe bottom layer of the decoder network to the top layer of theencoder network. To improve inference time, we employ low-precisionarithmetic for inference, which is further accelerated by specialhardware (Google’s Tensor Processing Unit, or TPU). To effectivelydeal with rare words, we use sub-word units (also known as“wordpieces”) [35] for inputs and outputs inour system. Using wordpieces gives a good balance between theflexibility of single characters and the efficiency of full words fordecoding, and also sidesteps the need for special treatment of unknownwords. Our beam search technique includes a length normalization procedure todeal efficiently with the problem of comparing hypotheses of differentlengths during decoding, and a coverage penalty to encourage the modelto translate all of the provided input.\begin{split}\mathbf{c}_{t}^{i},\mathbf{m}_{t}^{i}&=\mathrm{LSTM}_{i}(\mathbf{c}_{t-1}^{i},\mathbf{m}_{t-1}^{i},\mathbf{x}_{t}^{i-1};\mathbf{W}^{i})\\\mathbf{x}_{t}^{i}&=\mathbf{m}_{t}^{i}+\mathbf{x}_{t}^{i-1}\\\mathbf{c}_{t}^{i+1},\mathbf{m}_{t}^{i+1}&=\mathrm{LSTM}_{i+1}(\mathbf{c}_{t-1}^{i+1},\mathbf{m}_{t-1}^{i+1},\mathbf{x}_{t}^{i};\mathbf{W}^{i+1})\end{split}(6)Residual connections greatly improve the gradient flow in the backwardpass, which allows us to train very deep encoder and decodernetworks. In most of our experiments, we use 8 LSTM layers for the encoderand decoder, though residual connections can allow us to trainsubstantially deeper networks (similar to what was observedin [45]).In all experiments, our models consist of 8 encoder layers and 8 decoder layers.(Since the bottom encoder layer is actually bi-directional, in total there are9 logically distinct LSTM passes in the encoder.)The attention network is a simple feedforward network with one hidden layer with 1024 nodes.All of the models use 1024 LSTM nodes per encoder and decoder layers.	Authors used 12 LSTM layers for the encoder, and 12 LSTM layers for the decoder with residual connections for both networks, each layer has 1024 node.	Change number		
298	paper_128	What kinds of domain knowledge do the authors refer to in this context?	Authors refer to translation domain knowledge, as they refer to Luong's et al.  (2015) Neural Machine Translation as it reads through all source words until the end of a sentence, then starts translation by emitting one target word at a time as illustrated in Figure 1.	Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1.	Authors refer to translation domain knowledge, as they refer to Luong's et al.  (2015) Neural Machine Translation as it reads through certain source words until the end of a sentence, then starts translation by emitting one target word at a time as illustrated in Figure 1.	Change concept	 (all -> certain)	
299	paper_128	How does the size of a large neural network for NMT affect memory?	Large Neural network NMT has the ability to generalize well to very long word sequences so that it doesn't have to store gigantic phrase tables and language models, which results to having a small memory footprint.	Neural Machine Translation (NMT) achieved state-of-the-art performances inlarge-scale translation tasks such as from English to French [Luong et al., 2015] andEnglish to German [Jean et al., 2015]. NMT is appealing since it requires minimaldomain knowledge and is conceptually simple. The model by ?) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT [Koehn et al., 2003].	Large Neural network NMT doesn't have the ability to generalize well to very long word sequences so it has to store gigantic phrase tables and language models, which results to having a small memory footprint.	Opposite		
300	paper_128	What are the pros and cons of a global approach and a local approach?	A drawback of the global attention is it had to attend to all words on the source side for each target word, which is expensive and potentially will render it impractical to translate longer sequences, and despite that global attention gives a significant boost of +2. 8 BLEU making it better than the base attention system, but the local approach gave further improvement of +0. 9 BLEU on top of the global attention model.  Also the local approach achieved lower AERs.  Not to mention that the local approach is simpler, easier to implement and train, and computationally less expensive.  as it focus only on a small subset of the source positions per target word.	The global attention has a drawback that it has to attend to all words on thesource side for each target word, which is expensive and can potentially render it impractical totranslate longer sequences, e.g., paragraphs or documents.To address this deficiency, we propose a local attentional mechanism thatchooses to focus only on a small subset of the source positions per target word.Our local attention mechanism selectively focuses on a small window ofcontext and is differentiable. This approach has an advantage of avoiding the expensive computation incurred inthe soft attention and at the same time, is easier to train than the hardattention approach.In concrete details, the model first generates an aligned position p_{t} for each target word at time t. Thecontext vector \mbox{\boldmath{$c$}}_{t} is then derived as a weighted average over the set of source hidden states within the window [p_{t}-D,p_{t}+D]; D isempirically selected.888If the window crosses the sentence boundaries, wesimply ignore the outside part and consider words in the window. Unlike the global approach, the local alignment vector \mbox{\boldmath{$a$}}_{t} is now fixed-dimensional, i.e., \in\mathbb{R}^{2D+1}. We consider two variants of the model as below.In this work, we design, with simplicity and effectiveness in mind, two noveltypes of attention-based models: a global approach in which all sourcewords are attended and a local one whereby only a subset of source wordsare considered at a time. The former approach resembles the model of[Bahdanau et al., 2015] but is simpler architecturally. The latter can be viewed as aninteresting blend between the hard and soft attention modelsproposed in [Xu et al., 2015]: it is computationally less expensive than theglobal model or the soft attention; at the same time, unlike the hard attention,the local attention isdifferentiable almost everywhere, making it easier to implement andtrain.222There is a recent work by ?), which is verysimilar to our local attention and applied to the image generation task.However, as we detail later, our model is much simpler and can achieve good performance for NMT. Besides, we also examine variousalignment functions for our attention-based models.As shown in Table 1, we achieve progressive improvements when(a) reversing the source sentence, +1.3 BLEU, as proposed in [Sutskever et al., 2014]and (b) using dropout, +1.4 BLEU. On top of that, (c) the globalattention approach gives a significant boost of +2.8 BLEU, makingour model slightly better than the base attentional system of?) (row RNNSearch). When (d) using the input-feedingapproach, we seize another notable gain of +1.3 BLEU and outperform theirsystem. The local attention model with predictive alignments (row local-p) provesto be even better, giving us a further improvement of +0.9 BLEU on top of theglobal attention model.It is interesting to observe the trend previously reported in[Luong et al., 2015] that perplexity strongly correlates with translation quality.In total, we achieve a significant gain of5.0 BLEU points over the non-attentional baseline, which already includesknown techniques such as source reversing and dropout.We also found that the alignments produced by local attention models achievelower AERs than those of the global one. The AER obtained by the ensemble, whilegood, is not better than the local-m AER, suggesting the well-knownobservation that AER and translation scores are not well correlated [Fraser and Marcu, 2007].We show some alignment visualizations in Appendix A.	None			
301	paper_128	How are hard attention models different from soft attention models?	Soft attention model's weights are placed "softly" over all patches in the source image.  while Hard attention models selects one patch of the image to attend at a time.  it's also, none-differentiable, requires more complicated techniques and less expensive at inference time. 	This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by ?) to tackle the image captiongeneration task. In their work, soft attention refers to the global attentionapproach in which weights are placed “softly” over all patches in the sourceimage. The hard attention, on the other hand, selects one patchof the image to attend to at a time. While less expensive at inference time, thehard attention model is non-differentiable and requires more complicatedtechniques such as variance reduction or reinforcement learning to train.	Soft attention model's weights are placed "softly" over all patches in the source image.  while Hard attention models selects one patch of the image to attend at a time.  it's also, none-differentiable, requires less complicated techniques and more expensive at inference time. 	Change concept	 (less -> more)	
302	paper_128	What kinds of alignment functions are used for their attention-based models?	Authors used location, dot, general and concat alignment functions in their experiments.	We examine different attention models (global, local-m, local-p) and differentalignment functions (location, dot, general, concat) as described inSection 3. Due to limitedresources, we cannot run all the possible combinations.However, results in Table 4 do give us some idea aboutdifferent choices.The location-based function does not learn goodalignments: the global (location) model can only obtain a smallgain when performing unknown word replacement compared to using other alignmentfunctions.141414There is a subtle difference in how we retrieve alignmentsfor the different alignment functions. At time step t in which we receivey_{t-1} as input and then compute \mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$a$}}_{t},\mbox{\boldmath{$c$}}_{t}, and \mbox{\boldmath{$\tilde{h}$}}_{t} beforepredicting y_{t}, the alignment vector \mbox{\boldmath{$a$}}_{t} is used as alignmentweights for (a) the predicted word y_{t} in the location-basedalignment functions and (b) the input word y_{t-1} in the content-basedfunctions.For content-based functions, our implementation concat does not yield good performancesand more analysis should be done to understand thereason.151515With concat, the perplexities achieved by different models are 6.7 (global), 7.1(local-m), and 7.1 (local-p). Such high perplexities could be due to the factthat we simplify the matrix W_{a} to set the part that corresponds to \mbox{\boldmath{$\bar{h}$}}_{s}to identity. It is interesting to observe that dot workswell for the global attention and general is better for the localattention.Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.	None			
303	paper_128	What kinds of RNN architectures were used for the decoder in various prior research?	Previous work included vanilla RNN, LSTM and GRU in the decoder architecture.  as Sutskever and Luon stacked multiple layers of RNN with LSTM hidden unit for the decoder and encoder.  and Cho, Bahdanau and Jeal all adopted GRU.	Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state \mbox{\boldmath{$h$}}_{t} at the top layer of a stacking LSTM. The goal is then to derive a context vector \mbox{\boldmath{$c$}}_{t} that captures relevant source-side information to help predict the current target word y_{t}. While these models differ in how the context vector \mbox{\boldmath{$c$}}_{t} is derived, they share the same subsequent steps.?) used an RNN with the standard hidden unit for the decoder and aconvolutional neural network for encoding the source sentence representation. Onthe other hand, both ?) and ?) stackedmultiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit forboth the encoder and the decoder. ?), ?), and?) all adopted a different version of the RNN with anLSTM-inspired hidden unit, the gated recurrent unit (GRU), for bothcomponents.444They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.In more detail, one can parameterize the probability of decoding each word y_{j} as:p\left(y_{j}|y_{<j},\mbox{\boldmath{$s$}}\right)=\operatorname{softmax}\left(g\left(\mbox{\boldmath{$h$}}_{j}\right)\right)(2)with g being the transformation function that outputs a vocabulary-sizedvector.555One can provide g with other inputs such as the currentlypredicted word y_{j} as in [Bahdanau et al., 2015]. Here, \mbox{\boldmath{$h$}}_{j} is the RNN hiddenunit, abstractly computed as:\mbox{\boldmath{$h$}}_{j}=f(\mbox{\boldmath{$h$}}_{j-1},\mbox{\boldmath{$s$}}),(3)where f computes the current hidden state given the previous hidden state andcan be either a vanilla RNN unit, a GRU, or an LSTM unit. In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation s is only used once to initialize the decoder hidden state. On the other hand, in [Bahdanau et al., 2015, Jean et al., 2015] and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.	Previous work included vanilla RNN, CNN, LSTM and GRU in the decoder architecture.  as Sutskever and Luon stacked multiple layers of RNN with LSTM hidden unit for the decoder and encoder.  and Cho, Bahdanau and Jeal all adopted GRU.	Invent something didn't mentioned		
304	paper_128	How are the RNN architectures used for the decoder in prior research different from each other?	Kalchbrenner and Blunsom used a standard RNN hidden unit for the decoder.  and Sutskever and Luong stacked multiple layers of RNN with Long Short-Term Memory (LSTM) hidden unit for the encoder and the decoder.  on the other hand, Cho, Bahdanau and Jean all adopted different RNN architecture, with Gated Recurrent Unit (GRU) for encoder and decoder.	?) used an RNN with the standard hidden unit for the decoder and aconvolutional neural network for encoding the source sentence representation. Onthe other hand, both ?) and ?) stackedmultiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit forboth the encoder and the decoder. ?), ?), and?) all adopted a different version of the RNN with anLSTM-inspired hidden unit, the gated recurrent unit (GRU), for bothcomponents.444They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.In more detail, one can parameterize the probability of decoding each word y_{j} as:p\left(y_{j}|y_{<j},\mbox{\boldmath{$s$}}\right)=\operatorname{softmax}\left(g\left(\mbox{\boldmath{$h$}}_{j}\right)\right)(2)with g being the transformation function that outputs a vocabulary-sizedvector.555One can provide g with other inputs such as the currentlypredicted word y_{j} as in [Bahdanau et al., 2015]. Here, \mbox{\boldmath{$h$}}_{j} is the RNN hiddenunit, abstractly computed as:\mbox{\boldmath{$h$}}_{j}=f(\mbox{\boldmath{$h$}}_{j-1},\mbox{\boldmath{$s$}}),(3)where f computes the current hidden state given the previous hidden state andcan be either a vanilla RNN unit, a GRU, or an LSTM unit. In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation s is only used once to initialize the decoder hidden state. On the other hand, in [Bahdanau et al., 2015, Jean et al., 2015] and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.	None			
305	paper_128	In this sentence, do the current target state and all source states mean hidden states of the encoder?	However, it's not clear which sentence the questioner refers to and the question needs more elaboration.	The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \mbox{\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \mbox{\boldmath{$h$}}_{t} with each source hidden state \mbox{\boldmath{$\bar{h}$}}_{s}:\displaystyle\mbox{\boldmath{$a$}}_{t}(s)\displaystyle=\operatorname{align}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})(7)\displaystyle=\frac{\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\right)}{\sum_{s^{\prime}}\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s^{\prime}})\right)}Here, \operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\!=\!\begin{cases}\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it dot}}\\\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$W_{a}$}}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it general}}\\\mbox{\boldmath{$v$}}_{a}^{\top}\tanh\left(\mbox{\boldmath{$W_{a}$}}[\mbox{\boldmath{$h$}}_{t};\mbox{\boldmath{$\bar{h}$}}_{s}]\right)&\mbox{{\it concat}}\end{cases}Figure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h¯s. A global context vector ct is then computed as the weighted average, according to at, over all the source states.Figure 3: Local attention model – the model first predicts a single aligned position pt for the current target word. A window centered around the source position pt is then used to compute a context vector ct, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state ht and those source states h¯s in the window.	None			
306	paper_128	What does "variable-length alignment" mean?	a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, and the size of it equals the number of time steps on the source side as it's explained in Figure 2.	The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \mbox{\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \mbox{\boldmath{$h$}}_{t} with each source hidden state \mbox{\boldmath{$\bar{h}$}}_{s}:\displaystyle\mbox{\boldmath{$a$}}_{t}(s)\displaystyle=\operatorname{align}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})(7)\displaystyle=\frac{\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\right)}{\sum_{s^{\prime}}\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s^{\prime}})\right)}Here, \operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\!=\!\begin{cases}\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it dot}}\\\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$W_{a}$}}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it general}}\\\mbox{\boldmath{$v$}}_{a}^{\top}\tanh\left(\mbox{\boldmath{$W_{a}$}}[\mbox{\boldmath{$h$}}_{t};\mbox{\boldmath{$\bar{h}$}}_{s}]\right)&\mbox{{\it concat}}\end{cases}Figure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h¯s. A global context vector ct is then computed as the weighted average, according to at, over all the source states.	a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, but the size of it doesn't equal to the number of time steps on the source side.	Opposite		a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, and the size of it equals the number of time steps on the source side.
307	paper_128	Why did the authors use hidden states only at the top LSTM layers in both the encoder and decoder?	To derive a context vector that captures relevant source-side informations that help predicting the current target word.	Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state \mbox{\boldmath{$h$}}_{t} at the top layer of a stacking LSTM. The goal is then to derive a context vector \mbox{\boldmath{$c$}}_{t} that captures relevant source-side information to help predict the current target word y_{t}. While these models differ in how the context vector \mbox{\boldmath{$c$}}_{t} is derived, they share the same subsequent steps.	None			
309	paper_128	How is the tokenized BLEU different from the NIST BLEU?	A tonkenized BLEU: all text are tokenized with tonkenizer. perl and BLEU scores are computed with multi-bleu.  While NSIT BLEU: with meteval-v13a script as per WMT guideline.	We evaluate the effectiveness of our models on the WMT translation tasks betweenEnglish and German in both directions. newstest2013 (3000 sentences) is used asa development set to select our hyperparameters. Translation performances arereported in case-sensitive BLEU [Papineni et al., 2002] on newstest2014 (2737 sentences) andnewstest2015 (2169 sentences). Following [Luong et al., 2015], we reporttranslation quality using two types of BLEU: (a) tokenized121212All texts are tokenized with tokenizer.perl and BLEUscores are computed with multi-bleu.perl. BLEU to be comparable withexisting NMT work and (b) NIST131313With the mteval-v13ascript as per WMT guideline. BLEU to be comparablewith WMT results.	None			
310	paper_128	What was the size of the model?	The author's model consists of 4 layers of LSTM, each has 100 cells, and 1000-dimensional embeddings.	When training our NMT systems, following [Bahdanau et al., 2015, Jean et al., 2015], we filter outsentence pairs whose lengths exceed 50 words and shuffle mini-batches as weproceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and1000-dimensional embeddings. We follow [Sutskever et al., 2014, Luong et al., 2015] in trainingNMT with similar settings: (a) our parameters are uniformly initialized in[-0.1,0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learningrate schedule is employed – we start with a learning rate of 1; after 5 epochs,we begin to halve the learning rate every epoch, (d) our mini-batch size is 128,and (e) the normalized gradient is rescaled whenever its norm exceeds 5.Additionally, we also use dropout with probability 0.2 for our LSTMs as suggested by[Zaremba et al., 2015]. For dropout models, we train for 12 epochs and start halvingthe learning rate after 8 epochs. For localattention models, we empirically set the window size D=10.	The author's model consists of 8 layers of LSTM, each has 110 cells, and 1000-dimensional embeddings.	Change number		
311	paper_128	In Figure 6, why are the BLEU scores fluctuating when the sentence lengths are less than 40?	Author's model quality is more effective in handling long sentence as the quality doesn't degrade as sentences become longer.  Noting that in Figure 6, only one more measurement point was taken after 40.	We follow [Bahdanau et al., 2015] to group sentences of similar lengths together andcompute a BLEU score per group. Figure 6 shows thatour attentional models are more effective than the non-attentional one inhandling long sentences: the quality does not degrade as sentencesbecome longer. Our best model (the blue + curve) outperforms all other systems in all length buckets.	None			
313	paper_128	How did the attention method contribute to word alignments?	Local attention method had sharper alignment weights than global one, that's due to it's designed to only focus on a subset of words each time.	We visualize the alignment weights produced by our different attention models in Figure 7. The visualization of the local attention model is much sharper than that of the global one. This contrast matches our expectation that local attention is designed to only focus on a subset of words each time.	Local attention method had sharper alignment weights than global one, that's due to it's designed to focus on an entire set of words each time.	Change concept	 (a subset of words -> an entire set of words)	
315	paper_131	The cited papers are in the NLP domain, while this paper targets Text-to-Video generation. How did the authors have confidence in adopting unsupervised learning techniques that could perform well in this Text-to-Video domain as well?	Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) and this paper is inspired by these success.  Thus the authors have confidence in adopting unsupervised learning in Text-to-Video domain.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.	None			
316	paper_131	How short is it? What if the video that I want to generate is longer than its limitation? It would not be very pragmatic if it has too many restrictions in its length.	Leveraging frame rate conditioning, authours enable an additional augmentation method to tackle the limited volume of available videos at training timee, and provides additional control on the generated video at inference time by a varying number of frames-per-second.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Leveraging frame rate conditioning, authours enable an additional augmentation method to tackle the limited volume of available videos at training timee, and provides additional control on the generated video at inference time by a varying number of frames-per-minute.	Change concept	 (frames-per-second -> frames-per-minute)	
317	paper_131	What kind of text prompt does it contain? What were the criteria to set these prompts?	For a more thorough evaluation than existing literature in T2V, the authors collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts and filtered out prompts that were incomplete, too abstract, or offensive and then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.  It is used for zero-shot T2V human evaluation.	Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	For a more thorough evaluation than existing literature in T2V, the authors collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 500 prompts and filtered out prompts that were incomplete, too abstract, or offensive and then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.  It is used for zero-shot T2V human evaluation.	Change number		
318	paper_131	How can we define text-video paired dataset? For example, how long each video should be and how long should be the text description?	There is no alinged text and only the videos are used.  The authors use only public datasets (and no paired text for videos).  A text description describes an image frame in video so it has limitations to associate between text and phenomenon in video.  It needs to depict more detailed stories, is left for future work.  Moreover, for all of experiments they applied extrapolation network↑F with frame skip 5 to upsample a 16 frame video to 76 frames.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	There is no alinged text and only the videos are used.  The authors use only public datasets (and no paired text for videos).  A text description describes an image frame in video so it has limitations to associate between text and phenomenon in video.  It needs to depict more detailed stories, is left for future work.  Moreover, for all of experiments they applied extrapolation network↑F with frame skip 6 to upsample a 15 frame video to 76 frames.	Change number		
319	paper_131	To me, it sounds like an excuse for not collecting the dataset. How difficult is it to collect an open-source text-to-video dataset? What kind of procedure does it contain?	It is hard to collect datasets because a similarly sized (text, video) dataset cannot be easily collected.  For human evaluation, they employ some annotators and filtered out according to their criteria.  Therefore, they are not making an excuse about not collecting the dataset.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	It is not hard to collect datasets because a similarly sized (text, video) dataset can be easily collected.  For human evaluation, they employ some annotators and filtered out according to their criteria.  	Opposite		It is hard to collect datasets because a similarly sized (text, video) dataset cannot be easily collected.  For human evaluation, they employ some annotators and filtered out according to their criteria.  
320	paper_131	Can the number of frames be extended to more than 16? If exists, what is the upper bound for the number of frames?	They train a new masked frame interpolation and extrapolation network ↑F , capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.  Additionally, the spatial super-resolution models enable to increase a higher (controllable) frame rate.  Therefore, using the extrapolation network ↑F, it can possible to extend the video length from 16 frames to 76 frames.	In order to expand the two-dimensional (2D) conditional network into the temporal dimension, we modify the two key building blocks that now require not just spatial but also temporal dimensions in order to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2), discussed in the following two subsections. Other layers, such as fully-connected layers, do not require specific handling when adding an additional dimension, as they are agnostic to structured spatial and temporal information.Temporal modifications are made in most U-Net-based diffusion networks: the spatiotemporal decoder \operatorname{D^{t}} now generating 16 RGB frames, each of size 64\times 64, the newly added frame interpolation network \uparrow_{F}, increasing the effective frame rate by interpolating between the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks \operatorname{SR}_{l}^{t}.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.In addition to the spatiotemporal modifications discussed in Sec. 3.2, we train a new masked frame interpolation and extrapolation network \uparrow_{F}, capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.In order to increase the frame rate within memory and compute constraints, wefine-tune a spatiotemporal decoder \operatorname{D^{t}} on the task of masked frame interpolation, by zero-padding the masked input frames, enabling video upsampling. When fine-tuning on masked frame interpolation, we add an additional 4 channels to the input of the U-Net: 3 channels for the RGB masked video input and an additional binary channel indicating which frames are masked. We fine-tune with variable frame-skips and fps conditioning to enable multiple temporal upsample rates at inference time. We denote \uparrow_{F} as the operator that expands the given video tensor through masked frame interpolation. For all of our experiments we applied \uparrow_{F} with frame skip 5 to upsample a 16 frame video to 76 frames ((16-1)\times5+1). Note that we can use the same architecture for video extrapolation or image animation by masking frames at the beginning or end of a video.Using function-preserving transformations, we extend the spatial layers at the model initialization stage, to include temporal information.The extended spatial-temporal network includes new attention modules that learn temporal world dynamics from a collection of videos. This procedure significantly accelerates the T2V training process by instantaneously transferring the knowledge from a previously trained T2I network to a new T2V one. To enhance the visual quality, we train spatial super-resolution models as well as frame interpolation models. This increases the resolution of the generated videos, as well as enables a higher (controllable) frame rate.	They train a new masked frame interpolation and extrapolation network ↑F , capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.  Additionally, the spatial super-resolution models enable to increase a higher (controllable) frame rate.  Therefore, using the extrapolation network ↑F, it can possible to extend the video length from 16 frames to 86 frames.	Change number		
321	paper_131	How is this x converted to y using which network?	First, a prior network \operatorname{\textbf{P}}, that during inference generates image embeddings y_{e} given text embeddings x_{e} and BPE encoded text tokens \hat{x}.  Second, a decoder network \operatorname{\textbf{D}} that generates a low-resolution 64\times 64 RGB image \hat{y}_{l}, conditioned on the image embeddings y_{e}.  Finally, two super-resolution networks \operatorname{\textbf{SR}}_{\textbf{l}},\operatorname{\textbf{SR}}_{\textbf{h}} that increase the generated image \hat{y}_{l} resolution to 256\times 256 and 768\times 768 pixels respectively, resulting in the final222We then downsample to 512 using bicubic interpolation for a cleaner aesthetic.  Maintaining a clean aesthetic for high definition videos is part of future work.  generated image \hat{y}.	Make-A-Video’s final T2V inference scheme (depicted in Fig. 2) can be formulated as:yt^=SRh∘SRlt∘↑F∘Dt∘P∘(x^,Cx(x)),\hat{y_{t}}=\operatorname{SR}_{h}\circ\operatorname{SR}_{l}^{t}\circ\uparrow_{F}\circ\operatorname{D}^{t}\circ\operatorname{P}\circ(\hat{x},\operatorname{C}_{x}(x)),over^ start_ARG italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = roman_SR start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∘ roman_SR start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∘ ↑ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ∘ roman_D start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∘ roman_P ∘ ( over^ start_ARG italic_x end_ARG , roman_C start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ( italic_x ) ) ,(1)where \hat{y_{t}} is the generated video, \operatorname{SR}_{h},\operatorname{SR}_{l} are the spatial and spatiotemporal super-resolution networks (Sec. 3.2), \uparrow_{F} is a frame interpolation network (Sec. 3.3), \operatorname{D}^{t} is the spatiotemporal decoder (Sec. 3.2), \operatorname{P} is the prior (Sec. 3.1), \hat{x} is the BPE-encoded text, \operatorname{C}_{x} is the CLIP text encoder (Radford et al., 2021), and x is the input text. The three main components are described in detail in the following sections.We use the following networks to produce high-resolution images from text:(i) A prior network \operatorname{\textbf{P}}, that during inference generates image embeddings y_{e} given text embeddings x_{e} and BPE encoded text tokens \hat{x}, (ii) a decoder network \operatorname{\textbf{D}} that generates a low-resolution 64\times 64 RGB image \hat{y}_{l}, conditioned on the image embeddings y_{e}, and (iii) two super-resolution networks \operatorname{\textbf{SR}}_{\textbf{l}},\operatorname{\textbf{SR}}_{\textbf{h}} that increase the generated image \hat{y}_{l} resolution to 256\times 256 and 768\times 768 pixels respectively, resulting in the final222We then downsample to 512 using bicubic interpolation for a cleaner aesthetic. Maintaining a clean aesthetic for high definition videos is part of future work. generated image \hat{y}.	None			
322	paper_131	Does it have to be integrated into the network in an end-to-end manner? I guess it could make the network heavier.	They extend the spatial layers at the model initialization stage, to include temporal information, and the extended spatial-temporal network learn significantly accelerates the T2V training process by instantaneously transferring the knowledgefrom a previously trained T2I network to a new T2V one.  Because of the fact that using 3D convolutional layers is computationally heavy, they followed the work of (Ho et al. , 2022) extending dimension decomposition strategy to attention layers.  In contrast to VDM, they apply an additional 3x1x1 convolution projection (after each 1x3x3) such that the temporal information will also be passed through each convolution layer.	In order to expand the two-dimensional (2D) conditional network into the temporal dimension, we modify the two key building blocks that now require not just spatial but also temporal dimensions in order to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2), discussed in the following two subsections. Other layers, such as fully-connected layers, do not require specific handling when adding an additional dimension, as they are agnostic to structured spatial and temporal information.Temporal modifications are made in most U-Net-based diffusion networks: the spatiotemporal decoder \operatorname{D^{t}} now generating 16 RGB frames, each of size 64\times 64, the newly added frame interpolation network \uparrow_{F}, increasing the effective frame rate by interpolating between the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks \operatorname{SR}_{l}^{t}.A crucial component of T2I networks is the attention layer, where in addition to self-attending to extracted features, text information is injected to several network hierarchies, alongside other relevant information, such as the diffusion time-step. While using 3D convolutional layers is computationally heavy, adding the temporal dimension to attention layers is outright infeasible in terms of memory consumption.Inspired by the work of (Ho et al., 2022), we extend our dimension decomposition strategy to attention layers as well. Following each (pre-trained) spatial attention layer, we stack a temporal attention layer, whichas with the convolutional layers, approximates a full spatiotemporal attention layer. Specifically, given an input tensor h, we define flatten as a matrix operator that flattens the spatial dimension into h^{\prime}\in R^{B\times C\times F\times HW}. unflatten is defined as the inverse matrix operator. The Pseudo-3D attention layer therefore is therefore defined as:Factorized space-time attention layers have also been used in VDM (Ho et al., 2022) and CogVideo (Hong et al., 2022). CogVideo has added temporal layers to each (frozen) spatial layers whereas we train them jointly. In order to force their network to train for images and videos interchangeably, VDM has extended their 2D U-Net to 3D through unflattened 1x3x3 convolution filters, such that the subsequent spatial attention remains 2D, and added 1D temporal attention through relative position embeddings. In contrast, we apply an additional 3x1x1 convolution projection (after each 1x3x3) such that the temporal information will also be passed through each convolution layer.Using function-preserving transformations, we extend the spatial layers at the model initialization stage, to include temporal information.The extended spatial-temporal network includes new attention modules that learn temporal world dynamics from a collection of videos. This procedure significantly accelerates the T2V training process by instantaneously transferring the knowledge from a previously trained T2I network to a new T2V one. To enhance the visual quality, we train spatial super-resolution models as well as frame interpolation models. This increases the resolution of the generated videos, as well as enables a higher (controllable) frame rate.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	None			
323	paper_131	How can this prevent flickering artifacts? Any backup publications for further explanation?	To prevent flickering artifacts, they sustain hallucinating information to be consistent across frames.  They use the same noise initialization for each frame to encourage consistent detail hallucination.  For future works, they explain about thier several technical limitations such as learning association between text and phenomenon.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.	To prevent flickering artifacts, they sustain hallucinating information to be consistent across frames.  They use different noise initialization for each frame to encourage consistent detail hallucination.  For future works, they explain about thier several technical limitations such as learning association between text and phenomenon.	Change concept	 (the same -> different)	
324	paper_131	Does this bring better performance? If so, what is the reasoning for this?	In the limited volume of available videos at training time, conditioning on a varying number of frames-per-second, enables an additional augmentation method, and provides rovides additional control on the generated video at inference time.  In human evaluation experiments, it shows that raters choose this method more realistic motion more than the half of the time.  It is observed that this method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.  Table 1 demonstrates the quantitative results of Make-A-Video.	Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	In the limited volume of available videos at training time, conditioning on a varying number of frames-per-second, enables an additional augmentation method, and provides additional control on the generated video at inference time.  In human evaluation experiments, it shows that raters choose this method more realistic motion more than the half of the time.  It is observed that this method excels when there are little differences between frames where having real-world knowledge of how objects move is crucial.	Change concept	 (large -> little)	In the limited volume of available videos at training time, conditioning on a varying number of frames-per-second, enables an additional augmentation method, and provides additional control on the generated video at inference time.  In human evaluation experiments, it shows that raters choose this method more realistic motion more than the half of the time.  It is observed that this method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial
325	paper_131	How can this value be calculated? Why does the authors set the value as 0.5?	In NSFW images, there are toxic words in the text, or images with a watermark.  Therefore, authors filter out sample pairs with probability larger than 0.  As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones.  Compared to these models, T2I generation model was trained on data that removed NSFW content and toxic words.	Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.	None			
326	paper_131	What if the text prompt is exactly the same? I guess there can be cases where the text prompt is the same, but the videos are different. Did the authors remove such cases prior to running the evaluation?	Authors suggest future works that their our approach can not learn associations between text and phenomenon that can only be inferred in videos.  How to incorporate these (e. , generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	None			
327	paper_131	Does this also guarantee a generalizable performance over several domains? Did the authors evaluate the performance by specific domain?	They collect 300 text prompts for human evaluation and the prompts include 5 categories.  For quantitative results, Make-A-Video outperform CogVideo in both Chinese and English settings that it can infer that Make-AVideo has significantly better generalization capabilities than prior work.  Moreover, table 2 demonstrates that Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo.  It indicates that Make-A-Video can generalize better even to such a specific domain.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.	They collect 300 text prompts for human evaluation and the prompts include 3 categories.  For quantitative results, Make-A-Video outperform CogVideo in both Chinese and English settings that it can infer that Make-AVideo has significantly better generalization capabilities than prior work. 	Change number		They collect 300 text prompts for human evaluation and the prompts include 5 categories.  For quantitative results, Make-A-Video outperform CogVideo in both Chinese and English settings that it can infer that Make-AVideo has significantly better generalization capabilities than prior work. 
328	paper_131	We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release	They collect 300 text prompts and asked annotators what they would be interested in generating if there were a T2V system.  It is used for zero-shot T2V human evaluation which they plan to release.	Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	None			
329	paper_131	As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.	Text describing images does not capture the entirety of phenomena observed in videos.  That said, one can often infer actions and events from static images.  as done in image-based action recognition systems (Girish et al. , 2020).  Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.  the motion of waves at the beach, or of an elephant’s trunk).	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	None			
330	paper_131	Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020).	Unsupervised learning enables networks to learn from orders of magnitude more data.  This large quantity of data is important to learn representations of more subtle, less common concepts in the world.  Unsupervised learning has long had great success in advancing the field of natural language processing (NLP).	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.	Unsupervised learning enables networks to learn from orders of magnitude more data.  This large quantity of data is important to learn representations of less subtle, more common concepts in the world.  Unsupervised learning has long had great success in advancing the field of natural language processing (NLP).	Change concept	 (less -> more)	
331	paper_131	Does lower FVD value mean more coherent generation? What is a coherent video in the first place?	Coherent means a semantically similar video in spite of large differences between frames where having real-world knowledge of how objects move is crucial.	Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Examples of Make-A-Video’s generations are shown in Figure 1. In this section, we will show T2V generation comparison to CogVideo (Hong et al., 2022) and VDM (Ho et al., 2022), and video interpolation comparison to FILM (Reda et al., 2022).In addition, our models can be used for a variety of other tasks such as image animation, video variation, etc. Due to space constraint, we only show a single example of each.Figure 4 (a) shows the comparison of Make-A-Video to CogVideo and VDM. Make-A-Video can generate richer content with motion consistency and text correspondence.Figure 4 (b) shows an example of image animation where we condition the masked frame interpolation and extrapolation network \uparrow_{F} on the image and CLIP image embedding to extrapolate the rest of the video. This allows a user to generate a video using their own image – giving them the opportunity to personalize and directly control the generated video. Figure 4 (c) shows a comparison of our approach to FILM (Reda et al., 2022) on the task of interpolation between two images. We achieve this by using the interpolation model that takes the two images as the beginning and end frames and masks 14 frames in between for generation. Our model generates more semantically meaningful interpolation while FILM seems to primarily smoothly transition between frames without semantic real-world understanding of what is moving. Figure 4 (d) shows an example for video variation. We take the average CLIP embedding of all frames from a video as the condition to generate a semantically similar video.More video generation examples and applications can be found here: make-a-video.github.io.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	None			
332	paper_131	What are the reasons for such results? Why does it excel in such cases?	A model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method.  A diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.  They leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	A model that has only seen text describing images is not effective at generating short videos, as demonstrated by the authors' temporal diffusion-based method.	Opposite		A model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by the authors' temporal diffusion-based method.
333	paper_131	I believe T2I models can do this using latent exploration. What is the difference between them? What is novel about T2V’s interpolation?	A frame interpolation network for high frame rate generation can make a semantically similar video by taking the average CLIP embedding of all frames from a video as the condition.	Make-A-Video consists of three main components: (i) A base T2I model trained on text-image pairs (Sec. 3.1), (ii) spatiotemporal convolution and attention layers that extend the networks’ building blocks to the temporal dimension (Sec. 3.2), and (iii) spatiotemporal networks that consist of both spatiotemporal layers, as well as another crucial element needed for T2V generation - a frame interpolation network for high frame rate generation (Sec. 3.3).Using function-preserving transformations, we extend the spatial layers at the model initialization stage, to include temporal information.The extended spatial-temporal network includes new attention modules that learn temporal world dynamics from a collection of videos. This procedure significantly accelerates the T2V training process by instantaneously transferring the knowledge from a previously trained T2I network to a new T2V one. To enhance the visual quality, we train spatial super-resolution models as well as frame interpolation models. This increases the resolution of the generated videos, as well as enables a higher (controllable) frame rate.Examples of Make-A-Video’s generations are shown in Figure 1. In this section, we will show T2V generation comparison to CogVideo (Hong et al., 2022) and VDM (Ho et al., 2022), and video interpolation comparison to FILM (Reda et al., 2022).In addition, our models can be used for a variety of other tasks such as image animation, video variation, etc. Due to space constraint, we only show a single example of each.Figure 4 (a) shows the comparison of Make-A-Video to CogVideo and VDM. Make-A-Video can generate richer content with motion consistency and text correspondence.Figure 4 (b) shows an example of image animation where we condition the masked frame interpolation and extrapolation network \uparrow_{F} on the image and CLIP image embedding to extrapolate the rest of the video. This allows a user to generate a video using their own image – giving them the opportunity to personalize and directly control the generated video. Figure 4 (c) shows a comparison of our approach to FILM (Reda et al., 2022) on the task of interpolation between two images. We achieve this by using the interpolation model that takes the two images as the beginning and end frames and masks 14 frames in between for generation. Our model generates more semantically meaningful interpolation while FILM seems to primarily smoothly transition between frames without semantic real-world understanding of what is moving. Figure 4 (d) shows an example for video variation. We take the average CLIP embedding of all frames from a video as the condition to generate a semantically similar video.More video generation examples and applications can be found here: make-a-video.github.io.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	None			
334	paper_131	Is there any other method to do this instead of simply averaging the values out? There can be a smarter way to do this.	Pseudo-3D convolutional layers facilitates information sharing between the spatial and temporal axes, without succumbing to the heavy computational load of 3D conv layers.  Additionally, conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.Motivated by separable convolutions (Chollet, 2017), we stack a 1D convolution following each 2D convolutional (conv) layer, as shown in Fig. 3. This facilitates information sharing between the spatial and temporal axes, without succumbing to the heavy computational load of 3D conv layers. In addition, it creates a concrete partition between the pre-trained 2D conv layers and the newly initialized 1D conv layers, allowing us to train the temporal convolutions from scratch, while retaining the previously learned spatial knowledge in the spatial convolutions’ weights.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Pseudo-3D convolutional layers can't facilitates information sharing between the spatial and temporal axes without succumbing to the heavy computational load of 3D conv layers.  Additionally, conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Change concept	 (facilitates, without -> can't facilitate without)	
335	paper_131	the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today’s image generation models.	Modeling videos require expensive computational complexity that it is challenging in high-quality video data collection.  Thus, large-scale paired text-video is expensive as well.  Because of the limitations, the progress of T2V generation lags behind.	As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.Leveraging Image Priors for Video Generation.Due to the complexity of modeling videos and the challenges in high-quality video data collection, it is natural to consider leveraging image priors for videos to simplifying the learning process. After all, an image is a video with a single frame (Bain et al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates video generation as the task of finding a trajectory in the latent space of a pre-trained and fixed image generation model. In T2V generation, NÜWA (Wu et al., 2021b) combines image and video datasets in a multitask pre-training stage to improve model generalization for fine-tuning. CogVideo (Hong et al., 2022) uses a pre-trained and fixed T2I model for T2V generation with only a small number of trainable parameters to reduce memory usage during training. But the fixed autoencoder and T2I models can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable joint image and video generation. However, they sample random independent images from random videos as their source of images, and do not leverage the massive text-image datasets.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Modeling videos require expensive computational complexity that it is challenging in general video data collection.  Thus, large-scale paired text-video is expensive as well.  Because of the limitations, the progress of T2V generation lags behind.	Change concept	 (high-quality -> general)	
336	paper_131	How could this vastness be defined or quantitatively measured?	They employed annotators to make prompts and filtered out them correctly.  Evaluation is done about video quality and faithfulness.  For each comparison, 5 different annotators are employed.  They report FVD and IS on 10K samples and generate samples that follow the same class distribution as the training set.  Moreover, for MSR-VTT, FID and CLIPSIM are introduced.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.	They employed annotators to make prompts and filtered out them correctly.  Evaluation is done about video quality and faithfulness.  For each comparison, 5 different annotators are employed.  They report FVD and IS on 16K samples and generate samples that follow the same class distribution as the training set.  Moreover, for MSR-VTT, FID and CLIPSIM are introduced.	Change number		
337	paper_131	Does this use text input as well or not? I thought it should use a text prompt to reflect a natural flow of images, but it does not seem to.	Make-A-Video adopt unsupervised learning method by leveraging joint text-image prior that it is not need paried text-video data.  But, for training of the prior \operatorname{P}, text input is required.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Make-A-Video adopt supervised learning method by leveraging joint text-image prior that it is not need paried text-video data.  But, for training of the prior \operatorname{P}, text input is required.	Change concept	 (unsupervised -> supervised)	
338	paper_131	Can’t it be generated by video interpolation? I thought we can do this by giving two images and running interpolation.	Figure 4 (c) compares the task of interpolation between two images.  A frame interpolation network generates high frame rate and it can be interpreted as interpolating between two images.	Make-A-Video’s final T2V inference scheme (depicted in Fig. 2) can be formulated as:yt^=SRh∘SRlt∘↑F∘Dt∘P∘(x^,Cx(x)),\hat{y_{t}}=\operatorname{SR}_{h}\circ\operatorname{SR}_{l}^{t}\circ\uparrow_{F}\circ\operatorname{D}^{t}\circ\operatorname{P}\circ(\hat{x},\operatorname{C}_{x}(x)),over^ start_ARG italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = roman_SR start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∘ roman_SR start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∘ ↑ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ∘ roman_D start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∘ roman_P ∘ ( over^ start_ARG italic_x end_ARG , roman_C start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ( italic_x ) ) ,(1)where \hat{y_{t}} is the generated video, \operatorname{SR}_{h},\operatorname{SR}_{l} are the spatial and spatiotemporal super-resolution networks (Sec. 3.2), \uparrow_{F} is a frame interpolation network (Sec. 3.3), \operatorname{D}^{t} is the spatiotemporal decoder (Sec. 3.2), \operatorname{P} is the prior (Sec. 3.1), \hat{x} is the BPE-encoded text, \operatorname{C}_{x} is the CLIP text encoder (Radford et al., 2021), and x is the input text. The three main components are described in detail in the following sections.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Examples of Make-A-Video’s generations are shown in Figure 1. In this section, we will show T2V generation comparison to CogVideo (Hong et al., 2022) and VDM (Ho et al., 2022), and video interpolation comparison to FILM (Reda et al., 2022).In addition, our models can be used for a variety of other tasks such as image animation, video variation, etc. Due to space constraint, we only show a single example of each.Figure 4 (a) shows the comparison of Make-A-Video to CogVideo and VDM. Make-A-Video can generate richer content with motion consistency and text correspondence.Figure 4 (b) shows an example of image animation where we condition the masked frame interpolation and extrapolation network \uparrow_{F} on the image and CLIP image embedding to extrapolate the rest of the video. This allows a user to generate a video using their own image – giving them the opportunity to personalize and directly control the generated video. Figure 4 (c) shows a comparison of our approach to FILM (Reda et al., 2022) on the task of interpolation between two images. We achieve this by using the interpolation model that takes the two images as the beginning and end frames and masks 14 frames in between for generation. Our model generates more semantically meaningful interpolation while FILM seems to primarily smoothly transition between frames without semantic real-world understanding of what is moving. Figure 4 (d) shows an example for video variation. We take the average CLIP embedding of all frames from a video as the condition to generate a semantically similar video.More video generation examples and applications can be found here: make-a-video.github.io.Leveraging Image Priors for Video Generation.Due to the complexity of modeling videos and the challenges in high-quality video data collection, it is natural to consider leveraging image priors for videos to simplifying the learning process. After all, an image is a video with a single frame (Bain et al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates video generation as the task of finding a trajectory in the latent space of a pre-trained and fixed image generation model. In T2V generation, NÜWA (Wu et al., 2021b) combines image and video datasets in a multitask pre-training stage to improve model generalization for fine-tuning. CogVideo (Hong et al., 2022) uses a pre-trained and fixed T2I model for T2V generation with only a small number of trainable parameters to reduce memory usage during training. But the fixed autoencoder and T2I models can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable joint image and video generation. However, they sample random independent images from random videos as their source of images, and do not leverage the massive text-image datasets.	None			
339	paper_132	What does this initial results mean?	Video diffusion models present the first results on a large text-conditioned video generation tasks, and they achieve state-of-the-art results on popular video datasets.  They train the model with image-video jointly to improve sample quality.  Moreover, the conditional sampling method, introduced in Section 3. 1, shows better quality compared to the existing replacement method.	To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Table 5 reports results that verify the effectiveness of classifier-free guidance (Ho and Salimans, 2021) on text-to-video generation. As expected, there is clearimprovement in the Inception Score-like metrics with higher guidance weight, while the FID-like metrics improve and then degrade with increasing guidance weight. Similar findings have been reported on text-to-image generation (Nichol et al., 2021).Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.	Video diffusion models present the first results on a large text-conditioned image generation tasks, and they achieve state-of-the-art results on popular video datasets.  They train the model with image-video jointly to improve sample quality.	Change concept	 (video -> image)	Video diffusion models present the first results on a large text-conditioned video generation tasks, and they achieve state-of-the-art results on popular video datasets.  They train the model with image-video jointly to improve sample quality.
340	paper_132	Does making higher resolution have to be incorporated into the network? Can't we do this as a separate process?	Video diffusion models modify little of the archicture to accommodate video data within the memory constraints of deep learning accelerators.  They approach with the standard diffusion modelformalism.  In their method, one of skill to make high resolution video is the spatial upsampling introduced by Menick and Kalchbrenner (2019). Also, reconstruction guidance is extended to constuct the high-resolution model.  When they have low resolution ground truth videos, it upsamples them into high resolution videos using an unconditional high resolution diffusion model.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in which the mean squared error loss is imposed on a downsampled version of the model prediction, and backpropagation is performed through this downsampling.In this setting, we have low resolution ground truth videos \mathbf{x}^{a} (e.g. at the 64x64 spatial resolution), which may be generated from a low resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128 spatial resolution) using an unconditional high resolution diffusion model \hat{\mathbf{x}}_{\theta}. To accomplish this, we adjust the high resolution model as follows:𝐱~θ(𝐳t)=𝐱^θ(𝐳t)−wr⁢αt2∇𝐳t∥𝐱a−𝐱^θa(𝐳t)∥22\displaystyle\tilde{\mathbf{x}}_{\theta}(\mathbf{z}_{t})=\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t})-\frac{w_{r}\alpha_{t}}{2}\nabla_{\mathbf{z}_{t}}\lVert\mathbf{x}^{a}-\hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t})\rVert_{2}^{2}over~ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ∇ start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(8)where \hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t}) is our model’s reconstruction of the low-resolution video from \mathbf{z}_{t}, which is obtained by downsampling the high resolution output of the model using a differentiable downsampling algorithm such as bilinear interpolation.Note that it is also possible to simultaneously condition on low resolution videos while autoregressively extending samples at the high resolution using the same reconstruction guidance method. In Fig. 2, we show samples of this approach for extending 16x64x64 low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128 diffusion model.Our approach to video generation using diffusion models is to use the standard diffusion model formalism described in Section 2 with a neural network architecture suitable for video data. Each of our models is trained to jointly model a fixed number of frames at a fixed spatial resolution. To extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our models with a conditioning technique described later in Section 3.1.	Video diffusion models modify little of the archicture to accommodate video data within the memory constraints of deep learning accelerators.  They approach with the standard diffusion modelformalism.  In their method, the only to make high resolution video is the spatial upsampling introduced by Menick and Kalchbrenner (2019). Also, reconstruction guidance is extended to constuct the high-resolution model.  When they have low resolution ground truth videos, it upsamples them into high resolution videos using an unconditional high resolution diffusion model.	Change concept	 (one of the skills -> the only skill)	
341	paper_132	What does it mean to perform better in Text-to-Video generation? Does it mean that generated videos are aligned well with the text description?	The higher performance in Text-to-Video Generation requires not only excellent fidelity of video samples but also good handling of social bias in text-description given as a condition.	Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.Our goal with this work is to advance research on methods in generative modeling, and our methods have the potential to positively impact creative downstream applications. As with prior work in generative modeling, however, our methods have the potential for causing harmful impact and could enhance malicious or unethical uses of generative models, such as fake content generation, harassment, and misinformation spread, and thus we have decided not to release our models. Like all generative models, our models reflect the biases of their training datasets and thus may require curation to ensure fair results from sampling. In particular, our text-to-video models inherit the challenges faced by prior work on text-to-image models, and our future work will involve auditing for forms of social bias, similar to Buolamwini and Gebru (2018); Burns et al. (2018); Steed and Caliskan (2021); Cho et al. (2022) for image-to-text and image labeling models. We see our work as only a starting point for further investigation on video diffusion models and investigation into their societal implications, and we will aim to explore benchmark evaluations for social and cultural bias in the video generation setting and make the necessary research advances to address them.	The higher performance in Text-to-Video Generation requires only excellent fidelity of video samples.	Change concept	 (requires not only -> requires only)	
342	paper_132	What does this condition include? Text input?	Video Diffusion Model can be conditioned on text descriptions or image frame. When conditioned on a text description, they generate a video explaining the text.	We report our results on video diffusion models for unconditional video generation (Section 4.1), conditional video generation (video prediction) (Section 4.2), and text-conditioned video generation (Section 4.3). We evaluate our models using standard metrics such as FVD (Unterthiner et al., 2018), FID (Heusel et al., 2017), and IS (Salimans et al., 2016); details on evaluation are provided below alongside each benchmark.Samples and additional results are provided at https://video-diffusion.github.io/.Architecture hyperparameters, training details, and compute resources are listed in Appendix A.A common benchmark task for evaluating generative models of video is video prediction, where the model is given the first frame(s) of a video and is asked to generate the remainder. Models that do well on this conditional generation task are usually trained explicitly for this conditional setting, for example by being autoregressive across frames. Although our models are instead only trained unconditionally, we can adapt them to the video prediction setting by using the guidance method proposed in section 3.1. Here we evaluate this method on two popular video prediction benchmarks, obtaining state-of-the-art results.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.In the conditional generation setting, the data \mathbf{x} is equipped with a conditioning signal \mathbf{c}, which may represent a class label, text caption, or other type of conditioning. To train a diffusion model to fit p(\mathbf{x}|\mathbf{c}), the only modification that needs to be made is to provide \mathbf{c} to the model as \hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t},\mathbf{c}). Improvements to sample quality can be obtained in this setting by using classifier-free guidance (Ho and Salimans, 2021). This method samples using adjusted model predictions \tilde{{\boldsymbol{\epsilon}}}_{\theta}, constructed via\displaystyle\tilde{{\boldsymbol{\epsilon}}}_{\theta}(\mathbf{z}_{t},\mathbf{c})=(1+w){\boldsymbol{\epsilon}}_{\theta}(\mathbf{z}_{t},\mathbf{c})-w{\boldsymbol{\epsilon}}_{\theta}(\mathbf{z}_{t}),(6)where w is the guidance strength, {\boldsymbol{\epsilon}}_{\theta}(\mathbf{z}_{t},\mathbf{c})=\frac{1}{\sigma_{t}}(\mathbf{z}_{t}-\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t},\mathbf{c})) is the regular conditional model prediction, and {\boldsymbol{\epsilon}}_{\theta}(\mathbf{z}_{t}) is a prediction from an unconditional model jointly trained with the conditional model (if \mathbf{c} consists of embedding vectors, unconditional modeling can be represented as \mathbf{c}=\mathbf{0}). For w>0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal \mathbf{c}, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model (Ho and Salimans, 2021). The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(\mathbf{c}|\mathbf{z}_{t}) has high likelihood, and is an adaptation of the explicit classifier guidance method proposed by Dhariwal and Nichol (2021).	None			
343	paper_132	How does the authors accommodate the video datasets?	Other diffusion models that generate images use a 2D U-Net, but they use a 3D U-Net to handle video. A 3D U-Net diffusion model is used to generate a fixed number of video frames. A 2D U-Net is modified into each 2D convolution into a space-only 3D convolution, and inserted a temporal attention block that performs attention over the first axis and treats the spatial axes as batch axes. Authors concatenate random independent image frames to the end of each videosampled from the dataset and they choose these random independent images from random videos within the same dataset.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.We propose to extend this image diffusion model architecture to video data, given by a block of a fixed number of frames, using a particular type of 3D U-Net (Çiçek et al., 2016) that is factorized over space and time. First, we modify the image model architecture by changing each 2D convolution into a space-only 3D convolution, for instance, we change each 3x3 convolution into a 1x3x3 convolution (the first axis indexes video frames, the second and third index the spatial height and width). The attention in each spatial attention block remains as attention over space; i.e., the first axis is treated as a batch axis. Second, after each spatial attention block, we insert a temporalattention block that performs attention over the first axis and treats the spatial axes as batch axes.We use relative position embeddings (Shaw et al., 2018) in each temporal attention block so that the network can distinguish ordering of frames in a way that does not require an absolute notion of video time. We visualize the model architecture in Fig. 1.	A 3D U-Net diffusion model is used to generate a fixed number of video frames. A 2D U-Net is modified into each 2D convolution into a space-only 3D convolution, and inserted a temporal attention block that performs attention over the first axis and treats the spatial axes as batch axes. Authors concatenate ordered independent image frames to the end of each videosampled from the dataset and they choose these ordered independent images from indicated videos within the same dataset.	Change concept	 (random -> ordered, indicated)	
344	paper_132	I was wondering whether this results came from various settings (e.g., training only on video dataset).	Video Diffusion Models demonstrates their inital attempt to generate text-to-image generation results in various settings such as classifier-free guidance, jointly training of video-image, and unconditional and conditional generation. They consider several additional image frames for joint training of video-image. Moreover, they adjust the weight of classifier-free guidance, and conditioning method with the newly proposed reconstruction guidance for autoregressive extension and simultaneous spatial and temporal super-resolution.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.We report our results on video diffusion models for unconditional video generation (Section 4.1), conditional video generation (video prediction) (Section 4.2), and text-conditioned video generation (Section 4.3). We evaluate our models using standard metrics such as FVD (Unterthiner et al., 2018), FID (Heusel et al., 2017), and IS (Salimans et al., 2016); details on evaluation are provided below alongside each benchmark.Samples and additional results are provided at https://video-diffusion.github.io/.Architecture hyperparameters, training details, and compute resources are listed in Appendix A.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.	Video Diffusion Models demonstrates their inital attempt to generate text-to-image generation results in various settings such as classifier-free guidance, jointly training of video-image, video-text, text-image and unconditional and conditional generation. They consider several additional image frames for joint training of video-image. Moreover, they adjust the weight of classifier-free guidance, and conditioning method with the newly proposed reconstruction guidance for autoregressive extension and simultaneous spatial and temporal super-resolution.	Invent something didn't mentioned		
345	paper_132	Why does it have to be fixed? Can't we extend it to more frames?	Due to memory constraints of deep learning accelerators, a fixed number of video frames should be used. If memory constraints are addressed, a larger number of frames can be used. To address this issue, they introduce joint training on video and image. They concatenate random independent image frames to the end of each video sampled from the dataset to consider more frames during training and implement a memory optimization to fit more independent examples in a batch.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.	Due to memory constraints of deep learning accelerators, a random number of video frames should be used. If memory constraints are addressed, a larger number of frames can be used. To address this issue, they introduce joint training on video and image. They concatenate random independent image frames to the end of each video sampled from the dataset to consider more frames during training and implement a memory optimization to fit more independent examples in a batch.	Change concept	 (random -> ordered, indicated)	
346	paper_132	What is the reason for doing the joint training? Does it related to the model performance?	Due to memory limit, authors consider newly joint training method utilizing both image and video. As more independent image frames are added, we can see the reduced variane of the gradient at the expense of some bias for the video modeling. Table 4 shows that additional frames per video helps to improve in video and image sample quality metrics.	As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	Due to memory limit, authors consider newly joint training method utilizing text, image and video. As more independent image frames are added, we can see the reduced variane of the gradient at the expense of some bias for the video modeling. Table 4 shows that additional frames per video helps to improve in video and image sample quality metrics.	Invent something didn't mentioned		
347	paper_132	Is there a rule or criteria to have certain number of frames per second for a video? I think the number of frames per second can bring some bias in training.	To manage the computational requirements of training our models, they only train on a small subset of say 16 frames at a time.  But thier newly introduced joint training on video and image modeling, they concatenate random independent image frames to the end of each video sampled from the dataset.  Due to memory constraints, they use fixed number of frames but these randomly sampled frames helps to reduce bias in training. For evaluation, they adopt fixed number of conditioning samples and generating a sequence of video frames to compare other baselines.	The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.A common benchmark task for evaluating generative models of video is video prediction, where the model is given the first frame(s) of a video and is asked to generate the remainder. Models that do well on this conditional generation task are usually trained explicitly for this conditional setting, for example by being autoregressive across frames. Although our models are instead only trained unconditionally, we can adapt them to the video prediction setting by using the guidance method proposed in section 3.1. Here we evaluate this method on two popular video prediction benchmarks, obtaining state-of-the-art results.We evaluate video prediction performance on BAIR Robot Pushing (Ebert et al., 2017), a standard benchmark in the video literature consisting of approximately 44000 videos of robot pushing motions at the 64x64 spatial resolution. Methods for this benchmark are conditioned on 1 frame and generate the next 15. Results are listed in Table 3. Following the evaluation protocol of Babaeizadeh et al. (2021) and others, we calculate FVD (Unterthiner et al., 2018) using the I3D network (Carreira and Zisserman, 2017) by comparing 100\times 256 model samples against the 256 examples in the evaluation set.We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.	None			
348	paper_132	I understand that the quality of xb depends on xa. So the quality could get worse if we generate more frames?	It enables generating longer videos by applying this model autoregressively using a new method for a conditional generation.  Also, P4 explains that the conditioning method helps the model outperform the existing method.  The samples from the reconstruction guidance method are temporally coherent over the course of the entire autoregressive generation process and we can infer that the quality is not affected by generated frames.	The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.Figure 4 shows the samples of our reconstruction guidance method for conditional sampling compared to the replacement method (Section 3.1) for the purposes of generating long samples in a block-autoregressive manner (Section 4.3.3). The samples from the replacement method clearly show a lack of temporal coherence, since frames from different blocks throughout the generated videos appear to be uncorrelated samples (conditioned on \mathbf{c}). The samples from the reconstruction guidance method, by contrast, are clearly temporally coherent over the course of the entire autoregressive generation process. Figure 2 additionally shows samples of using the reconstruction guidance method to simultaneously condition on low frequency, low resolution videos while autoregressively extending temporally at a high resolution.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	It enables generating shorter videos by applying this model autoregressively using a new method for a conditional generation.  The authors explain that the conditioning method helps the model outperform the existing method.  The samples from the reconstruction guidance method are temporally coherent over the course of the entire autoregressive generation process and we can infer that the quality is not affected by generated frames.	Change concept	 (longer -> shorter)	It enables generating longer videos by applying this model autoregressively using a new method for a conditional generation.  The authors explain that the conditioning method helps the model outperform the existing method.  The samples from the reconstruction guidance method are temporally coherent over the course of the entire autoregressive generation process and we can infer that the quality is not affected by generated frames.
349	paper_133	Why did the authors focus on the verb? Is there any reason?	The T2V generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.  They use the pre-trained T2I model which is able to generate images that align well with the text, including the verb terms.	Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Formally, given a video \mathcal{V}=\left\{v_{i}|i\in[1,m]\right\} with m frames, accompanied with a corresponding textual description \mathcal{T}, our objective is to generate novel videos \hat{\mathcal{V}} driven by text prompts \hat{\mathcal{T}} using a pre-trained T2I model M, i.e., M(\hat{\mathcal{V}}|\mathcal{V},\mathcal{T},\hat{\mathcal{T}}), where \mathcal{T} and \hat{\mathcal{T}} share the same verbs.Examples of output variations driven by \hat{\mathcal{T}} include: changes of subject, background (e.g., the place where the subject is), attribute (e.g., color, age, etc.), and other semantic modifications (see Fig. 1 and Fig. 7).Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.	The T2V generator is expected to capture necessary emotion knowledge from the input video and synthesize novel videos guided by edited prompts.  They use the pre-trained T2I model which is able to generate images that align well with the text, including the adjective and verb terms.	Change concept	 (longer -> shorter)	
350	paper_133	Just out of curiosity, how humans are so well at this? Cab human's technique be used for the machines as well? or do we need a totally different approach?	One-Shot Tuning acquires temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SCAttn) and temporal self-attention (Temp-Attn).  It aptures spatial information and yields similar semantics as the training video to perform semantic mixing.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.One of the applications of our Tune-A-Video is to replace the subject in the training video. As shown in Fig. 7, Tune-A-Video is able to generate videos with customized subjects via changing the corresponding terms in text prompt, for example, replacing the polar bear with mammoth (the 2nd row of Fig. 7) in the “walking bear” example; replacing the man with King Kong or astronaut (the 7th and 8th row of Fig. 7) in the “running man” example.The generated videos are consistent in time and well-aligned with the modified text prompts.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.The models without One-Shot Tuning uses the weight of pre-trained T2I for inference. The tuning itself (the 2nd row of Fig. 10) captures spatial information and yields similar semantics as the training video (e.g., the panda skis like the man in training video). This implies that our one-shot tuning strategy is capable of performing semantic mixing as [24, 21], but with more flexibility that the object categories can be replaced. However, the temporal consistency cannot be maintained through individual frame attention (e.g., the pandas in the 2nd row of Fig. 10 are inconsistent). Although our Tune-A-Video w/o tuning (the 3nd row of Fig. 10) output consistent content across frames, it does not have a notion of motion since it is only trained on static images. With tuning, our full model (the 4nd row of Fig. 10) is able to generate the temporally-coherent video that incorporates the motion information (i.e., skiing) in the training video, demonstrating the effectiveness of our One-Shot Tuning.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	One-Shot Tuning doesn't aqcuire temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SCAttn) and temporal self-attention (Temp-Attn).  It captures spatial information and yields similar semantics as the training video to perform semantic mixing.	Opposite		One-Shot Tuning acquires temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SCAttn) and temporal self-attention (Temp-Attn).  It captures spatial information and yields similar semantics as the training video to perform semantic mixing.
351	paper_133	How efficient it is?	Tune-A-Video is based on a pre-trained T2I diffusion model and only updates the projection matrices in attention blocks, with the rest of parameters being frozen.  Moreover, SC-Attn reduce the computational complexity compared to CogView2.	We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.We provide quantitative and qualitative comparisons with CogVideo [19] (the only public333https://github.com/THUDM/CogVideo T2V generation model). CogVideo is based on a pre-trained T2I model CogView2 [4] and consists of 9.4 billion parameters (around 6\times larger than our Tune-A-Video). It is extensively trained on a large-scale dataset of 5.4 million captioned videos.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Tune-A-Video is based on a pre-trained T2I diffusion model and only updates the projection matrices in attention blocks without freezing other parameters.  Moreover, SC-Attn reduce the computational complexity compared to CogView2.	Change concept	 (longer -> shorter)	
352	paper_133	How about other terms like adjective?	The authors examine the ability of attribute modification with the adjectives like color annd chage.  Also, style transfer demonstrates the examples.	Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Formally, given a video \mathcal{V}=\left\{v_{i}|i\in[1,m]\right\} with m frames, accompanied with a corresponding textual description \mathcal{T}, our objective is to generate novel videos \hat{\mathcal{V}} driven by text prompts \hat{\mathcal{T}} using a pre-trained T2I model M, i.e., M(\hat{\mathcal{V}}|\mathcal{V},\mathcal{T},\hat{\mathcal{T}}), where \mathcal{T} and \hat{\mathcal{T}} share the same verbs.Examples of output variations driven by \hat{\mathcal{T}} include: changes of subject, background (e.g., the place where the subject is), attribute (e.g., color, age, etc.), and other semantic modifications (see Fig. 1 and Fig. 7).We provide several applications of our Tune-A-Video in generating videos with novel visual concepts (e.g., subjects, backgrounds, attributes, styles, etc.) guided by the text prompt. Here, we showcase two training examples: “a polar bear is walking on ice” (“walking bear”, the 1st row of Fig. 7), “a young man is running on the beach” (“running man”, the 5th row of Fig. 7), and their variations with edited prompts. More examples can be found in Fig. 12-14.We examine the ability of our Tune-A-Video in modifying the attributes such as color (e.g., a man wearing yellow to be in red, the 2nd row of Fig. 1) and age (e.g., a young man to become old, the 6th row of Fig. 7). We found that our Tune-A-Video even enables modifying the attribute that is not specified in the training prompt (e.g., we do not tell the model the skiing man in the training video wears yellow clothes). Notably, other properties in training video could be well preserved (e.g., the black backpack wore by the skiing man, and the blue shirts and black shorts wore by the running man).Thanks to the broad knowledge of pertrained T2I models, Tune-A-Video is able to transfer videos into different styles, which cannot be easily learned from video data [36]. By appending the style description (e.g., comic style) to the sentence, our Tune-A-Video yields outputs in that style with consistent motions and semantics (the 4th and 8th row of Fig .7).To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	None			
353	paper_133	What is the reason for adopting this?	To imitate human's ability to adapt new visual concets, One-Shot Video Generation task is proposed.  Tune-A-Video generates videos with novel visual concepts (e. , subjects, backgrounds, attributes, styles, etc. ) guided by the text prompt.  It is expensive to finetune T2I models on large-scale text-video datasets and not affordable to everyone.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We provide several applications of our Tune-A-Video in generating videos with novel visual concepts (e.g., subjects, backgrounds, attributes, styles, etc.) guided by the text prompt. Here, we showcase two training examples: “a polar bear is walking on ice” (“walking bear”, the 1st row of Fig. 7), “a young man is running on the beach” (“running man”, the 5th row of Fig. 7), and their variations with edited prompts. More examples can be found in Fig. 12-14.Our Tune-A-Video also enables changing the video background (i.e., place where the subject is), while keeping the motions of the subject and the temporal information consistent. For example, we can “send” a polar bear walking on the ice to Time Square (the 3rd of Fig. 7), and a man running on the beach to mountain (the 6th row of Fig. 7). We observe that some background semantic is tied to the subject, e.g., in the case of “walking bear”, although the background is replaced with Time Square, the color of the ground still remains similar to ice. We conjecture this is due to the strong regularities between “polar bear” and “ice”.We examine the ability of our Tune-A-Video in modifying the attributes such as color (e.g., a man wearing yellow to be in red, the 2nd row of Fig. 1) and age (e.g., a young man to become old, the 6th row of Fig. 7). We found that our Tune-A-Video even enables modifying the attribute that is not specified in the training prompt (e.g., we do not tell the model the skiing man in the training video wears yellow clothes). Notably, other properties in training video could be well preserved (e.g., the black backpack wore by the skiing man, and the blue shirts and black shorts wore by the running man).We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	To imitate human's ability to adapt new visual concets, One-Shot Video Generation task is proposed.  Tune-A-Video generates videos with novel visual concepts (e. , subjects, backgrounds, attributes, styles, etc. ) guided by the given images. 	Change concept	 (by the text prompt -> by the given images)	To imitate human's ability to adapt new visual concets, One-Shot Video Generation task is proposed.  Tune-A-Video generates videos with novel visual concepts (e. , subjects, backgrounds, attributes, styles, etc. ) guided by the text prompt.
354	paper_133	Does it have any performance degradation if we generate too many frames?	Figure 8 shows that the VDM baselines with factorized space-time attention fail to generate consistent content.  But Tune-A-Video can generate better temporal consistency video.	We first follow VDM baselines [17, 14] to use inflated 1\times 3\times 3 convolution kernels for video inputs and append temporal self-attention with causal mask for temporal modeling. However, we empirically observe that factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation (see Sec. 4.3). We further inflate the spatial self-attention to cross-frame attention as follows.There are alternative options for cross-frame attention in Fig. 6, including full attention and causal attention which also capture spatio-temporal consistency.However, such straightforward choices are actually not feasible in generating videos with continuous frames due to their high computational complexity.Specifically, given m frames and N sequences for each frame, the complexity for full attention is \mathcal{O}((mN)^{2}).It is not affordable if we need to generate long-form videos with a large value of m.Though causal attention only visits the previous frames, it actually shows the same complexity as full attention since the attention mask needs to accommodate the longest sequence, i.e., mN.In SC-Attn, the query to the first frame \mathbf{v}_{1} maintains the global coherence in terms of generated content, while the query to former frame \mathbf{v}_{i-1} learns the necessary motion between consecutive frames. This design is motivated by the observation that a simple cross-frame attention that attends the first video frame enables generating a sequence of frames that are consistent in content (see Fig. 2). Thus, spatial consistency can be maintained by querying the key and value of another frame for attention. For temporal consistency, attending previous nearest frame \mathbf{v}_{i-1} gives a direct guidance of the motion between two consecutive frames. Empirically, SC-Attn is able to generate temporally-coherent video frames with smooth transition, and meanwhile remain low computational complexity at \mathcal{O}(2m(N)^{2}). Moreover, as a diluted version of casual attention, SC-Attn naturally supports autoregressive generation of long video sequence through cross-frame attention to the intermediate features of early generated frames. Fig. 11 shows instances of long sequence generation.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	None			
355	paper_133	What is the reason that the space-time attention does not work well to generate consistent content?	Factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation.  The self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.  Using full attention in space-time leads to quadratic growth in computation.  It is thus infeasible for generating long-form videos with increasing frames.	We first follow VDM baselines [17, 14] to use inflated 1\times 3\times 3 convolution kernels for video inputs and append temporal self-attention with causal mask for temporal modeling. However, we empirically observe that factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation (see Sec. 4.3). We further inflate the spatial self-attention to cross-frame attention as follows.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.There are alternative options for cross-frame attention in Fig. 6, including full attention and causal attention which also capture spatio-temporal consistency.However, such straightforward choices are actually not feasible in generating videos with continuous frames due to their high computational complexity.Specifically, given m frames and N sequences for each frame, the complexity for full attention is \mathcal{O}((mN)^{2}).It is not affordable if we need to generate long-form videos with a large value of m.Though causal attention only visits the previous frames, it actually shows the same complexity as full attention since the attention mask needs to accommodate the longest sequence, i.e., mN.We fine-tune the inflated T2V models for One-Shot Video Generation. The objective of one-shot tuning is to acquire temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SC-Attn) and temporal self-attention (Temp-Attn). The SC-Attn models the one-way mapping from frame \mathbf{v}_{i} to its previous frames (i.e., \mathbf{v}_{1} and \mathbf{v}_{i-1}), and due to the causality, key and value features derived from previous frames are independent to the output of \mathbf{v}_{i}.Therefore, we propose to fix W^{K} and W^{V}, and only update W^{Q} in SC-Attn layers.On the other hand, we fine-tune the entire Temp-Attn layers, including W^{Q}, W^{K}, W^{V}, as they are newly added and randomly initialized.Moreover, we update the query projection in cross-attention (Cross-Attn) for better video-text alignment.Fine-tuning the attention blocks is computationally efficient, and keeps the property of diffusion-based T2I models unchanged.As shown in our experiments, this is sufficient to produce temporally-coherent videos with novel text prompts.Fig. 5 highlights the training pipeline and trainable parameters during the one-shot tuning process.As mentioned in Sec. 3.3, the VDM baselines [17, 14] factorize space and time by appending an additional temporal attention after each spatial attention block in T2I diffusion models. Specifically, the original 2D spatial blocks are kept in space only, and additional temporal convolution/attention blocks are added after the spatial layers to capture time-related information. For a fair comparison, we adopt the same training pipeline in Fig. 5 to fine-tune the VDM baselines for One-Shot Video Generation. As shown in Fig. 8, the VDM baselines with factorized space-time attention fail to generate consistent content (compare the appearance of the subjects across frames), whereas our Tune-A-Video with spatio-temporal cross-frame attention maintains better temporal consistency.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation.  The self-attention layers in T2I models are driven by spatial similarities and pixel positions.  Using full attention in space-time leads to quadratic growth in computation.  It is thus infeasible for generating long-form videos with increasing frames.	Change concept	 (only driven by spatial similarities rather than pixel positions -> driven by spatial similarities and pixel positions)	
356	paper_133	Is it inspired by transformer network?	The authors extend the spatial self-attention in the T2I model from one image to multiple images to maintain content consistency across frames.  It is useful in spatiotemporal domain like generating videos.	Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.More recently, Imagen Video [14] improves VDM with cascaded diffusion models and v-prediction parameterization to generate high definition videos. Phenaki [41] is the first work to generate videos from time variable prompts. To achieve this, they compresses videos to small representations of discrete tokens with causal attention in time, and thus can handle variable-length videos. To address the lack of video-text pair data, they joint train on a large scale of image-text pairs and a smaller number of video-text pairs, achieving better generalization results than available video datasets. Make-A-Video [36] shares similar motivation and aims to transfer the significant progress in T2I generation to T2V generation. They combine the appearance-text information from text-image data together with the world movements from unsupervised video footage, and achieve the state-of-the-art in T2V generation.We follow [36] to use pre-trained T2I diffusion models and propose Tune-A-Video for one-shot T2V generation. Differently, Tune-A-Video explores a more efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion changes.	The authors extend the spatial self-attention in the T2I model from one image and its description text to multiple images and their description texts to maintain content consistency across frames.  It is useful in spatiotemporal domain like generating videos.	Invent something didn't mentioned		
357	paper_133	How does this v1 related to generating consistent content?	Conditioning the first frame, it can autoregressively extend video frames with shared verb.	Formally, given a video \mathcal{V}=\left\{v_{i}|i\in[1,m]\right\} with m frames, accompanied with a corresponding textual description \mathcal{T}, our objective is to generate novel videos \hat{\mathcal{V}} driven by text prompts \hat{\mathcal{T}} using a pre-trained T2I model M, i.e., M(\hat{\mathcal{V}}|\mathcal{V},\mathcal{T},\hat{\mathcal{T}}), where \mathcal{T} and \hat{\mathcal{T}} share the same verbs.Examples of output variations driven by \hat{\mathcal{T}} include: changes of subject, background (e.g., the place where the subject is), attribute (e.g., color, age, etc.), and other semantic modifications (see Fig. 1 and Fig. 7).Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.	None			
359	paper_134	How well RoBERTa language modeling on Wiki-40B?	RoBERTa performs at about 2. 6 BPC on the MLM task with the Wiki-40B dataset.  RoBERTa performs better than BERT.	We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text.We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus.	RoBERTa performs at about 2.2 BPC on the MLM task with the Wiki-40B dataset.  RoBERTa performs better than BERT.	Change number		
361	paper_134	Why the authors suggest there is no truly monolingual pre-trained model?	Well-known pre-training resources already include multilingual data.	In this paper, we demonstrate that English pretrained models are exposed to a considerable amount of non-English data during pretraining, particularly in the case of more recent models that are trained on larger corpora derived from web crawls. We also find that this non-English text acts as a significant source of signal for cross-lingual transfer.We also only consider the effect of foreign language contamination for English pretrained models. It is unclear to what extent this phenomenon affects monolingual models for other languages; however, since many of the resources evaluated in this work are also used to pretrain non-English monolingual models (e.g., Wikipedia), similar effects would likely be observed.Overall, these results indicate that the considered models are actually multilingual and that their ability to transfer across languages is not zero-shot, despite what has been recently claimed.Given the effort required to fully remove all non-English data, we question whether it is practically possible to train truly monolingual models at scale.	None			
362	paper_134	What kind of pretrained language models they mentioned?	Authors mention BERT, RoBERTa, T5, mBERT, and XLM-R.	We now ask: how well do models pretrained on these putatively English corpora perform on non-English tasks? While the English data is more multilingual than previously thought, there are many differences between monolingual and multilingual pretraining; non-English data are often tokenized into more subword units333For example, the Basque UD treebank requires on average 1.78, 2.59, and 2.66 tokens per word to be encoded by XLMR, RoBERTa, and BERT, respectively.and are much less frequently observed during monolingual training.We evaluate popular English pretrained models on tasks in more than 50 languages: (masked) language modeling, POS probing, and finetuned POS tagging.We compare the performance of monolingual BERT Devlin et al. (2019), RoBERTa Liu et al. (2019), and T5 Raffel et al. (2020) against multilingual mBERT Delvin (2019) and XLM-R Conneau et al. (2020). We report average performance across five runs with different random seeds for the POS evaluations. The full results and all languages can be found in Appendix D.	None			
363	paper_134	How could English models performs well on non-English POS tasks?	Authors do not discuss how this performance is achieved.	Next, we evaluate how well monolingual English models perform on non-English downstream tasks, using part-of-speech (POS) tagging as a case study.	None			
364	paper_134	What is the difference ratio of non-English text in pretraining data between T5 and RoBERTa?	T5 data contains 0. 26%, and RoBERTa data contains 0.	This difference is likely due to two factors. First, in terms of relative percentages, RoBERTa is exposed to more non-English text than T5 (0.78% compared to only 0.22%). Secondly, RoBERTa’s subword vocabulary is robust to unexpected inputs and does not substitute an UNK token any input tokens; in contrast, T5 and BERT have high rates of UNK tokens for some non-Latin languages (Appendix B).555UNK tokens refer to placeholder tokens used when the model receives an input not covered by its vocabulary.However, for many high-resource languages the English models perform competitively, with T5 outperforming mBERT on German and Portuguese, among others.	None			
365	paper_134	What is training method used for decreasing the gap between monolingual model and multilingual model?	It is fine tuning.	To test if the effects of foreign language data carry through after finetuning, we also finetune a subset of the models (BERT{}_{base}, RoBERTa{}_{base}, mBERT, XLMR{}_{base}) for non-English POS tagging (Figure 1(c)). After finetuning, the gap between the mono- and multilingual models is much smaller: RoBERTa only averages 2.65 points worse than XLM-R, compared to 12.5 points when probing.	None			
366	paper_134	What are the two factors to show potential reason for cross-lingual generalization	They are quantity of target language data in the pre-training corpora and language similarity.	We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model’s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.	None			
367	paper_134	How did the authors find potential causes of cross-lingual transfer?	Authors do not discuss how they pointed to these potential causes.	We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model’s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.	None			
368	paper_134	What is the correlation value between target pretraining data size and model performance for latin data on T5?	It is 0.	We find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. BERT and T5 task performance are less correlated with observed pretrained data, likely due to tokenization artifacts (Appendix B). Indeed, when we control for languages not written with Latin script on T5, the correlation between performance and the amount of target pretraining data increases to \rho= 0.313.	None			
369	paper_134	Which factor is more related to model performance between pretraining data size and language similarity?	Pretraining data size is more related to model performance.	We then investigate the correlation between potential transfer causes and model performance (Table 2). Specifically, we consider the quantity of target language data found in the model’s pretraining corpus and the language similarity to English as potential causes of cross-lingual transfer.We find that across tasks, RoBERTa task performance is most strongly correlated with the amount of target language data seen during pretraining. BERT and T5 task performance are less correlated with observed pretrained data, likely due to tokenization artifacts (Appendix B). Indeed, when we control for languages not written with Latin script on T5, the correlation between performance and the amount of target pretraining data increases to \rho= 0.313.We also consider the effect of language similarity on task performance, which is often hypothesized to facilitate cross-lingual transfer. We use the syntactic distance of languages calculated by Malaviya et al. (2017); more similar languages score lower. However, we generally find that this is less correlated with performance than the quantity of target text, particularly for RoBERTa.	None			
370	paper_134	What is the role of non-English data for English pretrained models in the finding?	It can enhance cross-lingual transfer and generalization.	In this paper, we demonstrate that English pretrained models are exposed to a considerable amount of non-English data during pretraining, particularly in the case of more recent models that are trained on larger corpora derived from web crawls. We also find that this non-English text acts as a significant source of signal for cross-lingual transfer.However, the presence of foreign language data in pretraining corpora is not inherently problematic. Models trained on these datasets perform exceedingly well on their target languages and generalize to other languages much better than expected. Rather, it is important to remember that these models are not performing zero-shot transfer when used in other languages, given the scale and data with which they were pretrained.	The role of the non-English data during the pre-training is bad because they are considered as noises.	Change concept	 (non-English data becomes noise after twisting)	The role of the non-English data during the pre-training is that they enhance cross-lingual transfer and generalization.
371	paper_134	What is used for measure the quantities of non-English data?	Automatic language identification and manual qualitative analysis measure non-English data.  They are denominated in lines, tokens, and percentages across the paper.	We also see that non-English text makes up small percentages of the overall data, though this still leads to millions of tokens in large datasets.The largest individual languages after English only make up 0.01%, 0.15%, and 0.05% of the BERT, RoBERTa, and T5 training data, respectively.Multilingual pretraining work has shown that models generalize to new languages from varying amounts of data Delvin (2019); Lample and Conneau (2019); Conneau et al. (2020); however, these approaches intentionally select data across languages, and most upsample low-resource languages during training.Without these considerations, it is an open question how well the models trained on these relatively small amounts of non-English data generalize.We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.Our analysis finds that these corpora include very small percentages that amount to overall significant amounts of non-English text (Figure 1), particularly those derived from web-crawled data.Furthermore, the models trained on this data perform surprisingly well on other languages; this transfer is strongly correlated with the amount of target language data seen during pretraining. Notably, we find that the English T5 outperforms mBERT on POS tagging in multiple languages with no finetuning.We first measure how much non-English text exists in commonly used English pretraining corpora with two analyses: an automatic language identification to estimate the amount of foreign language data in these corpora, and a manual qualitative analysis of the text classified as non-English.	None			
372	paper_134	What are two kinds of pretrained language models?	They are monolingual and multilingual.	Pretrained language models have become an integral part of NLP systems. They come in two flavors: monolingual, where the model is trained on text from a single language, and multilingual, where the model is jointly trained on data from many different languages. Monolingual pretrained models are generally applied to tasks in the same language, whereas multilingual ones are used for cross-lingual tasks or transfer.	None			
373	paper_134	What is the range of the number of non-English tokens found in English corpus?	Non-English tokens make up 300k to 406M in the datasets investigated.	A summary of the language identification experiments is presented in Figure 1.111Full results of this evaluation are detailed in Appendix C. We see that every corpus contains notable quantities of non-English data, with our estimates ranging between 300k to 406M tokens. An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.	According to the authors, about 30k to 40M non-English tokens were found in the investigated English corpus.	Change number		According to the authors, about 300k to 406M non-English tokens were found in the investigated English corpus.
374	paper_134	Is there any problems from using web crawl data? If so, what is the problem?	Models perform worse on web-crawled data.	Our analysis also shows that the language classifier performs worse on the non-web crawled data. For example, it misclassified a quarter of the sampled lines from Stories as non-English when they in fact only contain English text; many of these lines stem from snippets of dialogue in the dataset. We generally observe that lines coded as En tend to be shorter than the correctly labeled lines and often contain non-standard English. The language classifier also struggles to handle noisy lines, for which it has no appropriate language label.Indeed, a major factor of language leakage is the method in which the data was collected: the datasets derived from web crawls contain higher percentages of non-English text (OpenWebText andCCNews). This is true even for C4, where the dataset was filtered with a classifier to exclude non-English text Raffel et al. (2020). Since automatic methods for language identification are imperfect, the datasets with more manual filtering (such as Wikipedia, which has human editors curating its content) are less prone to non-English data than those relying on classifiers.Due to these challenges, it is likely impossible to fully remove non-English text from a web-crawled dataset at scale.	None			
375	paper_134	How many categories used in non-English text classifier?	Non-English text classifier uses six categories.	We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.	The authors indicate that three categories are used in non-English text-classifier	Change number		The authors indicate that six categories are used in non-English text-classifier
377	paper_134	What are tasks to show how well English models tend to be multilingual?	Language composition estimation and POS tagging can measure multilingual performance.	We now ask: how well do models pretrained on these putatively English corpora perform on non-English tasks? While the English data is more multilingual than previously thought, there are many differences between monolingual and multilingual pretraining; non-English data are often tokenized into more subword units333For example, the Basque UD treebank requires on average 1.78, 2.59, and 2.66 tokens per word to be encoded by XLMR, RoBERTa, and BERT, respectively.and are much less frequently observed during monolingual training.More specifically, we quantify how multilingual English pretrained models are in two steps. First, we analyze common English pretraining corpora with a large-scale automatic evaluation to estimate their language composition, as well as a smaller-scale manual analysis. Second, we perform experiments across fifty languages on masked language modeling and part-of-speech (POS) tagging to measure how well the models trained on these pretraining corpora perform outside of English.	None			
379	paper_134	What are the pretraining datasets used in analyses?	English Wikipedia, BookCorpus, Stories, OpenWebText, CC-NEWS, and C4. En datasets were used in pretraining.	We consider the following pretraining datasets: English Wikipedia(11.8GB); BookCorpus (Zhu et al. 2015, 4.2GB); Stories (Trinh and Le 2018, 31GB); OpenWebText (Gokaslan and Cohen 2019, 38GB), which is an open-source version of WebText Radford et al. (2019); CC-NEWS (Liu et al. 2019, 76 GB); and C4.En (Raffel et al. 2020, 305GB), as provided by Dodge et al. (2021). We use the versions of Wikipedia, BookCorpus, and CC-NEWS used to pretrain RoBERTa.	The authors used English Wikipedia, BookCorpus, IMDB, ImageNet, Stories, OpenWebText, CC-NEWS, and C4. En datasets in pretraining.	Invent something didn't mentioned		The authors used English Wikipedia, BookCorpus, Stories, OpenWebText, CC-NEWS, and C4. En datasets in pretraining.
381	paper_137	Based on the results of the baseline and other models, will you rule out occurrence of overfitting in the data? How?	We can observe that there's little over fitment of data.  As it outperforms the other two baselines, especially on ProofWriter-PUD which contains Unknown labels (44% relative improvement compared to CoT and 56% compared to SI on Depth-5) as well as on the higher depths of PrOntoQA (37% relative improvement compared to CoT and 113% compared to SI on Depth-5).	The results for Lambada and the baselines on the two ProofWriter datasets are provided in Figure 1, and PrOntoQA results are shown in Figure 2. From the results, we observe that Lambada significantly outperforms the other two baselines, especially on ProofWriter-PUD which contains Unknown labels (44\% relative improvement compared to CoT and 56\% compared to SI on Depth-5) as well as on the higher depths of PrOntoQA (37\% relative improvement compared to CoT and 113\% compared to SI on Depth-5). These results show the merit of Lambada for logical reasoning and also show that backward chaining (which is the backbone of reasoning in Lambada) may be a better choice compared to forward chaining (the backbone in SI). The results also reveal a short-coming of the CoT approach in dealing with Unknown labels, as, unlike the examples for which the label is Proved or Disproved, there is no natural chain of thought for the examples whose labels are Unknown.	None			
382	paper_137	How were the 50 examples chosen for proof accuracy?	The 50 examples were randomly selected.	To understand the reason behind the high accuracy of CoT on higher depths of ProofWriter-PD, we randomly selected 50 examples from depth-5 of the dataset where CoT predicted the result correctly and manually verified if the proof chain is correct or not. For comparison, we also manually verified the proofs generated by Lambada following a similar procedure.	None			
383	paper_137	From the sign agreement, one can see improvement in accuracy with facts. Why?	when we allow the model to just choose one fact the accuracy is 0. 94 but that jumps to a near perfect accuracy when we enable the model to select two facts.  The Sign Agreement module likewise exhibits a near flawless accuracy.	Based on the results of largest PaLM model in Figure 4, the Rule Selection module has the lowest accuracy among the different modules followed by the Goal Decomposition. In the case of Fact Check, when we allow the model to only select one fact the accuracy is 0.94 but that increases to a near perfect accuracy when we allow the model to select two facts. The Sign Agreement module also shows a near perfect accuracy.	None			
384	paper_137	How can Lambada be adapted for other NLP tasks?	Lambada can be adopted for other NLP tasks as Lambada is an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the BC algorithm for high-level reasoning.  Lambada achieves significant improvements over existing approaches such as Chain-of-Thought and Selection-Inference in terms of prediction accuracy and proof accuracy.	We developed Lambada, an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the backward chaining (BC) algorithm for high-level reasoning. We showed that Lambada achieves significant improvements over competitive existing approaches such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy. Furthermore, we demonstrated how Lambada efficiently searches the entire proof space to accurately conclude that a statement can neither be proved nor disproved based on the theory.Although we only do experiments on formal reasoning problems and datasets, we believe our key insight on the efficacy of goal-directed reasoning with LMs is widely applicable and can be adapted to other NLP tasks where multi-step inference may be required. Going beyond the specific design of Lambada and its specialized modules, it would be useful to find other BC-inspired methods that might even incorporate BC into the LM directly e.g. a BC version of Chain-of-Thought.	Lambada can be adopted for other NLP tasks as Lambada is an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the BC algorithm for high-level reasoning.  However, Lambada doesn't achieve significant improvements over existing approaches such as Chain-of-Thought and Selection-Inference in terms of prediction accuracy and proof accuracy.	Opposite		
385	paper_137	How different would a BC version of chain of thought be than Lambada model?	Lambada, is an algorithm for text-based deductive logical reasoning that combines the ability of LMs to handle realistic text input with the backward chaining (BC) technique for high-level reasoning.   Lambada makes considerable gains over competing current techniques such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting whether a proposition can be proven or refuted based on a theory) and proof accuracy.  Furthermore, Lambada rapidly examines the full proof space to appropriately infer that a statement can neither be proven nor denied based on the theory.	We developed Lambada, an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the backward chaining (BC) algorithm for high-level reasoning. We showed that Lambada achieves significant improvements over competitive existing approaches such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy. Furthermore, we demonstrated how Lambada efficiently searches the entire proof space to accurately conclude that a statement can neither be proved nor disproved based on the theory.	None			
386	paper_137	In one of the examples in the paper, the longer rule gave validated fact check over short rule. Does that hinder your intuition?	the longer rule gave a validated fact check over the short rule in some examples but it doesn't hinder our intuition.	A number of approaches specifically look into whether LMs can generalize from examples requiring shorter reasoning chains (shown to them either as demonstration or as finetuning data) to examples requiring longer chains Anil et al. (2022); Tafjord et al. (2020). With our model, length generalization comes for free because the model learns the building blocks of solving the problem that are applied as many times as needed to solve the problem.Note that once a set of rules \mathcal{R}_{s} are selected, the algorithm proceeds in a depth-first manner (i.e. it exhaustively verifies one rule before going to the next rule). Therefore, if the algorithm can start with the rules that have a higher chance of succeeding at proving or disproving the goal, it can save computations and be less error prone. In this paper, we use a heuristic to rank the rules: we sort them based on their lengths with shorter rules being ranked higher.This heuristic is based on the intuition that shorter rules are likely to have fewer sub-goals in their antecedent. We leave more sophisticated ranking strategies as future work.	None			
387	paper_137	Does prediction of Unknown values have an influence on proved and disproved?	The prediction of Unknown values does not have an influence on proved and disproved.	The results for Lambada and the baselines on the two ProofWriter datasets are provided in Figure 1, and PrOntoQA results are shown in Figure 2. From the results, we observe that Lambada significantly outperforms the other two baselines, especially on ProofWriter-PUD which contains Unknown labels (44\% relative improvement compared to CoT and 56\% compared to SI on Depth-5) as well as on the higher depths of PrOntoQA (37\% relative improvement compared to CoT and 113\% compared to SI on Depth-5). These results show the merit of Lambada for logical reasoning and also show that backward chaining (which is the backbone of reasoning in Lambada) may be a better choice compared to forward chaining (the backbone in SI). The results also reveal a short-coming of the CoT approach in dealing with Unknown labels, as, unlike the examples for which the label is Proved or Disproved, there is no natural chain of thought for the examples whose labels are Unknown.	None			
390	paper_137	How can you come to the intuition that shorter rules have smaller sub goals?	If smaller LMs are utilised, then one may need to split the issue into sub-problems even more (e. , further decomposing the one-to-many comparisons in the selection module) (e.	We argue that the extent to which reasoning algorithms break the problem into sub-problem should be dependent on the scale and power of the LMs. If smaller LMs are used, then one may need to break the problem into sub-problems even further (e.g., further decomposing the one-to-many comparisons in the selection module). And as LMs become larger and stronger in the future, one could rely on them to solve problems even with a coarser-grained decomposition of the problem.	None			
391	paper_137	How is it better to decrease the depth by 1 over other values?	Decreasing the depth by 1 requires fewer calls as compared to other values.	Lambada and SI require multiple LM inference call per example. In Figure 5, we compare the two models with respect to the average number of inference calls they make to the LM per example, for the different depths of the ProofWriter-PUD dataset. We observe that Lambada requires significantly fewer inference calls, especially at higher depths. For example, for Depth-1, Lambada requires 3.8x fewer calls whereas for Depth-5 it requires 11.8x fewer calls.	None			
394	paper_137	Given the triggered sentences, how can this problem be rectified?	The triggered sentence is "One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs Garcez and Lamb ". 	One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs Garcez and Lamb (2020); Marcus (2020).In the classic literature, there are two major approaches to logical reasoning Poole and Mackworth (2010):1.Forward Chaining (FC) where one starts from the facts and rules (“theory”), and iterates between making new inferences and adding them to the theory until the goal statement can be proved or disproved,2.Backward Chaining (BC) where one starts from the goal and recursively decomposes it into sub-goals until the sub-goals can be proved or disproved based on the facts.Previous approaches to reasoning with LMs mostly incorporate elements of FC into LMs Tafjord et al. (2020); Creswell et al. (2022). FC requires selecting a subset of facts and rules from the entire set which might be difficult for an LM as it requires a combinatorial search over a large space.Moreover, deciding when to halt and declare failure to prove is challenging in FC Creswell et al. (2022), sometimes requiring specialized modules trained on intermediate labels Creswell and Shanahan (2022). Indeed, the classic automated reasoning literature is heavily weighted towards BC or goal-directed strategies for proof-finding.	None			
396	paper_138	How trustworthy are the ML decisions made by the system?	How trustworthy are ML decisions depends on many factors.  Human understanding and trust in ML concerns not only understanding promoted decisions, but also, evaluating these decisions in relation to limitations built into the ML model.  Limitations are introduced in ML systems by humans during the design phase.  The approach towards understanding ML decisions builds on connecting human understandable concepts to the ML models knowledge representations with the goal of making them explicable.  We here view contemporary ML as limited to local generalization within a single task or well-deﬁned set of tasks that only holds when the training data used is independent- and-identically-distributed (i.  ML is then limited when this does not hold or when it comes to causal inference and out-of-distribution (o. d) generalization.	We here view contemporary ML as limited to local gen- eralisation within a single task or well-deﬁned set of tasks that only holds when the training data used is independent- and-identically-distributed (i.i.d). ML is then limited when this does not hold or when it comes to causal inference and out-of-distribution (o.o.d) generalisation (Chollet 2019; Scholkopf et al. 2021).Human understanding and trust in ML concerns not only understanding promoted decisions 1 , but also, evaluating these decisions in relation to limitations built into the ML model. Limitations are introduced in ML systems by hu- mans during the design phase, for example; what to model, choice of algorithm, feature engineering, training data se- lection (Gillies et al. 2016). The need for explanations to convey understanding is pronounced in more complex ML models (Lipton 2016) and especially prominent in today’s dominating technology: neural networks.Our approach towards understanding ML decisions builds on connecting human understandable concepts to the ML models knowledge representations with the goal of making them explicable. Below follows an outline of the perspect- ives on explanations used in this paper.	None			
397	paper_138	How are other questions handled?	What if -questions and the centrality of concepts is the focus for this review where we examine how concepts are extracted from a neural network.  In our review, we use the structure of D-N explanations and three types why-questions, What if I see? , What if I do? and What if I had done? as an analytic lens to deepen and detail what we can expect, and not expect, from the research reviewed.	ML systems increasingly affect many aspects of human life, gaining trust in their decisions is a central and active re- search area. What if -questions and the centrality of concepts is the focus for this review where we examine how concepts are extracted from a neural network. We presuppose a situ- ation where a human, with domain knowledge, use concepts to answer why-questions. In our review, we use the structure of D-N explanations and three types why-questions, What if I see? , What if I do? and What if I had done? as an ana- lytic lens to deepen and detail what we can expect, and not expect, from the research reviewed.	None			
399	paper_14	How is the authors' work different from the “fast gradient sign” method?	The fast gradient sign method is very quick but may lead to sub-optimal perturbations thus damaging the overall robustness estimation, and fine-tuning with such adversarial samples may sometimes result in a drop in the overall performance of the model.  On the other hand, DeepFool creates adversarial perturbations that are closer to the absolute minimum compared to others thus giving us a more reliable tool in terms of robustness estimation and fine-tuning.	We compare the proposed DeepFool approach to state-of-the-art techniques to compute adversarial perturbations in [18] and [4]. The method in [18] solves a series of penalized optimization problems to find the minimal perturbation, whereas [4] estimates the minimal perturbation by taking the sign of the gradient\displaystyle\hat{\bm{r}}(\bm{x})=\epsilon\,\text{sign}\left(\nabla_{\bm{x}}J(\bm{\theta},\bm{x},y)\right),with J the cost used to train the neural network, \bm{\theta} is the model parameters, and y is the label of \bm{x}. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter \epsilon, we chose the smallest \epsilon such that 90\% of the data are misclassified after perturbation.555Using this method, we observed empirically that one cannot reach 100\% misclassification rate on some datasets. In fact, even by increasing \epsilon to be very large, this method can fail in misclassifying all samples.It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.We illustrate in Figure 1 perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.In this section, we fine-tune the networks of Table 1 on adversarial examples to build more robust classifiers for the MNIST and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool’s adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples.We fine-tune the networks by performing 5 additional epochs, with a 50\% decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5 extra epochs. For the sake of completeness, we also performed 5 extra epochs on the original data. The evolution of \hat{\rho}_{\text{adv}} for the different fine-tuning strategies is shown in Figures 5(a) to 5(d), where the robustness \hat{\rho}_{\text{adv}} is estimated using DeepFool, since this is the most accurate method, as shown in Table 1. Observe that fine-tuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN’s robustness is increased by about 40%. On the other hand, quite surprisingly, the method in [4] can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial perturbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations.To verify this hypothesis, we compare in Figure 7 the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by \alpha=1,2,3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see Figure 8). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations.Table 3 lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on generalization.666While the authors of [4] reported an increased generalization performance on the MNIST task (from 0.94\% to 0.84\%) using adversarial regularization, it should be noted that the their experimental setup is significantly different as [4] trained the network based on a modified cost function, while we performed straightforward fine-tuning.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.	The fast gradient sign method is very quick but may lead to sub-optimal perturbations thus damaging the overall robustness estimation, and fine-tuning with such adversarial samples may sometimes result in a drop in the overall performance of the model.  On the other hand, DeepFool creates adversarial perturbations that are closer to the absolute minimum compared to others thus giving us a less reliable tool in terms of robustness estimation and fine-tuning.	Change concept	 (more -> less)	
400	paper_14	What does an "adversarial perturbation" mean?	Adversarial perturbation is a small and unnoticeable change to the data that fool the given model (i. e give a different class after applying the perturbation).  It allows an understanding limits of existing architectures and calculation of the robustness of the models.	Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics [1, 16], speech [12, 6], and computer vision [10, 8]. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data [18]. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure 1). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation \bm{r} that is sufficient to change the estimated label \hat{k}(\bm{x}):\displaystyle\Delta(\bm{x};\hat{k}):=\min_{\bm{r}}\|\bm{r}\|_{2}\text{ subject to }\hat{k}(\bm{x}+\bm{r})\neq\hat{k}(\bm{x}),(1)where \bm{x} is an image and \hat{k}(\bm{x}) is the estimated label. We call \Delta(\bm{x};\hat{k}) the robustness of \hat{k} at point \bm{x}. The robustness of classifier \hat{k} is then defined as\rho_{\text{adv}}(\hat{k})=\mathbb{E}_{\bm{x}}\frac{\Delta(\bm{x};\hat{k})}{\|\bm{x}\|_{2}},(2)where \mathbb{E}_{\bm{x}} is the expectation over the distribution of data.The study of adversarial perturbations helps us understand what features are used by a classifier.The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples.Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models [18]. This can actually become a real concern from a security point of view.An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.	Adversarial perturbation is a small but very noticeable change to the data that fool the given model (i. e give a different class after applying the perturbation).  It allows an understanding limits of existing architectures and calculation of the robustness of the models.	Change concept	 (small and unnoticeable -> small but very noticeable)	
401	paper_14	What are the metrics used to compare the efficiency of different methods which compute the adversarial perturbations?	The metrics that are used to compare different methods of finding adversarial perturbations are: the average robustness of the model estimated in some type of norm (2-norm or infinity-norm in the paper).  and the average running time needed to find the estimated minimal perturbation.	In order to evaluate the robustness to adversarial perturbations of a classifier f, we compute the average robustness \hat{\rho}_{\text{adv}}(f), defined by\hat{\rho}_{\text{adv}}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{2}}{\|\bm{x}\|_{2}},(15)where \hat{\bm{r}}(\bm{x}) is the estimated minimal perturbation obtained using DeepFool, and \mathscr{D} denotes the test set444For ILSVRC2012, we used the validation data..We report in Table 1 the accuracy and average robustness \hat{\rho}_{\text{adv}} of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks.	Only one metric is used to compare different methods of finding adversarial perturbations: the average robustness of the model estimated in some type of norm (2-norm or infinity-norm in the paper)	Change concept	 (two metrics are used -> only 1 metric is used)	
402	paper_14	What does an "affine classifier" mean?	The affine classifier is the classifier in the form of an affine function.  The general form that is used in the paper is the function f: R^n -> R^m, where f(x) = W^T * x + B, for the given matrix and vector W and B.	Let f(\bm{x}) be an affine classifier, i.e., f(\bm{x})=\mathbf{W}^{\top}\bm{x}+\bm{b} for a given \mathbf{W} and \bm{b}. Since the mapping \hat{k} is the outcome of a one-vs-all classification scheme, the minimal perturbation to fool the classifier can be rewritten as follows\begin{split}&\operatorname*{arg\,min}_{\bm{r}}\|\bm{r}\|_{2}\\&\text{s.t. }\exists k:\bm{w}_{k}^{\top}(\bm{x}_{0}+\bm{r})+b_{k}\geq\bm{w}_{\hat{k}(\bm{x}_{0})}^{\top}(\bm{x}_{0}+\bm{r})+b_{\hat{k}(\bm{x}_{0})},\end{split}(6)where \bm{w}_{k} is the k^{\text{th}} column of \mathbf{W}. Geometrically, the above problem corresponds to the computation of the distance between \bm{x}_{0} and the complement of the convex polyhedron P,\displaystyle P=\bigcap_{k=1}^{c}\{\bm{x}:f_{\hat{k}(\bm{x}_{0})}(\bm{x})\geq f_{k}(\bm{x})\},(7)where \bm{x}_{0} is located inside P.We denote this distance by \text{{dist}}(\bm{x}_{0},P^{c}).The polyhedron P defines the region of the space where f outputs the label \hat{k}(\bm{x}_{0}). This setting is depicted in Figure 4. The solution to the problem in Eq. (6) can be computed in closed form as follows. Define \hat{l}(\bm{x}_{0}) to be the closest hyperplane of the boundary of P (e.g. \hat{l}(\bm{x}_{0})=3 in Figure 4). Formally, \hat{l}(\bm{x}_{0}) can be computed as follows\hat{l}(\bm{x}_{0})=\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f_{k}(\bm{x}_{0})-f_{\hat{k}(\bm{x}_{0})}(\bm{x}_{0})\right|}{\|\bm{w}_{k}-\bm{w}_{\hat{k}(\bm{x}_{0})}\|_{2}}.(8)The minimum perturbation \bm{r}_{*}(\bm{x}_{0}) is the vector that projects \bm{x}_{0} on the hyperplane indexed by \hat{l}(\bm{x}_{0}), i.e.,\bm{r}_{*}(\bm{x}_{0})=\frac{\left|f_{\hat{l}(\bm{x}_{0})}(\bm{x}_{0})-f_{\hat{k}(\bm{x}_{0})}(\bm{x}_{0})\right|}{\|\bm{w}_{\hat{l}(\bm{x}_{0})}-\bm{w}_{\hat{k}(\bm{x}_{0})}\|_{2}^{2}}(\bm{w}_{\hat{l}(\bm{x}_{0})}-\bm{w}_{\hat{k}(\bm{x}_{0})}).(9)In other words, we find the closest projection of \bm{x}_{0} on faces of P.As a multiclass classifier can be viewed as aggregation of binary classifiers, we first propose the algorithm for binary classifiers.That is, we assume here \hat{k}(\bm{x})=\text{sign}(f(\bm{x})), where f is an arbitrary scalar-valued image classification function f:\mathbb{R}^{n}\rightarrow\mathbb{R}. We also denote by \mathscr{F}\triangleq\{\bm{x}:f(\bm{x})=0\} the level set at zero of f.We begin by analyzing the case where f is an affine classifier f(\bm{x})=\bm{w}^{T}\bm{x}+b, and then derive the general algorithm, which can be applied to any differentiable binary classifier f.	None			
403	paper_14	What is the value of η used by the authors in experimentation?	The perturbation constant that is used is n = 0.	In practice, the above algorithm can often converge to a point on the zero level set \mathscr{F}. In order to reach the other side of the classification boundary, the final perturbation vector \hat{\bm{r}} is multiplied by a constant 1+\eta, with \eta\ll 1. In our experiments, we have used \eta=0.02.	None			
404	paper_14	The paper's algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation. Quantitatively, how far is the paper's approximation from the minimal perturbation?	The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it heavily depends on existing optimization methods.  In the paper, its effectiveness is proven relative to other state-of-the-art methods.  Although the analysis of how far the estimated perturbation from the actual minimal perturbation can be found in referenced papers, the more sophisticated analysis is not mentioned in the paper.	It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.In practice, the above algorithm can often converge to a point on the zero level set \mathscr{F}. In order to reach the other side of the classification boundary, the final perturbation vector \hat{\bm{r}} is multiplied by a constant 1+\eta, with \eta\ll 1. In our experiments, we have used \eta=0.02. It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it doesn't depend on existing optimization methods at all.  In the paper, its effectiveness is proven relative to other state-of-the-art methods.	Change concept	 (depends heavily on -> doesn't depend on at all)	The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it heavily depends on existing optimization methods.  In the paper, its effectiveness is proven relative to other state-of-the-art methods.
405	paper_14	Why did the authors choose a greedy approach for general classifier?	The DeepFool method is designed iteratively starting from very simple binary classifiers to more general non-linear differentiable classifiers.  The effectiveness of the greedy algorithm is justified by previous work and the results show very small perturbations, thus the authors claim that it is a viable method.	We then approximate, at iteration i, the distance between xi and the complement of P , dist(xi, P c), by dist(xi,  ̃P c i ). Specifically, at each iteration of the algorithm, the perturbation vector that reaches the boundary of the polyhedron  ̃Pi is computed, and the current estimate updated. The method is given in Algorithm 2. It should be noted that the proposed algorithm operates in a greedy way and is not guaranteed to converge to the optimal perturbation in (1). However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation.It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	The DeepFool method is designed iteratively starting from more general non-linear differentiable classifiers to very simple binary classifiers.  The effectiveness of the greedy algorithm is justified by previous work and the results show very small perturbations, thus the authors claim that it is a viable method.	Change concept	 (depends heavily on -> doesn't depend on at all)	
406	paper_14	Which deep neural network architectures were used for experimental comparison of DeepFool algorithm with existing methods?	Although the conclusion of the paper claims that 8 different classifiers were used, we can only see 6 classifiers with different datasets: 2-layer fully-connected network (MNIST), 2-layer LeNet (MNIST), 3-layer LeNet (CIFAR-10), NIN (CIFAR-10), CaffeNet (ILSVRC 2012), and GoogLeNet (ILSVRC 2012).	We now test our DeepFool algorithm on deep convolutional neural networks architectures applied to MNIST, CIFAR-10, and ImageNet image classification datasets. We consider the following deep neural network architectures:•MNIST: A two-layer fully connected network, and a two-layer LeNet convoluational neural network architecture [9]. Both networks are trained with SGD with momentum using the MatConvNet [20] package.•CIFAR-10: We trained a three-layer LeNet architecture, as well as a Network In Network (NIN) architecture [11].•ILSVRC 2012: We used CaffeNet [7] and GoogLeNet [17] pre-trained models.In this work, we proposed an algorithm, DeepFool, to compute adversarial examples that fool state-of-the-art classifiers. It is based on an iterative linearization of the classifier to generate minimal perturbations that are sufficient to change classification labels. We provided extensive experimental evidence on three datasets and eight classifiers, showing the superiority of the proposed method over state-of-the-art methods to compute adversarial perturbations, as well as the efficiency of the proposed approach. Due to its accurate estimation of the adversarial perturbations, the proposed DeepFool algorithm provides an efficient and accurate way to evaluate the robustness of classifiers and to enhance their performance by proper fine-tuning.The proposed approach can therefore be used as a reliable tool to accurately estimate the minimal perturbation vectors, and build more robust classifiers.	None			
407	paper_14	Why did the authors measure the perturbations using the L`2 norm?	The authors claim that the DeepFool algorithm is a well-founded baseline for finding adversarial perturbations for state-of-the-art models.  Although the use of the l-2 norm is not explicitly justified within the paper, it is a reasonable choice taking into account the scarcity of baseline methods.  Also, the method can be easily adapted to any l-p norm and the claims of the paper seem to hold for the l-infinity norm.	In this paper, we have measured the perturbations using the \ell_{2} norm. Our framework is however not limited to this choice, and the proposed algorithm can simply be adapted to find minimal adversarial perturbations for any \ell_{p} norm (p\in[1,\infty)). To do so, the update steps in line 10 and 11 in Algorithm 2 must be respectively substituted by the following updates\displaystyle\hat{l}\displaystyle\leftarrow\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f^{\prime}_{k}\right|}{\|\bm{w}^{\prime}_{k}\|_{q}},(11)\displaystyle\bm{r}_{i}\displaystyle\leftarrow\frac{|f^{\prime}_{\hat{l}}|}{\|\bm{w}^{\prime}_{\hat{l}}\|_{q}^{q}}|\bm{w}^{\prime}_{\hat{l}}|^{q-1}\odot\text{sign}(\bm{w}^{\prime}_{\hat{l}}),(12)where \odot is the pointwise product and q=\frac{p}{p-1}.333To see this, one can apply Holder’s inequality to obtain a lower bound on the \ell_{p} norm of the perturbation. In particular, when p=\infty (i.e., the supremum norm \ell_{\infty}), these update steps become\displaystyle\hat{l}\displaystyle\leftarrow\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f^{\prime}_{k}\right|}{\|\bm{w}^{\prime}_{k}\|_{1}},(13)\displaystyle\bm{r}_{i}\displaystyle\leftarrow\frac{|f^{\prime}_{\hat{l}}|}{\|\bm{w}^{\prime}_{\hat{l}}\|_{1}}\text{sign}(\bm{w}^{\prime}_{\hat{l}}).(14)An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.	None			
408	paper_14	How can the DeepFool algorithm be adapted to find minimal adversarial perturbations for any L`p norm?	To adapt the algorithm to use any l-p norm, only 2 lines in the algorithm (10 and 11) should be substituted with \displaystyle\hat{l}\displaystyle\leftarrow\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f^{\prime}_{k}\right|}{\|\bm{w}^{\prime}_{k}\|_{q}},(11)\displaystyle\bm{r}_{i}\displaystyle\leftarrow\frac{|f^{\prime}_{\hat{l}}|}{\|\bm{w}^{\prime}_{\hat{l}}\|_{q}^{q}}|\bm{w}^{\prime}_{\hat{l}}|^{q-1}\odot\text{sign}(\bm{w}^{\prime}_{\hat{l}}), where q = p/(p-1).	In this paper, we have measured the perturbations using the \ell_{2} norm. Our framework is however not limited to this choice, and the proposed algorithm can simply be adapted to find minimal adversarial perturbations for any \ell_{p} norm (p\in[1,\infty)). To do so, the update steps in line 10 and 11 in Algorithm 2 must be respectively substituted by the following updates\displaystyle\hat{l}\displaystyle\leftarrow\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f^{\prime}_{k}\right|}{\|\bm{w}^{\prime}_{k}\|_{q}},(11)\displaystyle\bm{r}_{i}\displaystyle\leftarrow\frac{|f^{\prime}_{\hat{l}}|}{\|\bm{w}^{\prime}_{\hat{l}}\|_{q}^{q}}|\bm{w}^{\prime}_{\hat{l}}|^{q-1}\odot\text{sign}(\bm{w}^{\prime}_{\hat{l}}),(12)where \odot is the pointwise product and q=\frac{p}{p-1}.333To see this, one can apply Holder’s inequality to obtain a lower bound on the \ell_{p} norm of the perturbation. In particular, when p=\infty (i.e., the supremum norm \ell_{\infty}), these update steps become\displaystyle\hat{l}\displaystyle\leftarrow\operatorname*{arg\,min}_{k\neq{\hat{k}(\bm{x}_{0})}}\frac{\left|f^{\prime}_{k}\right|}{\|\bm{w}^{\prime}_{k}\|_{1}},(13)\displaystyle\bm{r}_{i}\displaystyle\leftarrow\frac{|f^{\prime}_{\hat{l}}|}{\|\bm{w}^{\prime}_{\hat{l}}\|_{1}}\text{sign}(\bm{w}^{\prime}_{\hat{l}}).(14)	None			
409	paper_144	What is most important feature in hair fall disease model ? Is it false positive or false negative rate?	There is multiple important features in a model to consider: false positive and false negative rates, ignoring inter-class differences, model reliability, and overfitting problem.  But the paper doesn’t mention which is the most important feature of the model.	Overall, we observed very few works on hair diseases. The recent related works lack at least one of the following categories – discussion over false positive and false negative rates, ignoring inter-class differences, model reliability, and overfitting problem. In this work, we have attempted to fill these gaps by leveraging a convolutional neural networkAnother study [19] proposed a model for early alopecia detection. They used 100 samples for this research, with 80% as training data and the other 20% as testing data. They looked for four attributes, length of the hair, nail brittleness, amount of damage made to the hair, and hair follicle. Twolayer feed-forward network with a back propagation technique was used for detection purposes. The proposed model system consisting of 4 input neurons, 10 hidden neurons, and a linear output neuron, achieved 91% training accuracy with 86.7% validation accuracy. It showed the best performance at epoch 4 with a 0.059687 gradient. However, the study has some pitfalls, too, as they did not mention their data source or differentiate data classes with their respective sample sizes. Also, no image pre-processing was performed on the collected images. Although there is a possibility of overfitting without a proper data balancing technique, this report did not discuss the data balancing between the two classes. Furthermore, they did not calculate the model’s false positive and false-negative rates, which is crucial for a model specially developed for the healthcare system.				
410	paper_144	What are the possible reason androgenetic alopecia or MPB is less severe in women as compared to men?	MPB is an X-linked polygenicdisease, and males are more genetically prone to develop baldness at a mature age.  That's why MPB is less severe in women as compared to men.	Alopecia, folliculitis, and psoriasis are some common causes of hair loss. There is a difference between regular hair fall and alopecia; the latter develops coin-sized bald patches all over the scalp area. Alopecia or patchy hair loss can be of different types. Androgenetic alopecia or male-pattern baldness (MPB) is the most common form of alopecia where the hairline starts to recede, following a pattern where the frontal and temple area are most affected. 70% of men and 40% of women get this type of hair loss and thinning issue [3]. According to Liu et al., MPB is an X-linked polygenic disease, and males are more genetically prone to develop baldness at a mature age [5]. Topical minoxidil solution thickens the hair by 50% [3]. On the other hand, Alopecia areata (AA) is an autoimmune disease affecting individuals irrespective of age and sex. Primarily affecting the scalp area, AA can also spread in the beard, eyelashes, and eyebrows. In this case, the body’s immune cells cannot recognize hair follicles as ‘self.’ Instead, they consider these follicles as ‘foreign,’ which ultimately causes the hair follicles to be				
411	paper_144	What were the various treatment found in ayurved for hair loss?	The paper only says that the hair-loss treatment was found in ancient Ayurved by citing another paper, but it didn't discussed the various treatment found in ayurved for hair loss.	Hair, made of keratin protein, pertains to beauty and masculinity. Approximately 5 million hair follicles are present throughout our body [1]. Scalp Hair maintains body temperature and protects the brain from external heat. A typical hair growth cycle runs for 2-7 years, according to Patel et al. [2] and Wolff, Fischer, and Blume-Peytavi [3]. A healthy human has 100,000 hairs on the scalp, and 50-100 hair loss per day is considered normal. Hair loss is not a present-day issue. The hair-loss treatment was found in ancient Ayurveda scriptures 6000 years ago [2]. However, Hair and scalp-related issues are gaining more recognition nowadays compared to earlier years due to certain factors, such as environmental pollution, hormonal imbalance, autoimmune disease, gut microbiota alteration, elevated physical and mental stress levels in human lifestyle, seasonal change, unhealthy diet, micronutrient deficiency, genetic predisposition, and side-effects of drugs [2], [3]. According to Peyravian et al., 80 million Americans have hair loss- related issues to some extent [4]. Although most hair loss diseases are localized, some can spread to other locations. Some diseases require prescribed drugs and hair				
412	paper_144	How does immune therapy helps in resolving AA ?	P0 discussed that for AA treatment immune therapy is used.  But how does it helps is not discussed.	targeted and destroyed by the immune cells. It is an example of a hereditary disease. The study from Benigno et al. reported that, in the US alone, 700,000 individuals suffer from AA [6]. This disease, if diagnosed early, might resolve spontaneously. In severe cases, topical corticosteroid or immune therapy is used [3].				
413	paper_144	According to the author most hair and scalp disease is diagnosed in advanced stages. What could be the possible reason behind this ? How can we sensitize people for early diagnosis of hair disease?	Although early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test.   An AI-based application might pave the way to facilitate early disease detection.  Disease detection using machine learning approaches is gaining popularity in health informatics.  Therefore, AI-based approach for fast detection is a good way to make people go for the early diagnosis of the hair diseases.	Some scalp infections may be treatable if diagnosed early. Some but not all diseases may go on their own. Only an expert physician can detect the illness by visual observation. In some cases, early disease detection is beneficial for dermatologists to initiate the treatment. An early scalp inspection includes a dermatoscopic examination of the scalp for inflammation, itching, localized lesion, dandruff, follicular flakes, louse eggs (nits), and a scalp biopsy. Besides visual observation, the patient can undergo blood and hormone tests to detect the exact disease. Unfortunately, most hair and scalp diseases are diagnosed in advanced stages, which complicate the treatment options. All these factors lengthen the diagnosis and treatment process. Therefore, researchers are putting more effort into developing different mechanisms for the early detection of hair and scalp diseases.Disease detection using machine learning approaches is gaining popularity in health informatics. Many skin and scalp-related diseases can be detected using images of infected regions within a few seconds. In one study by Choudhary et al. [18], a framework is developed to differentiate alopecia areata from healthy hair. They obtained 200 healthy hair images from the figaro1K dataset and 68 alopecia areata hair images from DermNet. After a series of enhancement and segmentation, three key features wereAlthough early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test. An AI-based application might pave the way to facilitate early disease detection. In this study, we developed a machine learning model to accurately predict three hair and scalp-related diseases: alopecia, folliculitis, and psoriasis by feeding 150 preprocessed image data into a 2-D convolutional neural network model. After using 70% of the data to train the model, we analyzed remaining 30% of images for testing our model. After subsequent training, the model gave an overall 96.2% training accuracy on the training data and 91.1% validation accuracy for the test data, with a high precision and recall scores for each disease type. We have also provided our dataset with this study. Our proposed system would assist dermatologists and patients with a better understanding of disease classification and initiating early treatment options for the three most frequently occurred hair and scalp diseases.	Although early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test.   An AI-based application might pave the way to facilitate early disease detection.  Disease detection using device learning approaches is gaining popularity in health informatics.  Therefore, AI-based approach for fast detection is a good way to make people go for the early diagnosis of the hair diseases.	Tortured phrases	machine learning -> device learning	
414	paper_144	What were the various sources of data collection in the paper?	In this study, the authors extracted the images from different websites, such as DermQuest, DermNet, MedicineNet, DermnetNZ, and various medical professionals.	The most challenging part of using visual images for disease prediction and disease classification is data collection. Often, one can get fewer appropriate images for a specific illness found. Moreover, the pictures are scattered over the internet. In this study, the authors extracted the images from different websites, such as DermQuest, DermNet, MedicineNet, DermnetNZ, and various medical professionals.				
415	paper_144	What were the various pre processing techniques used before feeding the data to Neural network?	The three preprocessing steps used in this paper are image equalization, image enhancement, and data balancing.  First two parts are mainly for increasing image quality, and the last part is for model versatility.	In this section, we introduce the system workflow of our model and explain the functions of each module in details. As shown in Fig. 2, first, the captured image is sent to preprocessing steps which are divided into three parts: image equalization, image enhancement, and data balancing.Among these three, the first two parts are mainly for increasing image quality, and the last part is for model versatility. After the preprocessing steps, the image is passed to the Neural Network model for the classification task. We used a convolutional neural network that classifies an image successfully into three different classes: alopecia, folliculitis, and psoriasis.				
416	paper_144	Can CNN used in hair disease prediction really give very high accuracy,  given not enough dataset is present for training of model?	In all the models - CNN or SVM or FNN gets very high accuracy.  In all the experiment number of dataset is very limited.	Another study [19] proposed a model for early alopecia detection. They used 100 samples for this research, with 80% as training data and the other 20% as testing data. They looked for four attributes, length of the hair, nail brittleness, amount of damage made to the hair, and hair follicle. Two- layer feed-forward network with a back propagation technique was used for detection purposes. The proposed model system consisting of 4 input neurons, 10 hidden neurons, and a linear output neuron, achieved 91% training accuracy with 86.7% validation accuracy. It showed the best performance at epoch 4 with a 0.059687 gradient. However, the study has some pitfalls, too, as they did not mention their data source or differentiate data classes with their respective sample sizes. Also, no image pre-processing was performed on the collected images. Although there is a possibility of overfitting without a proper data balancing technique, this report did not discuss the data balancing between the two classes. Furthermore, they did not calculate the model’s false- positive and false-negative rates, which is crucial for a model specially developed for the healthcare system.Related work [20] was performed on skin disease detection, where machine learning was used to analyze the digital image of the affected skin area for identifying eczema, melanoma, and psoriasis. Their dataset consists of 80 images from different websites specific to skin diseases. By using a convolutional neural network for feature extraction and applying multiclass SVM on those features, they achieved 100% accuracy in disease classification. However, they did not explore other essential model performance matrices and overfitting issues. In another skin disease detection-based article [21], the authors proposed a scheme to classify skin lesions into five categories: healthy, acne, eczema, benign, and malignant melanoma, using a pre-trained CNN model, AlexNET for feature extraction and error correcting output codes support vector machine for classification. The dataset consists of 9144 images from different sources and achieved 84.21% accuracy using a 10-fold cross-validation technique.algorithm on hair disease images while maintaining high accuracy with good precision and recall scores.Although early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test. An AI-based application might pave the way to facilitate early disease detection. In this study, we developed a machine learning model to accurately predict three hair and scalp-related diseases: alopecia, folliculitis, and psoriasis by feeding 150 preprocessed image data into a 2-D convolutional neural network model. After using 70% of the data to train the model, we analyzed remaining 30% of images for testing our model. After subsequent training, the model gave an overall 96.2% training accuracy on the training data and 91.1% validation accuracy for the test data, with a high precision and recall scores for each disease type. We have also provided our dataset with this study. Our proposed system would assist dermatologists and patients with a better understanding of disease classification and initiating early treatment options for the three most frequently occurred hair and scalp diseases.				
417	paper_144	How does the author conclude that non-local means filter is the best filter for denoising the images ? Are there any other filters that can be used for the same task?	The author tested images on multiple filters including gaussian filter, median filter with kernel_size = 3, bilateral filter, and non-local means filter with patch_size = 3 and patch_distance = 5.  Comparing with other filter non-local means filter best result by preserving all the edges and reducing noise.	Noise is the degradation of image signals caused by external sources [23]. Noise introduces random variations of brightness or color information in the captured images. Most of the time, images on the internet have some noise associated with them. As we have collected most of the data samples from different dermatology websites, the noise in our dataset is not homogeneously distributed, which made it more complex. Therefore, we applied additional filters fordenoising the collected images. We started with the gaussian filter for a better image classification process. However, after using the gaussian filter, the images become completely blurred, which leads to the loss of important information and damage to the edges. We then applied the median filter, which worked better than the gaussian filter with kernel_size = 3. Though we achieved better accuracy using the bilateral filter, we got the best results while applying the non-local means filter with patch_size = 3 and patch_distance = 5. This non-local means filter preserved all the edges and reduced the noise better than the other filters for our application which is shown in Fig. 3.				
418	paper_144	How CLAHE is better than HE for image equalization?	Input image gets high contrast when pass through HE and hence loose information by adding noise.  Compare to that CLAHE is a adaptive histogram equalization method in which the contrast amplification is limited, so as to reduce this problem of noise amplification.	extracted from the images: texture, shape, and color. The researchers divided the dataset into 70%-30% train-test-split and applied a support vector machine (SNM) and k-nearest neighbor (KNN) for the classification task. Overall, they achieved 91.4% and 88.9% accuracy using SVM and KNN, respectively, with a 10-fold cross-validation approach. However, using other machine learning algorithms might increase in the accuracy rate, which should have been discussed. Besides, the application of Histogram Equalization (HE) for image enhancement complicated the process of getting accurate texture features from distorted images, as HE itself adds noise to the output image, distorting the signals. Moreover, this study only shed light on alopecia areata disease, ignoring the inter-class differences between other similar type diseases, which increased the likelihood of inaccurate prediction of other diseases as alopecia areata, thereby making this framework less reliable.Often the captured image doesn’t reflect the natural view and needs contrast enhancement to meet the level of realistic view [24]. Especially images with high color depth and after denoising effects need normalization for a better realistic view [25]. First, we applied histogram equalization (HE). However, the HE increases the contrast of the background when used in images with low color depth, and information is lost as the histogram is not confined to the local region. To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a histogram for each region. After clipping the histogram, we distributed the clipped value over the histogram equalization, which gives us control of the over-amplification of the contrast and generates the resultant image shown in Fig. 4.	Information picture gets high contrast when pass through HE and hence loose information by adding noise.  Compare to that CLAHE is a adaptive histogram equalization method in which the contrast amplification is limited, so as to reduce this problem of noise amplification.	Tortured phrases	Input image -> Information picture	
419	paper_144	What is 'autokeras' ? How it works?	Autokeras is best way to find model parameter.  It automatically tries different combination (in this case is 25) and find size of the model network.  In this case the best size is 3 hidden layer with 1 input and 1 output.	In this study, CNN is utilized for classification because it takes image’s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model’s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.	Autokeras is best way to find model parameter.  It automatically tries different combination (in this case is 25) and find size of the model network.  In this case the best size is 3 buried layer with 1 input and 1 output.	Tortured phrases	hidden layer -> buried layer	
420	paper_144	Author took batch_size to be 16 with 50 epochs while training the model . What was the intution behind taking these particular numbers?	Using grid search the batch_size and epochs is determined.  Since these are the optimal value hence used in the training.	In this study, CNN is utilized for classification because it takes image’s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model’s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.				
421	paper_144	What were the various hyperparameter used in 'grid search'?	Hyperparameters are Batch Size 16, Epoch 50, Kernel Size 3 x 3, Optimizer Adam, Dropout Rate 0. 3, Pooling Size 2 x 2, Activation Function ReLU.	We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.				
422	paper_144	Can using more epochs while training may increase the validation accuracy ? if no why ?	Optimal hyperparameter is used.  Hence for epoch it is the optimal value.  More epochs will not give us better accuracy.	We trained our CNN model using the optimal hyperparameters selected from the grid search. These hyperparameters are listed in Table II. We divided the dataset into 70%-30% train-test-split where 105 randomly selected images are used for training and 45 random images for testing. After applying the preprocessing steps, we used the training dataset to train the CNN model and evaluated the test dataset using the model.				
423	paper_144	Does the author believe using pretrained models may have resulted in better accuracy in classification?	The author discussed effectiveness of pre-trained network in previous work, however, they never used it for their model.	Related work [20] was performed on skin disease detection, where machine learning was used to analyze the digital image of the affected skin area for identifying eczema, melanoma, and psoriasis. Their dataset consists of 80 images from different websites specific to skin diseases. By using a convolutional neural network for feature extraction and applying multiclass SVM on those features, they achieved 100% accuracy in disease classification. However, they did not explore other essential model performance matrices and overfitting issues. In another skin disease detection-based article [21], the authors proposed a scheme to classify skin lesions into five categories: healthy, acne, eczema, benign, and malignant melanoma, using a pre-trained CNN model, AlexNET for feature extraction and error correcting output codes support vector machine for classification. The dataset consists of 9144 images from different sources and achieved 84.21% accuracy using a 10-fold cross-validation technique.				
424	paper_144	Why in the case of Alopecia areata the body's immune cell can't recognise hair follicle as 'self?	Alopecia areata (AA) is an autoimmune disease where the body’s immune cells cannot recognize hair follicles as ‘self. ’ Instead, they consider these follicles as ‘foreign'.  It is an example of a hereditary disease.  But the paper didn't fully discussed why the cell can't recognise hair follicle as 'self'.	Alopecia, folliculitis, and psoriasis are some common causes of hair loss. There is a difference between regular hair fall and alopecia; the latter develops coin-sized bald patches all over the scalp area. Alopecia or patchy hair loss can be of different types. Androgenetic alopecia or male-pattern baldness (MPB) is the most common form of alopecia where the hairline starts to recede, following a pattern where the frontal and temple area are most affected. 70% of men and 40% of women get this type of hair loss and thinning issue [3]. According to Liu et al., MPB is an X-linked polygenic disease, and males are more genetically prone to develop baldness at a mature age [5]. Topical minoxidil solution thickens the hair by 50% [3]. On the other hand, Alopecia areata (AA) is an autoimmune disease affecting individuals irrespective of age and sex. Primarily affecting the scalp area, AA can also spread in the beard, eyelashes, and eyebrows. In this case, the body’s immune cells cannot recognize hair follicles as ‘self.’ Instead, they consider these follicles as ‘foreign,’ which ultimately causes the hair follicles to betargeted and destroyed by the immune cells. It is an example of a hereditary disease. The study from Benigno et al. reported that, in the US alone, 700,000 individuals suffer from AA [6]. This disease, if diagnosed early, might resolve spontaneously. In severe cases, topical corticosteroid or immune therapy is used.				
425	paper_145	Is it true that More than 15% of all deaths in children younger than5 is  due to Pneumonia ?	The question is directly verify as True from the statement that, more than 15% of all deaths in children younger than 5 can be attributed to this cause.	Acute pulmonary infection (pneumonia) is a condition in which the lungs become inflamed due to infection with bacteria, viruses, or fungi; this leads to a condition known as pleural effusion, in which the lungs become swollen with fluid. More than 15% of all deaths in children younger than 5 can be attributed to this cause. Countries with high rates of population growth, pollution, and poor sanitation have the highest rates of pneumonia, and these countries also have				
426	paper_145	What is difference between CNN and D-CNN?	The paper directly didn't mentioned the difference between CNN and D-CNN.  But it is referable from P1 and P2 that, D-CNN multilayered, hierarchical and block-structure compare to CNN.	CNN has built a model of the human brain using the mixture of these networks. CNN layers are organized so that simpler patterns (lines, curves, etc.) are detected initially and more complicated patterns (faces, objects, etc.) are detected afterwards. However, CNN has drawn a lot of interest in data science since it has demonstrated its ability to locate, segment, and identify objects in images. In this study, the term "original CNN architecture" refers to a CNN network and algorithm that are available on Keras or Github. In this study, the CNN algorithm is used exactly as its creators and programmers intended it to be, with no modifications to its processing units, parameterization and hyper-parameter optimization methodologies, design patterns, or layer connections. A well-known CNN network was frequently created and improved by several researchers and programmers over the course of numerous difficulties.Deep learning is a crucial artificial intelligence tool for solving many complicated computer vision problems. Image categorization uses deep learning models, particularly convolutional neural networks (CNNs) problems. Such models work best with lots of data. Because professional clinicians must classify each image, obtaining such a large volume of labeled data for biomedical image classification tasks is expensive and time-consuming. Transfer learning circumvents this issue. This method applies network weights from a model trained on a large dataset to a small dataset problem. Biomedical image categorization often uses CNN models trained on ImageNet, which has over 14 million images. Data scientists have been drawn to the concept of utilizing Deep Convolutional Neural Network (D-CNN) in identifying, classifying, and segmenting brain tumors as a result of the visible benefits of Deep Convolutional Neural Network (D-CNN) in Medical Image Analysis. When it comes to segmenting the timorous region included inside a brain, D-CNN is a set of techniques that has the potential to produce better outcomes when compared to techniques that do not involve deep learning. The multilayered, hierarchical, and block structure of D-CNN allows for the extraction of low-, mid-, and high-level information from pictures of brain tumors. D-CNN shows an outstanding performance in solving the segmentation and classification issues that are based on time and effort consuming tasks like fractionalization of brain tumor cells in Medical Image scans. This is in contrast to the large amount of time and effort that is required for the segmentation process by doctors and radiologists due to the high quantity of data produced by scan centers.				
427	paper_145	Which ensemle learning avg. probablity or weighted avg. probablity is used by the author in modelling?	In P1 the author mentioned they developed a novel weight allocation method.  probability in modelling.	voting are some of the ensemble techniques that have been utilized in research in the literature most frequently. Each constituent base learner is given equal priority by the average probability-based ensemble. But for a specific issue, one basic classifier might be better equipped to gather data than another. Therefore, weighting all of the base classifiers is a better technique. However, the importance of the weights given to each classifier is the most important component in ensuring the ensemble's improved performance. The majority of methods base this number on the outcomes of experiments. In this study, we developed a novel weight allocation method in which the best weights for three base CNN models—SeresNet152, ResNet152v2, and DenseNet- 201, Vgg-19, and Resnext101—were determined using four evaluation metrics: precision, recall, f1-score, and area under the receiver operating characteristics (ROC) curve (AUC). For providing weights to the base learners in research in the literature, only classification accuracy was often taken into account [8], which may not be a sufficient metric, especially when the datasets are class imbalanced. Other indicators might offer more useful data for deciding how important the basic learner is.				
428	paper_145	How does the author take care of class imbalance problem?	Standard deviation was used as a model performance parameter in this study.	Because the dataset used in this experiment did not have any severe imbalances, the standard deviation was used as a model performance parameter in this study. Because this work deals with multi-class sorting, categorical cross- entropy was chosen as a loss task for all CNN architectures. The activation function employed in all transitional layers of the CNN architectures used in this study was relu, while the last layer's activation function was softmax. The following are the hyperparameters that were used: The dropout rate was 0.3, the learning rate was 0.0001, the batch size was 17, and there were 36 epochs. The model weights were updated using an adaptive moment estimation (Adam) optimizer. Before the resizing, all of the photographs were shrunk to the default image size for each architecture.				
429	paper_145	How does author take care of imbalanced class problem?	Standard deviation was used as a model performance parameter in this study.	Because the dataset used in this experiment did not have any severe imbalances, the standard deviation was used as a model performance parameter in this study. Because this work deals with multi-class sorting, categorical cross- entropy was chosen as a loss task for all CNN architectures. The activation function employed in all transitional layers of the CNN architectures used in this study was relu, while the last layer's activation function was softmax. The following are the hyperparameters that were used: The dropout rate was 0.3, the learning rate was 0.0001, the batch size was 17, and there were 36 epochs. The model weights were updated using an adaptive moment estimation (Adam) optimizer. Before the resizing, all of the photographs were shrunk to the default image size for each architecture.				
430	paper_145	What are the various category of architecture author talks about in this section?	The various CNN architecture author talks about are SecrensNet152, MobileNetV2, VGG19, ResNet152v2, ResNeXt101 and DenseNet201.	The performances of the six original individual CNN networks SecrensNet152, MobileNetV2, VGG19, ResNet152v2, ResNeXt100 and DenseNet201 are presented in this section. The models' classification performance is first presented. Following that, the overall measures for those models are discussed. gathering, in addition to descriptors, potential causes, and areas for improvement of results.The performances of the six original individual CNN networks SecrensNet152, MobileNetV2, VGG19, ResNet152v2, ResNeXt100 and DenseNet201 are presented in this section. The models' classification performance is first presented. Following that, the overall measures for those models are discussed. gathering, in addition to descriptors, potential causes, and areas for improvement of results.Six transfer learning CNN architectures' performance is presented in this section. SecrensNet152, MobileNetV2, VGG19, ResNet152v2, ResNeXt101 and DenseNet201 models all had high accuracies in the test sets, as shown in				
431	paper_145	How many extra image is generated for each class?	10 enhanced images were created from each original image.  We can refer that 10 times extra images is generated for each class.	We used data augmentation methods to achieve the goals in the training data. However, color enhancement, such as brightness, contrast, and saturation, as well as position enhancement, by way of scaling, cropping, flipping, and revolution, was used. The technique of data enhancement also included random rotations from -15 to 15 degrees, rotations of 90 degrees by accident, accidental distortion, bending, vertical reversal, horizontal reversal, skate, and luminous intensity conversion. In this approach, 10 enhanced images were created from each original image. The selection of a subset of transformations helps to enhance a heterogeneous image.				
432	paper_145	Was there any particular reason for using the set value of various parameter ? If yes then what were the reasons?	Since hyperparameters tunning need computation resources and those are limited, the authors used set value of various parameter.	Because the dataset used in this experiment did not have any severe imbalances, the standard deviation was used as a model performance parameter in this study. Because this work deals with multi-class sorting, categorical cross- entropy was chosen as a loss task for all CNN architectures. The activation function employed in all transitional layers of the CNN architectures used in this study was relu, while the last layer's activation function was softmax. The following are the hyperparameters that were used: The dropout rate was 0.3, the learning rate was 0.0001, the batch size was 17, and there were 36 epochs. The model weights were updated using an adaptive moment estimation (Adam) optimizer. Before the resizing, all of the photographs were shrunk to the default image size for each architecture.There are a number of limitations in the current stage of the research, which need to address in future work. The use of free-of-charge resources (Google Colab) limits the experiments of this study. As Google Colab offers the server				
433	paper_146	Why does author use K-mode instead of K-means?	K-Modes clustering is more accurate than using K-means clustering as the K-mode algorithm uses categorical data to form clusters.  Since the data in this paper is categorical, K-Modes is used.  This question is found directly in the paper.	The personality prediction is supported in this work by a questionnaire-based investigation. Openness to criticism, flexibility, team spirit, aspirations, and work ethics are among the traits that personality interview questions reveal. This aids to figure out how well a candidate may collaborate and work with team members. The responses to these queries give insight into the qualifications for the position The K- Modes clustering method is used in this survey-based investigation. The technique, which is simple to use and effective with vast amounts of data, is used to group categorical data. Based on the number of comparable categories between data points, clusters are defined. The k- modes clustering algorithm is an advancement over the k- means clustering method. K-means is the most widely used centre-based partitional clustering technique. Huang extends the k-means clustering method to the k-modes clustering algorithm to organize the categorical data:KModes clustering is one of the unsupervised Machine Learning algorithms that is used to cluster categorical variables. It is easy to implement and efficiently handles a large amount of data. A Kmodes technique uses a randomly selected starting cluster centre (modes) as a seed, whichCategorical data cannot be clustered using the K-means clustering method due to the different metrics it uses. The K- mode cluster algorithms are based on the K-mean pattern but eliminate the restriction on numerical data while maintaining their efficacy. By removing the restriction imposed by Kmeans after modification, this K-mode technique extends the K-mean pattern to cluster categorical data:The distance cannot be calculated for categorical data points, though [46]. KModes algorithm is what we choose. It makes use of the differences between the data points (total mismatches). Our data points are more comparable overall, the smaller the differences. Rather than using means, it employs modes [36].K-Modes clustering is more accurate than using K-means clustering as the K-mode algorithm uses categorical data to form clusters.	K-Modes clustering is more accurate than using K-implies bunching as the K-mode algorithm uses categorical data to form clusters.  Since the data in this paper is categorical, K-Modes is used.  This question is found directly in the paper.	Tortured phrases	K-means clustering -> K-implies bunching	
434	paper_146	What are 'AVI-AI' administrative systems?	AVI-AI is a AI based asynchronous video interviewing technology that helps to automate administrative tasks.	This article makes suggestions for using the Ocean model based on AVI (referred to as AVI-AI) or the Big Five personality traits to forecast a person's personality [3]. AVI- AI methods have drawn a lot of interest in the disciplines of computer sciences and human resources, particularly for autonomously evaluating personality traits [4] and communication skills [5]. Unknown are the reliability and accuracy of the ground-breaking employment selection tool known as AVI-AI. Automatically conducting interviews at a specific time is possible with the help of asynchronous video interviewing technology (AVI). The interview can be reviewed by employers at a later time. If employers want to examine how the candidates replied to the questions, they may also allow anyone to watch the recorded interview. It is difficult for human reviewers to accurately.The AVI is a clever administrative system, but there is no evidence to date that it is intelligent or has analytical capabilities. But analysis of this data is necessary [29]. AI- based AI needs to be trained properly. In a study by Hickman et al. in 2021, an AI was instructed to analyse a list of items to automatically assess a person's personality [37].Automated interviewing reduces the administrative burden on hiring teams, enhancing flexibility, efficiency, and automation of administrative tasks. By removing pointless processes, they assist businesses in finding top personnel more quickly.				
435	paper_146	What are various features used to judge person facial emotion and speech emotion ?	Face clues, personality scores, common sense knowledge and psycho-linguistic features are used to judge person facial emotion and speech emotion.	To estimate scores, several artificial neural networks (ANNs) were trained on a sizable labelled dataset. It utilised a cascade of ANNs to forecast personality traits from static face images to examine the connections between signals from stationary facial expression cues and self-reported personality traits. The finding provides strong evidence that multidimensional personality profiles can be predicted using ANNs trained on substantial labelled datasets from static facial photos. According to the study, advanced computer vision algorithms can be used to realise personality traits in real-world photos obtained in unexpected situations. shows unequivocally that each of the Big Five features is connected to a collection of face clues that can be gathered by using machine learning methods. [15]Evaluation of the best approaches for automated personality detection, which include advanced machine learning algorithms with a focus on multi-modal methods, a variety of data processing datasets, and potential uses [28]. The paper also explored that the most specific attributes for unimodal personality detection come from the visual modality. Combining inputs from multiple modalities frequently increases prediction accuracy. The accuracy was found to significantly increase when common sense knowledge and psycho-linguistic features were combined. This investigation only considered computational methods and excludes psychological studies on personality detection because it encompasses such a broad and varied topic as personality detection.end-to-end AI conducting interviews system. The above system performs automatic personality recognition based on features extracted from the AVIs and the genuine personality scores from the respondents' self-reported survey questions and facial gestures. Employers can later evaluate sound records using this method [32]. Based on the above studies to determine a person's personality, we have employed K-Model clustering and the OCEAN model to predict the personalities.				
436	paper_146	How does AVI-AI model functions?	AVI-AI is a end-to-end AI conducting interviews system.  The above system performs automatic personality recognition based on features extracted from the AVIs.	end-to-end AI conducting interviews system. The above system performs automatic personality recognition based on features extracted from the AVIs and the genuine personality scores from the respondents' self-reported survey questions and facial gestures. Employers can later evaluate sound records using this method [32]. Based on the above studies to determine a person's personality, we have employed K-Model clustering and the OCEAN model to predict the personalities.Once the company has determined the requirements of the position, it can use the AVI-AI-based system which uses OCEAN Model to evaluate candidates' various personality qualities. To support its analysis, a questionnaire-based study using the K-modes clustering algorithm is also used.				
437	paper_146	How does the proposed model increases the reliablity of the assesment?	Automated interviewing enhances evaluation consistency by establishing an organized and objective hiring procedure, assisting recruiters in quickly identifying the greatest fit.  As said in the paper, by developing an orderly and objective hiring approach proposed model increases the reliablity of the assesment.	Automated interviewing enhances evaluation consistency by establishing an organized and objective hiring procedure, assisting recruiters in quickly identifying the greatest fit.				
438	paper_146	Is it true the proposed model enhances the efficiency of the interviews?	It is true.  Proposed model enhances the efficiency of the interviews by conducting multiple at the same time also makes it place-independent.	Automated video interviews are efficient in more ways than one. They not only make it possible to schedule several interviews at once quickly, but they can also do it anywhere. As a result, the business can utilize the skills of a worker who is employed elsewhere in the world but is unable to relocate for a variety of reasons.Automated interviewing reduces the administrative burden on hiring teams, enhancing flexibility, efficiency, and automation of administrative tasks. By removing pointless processes, they assist businesses in finding top personnel more quickly. The following are some advantages of automated interviews:				
439	paper_146	What are the various component of individual work performance?	Task performance, contextual performance and counter productive to work behaviour are the three components of individual work performance.	To assess how well current employees are working, individual work performance (IWP), a useful and regularly used outcome measure, is often utilized. Job performance may be correlated with personality. The phrase "behaviours or acts that are related to the aims of the organization" [7] is a definition of IWP. IWP thus emphasizes employee behaviours or activities rather than the outcomes of those behaviours. Additionally, behaviours should be in the individual's control, omitting those that are limited by the environment [8]. The personalities of the employees at any given time can be ascertained from their answers to a series of questions that can be given to them. The first dimension, task performance, traditionally has received the most attention and can be defined as "the proficiency with which individuals perform the core substantive or technical tasks central to his or her job" [7]. The second dimension of IWP is contextual performance, defined as “behaviours that support the organizational, social and psychological environment in which the technical core must function” [9]. The third dimension of IWP is counterproductive to work behaviour, defined as “behaviour that harms the well-being of the organization” [8].				
440	paper_146	Is AVI followed by certain set of questionnaire for the implementation of model?	For the implementation of the model no set of questionnaire is used.  But to support the result of the model a questionnaire-based study used.	Once the company has determined the requirements of the position, it can use the AVI-AI-based system which uses OCEAN Model to evaluate candidates' various personality qualities. To support its analysis, a questionnaire-based study using the K-modes clustering algorithm is also used.				
441	paper_146	How does the author choose optimal number of cluster in the proposed model ?	Elbow method is used to choose optimal number of cluster in the proposed model.	To determine the optimal number of clusters, the Elbow method is used but it is modified to use within cluster difference. From the results of plotting within cluster differences for various values, the principle of the Elbow method takes the value of k at the point when the value does not decrease significantly with the addition of the value of k.				
442	paper_15	Can image content and style be "fully" or "completely" separated?	The paper suggests that it is impossible to completely separate the content and the style of the image.  But it is possible to extract their representations to then combine them with a loss function that allows the generation of visually appealing images that somewhat satisfy (not fully) the content and stylistic constraints.  It is important to mention that the artistic style representation is just a correlation of filter responses between layers in CNN.  The paper suggests that this is a plausible way to obtain the content-independent visual appearance of the image.  When the object recognition model is learning, it has to be able to extract features that are invariant to different variations of images.  Thus, it allows the separation of content and style representations.  Previous methods use non-parametric techniques that directly manipulate the pixels of the image without such separation of representations.	Again, we can visualise the information captured by these style feature spaces built on different layers of the network by constructing an image that matches the style representation of a given input image (Fig 1, style reconstructions). 10,11 Indeed reconstructions from the style features produce texturised versions of the input image that capture its general appearance in terms of colour and localised structures. Moreover, the size and complexity of local image structures from the input image increases along the hierarchy, a result that can be explained by the increasing receptive ﬁeld sizes and feature complexity. We refer to this multi-scale representation as style representation . The key ﬁnding of this paper is that the representations of content and style in the Convo- lutional Neural Network are separable. That is, we can manipulate both representations inde- pendently to produce new, perceptually meaningful images. To demonstrate this ﬁnding, we generate images that mix the content and style representation from two different source images. In particular, we match the content representation of a photograph depicting the “Neckarfront” in T ¨ ubingen, Germany and the style representations of several well-known artworks taken from different periods of art (Fig 2). The images are synthesised by ﬁnding an image that simultaneously matches the content representation of the photograph and the style representation of the respective piece of art (see Methods for details). While the global arrangement of the original photograph is preserved, the colours and local structures that compose the global scenery are provided by the artwork. Effectively, this renders the photograph in the style of the artwork, such that the appearance of the synthesised image resembles the work of art, even though it shows the same content as the photograph. As outlined above, the style representation is a multi-scale representation that includes mul- tiple layers of the neural network. In the images we have shown in Fig 2, the style representationincluding only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.Previous work on separating content from style was evaluated on sensory inputs of much lesser complexity, such as characters in different handwriting or images of faces or small ﬁgures in different poses. 12,13 In our demonstration, we render a given photograph in the style of a range of well-known artworks. This problem is usually approached in a branch of computer vision called non- photorealistic rendering (for recent review see 14 ). Conceptually most closely related are meth- ods using texture transfer to achieve artistic style transfer. 15–19 However, these previous ap- proaches mainly rely on non-parametric techniques to directly manipulate the pixel representa- tion of an image. In contrast, by using Deep Neural Networks trained on object recognition, we carry out manipulations in feature spaces that explicitly represent the high level content of an image. Features from Deep Neural Networks trained on object recognition have been previously used for style recognition in order to classify artworks according to the period in which they were created. 20 There, classiﬁers are trained on top of the raw network activations, which we call content representations. We conjecture that a transformation into a stationary feature space such as our style representation might achieve even better performance in style classiﬁcation. In general, our method of synthesising images that mix content and style from different sources, provides a new, fascinating tool to study the perception and neural representation of art, style and content-independent image appearance in general. We can design novel stimuli that introduce two independent, perceptually meaningful sources of variation: the appearance and the content of an image. We envision that this will be useful for a wide range of experimen- tal studies concerning visual perception ranging from psychophysics over functional imaging to even electrophysiological neural recordings. In fact, our work offers an algorithmic under- standing of how neural representations can independently capture the content of an image and the style in which it is presented. Importantly, the mathematical form of our style representa- 8tions generates a clear, testable hypothesis about the representation of image appearance down to the single neuron level. The style representations simply compute the correlations between different types of neurons in the network. Extracting correlations between neurons is a bio- logically plausible computation that is, for example, implemented by so-called complex cells in the primary visual system (V1). 21 Our results suggest that performing a complex-cell like computation at different processing stages along the ventral stream would be a possible way to obtain a content-independent representation of the appearance of a visual input. All in all it is truly fascinating that a neural system, which is trained to perform one of the core computational tasks of biological vision, automatically learns image representations that allow the separation of image content from style. The explanation could be that when learning object recognition, the network has to become invariant to all image variation that preserves object identity. Representations that factorise the variation in the content of an image and the variation in its appearance would be extremely practical for this task. Thus, our ability to abstract content from style and therefore our ability to create and enjoy art might be primarily a preeminent signature of the powerful inference capabilities of our visual system.				
443	paper_15	Which loss function is used by authors during image synthesis?	The loss function consists of 2 separate terms for content representation and artistic style representation.  The difference between the content representation of the original image and the reconstructed image is calculated by taking the squared-error loss of the two.  While the difference between the stylistic representation of the original artwork and the reconstructed image is calculated by taking the mean-square distance for each layer and combining them by averaging the weighted sum.  In the paper, the weights for each style representation of a layer are distributed equally.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsincluding only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.On top of the CNN responses in each layer of the network we built a style representation				
444	paper_15	To carry out manipulations in feature spaces, did the authors use a pretrained Deep Neural Networks or trained the model from scratch?	However, the basis of their model is VGG-Network without its fully connected layers.  Also, they obtained better gradient flow and better results when replacing the max pooling with the average pooling.	The results presented in the main text were generated on the basis of the VGG-Network,22				
445	paper_15	Why did the authors particularly use "Gradient Descent" instead of any other optimization algorithm?	The work uses gradient descent to transform the white noise image to match the stylistic and content representations of an artwork and a photograph respectively.  However, they do not discuss the reasons behind choosing gradient descent over other methods and do not provide alternatives.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	The work uses Mini-Batch gradient descent and Stochastic gradient descent to transform the white noise image to match the stylistic and content representations of an artwork and a photograph respectively.  However, they do not discuss the reasons behind choosing gradient descent over other methods and do not provide alternatives.	Change concept	gradient descent -> Mini-Batch gradient descent, Stochastic gradient descent	
446	paper_15	What does a "Gram" matrix mean?	The stylistic representation of the image in a single layer is calculated as the Gram matrix of vectorized feature maps of that layer.  The gram matrix is a matrix of the inner products of each vector.  In other words, G_i_j = \sum(V_i_k * V_j_k).	On top of the CNN responses in each layer of the network we built a style representation				
447	paper_15	What is the benefit of using "white noise" instead of any other noise like Salt-and-pepper or Gaussian noise?	The authors use the white noise image as a starting point for the loss function to turn it into a combination of given images.  And the results suggest that it works well.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	The authors use the Gaussian noise image as a starting point for the loss function to turn it into a combination of given images.  And the results suggest that it works well.	Change concept	white noise -> Gaussian noise	
448	paper_15	The authors measure mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. Which distance measure did they use (For example, Euclidean distance, Manhattan distance etc.)?	The authors calculate the difference between the stylistic representations of two images as the weighted average of the mean-squared distance of respective Gram matrices at each layer.  Specifically, the distance between two Gram matrices of certain layer l is calculated as E_l = \frac{1}{4*N_l^2*M_l^2} * (G_l_i_j^2 * A_l_i_j^2).  In other words, it is the mean of squared distance in Euclidean space.	On top of the CNN responses in each layer of the network we built a style representation	The authors calculate the difference between the stylistic representations of two images as the weighted average of the mean-squared distance of respective Gram matrices at each layer.  Specifically, the distance between two Gram matrices of certain layer l is calculated as E_l = \frac{1}{4*N_l^2*M_l^2} * (G_l_i_j^2 * A_l_i_j^2).  In other words, it is the mean of squared distance in Manhattan space.	Change concept	Euclidean distance -> Manhattan distance	
449	paper_15	How was the ratio α/β of weighting factors for content (α) and style reconstruction(β) used by the authors?	Since it is difficult to satisfy both content and stylistic constraints on the resulting image, the α and β weights in the loss function are used to manipulate the emphases on the content and stylistic representations respectively.  Several different ratios of α/β (10^-5, 10^-4, 10^-3, 10^-2) are explored to demonstrate the differences between synthesized images.  In general, it allowed smooth and continuous regulation of two separate terms of the loss function, thus producing more visually pleasing images.	including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.To generate the images that mix the content of a photograph with the style of a painting	Since it is difficult to satisfy both content and stylistic constraints on the resulting image, the α and β weights in the loss function are used to manipulate the emphases on the content and stylistic representations respectively.  Several different ratios of α/β (10^-5, 10^-4, 10^-3, 10^-2) are explored to demonstrate the differences between synthesized images and source images.  In general, it allowed smooth and continuous regulation of two separate terms of the loss function, thus producing more visually pleasing images.	Change concept	between synthesized images -> between synthesized images and source images. 	
450	paper_15	What is an example of usefulness of authors' work for experiments concerning electrophysiological neural recordings?	The work only claims that the idea of separating the sources of variation in visual perception might be useful for a range of experiments from psychophysics to electrophysiological neural recordings.  It does not go into detail about examples of such experiments.	In general, our method of synthesising images that mix content and style from different	The work not only claims that the idea of separating the sources of variation in visual perception might be useful for a range of experiments from psychophysics to electrophysiological neural recordings, but also go into detail about examples of such experiments.	Change concept	The work only claims ... It does not go into detail ... -> The work not only claims ..., but also ...	
451	paper_15	Would the reconstruction from higher layers be as good as reconstruction from the lower layers? Why or why not?	The content representation of the photograph resembles the pixel-wise image more in the lower layers, but encodes the more high-level contents in the higher layers.  To construct the results in Figure 2, the authors use the content representation from one of the highest layers 'conv_4_2', which means they fuse well with the extracted style representations.  Also, it is possible to change the emphasis between the content representation and style representation using the loss function.	The class of Deep Neural Networks that are most powerful in image processing tasks are called Convolutional Neural Networks. Convolutional Neural Networks consist of layers of small computational units that process visual information hierarchically in a feed-forward man- ner (Fig 1). Each layer of units can be understood as a collection of image ﬁlters, each of which extracts a certain feature from the input image. Thus, the output of a given layer consists of so-called feature maps: differently ﬁltered versions of the input image. When Convolutional Neural Networks are trained on object recognition, they develop a representation of the image that makes object information increasingly explicit along the pro- cessing hierarchy. 8 Therefore, along the processing hierarchy of the network, the input image is transformed into representations that increasingly care about the actual content of the im- age compared to its detailed pixel values. We can directly visualise the information each layer contains about the input image by reconstructing the image only from the feature maps in that layer 9 (Fig 1, content reconstructions, see Methods for details on how to reconstruct the im- age). Higher layers in the network capture the high-level content in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruc- tion. (Fig 1, content reconstructions d,e). In contrast, reconstructions from the lower layers simply reproduce the exact pixel values of the original image (Fig 1, content reconstructions a,b,c). We therefore refer to the feature responses in higher layers of the network as the content representation . To obtain a representation of the style of an input image, we use a feature space originally designed to capture texture information. 8 This feature space is built on top of the ﬁlter responses in each layer of the network. It consists of the correlations between the different ﬁlter responsesthe images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ ( w l = 1 / 5 in those layers, w l = 0 in all other layers) . The ratio α/β was either 1 × 10 − 3 (Fig 2 B,C,D) or 1 × 10 − 4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor w l was always equal to one divided by the number of active layers with a non-zero loss-weight w l .including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.	The content representation of the photograph resembles the pixel-wise image more in the higher layers, but encodes the more high-level contents in the lower layers.	Change concept	lower -> higher, higher -> lower	The content representation of the photograph resembles the pixel-wise image more in the lower layers, but encodes the more high-level contents in the higher layers.
452	paper_15	How did the authors ensure to keep the factor wl equal to one divided by the number of active layers with a non-zero loss-weight wl?	The weights w_l can manipulate the emphases between stylistic representations obtained from different layers.  To make the roles of each stylistic representation of each layer equal, the w_l is always kept at one divided by the number of active layers.  They are constants in the loss function that are set before starting to optimize the loss function.	On top of the CNN responses in each layer of the network we built a style representation that computes the correlations between the different filter responses, where the expectation is taken over the spatial extend of the input image. These feature correlations are given by the Gram matrix Gl ∈ RNl×Nl , where Gl ij is the inner product between the vectorised feature map and j in layer l: Gl ij = ∑ k F l ikF l jk. (3) To generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to find another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let ~a and ~x be the original image and the image that is generated and Al and Gl their respective style representations in layer l. The contribution of that layer to the total loss is then El = 1 4N 2 l M 2 l ∑ i,j (Gl ij − Al ij )2 (4) and the total loss is Lstyle(~a, ~x) = L∑ l=0 wlEl (5) where wl are weighting factors of the contribution of each layer to the total loss (see below for specific values of wl in our results). The derivative of El with respect to the activations in layer l can be computed analytically: ∂El ∂F l ij = { 1 N 2 l M 2 l ((F l)T (Gl − Al)) ji if F l ij > 0 0 if F l ij < 0 . (6) The gradients of El with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The five style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e).To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation of the photograph in one layer of the network and the style representation of the painting in a number of layers of the CNN. So let ~p be the photograph and ~a be the artwork. The loss function we minimise is Ltotal(~p, ~a, ~x) = αLcontent(~p, ~x) + βLstyle(~a, ~x) (7) where α and β are the weighting factors for content and style reconstruction respectively. For the images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (wl = 1/5 in those layers, wl = 0 in all other layers) . The ratio α/β was either 1×10−3 (Fig 2 B,C,D) or 1 × 10−4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor wl was always equal to one divided by the number of active layers with a non-zero loss-weight wl.	The weights w_l can manipulate the emphases between stylistic representations obtained from different layers.  To make the roles of each stylistic representation of each layer equal, the w_l is always kept at one divided by the number of active layers.  They are constants in the loss function that are set before starting to optimize the misfortune work.	Tortured phrases	loss functino -> misfortune work	
453	paper_151	What makes SBM-Transformer novel compared to existing efficient Transformer variants?	SBM-Transformer is the first Transformer architecture that can data-adaptively choose between linear to full attention with respective computational costs.	To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [2], each attention head samples a bipartite graph connecting queries to keys. Then, the adjacency of the sampled graph is used as an attention mask so that only attention scores corresponding to sampled edges are computed.The overall computational cost is linear in the number of edges, which can range from linear to quadratic in sequence length depending on the data and task under concern. Each attention head is equipped with its own underlying SBM, enabling the model to diversify the attention sparsity across heads and layers. By incorporating a straight-through estimator [4] in the discrete graph-sampling step, SBM-Transformer enjoys end-to-end differentiability and can find the proper attention sparsity based solely upon minimizing the predictive loss. The model can also easily be further regularized by penalizing the number of sampled edges, which results in a lighter model using less computational resources during inference. To the best of our knowledge, our method is the first Transformer architecture that can data-adaptively choose between linear to full attention with respective computational costs. To summarize, our main contributions are as follows:				
454	paper_151	What is a mixed-membership Stochastic Block Model?	The mixed-membership Stochastic Block Model (SBM) is a generative model that encodes the latent structure of graphs by assigning each node into multiple clusters.	The Stochastic Block Model (SBM) is a generative model that encodes the latent structure of graphs by grouping nodes into clusters. By modeling the cluster-membership of each node as well as inter-cluster relationships, SBMs can represent a wide variety of graph structures, which is a feature especially useful for generating new graphs or predicting missing edges in noisy data [1]. The standard SBM assigns each node to a single cluster, and the probability of an edge between two nodes strictly depends on the corresponding clusters. Several structural extensions include overlapping SBM [24] and mixed-membership SBM [2], which allow each node to be assigned to multiple clusters. The underlying SBM used by our framework mostly resembles these two variants, while the edge probability is modeled by a nonlinear function of two node embeddings rather than a bilinear one. There exist many other extensions including degree-corrected SBM [20] for multi-graphs and hierarchical SBM [31] for multiplex-graphs. Further details can be found in a recent survey [16].				
455	paper_151	In what way can SBM-Transformer be considered better than Reformer?	SBM-Transformer allows more flexible attention mask structures between linear to full attention with respective computational costs, while Reformer can only use block-diagonal masks that cannot model hierarchical contexts.	Table 8 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking first in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all five tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we find that flexibility of SBM is indeed beneficial, as Reformer struggles in ListOps, most likely due to the inability of block-diagonal masks to model hierarchical contexts.To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [2], each	SBM-Transformer allows more flexible attention mask structures between linear to full attention with respective computational costs, while Reformer can only use block-diagonal masks that can only model hierarchical contexts.	Change concept	cannot model -> can only model	
456	paper_151	What is a "Hamiltonian path"?	A Hamiltonian path is a path that visits all nodes in a graph.	Then, we can show that these three patterns form directed graphs that together satisfy the three				
457	paper_151	What is "local attention"?	Local attention is a Transformer model that uses a sliding window of some fixed context window size.	One way to remove the quadratic bottleneck from the attention score matrix is to apply a binary mask \bm{M}\in\{0,1\}^{n\times n} and compute the scaled dot-products \bm{Q}_{i}\bm{K}_{j}^{T}/\sqrt{d_{h}} only if \bm{M}_{ij}=1. In presence of an attention mask, the operation is modified to\displaystyle\texttt{Attn}_{\text{mask}}(\bm{X},\bm{M})=\sigma_{\bm{M}}\left(\bm{M}\odot\dfrac{\bm{Q}\bm{K}^{T}}{\sqrt{d_{h}}}\right)\bm{V}(3)\displaystyle\sigma_{\bm{M}}(\bm{A})_{ij}\coloneqq\begin{cases}\dfrac{\exp(\bm{A}_{ij})}{\sum_{k\in\{k^{\prime}|\bm{M}_{ik^{\prime}}=1\}}\exp(\bm{A}_{ik})}&\text{if}\;\;\bm{M}_{ij}=1\\\hfil 0&\text{otherwise}\end{cases}(4)where \odot indicates entry-wise multiplication. Note that the masked-softmax \sigma_{\bm{M}}(\cdot) operator only computes unmasked terms, ensuring that each (i,j)-th attention score survives as nonzero if and only if \bm{M}_{ij}=1. This is thus equivalent to filling in the (i,j)-th attention score with -\infty if \bm{M}_{ij}=0, then applying the standard softmax operator. Most sparsity-based efficient Transformers fall under this formulation, while using different methods to either manually fix or learn the mask \bm{M}. For instance, local attention [9, 3, 51] with a sliding window sets \bm{M}_{ij}=1 if |i-j|<c for some context window size c while Reformer [22] sets \bm{M}_{ij}=1 if \bm{Q}_{i} and \bm{K}_{j} are hashed into the same bucket.				
458	paper_151	How is the "average attention sparsity" measured in the experiments?	The average attention sparsity is measured by the densities of masks sampled in SBM-Transformer averaged across all attention heads.	To test if the model can effectively learn under a constraint on the computational cost, we also test the model under a sparsity-based regularizer that discourages excessive use of query-key edges. We penalize each sampled edge by adding to the predictive loss a weighted regularization term \lambda\mathcal{L}_{s}, where \mathcal{L}_{s} denotes the average mask density across all attention heads. Table 9 shows the performance of SBM-Transformer across varying regularization weights. Under strong regularization, the model surprisingly retains competitive performance while significantly reducing the average mask density.This indicates that similar local optima are shared across regimes with varying attention density in the loss landscape, and the regularization term is able to drive the model towards finding optimal attention scores with smaller density.We also compare the densities of masks sampled at each layer of SBM-Transformer during test time to examine whether our model is capable of diversifying sparsity across layers for better performance. Recall that this allows models to gather information in different levels, as seen in pretrained BERT where lower layers focus on the overall content via dense attention while upper layers gather syntactic information with tree-like patterns [11]. For each of the five tasks, we pick two highest-performing models (one for unregularized and another for regularized) for measurement. Figure 5 shows the average layer-wise mask densities of unregularized and regularized SBM-Transformers across different tasks. We find that under no regularization, the two layers can differ by more than 10% in tasks such as ListOps and Image. This may be due to the hierarchical and compositional structure of the two tasks. We also find that the variation is relatively low in Text with densities around 25%, indicating that the task requires broad attention overall. Lastly, the standard deviation is extremely large in upper layers for Pathfinder, showing that it samples a wide variety of masks depending on the input.				
459	paper_151	Why was the random edge exploration technique used during training of SBM-Transformer?	The random edge exploration technique allows SBM-Transformer to avoid the problem of having edge probabilities accidentally collapsing to zero and to explore new edges and resuscitate their sampling probabilities if necessary.	While this approach enables backpropagation in the same O(m) cost				
460	paper_151	Transformers are typically used with multiple attention layers and heads. Why did the authors use a single-layer single-head Transformer architecture for the synthetic task of finding repeated tokens?	Using a single-layer and single-head architecture forces a constrained setting where the sole head must perform full attention to compare each token to all the others in order to attain 100% accuracy.	For this task, we compare SBM-Transformer with k=128 clusters against various efficient Transformers: Linear Transformer [21], Linformer [45], Reformer [22], Performer [10], and Nyströmformer [48]. Across all methods, we use a single-layer and single-head architecture with 32 hidden dimensions. Note that due to this constrained setting, the sole head must perform full attention to compare each token to all the others in order to attain 100% accuracy. All models are trained for 2000 epochs where a new batch of sequences is sampled on-the-fly at each epoch. We use a batch size of 256 and learning rate of 1e-3.				
461	paper_151	The forward step of SBM-Transformer requires additional parameters and computation compared to the original Transformer architecture due to SBM sampling. Is this additional cost outweighed by exploiting sparsity?	SBM-Transformer is efficient compared to existing baselines in terms of FLOP count and peak memory use, but can result in longer runtimes due to sparse tensor operations being less optimized on GPU kernels.	Furthermore, we compare computational costs during inference by measuring FLOP count and peak memory usage. For SBM-Transformer, we test the model trained under \lambda=10^{-1}. Due to lack of support for sparse tensor operations in existing FLOP-counters, we measure FLOP counts by manually enumerating through each tensor operation. Table 3 shows that SBM-Transformer is comparably efficient across all tasks except for Text, where SBM-Transformer showed the largest average mask density. Note that while the cost of other baselines are fixed after initialization, the cost of SBM-Transformer is data-adaptive and can vary input-by-input. Further analysis and qualitative examples demonstrating the input-dependent attention mask densities can be found in Appendix C.Nonetheless, there are limitations due to sparse tensor operations being less optimized on GPU kernels. In the LRA experiments, we found that SBM-Transformer can result in longer runtimes compared to dense counterparts while its memory usage is much lower. While previous sparsity-based attention mechanisms with block-sparse attention are much more amenable for GPU computation [51, 9, 3], our work requires an architecture with better workload balancing and acceleration under unstructured sparsity, for which there is ongoing work [46, 54].				
462	paper_151	Why is each attention head equipped with a 2-layer MLP in particular?	The node embeddings are obtained by processing each query and key through the 2-layer MLP, mapping token representations into the node representation space.	For proper parameterization of the SBM, we must infer the nonnegative node-memberships and block matrix from the queries and keys. To do so, we equip each attention head a 2-layer MLPdh→dh with ReLU activation, and a set of k trainable cluster-embeddings C ∈ R k×dh . First, our model computes the block matrix Sˆ ∈ R k×k + by taking dot products amongst cluster-embeddings C followed by a 2-dimensional softmax activation. The node embeddings are obtained by processing each query and key through the MLPdh→dh				
463	paper_151	The proof of Theorem 1 is a direct application of previous results on sparse Transformers. What is the exact significance of this theoretical result?	We show that the low-rank structure of the underlying SBMs does not degrade the expressive power of Transformer, and that SBM-Transformer can universally approximate arbitrary functions with \mathcal{O}(n) connections.	Leveraging previous work on the theoretical expressiveness of sparse attention [50, 51], we show that SBM-Transformer with a small modification111Here we consider a variant of SBM-Transformer where self-loops are added manually (i.e. \bm{M}_{ii}=1 for all i). While this is useful in theoretical analysis, we find that not having self-loops slightly helps in empirical performance and hence omit self-loops for the main experiments. retains the same level of expressibility as full attention. Specifically, we show that the low-rank structure of the underlying SBMs does not degrade the expressive power of Transformer, and that SBM-Transformer can universally approximate arbitrary functions with \mathcal{O}(n) connections. For brevity, we provide a rough overview of the proof and defer further details to Appendix A.				
464	paper_152	Considering that GMPool requires matrix decomposition, how good is the efficiency aspect of the algorithm? Can the algorithm be used for large graphs?	GMPool may not be eligible to be used for large graphs as is due to the large cubic time complexity.	After acquiring the pooling operator, the pooling process becomes obvious. Nodes are in fundamental representation while edge features and adjacency matrix are in adjoint representation. Which leads to the following transformation rules.\displaystyle X_{i}^{(l+1)}=S^{(l)}X_{i}^{(l)}(13)\displaystyle E_{ij}^{(l+1)}=S^{(l)}E_{ij}^{(l)}S^{(l)T}(14)\displaystyle A_{ij}^{(l+1)}=S^{(l)}A_{ij}^{(l)}S^{(l)T}(15)If grouping is properly done, 0 (or close to 0) components will appear in the decomposed eigen value matrix. These zero eigenvalues arise naturally and play a role in disregarding group information; those are ineffective towards prediction. However, zero elements in the eigen values causes a major problem in the decomposition process since the matrix might carry a singular determinant.Eigen decomposition is based on an iterative approximation algorithm which includes unbounded terms if any two eigen values are small or close. One can see clearly about this matter in DBLP:journals/corr/IonescuVS15 .\Big{(}\frac{\partial{l}}{\partial{A}}\Big{)}=U\big{(}K^{T}\odot(U^{T}\frac{\partial{l}}{\partial{U}})+(\frac{\partial{l}}{\partial{\Lambda}})_{\textrm{diag}})(U^{T})(16)Here, \odot denotes element-wise product. Off-diagonal components of K=1/(\lambda_{i}-\lambda_{j}) causes the problem, since the value blows up to the infinity if any two eigen values are close or very small. However, there are some solutions for this matter by approximating gradient in different ways DBLP:journals/corr/abs-1906-09023 ; 9400752 ; DBLP:journals/corr/abs-2105-02498 . Those methods are developed further to achieve higher speed in the calculation DBLP:journals/corr/abs-2201-08663 . They claim that the method is noticeably faster, over 8 times, than the standard SVD which has the time complexity \mathcal{O}(n^{3}). Thus, we utilized this method in our work to stabilize and accelerate the learning process. However, since the algorithm achieves the higher speed by approximating gradients, the error compared to standard SVD grows bigger as the size of the matrix grows. Therefore, this method might not be valid with large sized graph data.				
465	paper_152	How does NGMPool work exactly? How is it different from GMPool?	NGMPool is a single-pooling variant of GMPool that does not perform SVD on the grouping matrix, but rather uses the grouping matrix as is.	To overcome this challenge, we propose GMPool, a general pooling framework that does not require an universal number of clusters as a user hyperparameter. Figure 1 depicts the overall framework of GMPool. The core intuition is that the product of a pooling matrix with itself forms a grouping matrix, where each (i,j)-th entry indicates the pairwise clustering similarity: whether the nodes i and j are pooled to the same clusters. For each graph, GMPool parameterizes the clustering similarities in its grouping matrix via a classification layer. Finally, we perform SVD on the grouping matrix to obtain the pooling matrix such that the overall rank represents the suitable number of clusters. We also test a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is. In real-world molecular property prediction tasks, we show that our approach outperforms previous baselines, while successfully learning suitable clusters.The main contributions of this paper are as follows:•We design a grouping matrix-based pooling operator that does not require users to specify the number of clusters a priori.•We propose GMPool and NGMPool. GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is.•We demonstrate the power of our methods both quantitatively and qualitatively on a wide range of real molecular property prediction tasks.				
466	paper_152	The paper mentions Eigenvalue Decomposition (EVD) as well as Singular Value Decomposition numerous times. How are the two related, and how are they different?	The grouping matrix is symmetric and real, which guarantees to have real eigen values as well as vectors.  The additional connection between SVD and EVD given this property of the grouping matrix cannot be found in this paper.	The pooling operator S is a square matrix with size of nl × nl, yet the eigen value Λ suppresses				
467	paper_152	How was the hyperparameters chosen for the baseline methods, and what were the chosen values for he experiments presented?	For baseline pooling methods, we perform grid search following previous work, and present best results.  We fix the final pooling size to 10 as the average size of most common 40 functional groups in bioactive molecules is 4.	For baseline pooling methods that require the cluster size as a hyperparameter, we perform grid search across candidates following previous work, and present best results.However, we fix the final pooling size to 10 as the average size of most common 40 functional groups in bioactive molecules is 4.25 ertl2020most , indicating that molecules under concern (statistics shown in Table 1) can have up to 10 clusters.The specific hyperparameter setups used for pooling baselines can be found in appendix.				
468	paper_152	What makes GMPool and NGMPool novel compared to existing graph pooling methods?	GMPool and NGMPool overcome the limitation of existing pooling frameworks that require a universal number of clusters as user parameter by first building a grouping matrix and decomposing the matrix into its square-root form.	In this section, we propose a novel differentiable pooling layer, GMPool, which obtains the pooling matrix by first building a grouping matrix that contains clustering similarities of pairwise nodes and then decomposing the matrix into its square-root form. We start the section with preliminary information, then outline the details of GMPool in later sections.To overcome this challenge, we propose GMPool, a general pooling framework that does not require an universal number of clusters as a user hyperparameter. Figure 1 depicts the overall framework of GMPool. The core intuition is that the product of a pooling matrix with itself forms a grouping matrix, where each (i,j)-th entry indicates the pairwise clustering similarity: whether the nodes i and j are pooled to the same clusters. For each graph, GMPool parameterizes the clustering similarities in its grouping matrix via a classification layer. Finally, we perform SVD on the grouping matrix to obtain the pooling matrix such that the overall rank represents the suitable number of clusters. We also test a single-pooling variant NGMPool that does not perform any decomposition, but rather uses the grouping matrix as is. In real-world molecular property prediction tasks, we show that our approach outperforms previous baselines, while successfully learning suitable clusters.The main contributions of this paper are as follows:•We design a grouping matrix-based pooling operator that does not require users to specify the number of clusters a priori.•We propose GMPool and NGMPool. GMPool performs SVD on the grouping matrix to obtain the pooling matrix, whereas NGMPool utilizes the grouping matrix as is.•We demonstrate the power of our methods both quantitatively and qualitatively on a wide range of real molecular property prediction tasks.However, the pooling methods above all share a common limitation: the number of clusters must be predefined for each layer as hyperparameters. This limitation is especially detrimental in inductive settings such as molecular property prediction, where each graph can have varying numbers of useful sub-structures. https://doi.org/10.1111/cbdd.12952 ; doi:10.1021/acs.jmedchem.0c00754 ; GUVENCH20161928  Allowing the model to pool towards varying number of clusters based on data is expected to enhance performance, and our proposed GMPool allows such variation through the rank of the grouping matrix. To the best of our knowledge, GMPool is the first to achieve high performance without the need to manually adjust the number of clusters through additional hyperparameter tuning.				
469	paper_152	Why did the authors choose to test the proposed graph pooling method specifically on molecular property prediction tasks?	The proposed graph pooling method was tested specifically on molecular property prediction tasks because predefining the number of clusters is especially detrimental in molecular property prediction where there is no single number of clusters that is suitable across all graphs.  The number of functional groups that determine useful characteristics and chemical behaviors can vary significantly across different molecules.	In most inductive settings, there is no single number of clusters that is suitable across all graphs in the dataset.Particularly in molecular graphs, the number of functional groups often determines useful characteristics and chemical behaviors, while varying significantly across different molecules.Nonetheless, existing pooling methods require the number of clusters as a hyperparameter, then operates under the assumption that all graphs share the same number of clusters ranjan2020asap . This is often undesirable as it not only requires additional hyperparameter tuning, but also imposes a strong inductive bias that deteriorates downstream performance.However, the pooling methods above all share a common limitation: the number of clusters must be predefined for each layer as hyperparameters. This limitation is especially detrimental in inductive settings such as molecular property prediction, where each graph can have varying numbers of useful sub-structures. https://doi.org/10.1111/cbdd.12952 ; doi:10.1021/acs.jmedchem.0c00754 ; GUVENCH20161928  Allowing the model to pool towards varying number of clusters based on data is expected to enhance performance, and our proposed GMPool allows such variation through the rank of the grouping matrix. To the best of our knowledge, GMPool is the first to achieve high performance without the need to manually adjust the number of clusters through additional hyperparameter tuning.				
470	paper_152	How do the authors deal with the numerical instability that may occur due to incorporating SVD into the proposed method?	GMPool decomposes the grouping matrix using a method that approximates gradients in SVD to stabilize gradient computations.	After acquiring the pooling operator, the pooling process becomes obvious. Nodes are in fundamental representation while edge features and adjacency matrix are in adjoint representation. Which leads to the following transformation rules.\displaystyle X_{i}^{(l+1)}=S^{(l)}X_{i}^{(l)}(13)\displaystyle E_{ij}^{(l+1)}=S^{(l)}E_{ij}^{(l)}S^{(l)T}(14)\displaystyle A_{ij}^{(l+1)}=S^{(l)}A_{ij}^{(l)}S^{(l)T}(15)If grouping is properly done, 0 (or close to 0) components will appear in the decomposed eigen value matrix. These zero eigenvalues arise naturally and play a role in disregarding group information; those are ineffective towards prediction. However, zero elements in the eigen values causes a major problem in the decomposition process since the matrix might carry a singular determinant.Eigen decomposition is based on an iterative approximation algorithm which includes unbounded terms if any two eigen values are small or close. One can see clearly about this matter in DBLP:journals/corr/IonescuVS15 .\Big{(}\frac{\partial{l}}{\partial{A}}\Big{)}=U\big{(}K^{T}\odot(U^{T}\frac{\partial{l}}{\partial{U}})+(\frac{\partial{l}}{\partial{\Lambda}})_{\textrm{diag}})(U^{T})(16)Here, \odot denotes element-wise product. Off-diagonal components of K=1/(\lambda_{i}-\lambda_{j}) causes the problem, since the value blows up to the infinity if any two eigen values are close or very small. However, there are some solutions for this matter by approximating gradient in different ways DBLP:journals/corr/abs-1906-09023 ; 9400752 ; DBLP:journals/corr/abs-2105-02498 . Those methods are developed further to achieve higher speed in the calculation DBLP:journals/corr/abs-2201-08663 . They claim that the method is noticeably faster, over 8 times, than the standard SVD which has the time complexity \mathcal{O}(n^{3}). Thus, we utilized this method in our work to stabilize and accelerate the learning process. However, since the algorithm achieves the higher speed by approximating gradients, the error compared to standard SVD grows bigger as the size of the matrix grows. Therefore, this method might not be valid with large sized graph data.While our model is useful and effective, there is still room for improvement. First of all, despite leveraging a method to decompose the grouping matrix with stable gradient computations, there exist corner cases with a small eigengap at which the model fails to converge. This event seldom happens (about 0.00018\% in our experiments), but can be non-negligible when one needs to learn with a large number of data points. Hence, one future direction would be to impose proper constraints on the loss to avoid such gradient blowup in the grouping matrix.				
471	paper_152	Would it be possible to reduce the asymptotic cost of GMPool from cubic to quadratic, yet retain its expressive power?	One future direction to enhance scalability of GMPool is to incorporate faster decomposition modules such as randomized approximation methods.  However, this is likely to incur loss in predictive performance.	Another future direction would be to enhance scalability of our methods to improve applicability to large-scale graphs. Since the grouping matrix decomposition step via SVD is the main computational bottleneck of GMPool, incorporating faster decomposition modules such as randomized approximation halko2011finding ; DBLP:journals/corr/abs-1710-02812  methods can lead to faster inference. However, this is likely to incur loss in predictive performance, and as the focus of this work lies in allowing variation in the number of clusters in small molecular graphs where scalability is not an issue, we defer improving the scalability to future work.				
472	paper_152	The paper mentions GMPool can be used with any GNN architecture besides DMPNN. Are there any results leveraging more recent GNN architectures such as GIN or Graph Transformers?	While the authors chose DMPNN due to its superior performance over GNN architectures, the proposed pooling layer is module-agnostic and can be combined with any GNN.  Results leveraging more recent GNN architectures such as GIN or Graph Transformers cannot be found in this paper.	As our backbone GNN, we adopt the Directed Message Passing Neural Network (DMPNN) doi:10.1021/acs.jcim.9b00237  which aggregates messages through directed edges. Note that while we chose DMPNN due to its superior performance over GNN architectures, our pooling layer is module-agnostic and can be combined with any GNN as long as node representations are returned as output.Given a graph, DMPNN first initializes the hidden state of each edge (i,j) based on its feature E_{ij} and the source-node’s feature X_{i}. At each timestep t, each directional edge gathers hidden states from incident edges into a message m_{ij}^{t+1} and updates its own hidden state to h_{ij}^{t+1} as follows\displaystyle m_{ij}^{t+1}=\sum_{k\in\mathcal{N}(i)\setminus j}h_{ki}^{t}(1)\displaystyle h_{ij}^{t+1}=\texttt{ReLU}(h_{ij}^{0}+W_{e}m_{ij}^{t+1})(2)Here, \mathcal{N}(i) denotes the set of neighboring nodes of node i and W_{e} a learnable weight. The hidden states of nodes are updated by aggregating the hidden states of incident edges into message m_{i}^{t+1}, and passing its concatenation with the node feature X_{i} into a linear layer followed by ReLU non-linearity\displaystyle m_{i}^{t+1}=\sum_{j\in\mathcal{N}(i)}h_{ij}^{t}(3)\displaystyle h_{i}^{t+1}=\texttt{ReLU}(W_{n}\texttt{concat}(X_{i},m_{i}^{t+1}))(4)Similarly, W_{n} denotes a learnable weight. Assuming DMPNN runs for T timesteps, we use (X_{out},E_{out})=\texttt{GNN}(A,X,E) to denote the output representation matrices containing hidden states of all nodes and edges, respectively (i.e., X_{out,i}=h_{i}^{T} and E_{out,ij}=h_{ij}^{T}).				
473	paper_155	Why is deduplication chosen as one of the baselines?	Deduplicating the pretraining corpora proves to mitigate privacy risks for LMs.	In this work, we compare our proposed method with a data preprocessing approach proposed by Kandpal et al. (2022) which shows that deduplicating the training corpora before pretraining helps pretrain LMs that show stronger robustness against extraction attacks than an LM pretrained under the same circumstances without deduplicating the pretraining corpora. However, we highlight that this approach, which may still be effective at mitigating the overall privacy risks, is not the most suitable approach when considering a realistic scenario of individuals requesting the removal of their information from the implicit parameters of the LMs.				
474	paper_155	Only a small number of examples (32) are randomly selected to be unlearned. Have the authors tried unlearning much larger portions of the training data and observing the effect on the resulting model?	Results show that forgetting 128 samples at once results in a severe degradation of general LM performance while forgetting 32 samples does not.	We show the effect of varying s (the # of data instances to be forgotten at once) in Figure 2a across model scales. We denote this approach as batch unlearning. As shown by the s=128 results, it is harder to forget more samples at once, resulting in substantial degradation of average LM performance regardless of how large the LM is. Since s\leq 32 does not show much degradation, we explore if sequentially unlearning can be a solution. In Figure 2b, we show the result of dividing the 128 samples into 4 chunks of 32 and performing sequential unlearning; we unlearn each chunk at a time until the chunk reaches the forgetting threshold. Surprisingly, as shown by the performance gap at s=128 between the dotted lines (the s=128 performance of Figure 2a) and straight lines, the end result is vastly different even though exactly the same instances were forgotten. Sequential unlearning shows almost no degradation of average LM performance. In Appendix G, we show that chunks once forgotten stay forgotten and that later chunks are forgotten much faster compared to the initial chunk. This result hints at the generalization of unlearning, which we do not further explore in the scope of this work. The result also suggests that knowledge unlearning can be continually applied to LMs when needed.				
475	paper_155	How much does the success of the EL metric vary depending on which n tokens are used as a prompt for this metric?	The average LM perfomance of varying n for the EL metric is shown in Table 13.	First, we show the Extraction Likelihood (EL) Forgetting Threshold values for n=[5,10,20,40] by measuring the value on the 10,000 validation instances unseen during training in Table 12. Next, we show the average LM performance (on the 9 classification benchmarks) where we perform unlearning on the LM on 32 samples until the target token sequences are forgotten (the EL & MA value are both lower than the threshold values) in Table 13. Performance shows the average of 5 random samplings.				
476	paper_155	Why not just use membership inference attack recall [1,2] and exposure metric [3], which are commonly used and established metrics? These two basically do what the currently proposed metrics do.	These metrics are dependent on the specific attacks, while ours is agnostic of the type of attack.	Previous work that explores to which extent LMs have memorized their training data approach the phenomenon with two different viewpoints. Some work view memorization of LMs simply as a threat to individual privacy (Carlini et al., 2021; 2022; Jagielski et al., 2022) and utilize metrics that quantify how much the LMs are susceptible to adversarial attacks. These metrics are mostly dependent on the specific types of attacks such as the membership inference attack (Shokri et al., 2017) and measure the privacy risks of LMs by quantifying the success rate of these attacks.				
477	paper_155	How was the value of n set to 10?	The n value is set to 10 because we consider an extraction attack to be successfuly when 10 token sequences are successfully extracted by the LM.	We set the n value to 10 since we empirically consider an extraction to be successful when 10 consecutive token sequences are successfully generated by the LM. We show varying the n with values from [5,10,20,40] in Appendix H.				
478	paper_155	What happens when we perform unlearning for really big LMs?	Larger LMs are stronger unlearners because they take fewer epochs for forgetting specific target token sequences and retains most of its previous capabilities compared to smaler LMs.	We highlight five main observations regarding the results. (1) OPT LMs show a much lower EL10 and MA than GPT-NEO LMs, confirming that deduplicating the pretraining corpora is indeed helpful for mitigating privacy risks. (2) NEO + DPD+ enables effective protection against extraction attacks demonstrated via the lowest EL and MA score; however, it brings severe degradation of generation capabilities measured via the Average F1 score of the 4 dialogue generation tasks. (3) NEO + UL+ results in severe degradation of both classification and dialogue tasks for the 125M, only severe degradation of dialogue tasks for 1.3B LM while for the 2.7B LMs, it enables retaining most of its previous capabilities. (4) While the LMs scale to larger sizes, it takes fewer epochs for the target sequences to be forgotten. Together with (3), this implies that larger LMs are strong unlearners. (5) While NEO + UL+ provides stronger privacy protection than OPT without sacrificing its performance from NEO for the 2.7B LM, it is much more computationally efficient (3,500,000x) than re-training the underlying LM, which is required for all data preprocessing approaches.				
479	paper_155	What does the author mean by “empirically” consider some token sequences to be forgotten?	Since the forgetting definition is dependent on a held-out validation corpora, it is considered 'empirically' forgotten.	By utilizing both \textsc{EL}_{n} and MA, we empirically define a specific token sequence \bm{x} to be forgotten and is no longer susceptible to extraction attacks when the following conditions are met:where D^{\prime} represents a validation corpora not seen during training. In other words, we define \bm{x} to be forgotten when the \textsc{EL}_{n}(\bm{x}) and MA(\bm{x}) reach a value that is lower than the average \textsc{EL}_{n} and MA on token sequences that were not seen during training.				
480	paper_155	What was Memorization Accuracy Metric first used to quantify?	MA was first used to quantify the training dynamics of large LMs.	MA quantifies how much f_{\theta} has memorized the given token sequences and was proposed by Tirumala et al. (2022) to analyze the training dynamics of large LMs.				
481	paper_155	What is the reason the standard deviation is not shown in the table?	The standard deviation is not shown in the table because it is shown in the Appendix.	For the actual target data used to quantify the privacy risks of the LMs, we sample instances from the Training Data Extraction Challenge 111https://github.com/google-research/lm-extraction-benchmark where 15,000 examples (each are 200 token sequences long) from 16 different domains of the Pile corpora that are identified to be somewhat easy-to-extract are provided. For our experiments, we randomly sample s samples from the 15,000 examples and make the underlying LM forget the s samples at once. As a default, we show the average results of 5 random samplings of s samples for all of our experimental settings. We only provide the average of the 5 samplings and do not separately report the standard deviation. Instead, we provide the results of each individual run in Appendix A.				
482	paper_156	How is the proposed work different from the previous works using Transformer-based VAE frameworks in terms of representation learning?	The proposed work is different from previous studies using Transformer-based VAE frameworks, which achieves representation including global or hierarchical information of the given data, in that the learned representation is disentangled according to domain-specific inductive bias to control generated chords.	We concretely use the variational Transformer inspired by Lin et al. [22]. They used a Transformer-based model extended by a conditional VAE framework to gener- ate a response from a conditional context . We leverage this seq2seq architecture to achieve a variational neural machine translation (VNMT) from a given melody to the chords [23]–[25]. To the best of our knowledge, we are the ﬁrst to apply the VNMT approach to music generation. In particular, our approach is different from previous music generation studies using the variational Transformer, which mostly served as an autoencoder [26], [27].Furthermore, we attempt to regularize the variational Transformer for controlling the chord outputs through a dis- entangled representation. Generating arbitrary sets of chords may not satisfy users who would like to create music based on their own tastes. In terms of building interactive music gen- eration systems as well as learning a good representation for sequential data, controllable generation with the VAE frame- work has mainly been approached by recent studies. These studies have aimed to learn disentangled representations for high-level musical features, such as pitch, rhythm, harmony, context, or arousal, through supervised learning [28]–[31]. Inspired by these studies, we use domain-speciﬁc induc- tive bias to achieve a disentangled representation for the well-summarized context of the target melody and chords.Furthermore, Choi et al. [26] proposed a Transformer-based autoencoder that achieved global representation for the musical contexts of polyphonic piano performance data. Jiang et al. [27] introduced a hierarchical Transformer VAE to learn context-sensitive melody representation with self-				
483	paper_156	Why would melody harmonization task be important for understanding human composition?	A melody harmonization task is important for understanding human composition since it aims to capture the long-term dependencies in music by constraining sets of chord progressions that can interact with a given melody.	A melody harmonization task requires capturing the long-term dependencies in music since a constrained sets of chord progressions can consistently interact with a given melody [4]. This has motivated the use of linguistic tech- niques such as context-free grammar [5], genetic algo- rithms [6], or hidden Markov models (HMMs) [3], [7], [8].				
484	paper_156	What is the benefit of using note-based representation over grid-based representations?	Note-based representation is better than grid-based representation in learning chord patterns.  The reason is that learning with the grid-based representation can result in generating chord progression with ambiguous patterns or hierarchies.  On the other hand, modeling the note-based representation can capture note patterns in a melody.	where e T , e N , S , and N denote the time-level embed- ding vectors, note-level embedding vectors, STHarm, and the number of melody notes, respectively, Embedding and Self-AttBlocks denote the embedding layer and L multi- head self-attention blocks that are identical to the vanilla Transformer, respectively [12], w ∗ denotes a sinusoidal posi- tional embedding scaled by a trainable weight [40], and TimeToNote is a novel method that we propose to convert the timewise embedding to the notewise embedding to capture the note patterns in a melody.Nevertheless, these LSTM-based studies had limitations in generating concrete chord structures. First, the models were unable to encode an original melodic structure despite their sequential architectures [4]. The notes in a melody were aggregated within a chord duration into a pitch-class histogram before being fed to the model. Second, the models did not explicitly consider capturing the patterns of chord pro- gressions. Chord labels correspond to the constant time grids (e.g., a bar or half-bar). Sequential modeling of grid-based chord labels is likely to result in ambiguous patterns or hier- archies of the generated outputs [8].				
485	paper_156	The authors claim that LSTM-based approaches have failed to capture realistic pattern of chords. Is it true?	LSTM-based approaches have failed to capture realistic patterns of chords due to two reasons.  The first reason is that they cannot encode an original melodic structure by aggregating melody notes for each chord into a pitch-class histogram before being fed to the model.  The second reason is that they capture ambiguous patterns or hierarchies in chord progressions since they recurrently model grid-based chord labels.  Empirically, it has been investigated that the LSTM-based models tend to generate some syncopated chord rhythms that can weaken the metrical boundaries, unlike real-world music.	Nevertheless, these LSTM-based studies had limitations in generating concrete chord structures. First, the models were unable to encode an original melodic structure despite their sequential architectures [4]. The notes in a melody were aggregated within a chord duration into a pitch-class histogram before being fed to the model. Second, the models did not explicitly consider capturing the patterns of chord pro- gressions. Chord labels correspond to the constant time grids (e.g., a bar or half-bar). Sequential modeling of grid-based chord labels is likely to result in ambiguous patterns or hier- archies of the generated outputs [8].Figs. 4 and 5 show some of the actual samples from the listening test for all ﬁve models as well as the human- composed music. These samples reveal the strengths of the proposed models. First, Fig. 4 mainly shows that the proposed models tend to reproduce the binary metrical structure of the chords compared to the baseline models. The binary metric structure is close to real-world music, most of which has been composed of four beats and strongly inﬂuenced by metrical boundaries [52]. In contrast, the chords generated from the baseline models show some syncopated rhythms, which can weaken the metrical boundaries. Fig. 5 illustrates another advantage of the proposed models, which is that the majority of the chord roots tend to shift in intervals either of perfect fourth or ﬁfth according to the circle-of- ﬁfths rule. This aspect reﬂects conventional Western music theory, which serves as domain knowledge for modeling real-world music [51], [54]. Moreover, the proposed models are shown to generate some natural chromatic progressions according to the given melody. On the other hand, the baseline models show some short transitions on the circle-of-ﬁfths at arbitrary spots, in contrast to the melody with regular phrasings.				
486	paper_156	Is TimeToNote method truly a novel idea to capture a musical hierarchy? It seems to be just a simple trick that also have been used in one of the previous music generation studies (MuseMorphose, 2021).	TimeToNote method is different from similar approaches to capture musical hierarchy.  First, it aims to aggregate grid-based information into musically meaningful units, while previous approaches map low-level musical units to high-level musical units, such as a bar.  Moreover, the aggregated information preserves the length information of the original representation, which is also different from the previous studies that simply average-pooled the representation.	In the Time2Note procedure, we add the scaled positional embedding w T to e (S) T . Then, we transfer it to the notewise embedding e (S) N with average pooling by an alignment matrix M ∈ { 0 , 1 } T × N as (2), where M indicates the alignment path between a piano roll and a series of notes. This process enables each frame of the notewise embedding to preserve the information of the original note duration :However, conventional Transformer-based studies encoded music as a series of musical events [15]. Using event-based representations differs from how humans perceive a rendered or score-written melody for harmonization [16]. Instead, a grid-based melody representation can be more intuitive for modeling melodic patterns synchronized with chord labels [4], [17], [18]. In our work, we convert a melody into a more intuitive note-based representation, where each frame represents one note. To this end, we use a novel time-to-note compression method to map a binary piano roll representation into a note-based embedding.				
487	paper_156	What does "global key signature" mean?	"Global key signature" means the harmonic context of music that is constrained to a certain range.  For example, the C major key is constrained to have functionally important chords such as C, G, and F major chords. 	The proposed architecture of VTHarm is inspired by [22]. VTHarm has an additional probabilistic encoder for a latent variable z , where z represents the global attribute of the aggregated melody and chords. We denote this encoder as the context encoder . We add a global key signature label as a conditional input token to the model. The key signature is essential for an arbitrary melody to obtain a certain harmonic context [41]. The key signature token can aid the model in specifying the latent space and sampling the outputs from thehuman-composed samples from CMD and HLSD include 72 different chord types with various amounts of musical tensions. 2) STHarm may generate common chords more fre- quently from the average chord distribution than the human- composed music, as shown in the lower diversity scores. Concretely, the most frequent chords in real-world music are diatonic chords such as the C, G, and F major chords in the C major key [9]. Since these chords have relatively less musical tension with respect to a melody, they are close to the melody under a music-theoretical space. Thus, these chords may obtain better coherence scores than other chords with more musical tension.Moreover, Human shows lower diversity scores than the variational models. We assume that this is because these mod- els can produce some infrequent chords far from the mean distribution of real-world music. The nature of stochastic generation models draws samples from the normal distribu- tion [49]. Some of the generated chords may violate the given key signature but increase the information outside the certain harmonic context. Hence, they may contribute to higher chord diversity than human-composed music.We conduct an ablation study to verify the beneﬁt of adding the conditional token c to VTHarm and rVTHarm. We assume that c provides key signature information that can efﬁciently constrain the latent space to a concrete harmonic context, improving the chord structuredness and reconstruction per- formance of the model. We compute the chord similarity metrics between the ground truth and generated chords from the VT models according to the presence of c . The results are demonstrated in Table 7. This table shows that the VT models without c mostly obtain worse scores for all similarity metrics than the models with c . This indicates that adding key signature information to the VT models in most cases not only enhances the one-by-one accuracy but also improves the structure of the generated chords to be more human-like.				
488	paper_156	How "chord coverage" can represent chord complexity, which cannot be simply defined without considering the human perception of music?	"Chord coverage" can represent chord complexity, as the corresponding scores are empirically correlated to "Complexity" scores that are collected by human participants during the listening test.  "Complexity" metric represents how complex a human listener perceives the chord progression to be. 	We expand the conventional criteria [10], [11] for deeper analysis of human judgment. Harmonicity measures how coherent the chords are with a given melody. Unexpected- ness measures how much the chords deviate from expecta- tion. Complexity measures how complex chord progression is perceived to be. Preference measures personal favor for chord progression [9].Moreover, Human shows lower diversity scores than the variational models. We assume that this is because these mod- els can produce some infrequent chords far from the mean distribution of real-world music. The nature of stochastic generation models draws samples from the normal distribu- tion [49]. Some of the generated chords may violate the given key signature but increase the information outside the certain harmonic context. Hence, they may contribute to higher chord diversity than human-composed music.Table 5 shows that the results mainly support the quantitative evaluation results. In contrast, STHarm shows the highest H score regardless of melody awareness. This suggests that STHarm outputs plausible chords to listen to than the baseline models. For U and C, VTHarm shows the highest scores,				
489	paper_156	Why does the objective for STHarm not include condition c?	The objective of STHarm does not include condition c as it aims to find mean distribution for chords that maximizes the likelihood given a certain melody.  STHarm may generate the chords that share the best-fit harmonic context with the melody through its objective.  Therefore, STHarm does not need extra information that constrains the harmonic context to better predict harmonically coherent chords.	constrained chord distributions. In contrast, STHarm does not use this token since it ﬁnds the mean distribution for chords that best ﬁt a given melody.The main objective for STHarm is maximizing the log likelihood of the estimated chord sequence y given the melody x :human-composed samples from CMD and HLSD include 72 different chord types with various amounts of musical tensions. 2) STHarm may generate common chords more frequently from the average chord distribution than the human-composed music, as shown in the lower diversity scores. Concretely, the most frequent chords in real-world music are diatonic chords such as the C, G, and F major chords in the C major key [9]. Since these chords have relatively less musical tension with respect to a melody, they are close to the melody under a music-theoretical space. Thus, these chords may obtain better coherence scores than other chords with more musical tension.				
490	paper_156	What is the benifit of using the HLSD dataset that does not contain various key signatures for evaluating the models?	Using the HLSD dataset without transposing to various key signatures, we can reproduce the baseline model performance with the same dataset setting to the previous studies and verify the proposed models compared to the baseline performance.  Therefore, it is beneficial over only using a new dataset.	HLSD [13] is an online database of melody and chord annota- tions that cover various genres, such as the pop, new age, and original soundtracks. This dataset has been constructed on a crowdsourcing platform called TheoryTab, 1 in which users have transcribed a large number of high quality melodies and chords. This dataset contains the raw annotations of melodies and chords in XML format, JSON data of the symbolic fea- tures of melodies and chords, and piano-roll ﬁgures depicting the melody and chords. We use the JSON data for 9,218 songs divided into 13,335 parts. We also normalize all songs into C major or C minor, as in previous studies [10], [11]. Fol- lowing Sun et al. [11], we use 500 parts for the test set and the other 500 parts for the validation set. As a result, we use 32,619, 1,346, and 809 samples for the training, validation, and test sets, respectively.				
491	paper_156	Were the baseline models implemented from scratch or from existing codes from the original authors?	The baseline models are implemented from scratch, where the experimental settings are referred to the original settings in the corresponding papers.	The models are implemented and evaluated in Python 3 and the PyTorch deep learning framework of version 1.5.0. For training each model, we use one NVIDIA GeForce GTX 1080 Ti. We mostly refer to the previous implemen- tations [40], [48] when implementing the vanilla Trans- former. For implementing and training BLSTM and ONADE, we use the original settings [9], [11]. The gradients are all clipped to 1 for the learning stability during training of all models. VTHarm, rVTHarm, and ONADE are assessed with 10 test samples per melody due to their randomness.				
492	paper_156	The authors seem to mention specific reasons only for lambda KL. Did the authors conduct any ablation study to decide lambda Reg?	Lambda Reg has been empirically set to 1 through several trials with various values.  The concrete results of such a process are not reported in the paper.	The embedding sizes of the melody and chord are 128 and 256, respectively. We use a hidden size of 256, attention head size of 4, number of attention blocks L of 4, and size of the latent variable z of 16. A dropout layer is used after every scaled positional encoding at a rate of 0.2. We use an Adam optimizer [46] with an initial learning rate of 1e-4, which is reduced to 95% after every epoch. We train the proposed models for 100 epochs with a batch size of 128. To select the value of λ KL , we refer to several studies on VAE-based music generation in which a scaling weight smaller than 1 encour- ages better reconstruction [21], [47]. Then, we empirically set λ KL and λ Reg to be 0.1 and 1, respectively, which results in the best performance.				
493	paper_156	What is the difference between TPSD anc DICD?	TPSD is based on the relationship of two adjacent chords in terms of the circle-of-fifths rule and the shared pitch-class indices in the four levels of the tonic space.  On the other hand, DICD is based on the pitch-class intervals between the two adjacent chords.	distance (DICD) measure the distance between two chord progressions:• Tonal pitch step distance (TPSD). TPSD computes the geometrical dissimilarity between the generated chords and the ground-truth chords in terms of the tonal pitch space (TPS) chord distance rule [53]. The TPS between chord x and chord y is computed as (16):where j is the least number of steps in one direction from the chordal root of x to that of y according to the circle-of-ﬁfths rule. In the circle-of-ﬁfths rule, all pitch classes are arranged in intervals of either perfect ﬁfth or fourth [54]. The variable k is the number of unique pitch class indices in the four levels (root, ﬁfths, triadic, diatonic) within the basic space of y compared to x [53]. That is, if the pitch class index is shared by y and x , it is not counted. We compute the TPS values between all pairs of adjacent chords within each progression,DICD computes the city block distance between the directed interval class (DIC) representation vectors for the chord transitions [55]. DIC is the histogram vector of the directional pitch interval classes, ranging from −5 to 6, computed between all pairs of chord notes from the two adjacent chords. We calculate each pitch interval from each note of the first chord to all notes of the				
494	paper_156	Generating average chords and low diversity score are not analogous. Is it true that STHarm generates "common" chords that are frequent in real-world music?	STHarm may have generated "common" chords that are frequent in real-world music.  Harmonicity and Preference scores are the highest for the STHarm, regardless of melody awareness, and those scores are evaluated by the human listeners who usually have listened to popular music where common chords are used. 	Table 5 shows that the results mainly support the quantita- tive evaluation results. In contrast, STHarm shows the highest H score regardless of melody awareness. This suggests that STHarm outputs plausible chords to listen to than the baseline models. For U and C, VTHarm shows the highest scores, andthe variational models show lower harmonicity and prefer- ence scores than STHarm. We assume that the variational models tend to generate more chords far from the mean distribution of the learned music data than STHarm. Such unique chords can reveal more inharmonicity than the fre- quent chords, and it may have provided the participants with unpleasant feelings. In addition, most participants listened to popular music, where common chords with less musical tension are used. Therefore, it may have led the participants providing poorer scores on preference as well as harmonicity. Nevertheless, VTHarm shows a better P score than ONADE with lower U and C scores. This means that VTHarm is more persuasive than the baseline model with lower chord complexity.				
495	paper_156	How is a harmonic similarity to human music connected to the structuredness of chord patterns?	A harmonic similarity to human music is connected to the structuredness of chord patterns because human-composed music is a ground truth representing the music that is well-structured and the objective of this paper is fundamentally generating chords similar to real-world music.	We investigate the harmonic similarity between the human-composed and generated chords. We use the samples from Human as the ground truth. This explicit comparison with Human can provide insight into whether the generated chords from each model are as well-structured as human- composed music [8].				
496	paper_156	Why can't training VTHarm guarantee a disentangled representation of the desired aspect?	VTHarm cannot guarantee a disentangled representation of the desired aspect because it does not aim a supervised learning that can decouple the representation by the high-level musical features.  Empirically, the learned representation from VTHarm has been shown to be less correlated to the target chord attribute than rVTHarm which regularizes the representation.	Furthermore, we attempt to regularize the variational Transformer for controlling the chord outputs through a dis- entangled representation. Generating arbitrary sets of chords may not satisfy users who would like to create music based on their own tastes. In terms of building interactive music gen- eration systems as well as learning a good representation for sequential data, controllable generation with the VAE frame- work has mainly been approached by recent studies. These studies have aimed to learn disentangled representations for high-level musical features, such as pitch, rhythm, harmony, context, or arousal, through supervised learning [28]–[31]. Inspired by these studies, we use domain-speciﬁc induc- tive bias to achieve a disentangled representation for the well-summarized context of the target melody and chords.Furthermore, we compute Pearson’s correlation coefﬁ- cients between α and the CC scores of the corresponding chord outputs. Table 4 shows that rVTHarm reveals higher correlation coefﬁcients than VTHarm for all datasets. This conﬁrms that rVTHarm derives a meaningful representation for the intended chord attribute compared to VTHarm.				
497	paper_156	What would be a proper measure to quantize how the attention maps differ by a value of alpha?	The proposed measure to quantize how the attention maps differ by a value of alpha would be one of the metrics that detect the diagonality of the matrix.	In addition, we examine the attention maps of rVTHarm with different values of α . We randomly sample z , where α is set to be one of {− 3 , 0 , 3 } , and generate the chords from z and the test melodies. We sum the attention matrices along the head dimension to see the aggregated weights. Fig. 3 shows that the attention weights become balanced and diagonal when α increases from − 3 to 3. This implies that the decoder of rVTHarm tends to focus on more melody notes when α increases.				
498	paper_156	What is the reason to select these three values for alpha?	The alpha has been selected to be one of {-3, 0, 3} since {-3, 3} can be the two extremes for the prior that is assumed to be the normal distribution, where the range from -3 to 3 includes 99. 7% of the probability distribution.	where V denotes VTHarm, Concatd denotes the concatenation over the feature dimension, Pool denotes the average pooling over time, and self-AttBlock denotes only one loop of				
499	paper_156	What is the benefit of rVTHarm compared to VTHarm, although it does not show the best scores in any of the metrics in Table 5?	rVTHarm is better than VTHarm in that it can control the desired attribute of chords with the latent representation while VTHarm is not guaranteed for controllable generation of the chords.  Although rVTHarm does not show the best scores in any metrics for the listening test, it shows higher preference scores than VTHarm with melody awareness.  Practically, the melody would be aware by the user as the melody is intentionally created or memorized by the user, hence the strength of rVTHarm in the situation with melody awareness can be more helpful than VTHarm.	Training VTHarm alone cannot guarantee a disentangled representation of the desired aspect. Therefore, rVTHarm aims to achieve a disentangled representation to control the generated chord outputs. We use the auxiliary loss by Pati et al. [32] to directly supervise the latent representation z . In this study, we choose the number of unique chords in the progression, or chord coverage , as a naive attribute for the chord complexity [10].Furthermore, we compute Pearson’s correlation coefﬁ- cients between α and the CC scores of the corresponding chord outputs. Table 4 shows that rVTHarm reveals higher correlation coefﬁcients than VTHarm for all datasets. This conﬁrms that rVTHarm derives a meaningful representation for the intended chord attribute compared to VTHarm.When the melody is unaware, BLSTM and rVTHarm obtain significantly lower Preference scores than when the melody is aware (p < 0.001). We further compute Pearson’s correlation coefficient of U with C or P scores, as shown in Table 6. As a result, rVTHarm reveals the most negative correlation of U with both C and P scores when the melody is aware. This indicates that 1) controlled chords				
500	paper_156	Is it valid to conclude that the baseline models are weaker than the proposed models since they generate some syncopated rhythms of chords?	The baseline models can be concluded to be weaker than the proposed models in that they generate some syncopated rhythms of chords which are not close to real-world music which is mostly composed of four or binary beats for a bar and strongly influenced by metrical boundaries.	Figs. 4 and 5 show some of the actual samples from the listening test for all ﬁve models as well as the human- composed music. These samples reveal the strengths of the proposed models. First, Fig. 4 mainly shows that the proposed models tend to reproduce the binary metrical structure of the chords compared to the baseline models. The binary metric structure is close to real-world music, most of which has been composed of four beats and strongly inﬂuenced by metrical boundaries [52]. In contrast, the chords generated from the baseline models show some syncopated rhythms, which can weaken the metrical boundaries. Fig. 5 illustrates another advantage of the proposed models, which is that the majority of the chord roots tend to shift in intervals either of perfect fourth or ﬁfth according to the circle-of- ﬁfths rule. This aspect reﬂects conventional Western music theory, which serves as domain knowledge for modeling real-world music [51], [54]. Moreover, the proposed models are shown to generate some natural chromatic progressions according to the given melody. On the other hand, the baseline models show some short transitions on the circle-of-ﬁfths at arbitrary spots, in contrast to the melody with regular phrasings.				
501	paper_156	In the decoder input, what is the "beginning" over which the latent variable z and the key signature token c are added? Is it a <bos> token?	The "beginning" of the decoder input is a sum of the latent variable z and the key signature token c, which is concatenated over the sequence dimension.  The concatenated embeddings are not added to any embedding such as that for the <bos> token. 	The encoder used in VTHarm is identical to the encoder used in STHarm, except that the conditional token c is con- catenated at the beginning of the note-based melody embed-where V denotes VTHarm, Concat d denotes the concatena- tion over the feature dimension, Pool denotes the average pooling over time, and self-AttBlock denotes only one loop of the self-attention block. The context encoder maps the chord input y 1 : O into the embedding e (V) O . Then, c is concatenated at the beginning of e (V) O over the sequence dimension before the multihead self-attention blocks. The self-attention output contains the harmonic context according to the key informa- tion. It is mean-aggregated over time so that it represents the global information of the chords [26]. The encoder output E ( c , x 1 : T ) is also mean aggregated over time to represent the global attribute of a melody. These two aggregated vectors are concatenated over the feature dimension and pass through the bottleneck, resulting in two parameters, µ , and σ . The latent code z is inferred from µ and σ through the reparam- eterization trick, and its prior is assumed to be the normal distribution [19].The right-shifted chord input is ﬁrst encoded with the same lookup table from the context encoder. The latent variable z and the key signature token c are added to the beginning, which corresponds to the ‘‘start-of-sequence’’ part of the chord embedding. The following attention network transfers the aggregated information from z and c to all frames of the embedding. The rest of the Transformer decoder reconstructs the target chords.	The "beginning" of the decoder input is a concatenation of the latent variable z and the key signature token c, which is summed over the sequence dimension. The concatenated embeddings are also added to the embedding for the <bos> token.	Opposite	he semantic meaning is made completely opposite by swapping the core operations ("sum" becomes "concatenation," "concatenated" becomes "summed") and by negating the final claim ("not added" becomes "also added to").	
502	paper_156	Key signature may be helpful to constrain harmonic context. Then, wouldn't it be more valid to conduct an ablation study on chord coherence rather than harmonic similarity?	It is valid enough to conduct an ablation study on harmonic similarity because constraining the harmonic context fundamentally aims to improve the chord structuredness and reconstruction performance.  Constraining the harmonic context with the key signature can help the model specify the latent space and increase the probability to generate the right chord sequence that is close to the human-composed data which is well-structured.	We conduct an ablation study to verify the beneﬁt of adding the conditional token c to VTHarm and rVTHarm. We assume that c provides key signature information that can efﬁciently constrain the latent space to a concrete harmonic context, improving the chord structuredness and reconstruction per- formance of the model. We compute the chord similarity metrics between the ground truth and generated chords from the VT models according to the presence of c . The results are demonstrated in Table 7. This table shows that the VT models without c mostly obtain worse scores for all similarity metrics than the models with c . This indicates that adding key signature information to the VT models in most cases not only enhances the one-by-one accuracy but also improves the structure of the generated chords to be more human-like.The proposed architecture of VTHarm is inspired by [22]. VTHarm has an additional probabilistic encoder for a latent variable z, where z represents the global attribute of the aggregated melody and chords. We denote this encoder as	None			
503	paper_156	What would be an example of the methods that explore the effect of melody awareness?	The method for exploring the effect of melody awareness can be one that deeply investigates how the awareness of the melody can affect the unexpectedness of the controlled chords and how this unexpectedness affects the perceived complexity and preference of the chords.	When the melody is unaware, BLSTM and rVTHarm obtain signiﬁcantly lower Preference scores than when the melody is aware ( p < 0 . 001). We further compute Pearson’s correlation coefﬁcient of U with C or P scores, as shown in Table 6. As a result, rVTHarm reveals the most nega- tive correlation of U with both C and P scores when the melody is aware. This indicates that 1) controlled chords are more unexpected and unpleasant with a familiar melody, and 2) some factors other than complexity seem to cause an increased unexpectedness in rVTHarm. However, the mean preference score of rVTHarm signiﬁcantly increases with melody awareness. This implies that the familiarity of the melody may strongly compensate for the high unexpect- edness of rVTHarm. This tendency needs further investi- gation to improve the robustness of controllable melody harmonization.	The method for exploring the effect of melody awareness can be one that deeply investigates how the awareness of the melody can affect the unexpectedness of the controlled chords and how this unexpectedness affects the perceived complexity and preference of the musical rhythm.	Change concept	"chords" to "musical rhythm"	
504	paper_157	What are some examples of "explicit planning"?	An example of "explicit planning" would be the plan or strategy of abruptly increasing dynamics for performing a climax within the music to highlight a certain emotion such as anger. 	However, these studies have constrained musical creativity. Maezawa et al. controlled musical expression only through quantized features from the musical scores. Tan et al. did not consider controlling tempo or timing with a latent representation. These methods may have restricted any potential for rendering piano performances with flexible musical expression. Musical creativity can be expanded not only by composers but also by performers who can elastically choose various strategies to highlight multiple nuances or emotions [13, 14, 15]. Moreover, the music generation field can be also broadened if static music created by automatic composition systems can be easily colored with realistic and elastic expression [16].	An example of "explicit planning" would be the plan or strategy of abruptly increasing tempo for performing a climax within the music to highlight a certain emotion such as anger.	Change concept	core musical concept is changed from "dynamics" (volume) to "tempo" (speed)	
505	paper_157	What is the difference between IsTopVoice and PositionInChord?	IsTopVoice is different from PositionInChord in that an index 1 of IsTopVoice represents the uppermost voice while that of PositionInChord represents the lowermost voice.  They are also different that IsTopVoice is binary while PositionInChord is multi-class.	Score Features. The features for a musical score represent eight categorical attributes for how the notes are composed:Pitch is a MIDI index number that ranges from 21 to 108.RelDuration and RelIOI are 11-class attributes of a quantized duration and IOI between a note onset and a previous chord, respectively. They range from 1 to 11, and each class represents a multiple of a 16th note’s length with respect to a given tempo [30, 31].IsTopVoice is a binary attribute of whether the note is the uppermost voice. It is heuristically computed regarding pitches and durations of surrounding notes.PositionInChord and NumInChord are 11-class attributes of a positional index of a note within its chord and the total number of notes in that chord, respectively, that range from 1 to 11. An index 1 for PositionInChord denotes the most bottom position.Staff is a binary attribute of the staff of a note, either of the G clef or F clef.IsDownbeat is a binary attribute of whether a note is at a downbeat or not.	PositionInChord is different from IsTopVoice in that an index 1 of PositionInChord represents the uppermost voice while that of IsTopVoice represents the lowermost voice.  They are also different that PositionInChord is binary while IsTopVoice is multi-class.	Opposite	"IsTopVoice" <=> PositionInChord, swap the positions of these two elements.	
506	paper_157	Why didn't the authors intend a "chord" to represent a more meaningful unit in music, such as a beat?	The authors intend a "chord" to represent simultaneous notes to intuitively models a polyphonic structure of piano performance that is defined by its temporal progression.  More fine-grained resolution than the beat-based resolution can reflect trivial changes in expression that varies by simultaneous note groups, such as a syncopation. 	We employ a self-supervised learning framework to force the latent representations to learn our target attributes [25, 26, 24].In addition, we facilitate independent control of the three expressive attributes–dynamics, articulation, and tempo–by utilizing an existing method that aligns the latent code with a target attribute [27, 28]. Finally, we design a novel mechanism that intuitively models a polyphonic structure of piano performance. In particular, we insert intermediate steps for chordwise encoding and decoding of the piano performance to our encoder-decoder architecture, where a chord denotes a group of simultaneous notes.Our approach has several contributions as follows:1) Our system aims to control musical expression while maintaining any characteristics induced by a given musical structure;2) We use self-supervised learning where new supervisory signals are involved in regularizing the latent representations effectively;3) Our system aims to control multiple expressive attributes independently of each other;4) Lastly, we leverage an intermediate step that projects a notewise representation into the chordwise in the middle of our system to intuitively model the polyphonic structure of piano performance.Inspired by previous studies [4, 8, 9, 32], we build a twostep encoder and decoder: An encoder models both notewise and chordwise dependencies of the inputs, and a decoder reconstructs the notewise dependency from the chordwise representation and the notewise condition. We denote a chord as a group of notes that are hit simultaneously, regardless of the staff, so that they sound together at an instant time [33]. Thus, learning the chordwise dependency is analogous to direct modeling of the temporal progression of the piano performance. Let M 2 RC N be a matrix that aligns serialized notes to their polyphonic structure, where C and N are the number of chords and the number of notes, respectively. Within the encoder, the	The authors intend a "chord" to represent a more meaningful unit in music, such as a beat.	Opposite	The authors didn't intend a "chord" to represent a more meaningful unit in music, such as a beat.	
507	paper_157	Is there no temporal dependency between the latent variable for explicit planning?	The latent variable for explicit planning has no temporal dependency.  The latent variable is derived from the standard normal distribution without the dependency on the score features.	Inference. A probabilistic encoder parameterized by \phi approximates the posterior distibutions of the latent representations z^{(\text{pln})} and z^{(\text{str})} from the performance input x and conditional score input y:\displaystyle q_{\phi}(z^{(\text{pln})},z^{(\text{str})}|x,y)=\displaystyle q_{\phi}(z^{(\text{pln})}|x^{(\text{chd})})(3)\displaystyle\prod_{c=1}^{C}q_{\phi}(z^{(\text{str})}_{c}|x^{(\text{chd})}_{\leq c},y^{(\text{chd})}_{\leq c})where x^{(\text{chd})}=\text{N2C}(e_{x}) is the chordwise embedding, and e_{x} is the notewise embedding for x. The posterior distributions of z^{(\text{pln})}_{c} and z^{(\text{str})}_{c} are approximated by distribution parameters encoded by f^{(\text{pln})}(x^{(\text{chd})}) and f^{(\text{str})}(x^{(\text{chd})},y^{(\text{chd})}), where f^{(\text{pln})} and f^{(\text{str})} are bidirectional and unidirectional recurrent neural networks, respectively.We note that z^{(\text{pln})} is independent of the score features y. This allows a flexible transfer of the explicit planning among other musical pieces. On the other hand, z^{(\text{str})} is constrained by y since the structural attributes are dependent on the note structure.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.where y(chd) = N2C(ey) is the chordwise embedding, and ey is the notewise embedding for y. We assume that the prior of z(pln) c is a standard normal distribution. In contrast, z(str) c is sampled from a sequential prior [24, 36, 37], conditioned on both previous latent variables and chordwise score features: z(str) c   N( (prior); diag( (prior)2), where [ (prior);  (prior)] = f(prior)(z(str)	The latent variable for explicit planning has a strong temporal dependency. The latent variable is derived from the standard normal distribution with a direct dependency on the score features.	Opposite	"no temporal dependency" to "a strong temporal dependency" and "without the dependency" to "with a direct dependency"	
508	paper_157	Why did the authors use a polynomial function to extract explicit planning of the performance data?	The authors use a polynomial function to extract explicit planning as explicit planning is defined to be a high-level sketch that the performer draws as the bigger plan of progressing musical expression throughout the piece.  Such a sketch is assumed to be "smoothed" since it would derive from human thought that memorizes or imagines musical expression that can be also represented as an aural form by  "singing out" the musical progression. 	Prediction Tasks. We extract new supervisory signals for additional prediction tasks from the input data [24]. We define a signal of explicit planning I^{(\text{pln})} as a set of smoothed contours of the expressive parameters. It is extracted as a polynomial function predicted from the chordwise performance parameters k. We also derive a signal of structural attribute as I^{(\text{str})}=\text{sign}(k-I^{(\text{pln})}) which represents normalized directions of the performance parameters.We train two discriminators D^{(\text{pln})} and D^{(\text{str})} that directly receive z^{(\text{pln})} and z^{(\text{str})}, respectively. D^{(\text{pln})} is composed of A sub-discriminators where each discriminator D^{(\text{pln})}_{a} predicts a signal I^{(\text{pln})}_{a} for each expressive attribute a from z^{(\text{pln})}_{a}\in\mathbb{R}^{C\times(d^{(\text{pln})}/A)}, where z^{(\text{pln})}_{a} is a constituent part of z^{(\text{pln})}, and A is the number of expressive attributes. This setting is for a clear disentanglement among the expressive attributes. On the other hand, D^{(\text{str})} predicts the signal I^{(\text{str})} at once for all expressive attributes that belong to the same musical structure. All discriminators are jointly trained with the generative model, and the costs \mathcal{L}_{\text{pln}} and \mathcal{L}_{\text{str}} are minimized as \mathcal{L}_{\text{pln}}=\frac{1}{A}\sum_{a}\text{MSE}(D^{(\text{pln})}_{a}(z^{(\text{pln})}_{a}),I^{(\text{pln})}_{a}) and \mathcal{L}_{\text{str}}=\text{MSE}(D^{(\text{str})}(z^{(\text{str})}),I^{(\text{str})}), respectively.Therefore, we attempt a new approach that renders piano performances with flexible musical expressions. We disregard a typical assumption from previous studies that a performer must follow a composer’s intent [17, 18, 19, 4]. According to the literature, performers learn to identify or imitate "expressive models", or explicit planning, of existing piano performances [20]. We focus on this attribute, defining it as a higher-level sketch of the expressive attributes (i.e. dynamics, articulation, and tempo [21]) that the performer draws based on a personal interpretation of the musical piece [20, 4, 11]. We also assume that the remaining attribute represents common performing strategies that are connected to certain musical patterns, while these strategies slightly differ across performers [22, 23]. We call this attribute as a structural attribute that belongs to given note structures of a musical piece.	None			
509	paper_157	Why did the authors use only one composer rather than several composers together?	The authors use only one composer, Chopin, rather than several composers together because Chopin's music has been one of the most common resources that are analyzed by literature to investigate the development in Western musical expression with respect to various musical structures.  In other words, modeling music only from Chopin is assumed to be enough for learning Western musical expression derived from various musical patterns.	We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [22, 41, 6, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers who represent various eras of Western music. For subjective evaluation, we collect 42 songs of non-Classical songs from online source222http://www.ambrosepianotabs.com/page/library which are less constrained to written expression than most Classical excerpts.	The authors avoid using only one composer like Chopin, and instead use several composers together because a single composer's work is too limited to be a common resource for analyzing the development in Western musical expression. In other words, modeling music only from Chopin is assumed to be insufficient for learning Western musical expression derived from various musical patterns.	Opposite	The semantic meaning is made completely opposite by inverting the rationale and the conclusion, stating that multiple composers are necessary and a single one is insufficient.	
510	paper_157	Why didn't the authors use the previous studies mentioned in the Introduction section as baseline models?	The authors did not use the previous studies as the baseline models since the proposed work attempts a new approach that disregards a typical assumption from the previous studies.  There has been no identical assumption in the previous studies that musical expression can vary regardless of the written expression provided by the composers.	Therefore, we attempt a new approach that renders piano performances with flexible musical expressions. We disregard a typical assumption from previous studies that a performer must follow a composer’s intent [4, 17–19]. According to the literature, performers learn to identify or imitate "expressive models", or explicit planning, of existing piano performances [20]. We focus on this attribute, defining it as a higher-level sketch of the expressive attributes (i.e. dynamics, articulation, and tempo [21]) that the performer draws based on a personal interpretation of the musical piece [4, 11, 20]. We also assume that the remaining attribute represents common performing strategies that are connected to certain musical patterns, while these strategies slightly differ across performers [22, 23]. We call this attribute as a structural attribute that belongs to given note structures of a musical piece.	The authors did not use the previous studies as the baseline models since the proposed work attempts a new approach that disregards a typical assumption from the previous studies. There has been no identical assumption in the previous studies that musical expression can vary regardless of the performer's interpretation.	Change concept	The core concept of the source of variation is changed from "the written expression provided by the composers" to "the performer's interpretation," shifting the focus from the score to the performance.	
511	paper_157	What is the reason for using the reconstruction metric calculated from zero explicit planning?	The reconstruction metric that measures the performance for predicting the structure attribute is calculated from zero explicit planning.  The reason is that using a flat expression derived by the zero explicit planning can let the generated structural attribute be solely exposed, not mixed with any musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	The reconstruction metric that measures the performance for predicting the dynamics attribute is calculated from zero explicit planning. The reason is that using a flat expression derived by the zero explicit planning can let the generated dynamics attribute be solely exposed, not mixed with any musical expression.	Change concept	The core concept of the attribute being measured is changed from "structure" to "dynamics", which is a different musical property.	
512	paper_157	Why did the authors use the randomly sampled z(str) to measure explicit planning, while using z(pln) from zero explicit planning to measure structural attributes?	The authors use the randomly sampled z(str) to measure explicit planning as they aim to disentangle explicit planning from any structural attribute.  They also use z(pln) from zero explicit planning to measure the structural attributes since a flat expression can expose any structural attribute that is not mixed with arbitrary musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	The authors use the z(pln) from zero explicit planning to measure explicit planning as they aim to disentangle explicit planning from any structural attribute.  They also use randomly sampled z(str) to measure the structural attributes since a flat expression can expose any structural attribute that is not mixed with arbitrary musical expression.	Opposite	Swap the positions of "randomly sampled z(str)" and "z(pln) from zero explicit planning"	
513	paper_157	Why didn't the authors try the listening test for the samples from non-zero, realistic explicit planning, like other performance rendering studies?	The authors didn't try the listening test for the samples from non-zero, realistic explicit planning due to the following reason.  Such realistic explicit planning should be inserted by the user, or inferred from the posterior distribution with respect to the ground truth data, maybe Classical music with various musical expressions, but the existing expressions can be already constrained by the written guidelines.  The written expression can be a strong bias to the listeners so that the new expression against the original expression can be perceived as awkward regardless of how natural the expression itself is.	We conduct a listening test to compare the proposed model architecture to Notewise and CVAE. We qualitatively evaluate the base quality of the samples that have flat expressions, so that quality judgments are independent of any preference of arbitrary explicit planning. We generate each sample using z^{(\text{pln})}_{0}. A listening test is composed of 30 trials where each participant chooses a more "human-like" sample out of the generated sample and its plain MIDI [9]. Both samples have the same length which is a maximum of 15 seconds, rendered with TiMidity++333https://sourceforge.net/projects/timidity/ without any pedal effect. Human-likeness denotes how similar the sample is to an actual piano performance that commonly appears in popular music. A total of 28 participants are involved, and 6 participants are professionally trained in music.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [6, 22, 41, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers	The authors conducted a listening test for the samples from non-zero, realistic explicit planning due to the following reason. Such realistic explicit planning should be inserted by the user, or inferred from the posterior distribution with respect to the ground truth data, maybe Classical music with various musical expressions, but the existing expressions can be freely interpreted beyond the written guidelines. The written expression can be a weak suggestion to the listeners so that the new expression against the original expression can be perceived as natural and innovative if the expression itself is natural.	Opposite	change "didn't try" to "conducted", "constrained by" to "freely interpreted beyond", "strong bias" to "weak suggestion" and "perceived as awkward" to "perceived as natural and innovative"	
514	paper_157	How can the difference between the black and orange lines, which represent two samples from different z(str), be specificaly interpreted from a musical perspective?	The difference between the black and orange lines can be interpreted as a granular variety in the performing strategies with respect to the given musical structure by different performers.  Those different strategies can represent the common technique that the performers may choose to represent the musical structure, but they may vary since they are induced from two human behaviors that cannot be identical to each other.	Therefore, we attempt a new approach that renders piano performances with flexible musical expressions. We disregard a typical assumption from previous studies that a performer must follow a composer’s intent [17, 18, 19, 4]. According to the literature, performers learn to identify or imitate "expressive models", or explicit planning, of existing piano performances [20]. We focus on this attribute, defining it as a higher-level sketch of the expressive attributes (i.e. dynamics, articulation, and tempo [21]) that the performer draws based on a personal interpretation of the musical piece [20, 4, 11]. We also assume that the remaining attribute represents common performing strategies that are connected to certain musical patterns, while these strategies slightly differ across performers [22, 23]. We call this attribute as a structural attribute that belongs to given note structures of a musical piece.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.				
515	paper_157	What is the difference between conducting polynomial regression and predicting explicit planning with the learned representation?	Conducting polynomial regression is different from predicting explicit planning from the learned representation since polynomial regression would be based on a finite set of data in a certain length.  In other words, different lengths of the input data of the polynomial function can result in different polynomial curves.  On the other hand, the prediction of explicit planning from the latent representation is not affected by the input length.	Prediction Tasks. We extract new supervisory signals for additional prediction tasks from the input data [24]. We define a signal of explicit planning I^{(\text{pln})} as a set of smoothed contours of the expressive parameters. It is extracted as a polynomial function predicted from the chordwise performance parameters k. We also derive a signal of structural attribute as I^{(\text{str})}=\text{sign}(k-I^{(\text{pln})}) which represents normalized directions of the performance parameters.We train two discriminators D^{(\text{pln})} and D^{(\text{str})} that directly receive z^{(\text{pln})} and z^{(\text{str})}, respectively. D^{(\text{pln})} is composed of A sub-discriminators where each discriminator D^{(\text{pln})}_{a} predicts a signal I^{(\text{pln})}_{a} for each expressive attribute a from z^{(\text{pln})}_{a}\in\mathbb{R}^{C\times(d^{(\text{pln})}/A)}, where z^{(\text{pln})}_{a} is a constituent part of z^{(\text{pln})}, and A is the number of expressive attributes. This setting is for a clear disentanglement among the expressive attributes. On the other hand, D^{(\text{str})} predicts the signal I^{(\text{str})} at once for all expressive attributes that belong to the same musical structure. All discriminators are jointly trained with the generative model, and the costs \mathcal{L}_{\text{pln}} and \mathcal{L}_{\text{str}} are minimized as \mathcal{L}_{\text{pln}}=\frac{1}{A}\sum_{a}\text{MSE}(D^{(\text{pln})}_{a}(z^{(\text{pln})}_{a}),I^{(\text{pln})}_{a}) and \mathcal{L}_{\text{str}}=\text{MSE}(D^{(\text{str})}(z^{(\text{str})}),I^{(\text{str})}), respectively.				
516	paper_157	What would be the possible genres or composers to use in the experiments for further investigation?	The possible genres or composers to use in the experiments for further investigation would be more contemporary genres, such as jazz or blues, since the trained dataset is completely Classical while the test dataset is more contemporary.	We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [22, 41, 6, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers who represent various eras of Western music. For subjective evaluation, we collect 42 songs of non-Classical songs from online source222http://www.ambrosepianotabs.com/page/library which are less constrained to written expression than most Classical excerpts.				
518	paper_162	Why does negative transfer occur when learning with auxiliary tasks?	Negative transfer happens when the learning of an auxiliary task negatively impacts the performance of the primary task.  In the case of graph-based tasks, it can happen because the graph structure, such as the number of nodes, edges, and diameter, can be vastly different between domains.  This causes confusion for the model, resulting in poor generalization of the primary task.	Pre-training with an auxiliary task is a common technique for deep neural networks.Indeed, it is the de facto standard step in natural language processing and computer vision to learn a powerful backbone networks such as BERT devlin2018bert  and ResNet he2016deep  leveraging large datasets such as BooksCorpus zhu2015aligning , English Wikipedia, and ImageNet deng2009imagenet .The models trained on the auxiliary task are often beneficial for the primary (target) task of interest.Despite the success of pre-training, few approaches have been generalized to graph-structured data due to their fundamental challenges.First, graph structure (e.g., the number of nodes/edges, and diameter) and its meaning can significantly differ between domains. So the model trained on an auxiliary task can harm generalization on the primary task, i.e., negative transfer pan2009survey .Also, many graph neural networks are transductive approaches. This often makes transfer learning between datasets inherently infeasible.So, pre-training on the target dataset has been proposed using auxiliary tasks: graph kernel  navarin2018pre , graph reconstruction zhang2020graph , and attribute masking  hu2020strategies . These assume that the auxiliary tasks for pre-training are carefully selected with substantial domain knowledge and expertise in graph characteristics to assist the primary task.Since most graph neural networks operate on homogeneous graphs, which have a single type of nodes and edges, the previous pre-training/auxiliary tasksare not specifically designed for heterogeneous graphs, which have multiple types of nodes and edges.Heterogeneous graphs commonly occur in real-world applications, for instance, a music dataset has multiple types of nodes (e.g., user, song, artist) and multiple types of relations (e.g., user-artist, song-film, song-instrument).Our framework SELAR is learning to learn a primary task with multiple auxiliary tasks to assist the primary task.This can be formally written asmin𝐰,Θ⁡𝔼⁢[ℒp⁢r⁢(𝐰∗⁢(Θ))](x,y)∼Dp⁢r⁢ s.t. ⁢𝐰∗⁢(Θ)=arg⁡min𝐰⁡𝔼⁢[ℒp⁢r+a⁢u⁢(𝐰;Θ)](x,y)∼Dp⁢r+a⁢u,\displaystyle\min_{\mathbf{w},\Theta}\;\;\underset{(x,y)\sim D^{pr}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;}{\text{\large$\mathbb{E}$}\;\;\left[\;\;\mathcal{L}^{pr}(\mathbf{w}^{\ast}(\Theta))\;\;\right]}\;\;\text{ s.t. }\;\;\mathbf{w}^{\ast}(\Theta)=\operatorname*{\arg\!\min}_{\mathbf{w}}\underset{(x,y)\sim D^{pr+au}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;}{\;\;\mathbb{E}\;\;\left[\;\;\mathcal{L}^{pr+au}(\mathbf{w};\Theta)\;\;\right]},roman_min start_POSTSUBSCRIPT bold_w , roman_Θ end_POSTSUBSCRIPT start_UNDERACCENT ( italic_x , italic_y ) ∼ italic_D start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG blackboard_E [ caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( bold_w start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( roman_Θ ) ) ] end_ARG s.t. bold_w start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( roman_Θ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_w end_POSTSUBSCRIPT start_UNDERACCENT ( italic_x , italic_y ) ∼ italic_D start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG blackboard_E [ caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r + italic_a italic_u end_POSTSUPERSCRIPT ( bold_w ; roman_Θ ) ] end_ARG ,(2)where \mathcal{L}^{pr}(\cdot) is the primary task loss function to evaluate the trained model f(x;\mathbf{w}^{\ast}(\Theta)) on meta-data (a validation for meta-learning han2018coteaching ) D^{pr} and \mathcal{L}^{pr+au} is the loss function to train a model on training data D^{pr+au} with the primary and auxiliary tasks. To avoid cluttered notation, f, x, and y are omitted. Each task \mathcal{T}_{t} has N_{t} samples and \mathcal{T}_{0} and \{\mathcal{T}_{t}\}_{t=1}^{T} denote the primary and auxiliary tasks respectively.The proposed formulation in Eq. (2) learns how to assist the primary task by optimizing \Theta via meta-learning. The nested optimization problem given \Theta is a regular training with properly adjusted loss functions to balance the primary and auxiliary tasks. The formulation can be more specifically written as\displaystyle\min_{\mathbf{w},\Theta}∑i=1M01M0ℓ0(yi(0,m⁢e⁢t⁢a),f(xi(0,m⁢e⁢t⁢a);𝐰∗(Θ))\displaystyle\sum_{i=1}^{M_{0}}\frac{1}{M_{0}}\ell^{0}(y_{i}^{(0,meta)},f(x_{i}^{(0,meta)};\mathbf{w}^{\ast}(\Theta))∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG roman_ℓ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT , italic_f ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 , italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT ; bold_w start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( roman_Θ ) )(3)s.t.\displaystyle\mathbf{w}^{\ast}(\Theta)=\operatorname*{\arg\!\min}_{\mathbf{w}}\sum_{t=0}^{T}\sum_{i=1}^{N_{t}}\frac{1}{N_{t}}\mathcal{V}(\xi^{(t,train)}_{i};\Theta)\ell^{t}(y_{i}^{(t,train)},f^{t}(x_{i}^{(t,train)};\mathbf{w})),(4)where \ell^{t} and f^{t} denote the loss function and the model for task t. We overload \ell^{t} with its function value, i.e., \ell^{t}=\ell^{t}(y_{i}^{(t,train)},f^{t}(x_{i}^{(t,train)};\mathbf{w})). \xi^{(t,train)}_{i} is the embedding vector of i_{th} sample for task t. It is the concatenation of one-hot representation of task types, the label of the sample (positive/negative), and its loss value, i.e., \xi^{(t,train)}_{i}=\left[\ell^{t};e_{t};y_{i}^{(t,train)}\right]\in\textbf{R}^{T+2}.To derive our learning algorithm,we first shorten the objective function in Eq. (3) and Eq. (4) as \mathcal{L}^{pr}(\mathbf{w}^{\ast}(\Theta)) and \mathcal{L}^{pr+au}(\mathbf{w};\Theta).This is equivalent to Eq. (2) without expectation.Then, our formulation is given as\min_{\mathbf{w},\Theta}\mathcal{L}^{pr}(\mathbf{w}^{\ast}(\Theta))\;\;\text{ s.t. }\mathbf{w}^{\ast}(\Theta)=\operatorname*{\arg\!\min}_{\mathbf{w}}\mathcal{L}^{pr+au}(\mathbf{w};\Theta),(5)To circumvent the difficulty of the bi-level optimization, as previous works MAML ; han2018coteaching  in meta-learning we approximate it with the updated parameters \hat{\mathbf{w}} using the gradient descent update as\displaystyle\mathbf{w}^{\ast}(\Theta)\approx\hat{\mathbf{w}}^{k}(\Theta^{k})=\mathbf{w}^{k}-\alpha\nabla_{\mathbf{w}}\mathcal{L}^{pr+au}(\mathbf{w}^{k};\Theta^{k}),(6)where \alpha is the learning rate for \mathbf{w}.We do not numerically evaluate \hat{\mathbf{w}}^{k}(\Theta) instead we plug the computational graph of \hat{\mathbf{w}}^{k} in \mathcal{L}^{pr}(\mathbf{w}^{\ast}(\Theta)) to optimize \Theta.Let \nabla_{\Theta}\mathcal{L}^{pr}(\mathbf{w}^{\ast}(\Theta^{k})) be the gradient evaluated at \Theta^{k}.Then updating parameters \Theta is given as\displaystyle\Theta^{k+1}=\Theta^{k}-\beta\nabla_{\Theta}\mathcal{L}^{pr}(\hat{\mathbf{w}}^{k}(\Theta^{k})),(7)where \beta is the learning rate for \Theta. This update allows softly selecting useful auxiliary tasks (meta-paths) and balance them with the primary task to improve the performance of the primary task. Without balancing tasks with the weighting function \mathcal{V}(\cdot;\Theta), auxiliary tasks can dominate training and degrade the performance of the primary task.				
519	paper_162	How did the authors design the meta-path prediction task?	The authors designed the meta-path prediction task as a variation of link prediction.  In meta-path prediction, instead of just predicting links between two nodes, the task is to predict the presence of a specific sequence of heterogeneous composite relations, called a meta-path.  The prediction is done in the same way as link prediction, by assigning a binary label (1 or 0) to indicate whether the two nodes are connected by the meta-path.  The labels for the task can be generated automatically from the heterogeneous graph, by calculating the product of the adjacency matrices of the edge types in the meta-path.	Meta-path prediction is similar to link prediction but meta-paths allow heterogeneous composite relations.The meta-path prediction can be achieved in the same manner as link prediction.If two nodes u and v are connected by a meta-path p with the heterogeneous edges (t_{1},t_{2},\ldots t_{\ell}), then y_{u,v}^{p}=1, otherwise y_{u,v}^{p}=0. The labels can be generated from a heterogeneous graph without any manual labeling.They can be obtained by A_{p}=A_{t_{l}}\ldots A_{t_{2}}A_{t_{1}}, where A_{t} is the adjacency matrix of edge type t. The binarized value at (u,v) in A_{p} indicates whether u and v are connected with the meta-path p.In this paper, we use meta-path prediction as a self-supervised auxiliary task.Meta-Path [46, 49] is a path on a heterogeneous graph G that a sequence of nodes connected with heterogeneous edges, i.e., v1 t1 −→ v2 t2 −→ . . . tl −→ vl+1, where tl ∈ T e denotes an l-th edge type of the meta-path. The meta-path can be viewed as a composite relation R = t1 ◦ t2 . . . ◦ tl between node v1 and vl+1, where R1 ◦ R2 denotes the composition of relation R1 and R2. The definition of meta-path generalizes multi-hop connections and is shown to be useful to analyze heterogeneous graphs. For instance, in Book-Crossing dataset, ‘user-item-written.series-item-user’ indicates that a meta-path that connects users who like the same book series.				
520	paper_162	What do challenging auxiliary tasks mean?	Challenging auxiliary tasks refer to tasks that are difficult for the model to learn, which can negatively impact the performance of the primary task.  In the case of meta-path prediction, it is considered more challenging than link prediction and node classification because it requires the understanding of long-range relations across heterogeneous nodes.  The task becomes even more difficult when mini-batch training is necessary due to the large size of datasets or models, as important nodes and edges for meta-paths may not be available within a mini-batch.	Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes. The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models. Within a mini-batch, important nodes and edges for meta-paths are not available. Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations. The challenges can hinder representation learning and damage the generalization of the primary task. We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner’s need. Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. 2.				
521	paper_162	How can Hint Network help with challenging auxiliary tasks?	The amount of help (correction) provided by the HintNet is optimized to maximize the learner's gain, and the help is determined by weighting functions for HintNet, which are optimized by meta-learning.	The amount of help (correction) by HintNet is optimized maximizing the learner’s gain.Let \mathcal{V}_{H}(\cdot) and \Theta_{H} be a weight function to determine the amount of hint and its parameters which are optimized by meta-learning. Then, our formulation with HintNet is given as\displaystyle\min_{\mathbf{w},\Theta}\sum_{i=1}^{M_{0}}\frac{1}{M_{0}}\ell^{0}(y_{i}^{(0,meta)},f(x_{i}^{(0,meta)};\mathbf{w}^{\ast}(\Theta,\Theta_{H})))(10)\displaystyle\text{s.t. }\mathbf{w}^{\ast}(\Theta)=\operatorname*{\arg\!\min}_{\mathbf{w}}\sum_{t=0}^{T}\sum_{i=1}^{N_{t}}\frac{1}{N_{t}}\mathcal{V}(\xi^{(t,train)}_{i},\ell^{t};\Theta)\ell^{t}(y_{i}^{(t,train)},\hat{y}_{i}^{(t,train)}(\Theta_{H})),(11)where \hat{y}_{i}^{(t,train)}(\Theta_{H}) denotes the convex combination of the learner’s answer and HintNet’s answer, i.e., \mathcal{V}_{H}(\xi^{(t,train)}_{i};\Theta_{H})f^{t}(x_{i}^{(t,train)};\mathbf{w})+(1-\mathcal{V}_{H}(\xi^{(t,train)}_{i};\Theta_{H}))f_{H}^{t}(x_{i}^{(t,train)};\mathbf{w}). The sample embedding is\xi^{(t,train)}_{i}=\left[\ell^{t};\ell^{t}_{H};e_{t};y_{i}^{(t,train)}\right]\in\textbf{R}^{T+3}.Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes. The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models. Within a mini-batch, important nodes and edges for meta-paths are not available. Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations. The challenges can hinder representation learning and damage the generalization of the primary task. We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner’s need. Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig. 2.				
522	paper_162	How does this paper experimentally show that auxiliary tasks are not beneficial?	This paper experimentally shows that auxiliary tasks are not always beneficial by comparing four different learning strategies.  The first strategy, "Vanilla," involves standard training of base models only with the primary task samples.  "w/ meta-path," involves training with the primary task and auxiliary tasks using a standard loss function.  By comparing the performance of these different strategies, the paper shows the impact of using auxiliary tasks, such as meta-path predictions, on the primary task and demonstrates that auxiliary tasks are not always beneficial. 	Baselines. We evaluate our methods with five graph neural networks : GCN GCN , GAT GAT , GIN xu2018powerful , SGConv wu2019simplifying  and GTN yun2019graph . Our methods can be applied to both homogeneous graphs and heterogeneous graphs. We compare four learning strategies: Vanilla, standard training of base models only with the primary task samples; w/o meta-path, learning a primary task with sample weighting function \mathcal{V}(\xi;\Theta); w/ meta-path, training with the primary task and auxiliary tasks (meta-path prediction) with a standard loss function; SELAR proposed in Section 3.2, learning the primary task with optimized auxiliary tasks by meta-learning; SELAR+Hint introduced in Section 3.3.In all the experiments, we report the mean performance of three independent runs.Implementation details are in the supplement. Our experiments were mainly performed based on NAVER Smart Machine Learning platform (NSML) sung2017nsml ; kim2018nsml .The goal of our framework is to learn with multiple auxiliary tasks to improve the performance of the primary task.In this work, we demonstrate our framework with meta-path predictions as auxiliary tasks. But our framework could be extended to include other auxiliary tasks.The meta-paths capture diverse and meaningful relations between nodes on heterogeneous graphs HAN .However, learning with auxiliary tasks has multiple challenges: identifying useful auxiliary tasks, balancing the auxiliary tasks with the primary task, and converting challenging auxiliary tasks into solvable (and relevant) tasks.To address the challenges, we propose SELf-supervised Auxiliary LeaRning (SELAR).Our framework consists of two main components:1) learning weight functions to softly select auxiliary tasks and balance them with the primary task via meta-learning, and2) learning Hint Networks to convert challenging auxiliary tasks into more relevant and solvable tasks to the primary task learner.				
523	paper_162	Have the authors experimented with extending to other auxiliary tasks other than meta-path prediction?	In this paper, the authors did not conduct experiments on extending the framework to other auxiliary tasks besides meta-path prediction.  However, the authors mention that it is a possible direction for future work.	We proposed meta-path prediction as self-supervised auxiliary tasks on heterogeneous graphs.Our experiments show that the representation learning on heterogeneous graphscan benefit from meta-path prediction which encourages to capture rich semantic information.The auxiliary tasks can be further improved by our proposed method SELAR, which automatically balances auxiliary tasks to assist the primary task via a form of meta-learning.The learnt weighting function identifies more beneficial meta-paths for the primary tasks.Within a task, the weighting function can adjust the cross entropy like the focal loss, which focuses on hard examples by decreasing weights for easy samples.Moreover, when it comes to challenging and remotely relevant auxiliary tasks,our HintNet helps the learner by correcting the learner’s answer dynamically and further improves the gain from auxiliary tasks.Our framework based on meta-learning provides learning strategies to balance primary task and auxiliary tasks, and easy/hard (and positive/negative) samples.Interesting future directions include applying our framework to other domains and various auxiliary tasks.Our code is publicly available at https://github.com/mlvlab/SELAR.The goal of our framework is to learn with multiple auxiliary tasks to improve the performance of the primary task.In this work, we demonstrate our framework with meta-path predictions as auxiliary tasks. But our framework could be extended to include other auxiliary tasks.The meta-paths capture diverse and meaningful relations between nodes on heterogeneous graphs HAN .However, learning with auxiliary tasks has multiple challenges: identifying useful auxiliary tasks, balancing the auxiliary tasks with the primary task, and converting challenging auxiliary tasks into solvable (and relevant) tasks.To address the challenges, we propose SELf-supervised Auxiliary LeaRning (SELAR).Our framework consists of two main components:1) learning weight functions to softly select auxiliary tasks and balance them with the primary task via meta-learning, and2) learning Hint Networks to convert challenging auxiliary tasks into more relevant and solvable tasks to the primary task learner.				
524	paper_162	What is a meta-path? Please explain with examples.	A meta-path is a sequence of node types and edge types in a graph that describes a specific type of relationship between nodes.  An example is in a recommendation system, a meta-path could be "user-item-written. series-item-user" which describes a relationship between users who like the same book series. 	Meta-Path HAN ; sun2011pathsim  is a path on a heterogeneous graph G that a sequence of nodes connected with heterogeneous edges, i.e., {v}_{1}\xrightarrow{t_{1}}{v}_{2}\xrightarrow{t_{2}}\ldots\xrightarrow{t_{l}}{v}_{l+1},where t_{l}\in\mathcal{T}^{e} denotes an l-th edge type of the meta-path.The meta-path can be viewed as a composite relation R=t_{1}\circ t_{2}\ldots\circ t_{l} between node {v}_{1} and {v}_{l+1}, where R_{1}\circ R_{2} denotes the composition of relation R_{1} and R_{2}.The definition of meta-path generalizes multi-hop connections and is shown to be useful to analyze heterogeneous graphs.For instance, in Book-Crossing dataset, ‘user-item-written.series-item-user’ indicates that a meta-path that connects users who like the same book series.Meta-path prediction is similar to link prediction but meta-paths allow heterogeneous composite relations.The meta-path prediction can be achieved in the same manner as link prediction.If two nodes u and v are connected by a meta-path p with the heterogeneous edges (t_{1},t_{2},\ldots t_{\ell}), then y_{u,v}^{p}=1, otherwise y_{u,v}^{p}=0. The labels can be generated from a heterogeneous graph without any manual labeling.They can be obtained by A_{p}=A_{t_{l}}\ldots A_{t_{2}}A_{t_{1}}, where A_{t} is the adjacency matrix of edge type t. The binarized value at (u,v) in A_{p} indicates whether u and v are connected with the meta-path p.In this paper, we use meta-path prediction as a self-supervised auxiliary task.Meta-path prediction is generally more challenging than link prediction and node classification since it requires the understanding of long-range relations across heterogeneous nodes.The meta-path prediction gets more difficult when mini-batch training is inevitable due to the size of datasets or models.Within a mini-batch, important nodes and edges for meta-paths are not available.Also, a small learner network, e.g., two-layer GNNs, with a limited receptive field, inherently cannot capture long-range relations.The challenges can hinder representation learning and damage the generalization of the primary task.We proposed a Hint Network (HintNet) which makes the challenge tasks more solvable by correcting the answer with more information at the learner’s need.Specifically, in our experiments, the HintNet corrects the answer of the learner with its own answer from the augmented graph with hub nodes, see Fig.  2.				
525	paper_162	What is the role of meta-data in the proposed method?	In the proposed method, meta-data serves as a signal to guide the update of the model's parameters in a way that improves the primary task.  It is used in the outer loop of the bi-level optimization process to evaluate the performance of the model on the primary task, represented by the primary task loss function Lpr(·).  In other words, meta-data is used to provide guidance for the learning process in a way that improves the primary task.	Our framework SELAR is learning to learn a primary task with multiple auxiliary tasks to assist the primary task. This can be formally written as min w,Θ E [ L pr(w∗ (Θ)) ] (x,y)∼Dpr s.t. w∗ (Θ) = argmin w E L pr+au(w; Θ) (x,y)∼Dpr+au , (2) where L pr(·) is the primary task loss function to evaluate the trained model f(x; w∗ (Θ)) on metadata (a validation for meta-learning [40]) Dpr and L pr+au is the loss function to train a model on training data Dpr+au with the primary and auxiliary tasks. To avoid cluttered notation, f, x, and y are omitted. Each task Tt has Nt samples and T0 and {Tt} T t=1 denote the primary and auxiliary tasks respectively. The proposed formulation in Eq. (2) learns how to assist the primary task by optimizing Θ via meta-learning. The nested optimization problem given Θ is a regular training with properly adjusted loss functions to balance the primary and auxiliary tasks. The formulation can be more specifically written as				
526	paper_162	Why is bi-level optimization for meta-learning difficult?	The goal is to optimize these parameters in a way that improves the performance of the primary task by utilizing the auxiliary tasks.  The optimization process becomes difficult because the primary task and auxiliary tasks may have conflicting objectives, making it challenging to find a set of parameters that work well for both.  Additionally, the nested optimization problem can become computationally expensive.	The model parameters \mathbf{w}^{k} for tasks can be updated with optimized \Theta^{k+1} in (7) as\displaystyle\mathbf{w}^{k+1}=\mathbf{w}^{k}-\alpha\nabla_{\mathbf{w}}\mathcal{L}^{pr+au}(\mathbf{w}^{k};\Theta^{k+1}).(8)Remarks. The proposed formulation can suffer from the meta-overfitting antoniou2018train ; zintgraf2018fast  meaning that the parameters \Theta to learn weights for softly selecting meta-paths and balancing the tasks with the primary task can overfit to the small meta-dataset.In our experiment, we found that the overfitting can be alleviated by meta-validation sets antoniou2018train .To learn \Theta that is generalizable across meta-training sets, we optimize \Theta across k different meta-datasets like k-fold cross validation using the following equation:Θk+1=Θk−β𝔼[∇Θℒp⁢r(𝐰^k(Θk))],Dp⁢r⁢(m⁢e⁢t⁢a)∼CV\displaystyle\Theta^{k+1}\;=\;\underset{D^{pr(meta)}\sim CV\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;}{\Theta^{k}\;-\;\;\beta\;\;\mathbb{E}\left[\;\nabla_{\Theta}\mathcal{L}^{pr}(\hat{\mathbf{w}}^{k}(\Theta^{k}))\;\right],}roman_Θ start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT = start_UNDERACCENT italic_D start_POSTSUPERSCRIPT italic_p italic_r ( italic_m italic_e italic_t italic_a ) end_POSTSUPERSCRIPT ∼ italic_C italic_V end_UNDERACCENT start_ARG roman_Θ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_β blackboard_E [ ∇ start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT italic_p italic_r end_POSTSUPERSCRIPT ( over^ start_ARG bold_w end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( roman_Θ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ) ] , end_ARG(9)where D^{pr(meta)}\sim CV is a meta-dataset from cross validation. We used 3-fold cross validation and the gradients of \Theta w.r.t different meta-datasets are averaged to update \Theta^{k}, see Algorithm 1. The cross validation is crucial to alleviate meta-overfitting and more discussion is Section 4.3.To circumvent the difficulty of the bi-level optimization, as previous works [39, 40] in meta-learning we approximate it with the updated parameters wˆ using the gradient descent update as w∗ (Θ) ≈ wˆ k (Θk ) = wk − α∇wL pr+au(wk ; Θk ), (6) where α is the learning rate for w. We do not numerically evaluate wˆ k (Θ) instead we plug the computational graph of wˆ k in L pr(w∗ (Θ)) to optimize Θ. Let ∇ΘL pr(w∗ (Θk )) be the gradient evaluated at Θk . Then updating parameters Θ is given as Θ k+1 = Θk − β∇ΘL pr(wˆ k (Θk )), (7) where β is the learning rate for Θ. This update allows softly selecting useful auxiliary tasks (metapaths) and balance them with the primary task to improve the performance of the primary task.				
527	paper_163	What is the anchor point in this paper?	In this paper, the anchor points are a subset of a set of points (denoted as P) that are selected using the Farthest Point Sampling (FPS) algorithm.  The anchor points are chosen by first selecting a random point and then sequentially choosing the farthest points from the previous points.	Sampling anchor points is the first step of our framework to locate multiple local transformations.To minimize the redundancy between local transformations, the anchor points \mathcal{P}^{\mathcal{A}}\subset\mathcal{P} are selected by the Farthest Point Sampling (FPS) algorithm.FPS randomly chooses the first point and then sequentially chooses the farthest points from previous points.This maximizes the coverage of anchor points and allows diverse transformations.				
528	paper_163	What does it mean the realistic sample?	A realistic sample in this context refers to a 3D object that has undergone a smooth deformation, meaning that the shape of the object changes gradually rather than abruptly.  The realistic samples that the authors aim to generate are those that resemble real-world objects with diverse shapes and deformations, such as airplanes with varying wing lengths and directions, guitars with different sizes and aspect ratios, and people with different heights and postures.	Smooth deformations are key to generate realistic and locally transformed samples.A naïve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts. It has a high chance to lose discriminative structures.Instead, we employ the Nadaraya-Watson kernel regression [27, 28] to smoothly interpolate the local transformations in the 3D space.Given M local transformations \{T_{j}\}_{j=1}^{M}, our smoothly varying transformation at an arbitrary point \mathbf{p}_{i} is given as:\small\hat{T}(\mathbf{p}_{i})=\frac{\sum_{j=1}^{M}{K_{h}}(\mathbf{p}_{i},\mathbf{p}^{\mathcal{A}}_{j})T_{j}}{\sum_{k=1}^{M}{K_{h}}(\mathbf{p}_{i},\mathbf{p}^{\mathcal{A}}_{k})},(3)where K_{h}(\cdot,\cdot) is a kernel function with bandwidth h, and T_{j} is the local transformation in (2) centered at \mathbf{p}^{\mathcal{A}}_{j}.To define \hat{T}(\mathbf{p}_{i}) at any point in the 3D space, we use a kernel function that has a strictly positive value for any pair of points,i.e., K_{h}(\mathbf{p}_{i},\mathbf{p}_{j})>0 for \forall\mathbf{p}_{i},\forall\mathbf{p}_{j}.The following proposition theoretically guarantees that our augmentation is a smooth transformation under mild conditions. The proof is in the supplement.Thus, CDA is simply a similarity transformation with small jittering that cannot simulate diverse shapes and deformable objects.Unlike synthetic datasets like ModelNet [14] and ShapeNet [26], a real-world dataset like ScanObjectNN [1] further necessitates the generation of sophisticated deformations such as a mixture of local transformations.These are exemplified in Figure 1: airplanes with varying lengths and directions of wings and body, guitars in varying sizes and aspect ratios, and people with different heights and postures (e.g., crossing legs).				
529	paper_163	Why should the proposed method have smoothly varying weights for transformations?	The proposed method uses smoothly varying weights for transformations to generate realistic and locally transformed samples.  The reason for this is that a naive application of a random local transformation within its finite neighborhood can result in a discontinuous shape and an overlap of different parts, leading to loss of discriminative structures, which can make the augmented object unrealistic.  By using smoothly varying weights, the Nadaraya-Watson kernel regression is able to interpolate the local transformations in the 3D space smoothly, resulting in a more realistic and locally transformed sample.	Smooth deformations are key to generate realistic and locally transformed samples.A naïve application of a random local transformation within its finite neighborhood may result in a discontinuous shape and an overlap of different parts. It has a high chance to lose discriminative structures.Instead, we employ the Nadaraya-Watson kernel regression [27, 28] to smoothly interpolate the local transformations in the 3D space.Given M local transformations \{T_{j}\}_{j=1}^{M}, our smoothly varying transformation at an arbitrary point \mathbf{p}_{i} is given as:\small\hat{T}(\mathbf{p}_{i})=\frac{\sum_{j=1}^{M}{K_{h}}(\mathbf{p}_{i},\mathbf{p}^{\mathcal{A}}_{j})T_{j}}{\sum_{k=1}^{M}{K_{h}}(\mathbf{p}_{i},\mathbf{p}^{\mathcal{A}}_{k})},(3)where K_{h}(\cdot,\cdot) is a kernel function with bandwidth h, and T_{j} is the local transformation in (2) centered at \mathbf{p}^{\mathcal{A}}_{j}.To define \hat{T}(\mathbf{p}_{i}) at any point in the 3D space, we use a kernel function that has a strictly positive value for any pair of points,i.e., K_{h}(\mathbf{p}_{i},\mathbf{p}_{j})>0 for \forall\mathbf{p}_{i},\forall\mathbf{p}_{j}.The following proposition theoretically guarantees that our augmentation is a smooth transformation under mild conditions. The proof is in the supplement.				
530	paper_163	Why is it necessary to maximize the coverage of anchor points?	Maximizing the coverage of anchor points is necessary in order to ensure that the local transformations are being applied evenly across the entire input space.  This allows for a more diverse set of augmented samples to be generated, which can help to improve the robustness and generalization of a model trained on the augmented data.	We present a simple yet effective point cloud augmentation with weighted local transformations (PointWOLF).Our method generates deformation for point clouds by a convex combination of multiple transformations with smoothly varying weights.PointWOLF first selects several anchor points and locates random local transformations (e.g., similarity transformations) at the anchor points.Based on the distance from a point in the input to the anchor points, our method differentially applies the local transformations.The smoothly varying weights based on the distance to the anchor points allow spatially continuous augmentation and generate realistic samples.Our framework can be viewed as a kernel regression with transformations.Sampling anchor points is the first step of our framework to locate multiple local transformations.To minimize the redundancy between local transformations, the anchor points \mathcal{P}^{\mathcal{A}}\subset\mathcal{P} are selected by the Farthest Point Sampling (FPS) algorithm.FPS randomly chooses the first point and then sequentially chooses the farthest points from previous points.This maximizes the coverage of anchor points and allows diverse transformations.				
531	paper_163	What is the difficulty of augmentation on point clouds compared to augmentation on traditional 2d images?	The difficulty of augmentation on point clouds compared to traditional 2D images is primarily due to the unordered and unstructured nature of point clouds.  Unlike 2D images, which have a well-defined grid structure and pixels with fixed locations, point clouds are just a collection of points in 3D space.  This makes it harder to apply standard image augmentation techniques, such as rotation and scaling, to point clouds.  Additionally, point clouds often have missing or incomplete data, which can make it difficult to generate realistic augmentations.	Modern deep learning techniques, which established their popularity on structured data, began showing success on point clouds.Unlike images with clear lattice structures, each point cloud is an unordered set of points with no inherent structures that globally represent various 3D objects.Recent deep learning efforts have focused on enabling neural networks to operate on point clouds.While several point cloud datasets appeared, a particular dataset of scanned real-world objects [1] required a much greater understanding of the point cloud structures to identify highly complex real-world objects.In response, the approaches have evolved from extracting point-wise information with no structural information [2] to explicitly encoding the local structure [3].These works on network development have been making steady progress despite the scarcity of point cloud data.				
532	paper_17	What does non-linguistic means?	Non-linguistic is something which is not related to linguistic information, and it includes the tasks such as quantitative computation and decimal operation.	In this paper, we explore whether pretraining on text is inherently about learning language, or if pretraining also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning (for example, performing quantitative computation such as finding the median of a set of numbers, recognizing regular expressions, or identifying whether a string is a palindrome, as shown in Figure 1). In other words, we investigate whether and how pretraining develops helpful inductive biases for non-linguistic reasoning. For this analysis, we create a set of 19 tasks from three categories of task paradigms: quantitative computation (§3.1), recognizing regular expressions (§3.2), and string reasoning (§3.3). Figure 1 shows an example for each category, and the full list of tasks is described in the table 1. We experiment with transformer and RNN based LMs (§4) for learning these tasks, and perform a comparative analysis with (non-pretrained) neural model variants from the perspective of learning metrics such as accuracy and sample efficiency.Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts.				
533	paper_17	Is this true? Despite using three different pretraining data (text domain), the model shows similar accuracy in big sample case.	It's false.  Models which pretrained using three different data outperform all non-pretrained data.	Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts. In Table 2, we note that all three non-English pretrained LMs significantly outperformed non-pretrained models, with the best performance being comparable or marginally lower than English versions. In fact, Code-BERT surprisingly surpasses ROC by 5%. These findings strongly indicate that the advantages from pretraining have little to do with the format of the tasks, since they persist for scenarios with little shared linguistic structure.				
536	paper_17	According to the paper, does BERT is overfitted?	They say that the reason of good performance of fine-tuned model is not caused by task specific knowledge.	Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts.				
537	paper_17	Is odd classification task is linguistic?	Odd classification is one of linguistic task because it does not included in six non-linguistic tasks.	Tables 2 and 3 shows the average accuracy of six non-linguistic tasks (palindrome classification, isogram classification, tautonym classification, odd even classification, decimal operation and median) fine-tuned using different BERT and DeBERTA representations respectively. We note that the models pretrained on all three domains outperformed the non-pretrained model (NP). This suggests that the results of experiments in Section 5 generalize to new text corpora for pretraining, and do not rely on having access to text on specific topics during pretraining. This is a non-trivial result, since it suggests for example, that the higher performance of pretrained models on tasks such as palindrome and anagram classification is not due to the pretrained models having seen information about such concepts during pretraining. This is especially so since the results even generalize to ROC stories, which contain no information on such technical concepts.				
538	paper_17	Is this true?: Calculating length of a string is string reasoning task.	Calculating length of a string is not a string reasoning task because it does not require character composition within or with another string.	This task paradigm focuses on reasoning tasks over individual strings or pairs of strings. Palindrome classification.A string is a palindrome if it reads the same forward and backward. The task is to classify whether a given string is a palindrome. The string length ranges from 1 to 15.Anagram classification.Two strings are anagrams if one is formed by rearranging letters from the other. The task is to classify if a pair of strings are anagrams. The string length ranges from 2 to 15.Isogram classification.A string is an isogram if it has no repeating characters. The task is to classify whether a given string is an isogram. The string length ranges from 1 to 52.Tautonym classification.A tautonym is a word which can be broken down into two identical parts, with the same spelling. The task is to classify whether a given string is a tautonym. The string length ranges from 1 to 10. Length of a string.Output the length of a given string. The string length ranges from 1 to 10.Count of unique characters.Given a string, count the number of unique characters in it. The string lengths ranges from 10 to 30.Parity check.Given a binary string, output if the counts of ones and zeros are the same. The maximum length of the binary string is 20.Vowels classification.Given a string, classify if the string contains only vowel characters. The string length ranges from 3 to 10. Maximum frequent character.Given a string, output the character with the maximum frequency. The string length ranges from 5 to 30.String reasoning: Figures 6 show the results on Palindrome, Anagram, Isogram and Tautonym classification. These tasks require character comparison within the string or with another string. Again, the pretrained variants consistently outperformed non-pretrained models variants in all of these tasks. In particular, the non-pretrained models completely fail to learn the Anagram and Palindrome tasks even for the largest training set size. Again, Transformer based LMs outperform LSTM based LMs.				
539	paper_17	Is this true? NILM has only classification tasks.	It's true, becase NILM has three kinds of tasks, and all tasks it classification task.	In this section, we describe the tasks used for our analysis, which we refer to as NILM (measuring Non-linguistic Inductive bias in Language Models). The tasks correspond to three task paradigms: (1) quantitative computation, (2) regular expressions, and (3) string reasoning. Each task in NILM is posed as a classification task. The descriptions for all the tasks with input and output examples, class labels and the input range are shown in Table 1. Each task has a synthetically generated dataset with train/dev/test splits222The training set size for all tasks is 10K, dev set size is 1K and test set size is 1K, except for tasks on recognizing regular expressions, where the test set size is 2K following previous work Bhattamishra et al. (2020).. To avoid biases in the datasets, relevant numbers and strings in individual examples are uniformly sampled from the appropriate ranges.				
540	paper_17	How is SNLI sort and SNLI shuffle different?	SNLI short consists of sentences with sorted words.  However, SNLI shuffle consists of sentences with randomly shuffled words.	SNLI sort. The words in the sentences of SNLI dataset are sorted based on alphabetical order. SNLI shuffle. We randomly shuffle words in sentences in the SNLI dataset. Amazon reviews sort. Similar to SNLI sort, the words in sentences are alphabetically sorted. Amazon reviews shuffle. We randomly shuffle words in sentences in the Amazon reviews dataset.				
541	paper_17	How Zipf distribution and Uniform distribution different?	Zipf distribution consists of words which picked with a unigram probability that follows Zipf's law.  However, uniform distribution consists of words that sampled with a uniform unigram probability.	Zipf distribution. We select 30k words (types) from the Amazon reviews dataset. Words are picked with a unigram probability that follows Zipf’s word frequency law, which all natural languages empirically follow Piantadosi (2014). For the Zipf distribution, we chose \alpha=1 and \beta=2.7, to match the parameters of most natural languages. The text does not follow any word order.Uniform distribution. In this dataset, words are sampled from the same vocabulary as in ‘Zipf distribution’, but with a uniform unigram probability. The text does not follow any word order.Synthetic Vocabulary. Words are selected with uniform distribution from a vocabulary to form sentences. However, instead of a vocabulary of English words, the words in the vocabulary are also synthetically generated (3 letter combinations of lower-case alphabets). In this text, the words do not possess morphology in addition to no syntax.				
542	paper_17	How is this paper and other previous works which have explored the ability of RNN and Transformer architecture?	Previous works only focus on the learnability of tasks.  They do not concentrate in pretrained LMS.  However, this paper focus on it.	Some previous works have explored the ability of RNN and Transformer architectures for learning regular languages Weiss et al. (2018); Sennhauser and Berwick (2018); Suzgun et al. (2019b); Bhattamishra et al. (2020), closing brackets Skachkova et al. (2018), and dynamic counting Suzgun et al. (2019a). However, they focus on the learnability of these tasks with specific architectures, and do not look at pretrained LMs, which are our focus here.				
543	paper_17	What does NILM means? Is it different to GLUE?	NILM is the dataset of measuring Non-linguistic Inductive bias in Language Models.  It is different with GLUE since GLUE focus on tasks require linguistic knowledge and reasoning.	Pretrained Language Models (LMs) have shown singular succcess on a range of natural language understandings tasks, to the extent that they have become foundational for contemporary NLP systems. Several works have investigated why pretraining works so well Warstadt et al. (2019); Zhao et al. (2020). In particular, studies have shown that the pretrained LMs like BERT capture linguistic knowledge about syntax Lin et al. (2019); Wu et al. (2020), semantics Vulić et al. (2020b, a) and morphology Hofmann et al. (2020, 2021). In fact, Tenney et al. (2019) demonstrated that learned representations in pretrained LMs even internally reflect the classical NLP pipeline. Since most NLP benchmarks such as SuperGLUE Wang et al. (2019) naturally are focused on tasks such as textual entailment and reading comprehension that require linguistic knowledge and reasoning, it is unsurprising that LMs have achieved strong results on these tasks. On the other hand, little work so far has explored the abilities of pretrained LMs for learning non-linguistic tasks. In this section, we describe the tasks used for our analysis, which we refer to as NILM (measuring Non-linguistic Inductive bias in Language Models). The tasks correspond to three task paradigms: (1) quantitative computation, (2) regular expressions, and (3) string reasoning. Each task in NILM is posed as a classification task. The descriptions for all the tasks with input and output examples, class labels and the input range are shown in Table 1. Each task has a synthetically generated dataset with train/dev/test splits222The training set size for all tasks is 10K, dev set size is 1K and test set size is 1K, except for tasks on recognizing regular expressions, where the test set size is 2K following previous work Bhattamishra et al. (2020).. To avoid biases in the datasets, relevant numbers and strings in individual examples are uniformly sampled from the appropriate ranges.				
544	paper_17	What does SNLI means? Is it a model?	SNLI is one of benchmark dataset published in 2015.	SNLI. We pretrained BERT small from scratch on SNLI data Bowman et al. (2015). It has 1000k sentences (570k pairs of text and hypothesis). Amazon reviews. We selected 500k movies and tv reviews from the larger Amazon reviews dataset He and McAuley (2016) and used for pretraining. Since reviews are in a free-text format, and their collection was not tailored with a NLP task in mind, they might be more representative of the complexity of real-world language use than SNLI.ROC. ROC is a corpora of 100K children stories, each made up of five sentences Mostafazadeh et al. (2017). The language in ROC is relatively simple in both vocabulary and sentence structure.				
545	paper_17	Explain Mode task in Decimal & word operation with examples.	Decimal & word operation is task of subtracting or dividing two numbers.  Operands in this task are represented in decimal or word notation.	This task paradigm focuses on tasks involving arithmetic and set statistics. Odd classification.Classify if a number is odd. Even classification.Classify if a number is even. Odd even classification.For a given number N and a string “even” or “odd”, classify if the number satisfies the string condition. Decimal operation. Subtract or divide two numbers. Operands are represented in decimal notation. Decimal & word operation. Subtract or divide two numbers. Operands are represented in decimal or word notation. Mean. Given a set of numbers, output the mean.Median. Given a set, output the median. Mode. Given a set of numbers, output the mode.				
546	paper_17	What does inductive bias means?	Inductive bias is performance gain of pretrained model in different linguistic structure.	Finally, in our discussion, we conceptually stretch the notion of inductive bias. The idea of inductive bias is usually associated with specific model types McCoy et al. (2020); Kharitonov and Chaabouni (2021), architectures Xu et al. (2021); Brutzkus and Globerson (2021) and regularization approaches Helmbold and Long (2015). We believe that extending this to refer to learning tasks with pretrained LMs is both reasonable and useful.				
547	paper_17	Explain the motivation of this paper	The motivation of this paper is analyzing whether pretraining on text is inherently about learning language or if pretraining inject non-linguisitc reasoning to LMs.	In this paper, we explore whether pretraining on text is inherently about learning language, or if pretraining also imbues LMs with skills for symbolic manipulation and non-linguistic reasoning (for example, performing quantitative computation such as finding the median of a set of numbers, recognizing regular expressions, or identifying whether a string is a palindrome, as shown in Figure 1). In other words, we investigate whether and how pretraining develops helpful inductive biases for non-linguistic reasoning. For this analysis, we create a set of 19 tasks from three categories of task paradigms: quantitative computation (§3.1), recognizing regular expressions (§3.2), and string reasoning (§3.3). Figure 1 shows an example for each category, and the full list of tasks is described in the table 1. We experiment with transformer and RNN based LMs (§4) for learning these tasks, and perform a comparative analysis with (non-pretrained) neural model variants from the perspective of learning metrics such as accuracy and sample efficiency.				
548	paper_17	What is the major one structural difference between ELMO model and others (BERT small, BERT large, DeBERTa)	ELMO is LSTM based language model, but BERT and DeBERTa is transformer based language model.	Next, we describe the LMs and their variants used in NILM. We experiment with four language models, based on both Transformer and RNN architectures. BERT small.This is the bert-base-uncased model with 12 transformer encoder layers and the dimension of the representations is 768. BERT tokenizer is based on the WordPiece model Wu et al. (2016). BERT large.This is the bert-large-uncased model which has 24 transformer encoders and representations have 1024 dimensions. DeBERTa.This is a transformer based language model and its tokenizer is built using Byte Pair Encoding Sennrich et al. (2016). We consider the DeBERTa base model. It has 12 transformer encoder layers and representations have 768 dimensions. ELMO.This is an LSTM based language model Peters et al. (2018).It has 3 layers and the output representations have 1024 dimensions.				
549	paper_17	Why author said that underperformance of non-pretrained models comes from small data?	Author said that underperformance of non-pretrained models comes from small data because if the model parameter size is too large compare to the data size, model training can be suffured under-fitting.	As previously mentioned, a possible explanation for the underperformance of non-pretrained models ise that the large number of parameters of the architecture relative to the sizes of the training data might be leading to under-fitting. To test this, we experiment with smaller Transformer-based models with varying numbers of parameters.				
550	paper_17	Why author did 7.3 Non-english and computer languages tast? What is the objective of this section?	They did Non-english and computer languages test to shows that the benefits of from pretraining have little to do with the format of the tasks.  Therefore, objective of this section is to show that advantage of pretraining persist with various degrees.	Finally, we investigate the role that pretraining data plays in influencing task performance on non-linguistic tasks (§7). We experiment with pretraining on different domains of text, pretraining on perturbed representations of natural language text (such as shuffled word order), pretraining on text of computer programs (no linguistic properties of natural languages), pretraining on multi-lingual and non-English text, and pretraining with synthetic text (data sampled from synthetic distributions). Our analysis reveals that the advantages of pretraining surprisingly persist with various degrees across these variations, suggesting hithertho unexplored connections between pretraining and the learning abilities of language models. Our contributions are:In Table 2, we note that all three non-English pretrained LMs significantly outperformed non-pretrained models, with the best performance being comparable or marginally lower than English versions. In fact, Code-BERT surprisingly surpasses ROC by 5%. These findings strongly indicate that the advantages from pretraining have little to do with the format of the tasks, since they persist for scenarios with little shared linguistic structure.				
551	paper_17	what is the evidence for auther’s saying: “Our observation that is behavior is seen even when pretraining on synthetically generated languages”?	Author said  “Our observation that is behavior is seen even when pretraining on synthetically generated languages” since they showed that the benefits of pretraining persist with various degrees in non-linguistic tasks.	Finally, we investigate the role that pretraining data plays in influencing task performance on non-linguistic tasks (§7). We experiment with pretraining on different domains of text, pretraining on perturbed representations of natural language text (such as shuffled word order), pretraining on text of computer programs (no linguistic properties of natural languages), pretraining on multi-lingual and non-English text, and pretraining with synthetic text (data sampled from synthetic distributions). Our analysis reveals that the advantages of pretraining surprisingly persist with various degrees across these variations, suggesting hithertho unexplored connections between pretraining and the learning abilities of language models. Our contributions are:				
552	paper_17	Look Figure 4.  Give your one observation by comparing (a) and (b), or pretrained and non-pretrained. Reason them.	pretrained LMs can perfectly learn the tasks with many fewer labeled examples, compared to the non-pretrained models in both tasks.	Recognizing regular expressions: Figure 4 shows the comparative performance of pretrained LMs on non-pretrained models on the two tasks involving recognizing regular expressions. For both tasks, we note that the pretrained LMs can perfectly learn the tasks with many fewer labeled examples compared to the non-pretrained models. In both cases, the non-pretrained Transformer-based models eventually reach optimal performance as well. However, curiously the ELMO based non-pretrained models struggle with learning both tasks.				
554	paper_172	How does the author show that the algorithm is free from the issue of diverging from human-language?	The authors show it by conducting the human evaluation on Amazon Mechanical Turk (AMT).	We also conduct human evaluation on Amazon Mechanical Turk (AMT) to assess the quality of gen- erated responses of GPT-Critic and baseline algorithms, using the evaluation protocol as in (Yang et al., 2021; Lin et al., 2020; Zhang et al., 2020). Speciﬁcally, human workers on AMT were asked to read the context and generated response by interactive simulation via ConvLab, then score the following two evaluation metrics on a Likert scale (1-5): 1) Appropriateness : evaluates whether the generated responses are appropriate for the given context, 2) Fluency : evaluates whether the gen- erated responses are comprehensible and human-like. We compare the performance of GPT-Critic with same baselines on ConvLab evaluation. Figure 3 summarizes the overall results of human eval- uation, where 60 workers evaluate the quality of 30 randomly selected dialogues for each algorithm. The results show that GPT-Critic signiﬁcantly outperforms baseline algorithms in appropriateness which is related to task accomplishment. Moreover, the result of ﬂuency shows that GPT-Critic does not hurt the agent’s capability to generate human-like sentences.				
556	paper_172	What are the metrics used to evaluate the naturalness of the sentences generated by the policy?	They evaluated the naturalness of the generated sentences by the fluency metric in the human evaluation.	We also conduct human evaluation on Amazon Mechanical Turk (AMT) to assess the quality of gen- erated responses of GPT-Critic and baseline algorithms, using the evaluation protocol as in (Yang et al., 2021; Lin et al., 2020; Zhang et al., 2020). Speciﬁcally, human workers on AMT were asked to read the context and generated response by interactive simulation via ConvLab, then score the following two evaluation metrics on a Likert scale (1-5): 1) Appropriateness : evaluates whether the generated responses are appropriate for the given context, 2) Fluency : evaluates whether the gen- erated responses are comprehensible and human-like. We compare the performance of GPT-Critic with same baselines on ConvLab evaluation. Figure 3 summarizes the overall results of human eval- uation, where 60 workers evaluate the quality of 30 randomly selected dialogues for each algorithm. The results show that GPT-Critic signiﬁcantly outperforms baseline algorithms in appropriateness which is related to task accomplishment. Moreover, the result of ﬂuency shows that GPT-Critic does not hurt the agent’s capability to generate human-like sentences.				
557	paper_172	Did the authors have an experiment with training the state-of-the-art offline RL algorithm with MultiWOZ dataset?	The authors provide experimental results of CRR and Decision Transformer as baselines of offline RL algorithm.	In this section, we show the experimental results of GPT-critic on both automatic evaluation and human evaluation. First, we evaluate the performances of GPT-Critic on the MultiWOZ 2.0 (Budzianowski et al., 2018) as dataset-based automatic evaluation, compared with baseline methods including ofﬂine RL algorithms. Second, for more realistic evaluation, we conduct a simulator-based evaluation on the ConvLab framework (Zhu et al., 2020). Third, we also conduct the human eval- uation to evaluate the quality of generated responses. Finally, we give a qualitative analysis of our method using generated dialogue examples on the training dataset of MultiWOZ 2.0, which shows how GPT-Critic improves the performance through the behavior cloning of self-generated dialogues. The qualitative analysis with generated dialogue examples can be found in Appendix B.In addition, we also compare with recent ofﬂine RL algorithms that are free from the issue of di- verging from human language: 1) CRR (Wang et al., 2020), a value-ﬁltered regression method that performs weighted behavior cloning of ofﬂine dataset, 2) Decision Transformer (Chen et al., 2021), a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. For a fair comparison, we use the same pre-trained GPT-2 model as a policy network to train the CRR and the Decision Transformer. Moreover, to show that the policy-gradient-based standard RL algorithms suffer from diverging from human language, we also provide examples of responses generated by policy-gradient-based standard RL algorithm in Appendix D.For the results of ofﬂine RL baselines, CRR and Decision Transformer show the results that do not diverge from human-language, since their policy is also trained by behavior cloning. However, both algorithms show limited performance because they perform behavior cloning on a ﬁxed dataset. CRR has achieved remarkable success in continuous control tasks by performing weighted behavior cloning of training dataset ﬁltered by critic, but it does not effectively perform in the task-oriented dialogues because of data scarcity. Furthermore, to evaluate the Decision Transformer, we adopt a delayed return where the agent receives the cumulative reward at the end of dialogue, since the agent cannot observe user goal. Therefore, without observing the user goal at test time, Decision Transformer reduces to the behavior cloning of successful dialogues.				
558	paper_172	CRR is also an algorithm that is free from issues that diverge from human language. What are the advantages compared to CRR?	CRR is a variant of weighted behavior cloning approaches that perform behavior cloning with a learned weight on a fixed dataset.  In contrast to the CRR, where the action choice is restricted to the support in the given dataset, the proposed algorithm can effectively improve the policy by revising unsuccessful dialogues into successful ones.	Ofﬂine Reinforcement Learning. There have been extensive studies on ofﬂine RL (Fujimoto et al., 2019; Levine et al., 2020; Kumar et al., 2020; Wang et al., 2020). Most of prior works are built on the off-policy actor-critic framework, and they focus on the overestimation issue by taking the OOD actions (Kumar et al., 2019; Lee et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al., 2020). However, a naive application of these ofﬂine RL methods suffer from the issue of diverging from human language in the task-oriented dialogues (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020). On the other hand, there are a number of recent works on weighted behavior cloning, where a policy is trained by a variant of supervised learning loss (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). The weighted behavior cloning approaches ﬁlter out bad actions, then perform behavior cloning on high-quality data. However, in the task-oriented dialogues, simply dropping the unsuccessful dialogues from the corpus is undesirable, since they may contain some task-speciﬁc information that is useful to properly respond to user requests. Our GPT-Critic aims to revise unsuccessful dialogues into successful ones, which is in contrast to the weighted behavior cloning on the ﬁxed training dataset, where the action choice is restricted to the support in the dataset (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). More recently, Chen et al. (2021) introduce Decision Transformer, a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. These ofﬂine RL methods based on behavior cloning are directly applied to the task-oriented dialogues without aforementioned issue, but their results are similar to that of behavior cloning in the task-oriented dialogues.For the results of ofﬂine RL baselines, CRR and Decision Transformer show the results that do not diverge from human-language, since their policy is also trained by behavior cloning. However, both algorithms show limited performance because they perform behavior cloning on a ﬁxed dataset. CRR has achieved remarkable success in continuous control tasks by performing weighted behavior cloning of training dataset ﬁltered by critic, but it does not effectively perform in the task-oriented dialogues because of data scarcity. Furthermore, to evaluate the Decision Transformer, we adopt a delayed return where the agent receives the cumulative reward at the end of dialogue, since the agent cannot observe user goal. Therefore, without observing the user goal at test time, Decision Transformer reduces to the behavior cloning of successful dialogues.				
559	paper_172	They claim that the proposed algorithm can maintain the GPT-2’s ability to generate human-like responses while improving the task performance. Is this true?	The authors claim that the proposed algorithm can maintain the GPT-2’s ability to generate human-like responses while improving the task performance.  In the experiments, they show that the proposed method improves the task performance without the issue of diverging from human language.	Table 3 summarizes the overall performance of GPT-Critic and baseline algorithms in end-to-end response generation setting, where the generated dialogue state and generated dialogue act are used for the DB search and response generation. The results show that GPT-Critic achieved the best performance in terms of inform rate, success rate, and combined score. Moreover, the performance of GPT-Critic on the BLEU score matches those of other pre-trained LM-based methods, since GPT-Critic inherits GPT-2’s ability to generate human-like responses through the behavior cloning of responses generated by GPT-2. The results show that GPT-Critic improves the task performance of the agent without the issue of diverging from human language. In addition, as can be shown in Table 3, the naive data augmentation is not effective since it will not change the GPT’s sampling distribution in principle.We also conduct human evaluation on Amazon Mechanical Turk (AMT) to assess the quality of gen- erated responses of GPT-Critic and baseline algorithms, using the evaluation protocol as in (Yang et al., 2021; Lin et al., 2020; Zhang et al., 2020). Speciﬁcally, human workers on AMT were asked to read the context and generated response by interactive simulation via ConvLab, then score the following two evaluation metrics on a Likert scale (1-5): 1) Appropriateness : evaluates whether the generated responses are appropriate for the given context, 2) Fluency : evaluates whether the gen- erated responses are comprehensible and human-like. We compare the performance of GPT-Critic with same baselines on ConvLab evaluation. Figure 3 summarizes the overall results of human eval- uation, where 60 workers evaluate the quality of 30 randomly selected dialogues for each algorithm. The results show that GPT-Critic signiﬁcantly outperforms baseline algorithms in appropriateness which is related to task accomplishment. Moreover, the result of ﬂuency shows that GPT-Critic does not hurt the agent’s capability to generate human-like sentences.				
560	paper_172	Which of the baseline algorithms are the offline RL algorithms?	CRR and Decision Transformer are used as offline RL baseline algorithms in the paper.	In addition, we also compare with recent ofﬂine RL algorithms that are free from the issue of di- verging from human language: 1) CRR (Wang et al., 2020), a value-ﬁltered regression method that performs weighted behavior cloning of ofﬂine dataset, 2) Decision Transformer (Chen et al., 2021), a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. For a fair comparison, we use the same pre-trained GPT-2 model as a policy network to train the CRR and the Decision Transformer. Moreover, to show that the policy-gradient-based standard RL algorithms suffer from diverging from human language, we also provide examples of responses generated by policy-gradient-based standard RL algorithm in Appendix D.For the results of ofﬂine RL baselines, CRR and Decision Transformer show the results that do not diverge from human-language, since their policy is also trained by behavior cloning. However, both algorithms show limited performance because they perform behavior cloning on a ﬁxed dataset. CRR has achieved remarkable success in continuous control tasks by performing weighted behavior cloning of training dataset ﬁltered by critic, but it does not effectively perform in the task-oriented dialogues because of data scarcity. Furthermore, to evaluate the Decision Transformer, we adopt a delayed return where the agent receives the cumulative reward at the end of dialogue, since the agent cannot observe user goal. Therefore, without observing the user goal at test time, Decision Transformer reduces to the behavior cloning of successful dialogues.				
561	paper_172	How are actions defined in task-oriented dialogue?	In task-oriented dialogues, the actions are defined as a sequence of tokens which represents dialogue act and system response.	We consider the task-oriented dialogue system that can be modeled as a partially observable Markov decision process (POMDP) (Williams & Young, 2007) deﬁned by tuple (cid:104) S, A, O, T, Z, R, γ (cid:105) where S is the set of environment states s = (cid:104) g, h (cid:105) (underlying state that consists of the user goal g and dialogue history h ), A is the set of actions a (a sequence of tokens which represents dialogue act and system response ), O is the set of observations o (user utterance), T ( s (cid:48) | s, a ) = Pr( s t +1 = s (cid:48) | s t = s, a t = a ) is the transition function, Z ( o | s (cid:48) , a ) = Pr( o t +1 = o | s t +1 = s (cid:48) , a t = a ) is the observation probability, R ( g, h, a ) is the reward function indicating the utility of executing action a in history h and the user goal g , and γ ∈ (0 , 1) is a discount factor. The history at time step t , h t = { o 0 , a 0 , . . . o t − 1 , a t − 1 , o t } , is a sequence of all previous observations and actions. Since the underlying state s (e.g. user goal) is not directly observable, the agent makes decisions based on the entire observation-action history. The policy π ( a t | h t ) is mapping from history h t to a probability distribution over A . The goal is to ﬁnd an optimal policy π ∗ that maximizes the expected cumulative rewards, i.e. π ∗ = arg max π E π [ (cid:80) ∞ t =0 γ t R ( g, h t , a t )] . The action-value function of policy π is deﬁned as Q π ( h, a ) := E π [ (cid:80) ∞ t =0 γ t R ( g, h t , a t ) | h 0 = h, a 0 = a ] , where Q π is a unique solution of the Bellman equation: Q π ( h, a ) = E g [ R ( g, h, a )] + γ E π [ Q π ( h (cid:48) , a (cid:48) )] .				
562	paper_172	What does “KL control” means?	KL control means that the regularization technique to restrict the policy to stay close to its prior policy.	Reinforcement Learning for Task-Oriented Dialogue Systems. Applying the standard RL meth- ods straightforwardly to optimize a task-oriented dialogue agent causes the issue of diverging from human language. To address this problem, interleaving reinforcement learning with supervised learning has been proposed but it is still not free from the issue of diverging from human language (Lewis et al., 2017). Recently, the latent representation models for language actions have been in- troduced to address the aforementioned problem (Zhao et al., 2019; Yarats & Lewis, 2018). They disentangle the semantics of the utterance and the natural language generation, and then perform goal-based training in the space of the latent variables instead of directly optimizing utterances. However, they cannot be directly applied to large-scale pre-trained language models that are not designed in a way that works inherently with discrete latent variables. Jaques et al. (2020) use KL- control to restrict the policy to stay close to its prior policy, but it still suffers from divergence from human language even with carefully chosen hyper-parameters. Furthermore, Jang et al. (2020) adopt Bayes-adaptive Monte-Carlo planning to negotiation dialogue then use it as a policy improvement operator. This approach can prevent the issue of diverging from human language through the policy improvement based on behavior cloning of self-generated dialogues. However, they assume a user model that is difﬁcult enough to be considered another problem.				
563	paper_172	What does “offline RL” means?	Offline RL is one of the reinforcement learning settings that assumes the agent aims to optimize the policy solely from the ﬁxed dataset without online environment interaction.	Training a task-oriented dialogue agent can be naturally formulated as ofﬂine rein- forcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasi- ble (syntactically and semantically correct) actions are very sparse. Thus, standard RL methods easily fail and generate responses diverging from human language, even when ﬁne-tuning a powerful pre-trained language model. In this paper, we introduce GPT-Critic, an ofﬂine RL method for task-oriented dialogue. GPT-Critic is built upon GPT-2, ﬁne-tuning the language model through behavior cloning of the critic-guided self-generated sentences. GPT-Critic is essentially free from the issue of diverging from human language since it learns from the sentences sam- pled from the pre-trained language model. In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.Training a task-oriented conversational agent from a dialogue corpus can be naturally formulated as ofﬂine reinforcement learning (RL) problem (Levine et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020), which offers the prospect to optimize the policy solely from the ﬁxed dataset without online environment interaction. Most of the existing ofﬂine RL methods are built on the off-policy Actor- Critic framework, which performs iterative optimization of the policy (i.e. actor) and the action- value function (i.e. critic) (Fujimoto et al., 2019; Janner et al., 2019; Kumar et al., 2020). Yet, a naive application of these ofﬂine RL methods generally results in poor dialogue strategies which generate responses in no way similar to human language (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020).				
564	paper_172	What is “overestimation issue” in RL?	The overestimation issue means the problem when the action values are overestimated by using out-of-distribution actions in RL.	where ¯ φ is the parameters of the target network. As discussed in the prior work (Fujimoto et al., 2019; Kumar et al., 2020), optimizing this loss can be challenging in the ofﬂine RL setting due to the overestimation issue in the bootstrapping process by taking out-of-distribution (OOD) actions to evaluate the value of the next state.Ofﬂine Reinforcement Learning. There have been extensive studies on ofﬂine RL (Fujimoto et al., 2019; Levine et al., 2020; Kumar et al., 2020; Wang et al., 2020). Most of prior works are built on the off-policy actor-critic framework, and they focus on the overestimation issue by taking the OOD actions (Kumar et al., 2019; Lee et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al., 2020). However, a naive application of these ofﬂine RL methods suffer from the issue of diverging from human language in the task-oriented dialogues (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020). On the other hand, there are a number of recent works on weighted behavior cloning, where a policy is trained by a variant of supervised learning loss (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). The weighted behavior cloning approaches ﬁlter out bad actions, then perform behavior cloning on high-quality data. However, in the task-oriented dialogues, simply dropping the unsuccessful dialogues from the corpus is undesirable, since they may contain some task-speciﬁc information that is useful to properly respond to user requests. Our GPT-Critic aims to revise unsuccessful dialogues into successful ones, which is in contrast to the weighted behavior cloning on the ﬁxed training dataset, where the action choice is restricted to the support in the dataset (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). More recently, Chen et al. (2021) introduce Decision Transformer, a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. These ofﬂine RL methods based on behavior cloning are directly applied to the task-oriented dialogues without aforementioned issue, but their results are similar to that of behavior cloning in the task-oriented dialogues.				
565	paper_172	What are the examples of offline RL algorithms that are applicable to the task-oriented dialogue domain without diverging from human language?	CRR and Decision Transformer are the examples of offline RL algorithms that are applicable to the task-oriented dialogue domain without diverging from human language.	In addition, we also compare with recent ofﬂine RL algorithms that are free from the issue of di- verging from human language: 1) CRR (Wang et al., 2020), a value-ﬁltered regression method that performs weighted behavior cloning of ofﬂine dataset, 2) Decision Transformer (Chen et al., 2021), a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. For a fair comparison, we use the same pre-trained GPT-2 model as a policy network to train the CRR and the Decision Transformer. Moreover, to show that the policy-gradient-based standard RL algorithms suffer from diverging from human language, we also provide examples of responses generated by policy-gradient-based standard RL algorithm in Appendix D.For the results of ofﬂine RL baselines, CRR and Decision Transformer show the results that do not diverge from human-language, since their policy is also trained by behavior cloning. However, both algorithms show limited performance because they perform behavior cloning on a ﬁxed dataset. CRR has achieved remarkable success in continuous control tasks by performing weighted behavior cloning of training dataset ﬁltered by critic, but it does not effectively perform in the task-oriented dialogues because of data scarcity. Furthermore, to evaluate the Decision Transformer, we adopt a delayed return where the agent receives the cumulative reward at the end of dialogue, since the agent cannot observe user goal. Therefore, without observing the user goal at test time, Decision Transformer reduces to the behavior cloning of successful dialogues.				
566	paper_172	How is learning a task-oriented dialogue agent different from the problems in the RL domain?	In task-oriented dialogue, the action space is combinatorially large and a naive application of RL algorithms suffer from the issue of diverging from human language.	Training a task-oriented dialogue agent can be naturally formulated as ofﬂine rein- forcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasi- ble (syntactically and semantically correct) actions are very sparse. Thus, standard RL methods easily fail and generate responses diverging from human language, even when ﬁne-tuning a powerful pre-trained language model. In this paper, we introduce GPT-Critic, an ofﬂine RL method for task-oriented dialogue. GPT-Critic is built upon GPT-2, ﬁne-tuning the language model through behavior cloning of the critic-guided self-generated sentences. GPT-Critic is essentially free from the issue of diverging from human language since it learns from the sentences sam- pled from the pre-trained language model. In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.Building an end-to-end task-oriented dialogue agent is one of the promising applications of natural language processing (NLP) tasks, yet challenging due to large language action spaces and limited availability of human-annotated data. Recently, large-scale pre-trained language models (LM) have achieved remarkable successes in various NLP tasks with prohibitively large vocabulary (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Raffel et al., 2019). The current best performing end-to-end conversational agents for a task-oriented dialogue system utilize a pre-training on large- scale corpus and ﬁne-tuning on downstream tasks (Ham et al., 2020; Yang et al., 2021; Lin et al., 2020; Peng et al., 2021). This combination of pre-training and ﬁne-tuning signiﬁcantly improves overall performance in the task-oriented dialogues. However, supervised ﬁne-tuning (i.e. imitation learning of the dialogue corpus) alone may not be sufﬁcient to learn an optimal dialogue strategy since the corpus often contains suboptimal dialogues collected from human participants of diverse expertise levels. Thus, in order to optimize the task performance of the conversational agent, goal- oriented training (i.e. reinforcement learning) is an essential and promising direction to pursue.Training a task-oriented conversational agent from a dialogue corpus can be naturally formulated as ofﬂine reinforcement learning (RL) problem (Levine et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020), which offers the prospect to optimize the policy solely from the ﬁxed dataset without online environment interaction. Most of the existing ofﬂine RL methods are built on the off-policy Actor- Critic framework, which performs iterative optimization of the policy (i.e. actor) and the action- value function (i.e. critic) (Fujimoto et al., 2019; Janner et al., 2019; Kumar et al., 2020). Yet, a naive application of these ofﬂine RL methods generally results in poor dialogue strategies which generate responses in no way similar to human language (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020).Reinforcement Learning for Task-Oriented Dialogue Systems. Applying the standard RL meth- ods straightforwardly to optimize a task-oriented dialogue agent causes the issue of diverging from human language. To address this problem, interleaving reinforcement learning with supervised learning has been proposed but it is still not free from the issue of diverging from human language (Lewis et al., 2017). Recently, the latent representation models for language actions have been in- troduced to address the aforementioned problem (Zhao et al., 2019; Yarats & Lewis, 2018). They disentangle the semantics of the utterance and the natural language generation, and then perform goal-based training in the space of the latent variables instead of directly optimizing utterances. However, they cannot be directly applied to large-scale pre-trained language models that are not designed in a way that works inherently with discrete latent variables. Jaques et al. (2020) use KL- control to restrict the policy to stay close to its prior policy, but it still suffers from divergence from human language even with carefully chosen hyper-parameters. Furthermore, Jang et al. (2020) adopt Bayes-adaptive Monte-Carlo planning to negotiation dialogue then use it as a policy improvement operator. This approach can prevent the issue of diverging from human language through the policy improvement based on behavior cloning of self-generated dialogues. However, they assume a user model that is difﬁcult enough to be considered another problem.				
567	paper_172	What is main different between the experiments on MultiWOZ and ConvLab?	The main difference is that MultiWOZ banchmark provides dataset-based automatic evaluation and ConvLab framework provides a simulator-based evaluation.	In this section, we show the experimental results of GPT-critic on both automatic evaluation and human evaluation. First, we evaluate the performances of GPT-Critic on the MultiWOZ 2.0 (Budzianowski et al., 2018) as dataset-based automatic evaluation, compared with baseline methods including ofﬂine RL algorithms. Second, for more realistic evaluation, we conduct a simulator-based evaluation on the ConvLab framework (Zhu et al., 2020). Third, we also conduct the human eval- uation to evaluate the quality of generated responses. Finally, we give a qualitative analysis of our method using generated dialogue examples on the training dataset of MultiWOZ 2.0, which shows how GPT-Critic improves the performance through the behavior cloning of self-generated dialogues. The qualitative analysis with generated dialogue examples can be found in Appendix B.				
568	paper_172	How is the proposed method free from the issue of diverging from human language?	Since the proposed method updates the policy through behavior cloning of the self-generated human-like responses, it is essentially free from the issue of diverging from human language.	Training a task-oriented dialogue agent can be naturally formulated as ofﬂine rein- forcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasi- ble (syntactically and semantically correct) actions are very sparse. Thus, standard RL methods easily fail and generate responses diverging from human language, even when ﬁne-tuning a powerful pre-trained language model. In this paper, we introduce GPT-Critic, an ofﬂine RL method for task-oriented dialogue. GPT-Critic is built upon GPT-2, ﬁne-tuning the language model through behavior cloning of the critic-guided self-generated sentences. GPT-Critic is essentially free from the issue of diverging from human language since it learns from the sentences sam- pled from the pre-trained language model. In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.We presented GPT-Critic, an ofﬂine RL algorithm for task-oriented dialogue system, which can be adopted for any generative pre-trained language model. GPT-Critic aims to learn an end-to-end task-oriented dialogue agent without the issue of diverging from human language. GPT-Critic starts with ﬁne-tuning the GPT-2 model and learning the critic using the dialogue corpus. Then, GPT- Critic updates the policy through the behavior cloning of the critic-guided self-generated responses, thus it is essentially free from the issue of diverging from human language. In the experiments, we demonstrated that GPT-Critic outperforms the state-of-the-art algorithms in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.revised responses. Moreover, Table 5 shows that the generated dialogues do not diverge from hu- man language. Since GPT-Critic updates the policy through behavior cloning of the self-generated human-like responses, GPT-Critic is essentially free from the issue of diverging from human lan- guage.				
569	paper_172	How does the proposed method address the issue of large action spaces?	The proposed method consider the set of response candidates that are generated from the ﬁne-tuned GPT-2 as action spaces.	In order to address the prohibitively large language action spaces, we explicitly consider the set of response candidates that are generated from the ﬁne-tuned GPT-2. The GPT-Critic selects the				
571	paper_172	Why are most of the existing offline RL algorithms not straightforward to apply to the task-oriented dialogue?	Since a naive application of existing offline RL algorithms suffer from the issue of diverging from human language, it is not straightforward to apply them to the task-oriented dialogue.	Training a task-oriented conversational agent from a dialogue corpus can be naturally formulated as ofﬂine reinforcement learning (RL) problem (Levine et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020), which offers the prospect to optimize the policy solely from the ﬁxed dataset without online environment interaction. Most of the existing ofﬂine RL methods are built on the off-policy Actor- Critic framework, which performs iterative optimization of the policy (i.e. actor) and the action- value function (i.e. critic) (Fujimoto et al., 2019; Janner et al., 2019; Kumar et al., 2020). Yet, a naive application of these ofﬂine RL methods generally results in poor dialogue strategies which generate responses in no way similar to human language (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020).Reinforcement Learning for Task-Oriented Dialogue Systems. Applying the standard RL meth- ods straightforwardly to optimize a task-oriented dialogue agent causes the issue of diverging from human language. To address this problem, interleaving reinforcement learning with supervised learning has been proposed but it is still not free from the issue of diverging from human language (Lewis et al., 2017). Recently, the latent representation models for language actions have been in- troduced to address the aforementioned problem (Zhao et al., 2019; Yarats & Lewis, 2018). They disentangle the semantics of the utterance and the natural language generation, and then perform goal-based training in the space of the latent variables instead of directly optimizing utterances. However, they cannot be directly applied to large-scale pre-trained language models that are not designed in a way that works inherently with discrete latent variables. Jaques et al. (2020) use KL- control to restrict the policy to stay close to its prior policy, but it still suffers from divergence from human language even with carefully chosen hyper-parameters. Furthermore, Jang et al. (2020) adopt Bayes-adaptive Monte-Carlo planning to negotiation dialogue then use it as a policy improvement operator. This approach can prevent the issue of diverging from human language through the policy improvement based on behavior cloning of self-generated dialogues. However, they assume a user model that is difﬁcult enough to be considered another problem.Ofﬂine Reinforcement Learning. There have been extensive studies on ofﬂine RL (Fujimoto et al., 2019; Levine et al., 2020; Kumar et al., 2020; Wang et al., 2020). Most of prior works are built on the off-policy actor-critic framework, and they focus on the overestimation issue by taking the OOD actions (Kumar et al., 2019; Lee et al., 2020; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al., 2020). However, a naive application of these ofﬂine RL methods suffer from the issue of diverging from human language in the task-oriented dialogues (Lewis et al., 2017; Zhao et al., 2019; Jang et al., 2020). On the other hand, there are a number of recent works on weighted behavior cloning, where a policy is trained by a variant of supervised learning loss (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). The weighted behavior cloning approaches ﬁlter out bad actions, then perform behavior cloning on high-quality data. However, in the task-oriented dialogues, simply dropping the unsuccessful dialogues from the corpus is undesirable, since they may contain some task-speciﬁc information that is useful to properly respond to user requests. Our GPT-Critic aims to revise unsuccessful dialogues into successful ones, which is in contrast to the weighted behavior cloning on the ﬁxed training dataset, where the action choice is restricted to the support in the dataset (Wang et al., 2020; Peng et al., 2019; Siegel et al., 2020). More recently, Chen et al. (2021) introduce Decision Transformer, a Transformer-based architecture that casts the problem of RL as conditional sequence modeling. These ofﬂine RL methods based on behavior cloning are directly applied to the task-oriented dialogues without aforementioned issue, but their results are similar to that of behavior cloning in the task-oriented dialogues.				
572	paper_172	Why does the proposed algorithm not outperforms in turn metric in the results of ConvLab experiments?	Since a proposed method is trained by maximizing the success rate without considering the dialogue turn, the proposed algorithm does not outperforms in turn metric in the results.	performance in all metrics related to task accomplishment. However, they also show that GPT-Critic takes longer dialogue turn for the task accomplishment because GPT-Critic is trained by maximizing the success rate without considering the dialogue turn.				
573	paper_172	In the human evaluation, what does the author want to show differently from MultiWOZ and Convlab experiments?	The author want to show that the proposed method does not suffer from the issue of diverging from human language.	In this section, we show the experimental results of GPT-critic on both automatic evaluation and human evaluation. First, we evaluate the performances of GPT-Critic on the MultiWOZ 2.0 (Budzianowski et al., 2018) as dataset-based automatic evaluation, compared with baseline methods including ofﬂine RL algorithms. Second, for more realistic evaluation, we conduct a simulator-based evaluation on the ConvLab framework (Zhu et al., 2020). Third, we also conduct the human eval- uation to evaluate the quality of generated responses. Finally, we give a qualitative analysis of our method using generated dialogue examples on the training dataset of MultiWOZ 2.0, which shows how GPT-Critic improves the performance through the behavior cloning of self-generated dialogues. The qualitative analysis with generated dialogue examples can be found in Appendix B.We also conduct human evaluation on Amazon Mechanical Turk (AMT) to assess the quality of gen- erated responses of GPT-Critic and baseline algorithms, using the evaluation protocol as in (Yang et al., 2021; Lin et al., 2020; Zhang et al., 2020). Speciﬁcally, human workers on AMT were asked to read the context and generated response by interactive simulation via ConvLab, then score the following two evaluation metrics on a Likert scale (1-5): 1) Appropriateness : evaluates whether the generated responses are appropriate for the given context, 2) Fluency : evaluates whether the gen- erated responses are comprehensible and human-like. We compare the performance of GPT-Critic with same baselines on ConvLab evaluation. Figure 3 summarizes the overall results of human eval- uation, where 60 workers evaluate the quality of 30 randomly selected dialogues for each algorithm. The results show that GPT-Critic signiﬁcantly outperforms baseline algorithms in appropriateness which is related to task accomplishment. Moreover, the result of ﬂuency shows that GPT-Critic does not hurt the agent’s capability to generate human-like sentences.				
574	paper_173	What is different between the results denoted by planning and learning in Table 2.	The results denoted by planning report the performance of planning through the simulation, and the results denoted by learning report the performance without further simulation.	In order to understand the effectiveness of MC-LAVE as a policy improvement operator, we compare the performances of PUCT-RL and MC-LAVE-RL in Z ORK 1. Table 2 reports the intermediate results of planning and supervised learning in each iteration of the policy iteration. In each iteration, the policy and the Q-function are trained using planning trajectories and experience replay collected from 25 independent planning agents. As can be seen in Table 2, the performance of MC-LAVE-RL is improved more consistently than PUCT-RL, both in planning and learning. At the beginning of the policy iteration, PUCT-RL improves the performance, but it fails to overcome bottleneck and converges to a suboptimal policy: PUCT utilizes the prior policy learned by imitating the planning results of the previous iteration to estimate the exploration bonus, but this uncertainty-based method is not much effective to encourage the agent to explore the action space that is not sufﬁciently covered. On the other hand, MC-LAVE-RL not only uses the prior policy, but also uses Q-Network for credit assignment to language actions. This allows a more focused exploration on semantically promising actions and consequently overcomes the bottleneck to further improve the performance.				
575	paper_173	Did the authors have an experiment with training the state-of-the-art RL algorithm on the Jericho envrionment?	The authors compare the proposed method with state-of-the-art RL algorithm MC. Q*BERT on Jericho environment.	First, we compare the performance of MC-LAVE-RL with the following algorithms: (1) DRRN (Hausknecht et al., 2020), a variant of the DQN algorithm (Mnih et al., 2013) for natural lan- guage action space, (2) TDQN (Hausknecht et al., 2020), an extension of LSTM-DQN algorithm (Narasimhan et al., 2015) incorporating with template-based action generation, (3) KG-A2C (Am- manabrolu & Hausknecht, 2020), an actor-critic method with knowledge graph state representation, (4) MC!Q*BERT (Ammanabrolu et al., 2020), an extension of KG-A2C with BERT-based knowl- edge graph construction and knowledge-graph-based intrinsic reward. In addition, we also compare MC-LAVE-RL with our baseline called PUCT-RL, which uses PUCT as a policy improvement op- erator.				
576	paper_173	Which of the baseline algorithms are the planning-based RL algorithms?	PUCT-RL is a planning-based RL algorithm.	First, we compare the performance of MC-LAVE-RL with the following algorithms: (1) DRRN (Hausknecht et al., 2020), a variant of the DQN algorithm (Mnih et al., 2013) for natural lan- guage action space, (2) TDQN (Hausknecht et al., 2020), an extension of LSTM-DQN algorithm (Narasimhan et al., 2015) incorporating with template-based action generation, (3) KG-A2C (Am- manabrolu & Hausknecht, 2020), an actor-critic method with knowledge graph state representation, (4) MC!Q*BERT (Ammanabrolu et al., 2020), an extension of KG-A2C with BERT-based knowl- edge graph construction and knowledge-graph-based intrinsic reward. In addition, we also compare MC-LAVE-RL with our baseline called PUCT-RL, which uses PUCT as a policy improvement op- erator.				
577	paper_173	Which of the baseline algorithms require the resettable simulator?	Q*BERT and PUCT-RL require the resettable simulator assumption.	Table 1 summarizes handicaps leveraged in each algorithm and the performance of MC-LAVE-RL and baseline algorithms across 9 IF games included in the Jericho environment. The results show that MC-LAVE-RL outperforms or matches the state-of-the-art results on 8 out of 9 games. Although MC-LAVE-RL requires more handicap or assumption, it performs the same or better than strong baseline MC!Q*BERT which requires similar assumptions and more requirements. In addition, MC-LAVE-RL achieves higher game scores on overall games compared to PUCT-RL, which is a baseline algorithm that only excludes language-driven exploration strategy from MC-LAVE-RL. Furthermore, MC-LAVE-RL performs signiﬁcantly better than other methods on difﬁcult games such as Z ORK 1, D EEPHOME , and L UDICORP , which are categorized by Hausknecht et al. (2020) as a relatively challenging game due to the large action space and sparse rewards.				
578	paper_175	What is the hyperparameter of the proposed method? How did you tune it?	This is as it is.  We mostly follow the practice of Caron et al.	Training details We train our model on the training set of the ILSVRC-2012 ImageNet-1k dataset [18] without using class labels. We use the same data augmentation scheme (color jittering, Gaussian blur, and solarization) and multi-crop strategy (two 224 × 224 and six 96 × 96) used in Caron et al. [9]. We use a batch size of 4096 and employ the LARS optimizer [52] with a weight decay of 10−6. We use linearly scaled learning rate of lr × batch size/256 [27] with a base learning rate of 0.3. 5 We adjust the learning rate with 10 epochs of a linear warmup followed by cosine scheduling. We also use an exponential moving average (EMA) network by default.				
580	paper_175	Why should we care about the batch size in cost of performance on the unsupervised representation learning methods?	Curernt trend of self-supervised learning methods employ a large-scale dataset.  We care about batch size since this corresponds to speed of the method.	There has been a growing interest in using a large-scale dataset to build powerful machine learning models Radford et al. (2021).Self-supervised learning (SSL), which aims to learn a useful representation without labels, is suitable for this trend; it is actively studied in the fields of natural language processing Devlin et al. (2019); Du et al. (2021) and computer vision Chen et al. (2020a); He et al. (2020).In the vision domain, recent SSL methods commonly use data augmentations and induce their visual representation to be augmentation-invariant.They have achieved state-of-the-art performance surpassing supervised representation in a variety of visual tasks, including semi-supervised learning Caron et al. (2020); Zbontar et al. (2021), transfer learning Ericsson et al. (2021), and object detection Chen et al. (2020c).We apply MIRA to clustering-based representation learning and verify the representation quality on several standard self-supervised learning benchmarks.We demonstrate its state-of-the-art performance on linear/k-NN evaluation, semi-supervised learning, and transfer learning benchmark.We further experiment with convergence speed, scalability, and different components of our method.				
581	paper_175	Is it possible to say the proposed method learns better representation? What is the meaning of the downstream tasks?	SSL methods learn useful representation by solving pretext tasks without labels.  In P7, we can get a hint that this benchmarks are testbed for evaluating SSL methods.	There has been a growing interest in using a large-scale dataset to build powerful machine learning models Radford et al. (2021).Self-supervised learning (SSL), which aims to learn a useful representation without labels, is suitable for this trend; it is actively studied in the fields of natural language processing Devlin et al. (2019); Du et al. (2021) and computer vision Chen et al. (2020a); He et al. (2020).In the vision domain, recent SSL methods commonly use data augmentations and induce their visual representation to be augmentation-invariant.They have achieved state-of-the-art performance surpassing supervised representation in a variety of visual tasks, including semi-supervised learning Caron et al. (2020); Zbontar et al. (2021), transfer learning Ericsson et al. (2021), and object detection Chen et al. (2020c).We apply MIRA to clustering-based representation learning and verify the representation quality on several standard self-supervised learning benchmarks.We demonstrate its state-of-the-art performance on linear/k-NN evaluation, semi-supervised learning, and transfer learning benchmark.We further experiment with convergence speed, scalability, and different components of our method.SSL methods are designed to learn the representation by solving pretext tasks, and recent state-of-the-art methods encourage their learned representations to be augmentation invariant.They are based on various pretext tasks: instance discrimination Chen et al. (2020a, b, c, 2021), metric learning Grill et al. (2020); Chen and He (2021), self-training Zheng et al. (2021); Caron et al. (2021), and clustering Asano et al. (2020); Caron et al. (2018, 2020); only a few account for encoding the semantic structure of data.While some works Wang et al. (2020); Dwibedi et al. (2021); Koohpayegani et al. (2021) consider the nearest neighbors in the latent space, our method belongs to the clustering-based SSL method that flexibly accounts for inter-data similarity.Meanwhile, many SSL methods are prone to collapsing into a trivial solution where every representation is mapped into a constant vector.Various schemes and mechanisms are suggested to address this, e.g., the asymmetric structure, redundancy reduction, etc.We will review more relevant works in detail below.				
582	paper_175	Why is it adequate to say this problem is strictly convex?	We prove that hessian is a positive definite matrix.	We first prove the strict convexity of the optimization function f : R Lemma 1. For x ∈ R N×1+ , s(x) = P Ni xi log xiis a strictly convex function of x. Proof. Since the Hessian of s is a diagonal matrix with positive elements ∇2xs(x)i,i = 1/xi, s is astrictly convex function.				
583	paper_175	How does the proposed method address the issue of cluster collapse?	Mutual information regularizer unfavors collapsed representation.	The MI term in Eq. 4 takes a minimum value when collapsing happens.MIRA naturally avoids collapsed solution via penalizing assignment that exhibits low MI.Specifically, unless starting from the collapsed state, MIRA finds MI-maximizing points around the model prediction; it will not choose collapsed pseudo-labels.Hence, the iterative training to predict such labels will not collapse whenever the prediction of pseudo-labels is achievable.Our empirical results verify that MIRA does not require extra training techniques or artificial constraints to address collapsing.				
584	paper_175	How does the cluster-based method learn meaningful representation from scratch?	Clustering methods encourage the representations to encode the semantic structures of the data.  While this can be prone to collapse, they rely on extra techniques.	Meanwhile, a line of work uses clustering for un-/self-supervised representation learning.They explicitly assign pseudo-labels to embedded representation via clustering, and the model is thereby trained to predict such labels.These clustering-based methods can account for inter-data similarity; representations are encouraged to encode the semantic structure of data.Prior works Yang et al. (2016); Xie et al. (2016); Bautista et al. (2016); Hu et al. (2017) have shown encouraging results in small-scaled settings; Caron et al. (2018) show that it can also be applied to the large-scaled dataset or even to a non-curated dataset Caron et al. (2019).Recently, several works Asano et al. (2020); Caron et al. (2020); Li et al. (2021) have adopted the philosophy of augmentation invariance and achieved strong empirical results.They typically assign pseudo-labels using augmented views while predicting the labels by looking at other differently augmented views.Despite its conceptual simplicity, a naive application of clustering to representation learning is hard to achieve, especially when training with large-scale datasets.This is because clustering-based methods are prone to collapse, i.e., all samples are assigned to a single cluster; hence, recent methods heavily rely on extra training techniques or artificial constraints, such as pre-training Yan et al. (2020), sampling strategy Caron et al. (2018), equipartition constraints Asano et al. (2020); Caron et al. (2020), to avoid collapsing.However, it is unclear if these additions are appropriate or how such components will affect the representation quality.Many SSL approaches rely on extra training techniques and artificial assumptions to prevent collapsing.In clustering-based methods, DeepCluster Caron et al. (2018) adapts a sampling strategy to sample elements uniformly across pseudo-labels to deal with empty clusters; SeLa Asano et al. (2020) and SwAV Caron et al. (2020) impose equipartition constraints to balance the cluster distribution.Similarly, SelfClassifier Amrani et al. (2021) uses a uniform pseudo-label prior, and PCL Li et al. (2021) employs concentration scaling.DINO Caron et al. (2021) and ReSSL Zheng et al. (2021) address collapsing by specific combinations of implementation details, i.e., centering and scaling with an exponential moving average network; their mechanism for preventing collapse is unclear.In this work, we show our method can naturally avoid collapsing without any of these assumptions or training techniques.We achieve results better than baselines with a simple but novel information regularization algorithm.We take a more detailed comparison with SeLa and SwAV after explaining our method in Sec. 3.3.				
585	paper_175	Can clustering-based self-supervised approaches learn a piece of local information? If not, task applicability would be limited.	MIRA does not perform well in the detection task.	A.7 Experiments on the detection and segmentation task We test our method on detection segmentation of the COCO 2017 dataset with Masked R-CNN, R50-C4 on a 2x scheduled setting. We use the configuration from the MoCo official implementation. MIRA performs better than the supervised baseline and is comparable to MoCo; it is not as dominating as in the classification tasks.				
586	paper_175	How is MIRA similar or different compared to other clustering-based methods? (e.g. SwaV)	MIRA does not require any artificial constraints or techniques in training, unlike other self-supervised methods.  However, MIRA uses some of the techniques used in the other paper.	The pseudo-code of MIRA for representation learning with Eq. 8 is provided in the Appendix.In the following experiments, we verify the effectiveness of MIRA for a representation learning purpose.We note that MIRA can integrate recently suggested self-supervised learning components, such as exponential moving average (EMA) or multi-crop (MC) augmentation strategy following the baselines Chen et al. (2021); Caron et al. (2020, 2021).For convenience, in the rest of this paper, we call the representation learning with MIRA also as MIRA.We discuss some further details as follows:Despite its conceptual simplicity, a naive application of clustering to representation learning is hard to achieve, especially when training with large-scale datasets.This is because clustering-based methods are prone to collapse, i.e., all samples are assigned to a single cluster; hence, recent methods heavily rely on extra training techniques or artificial constraints, such as pre-training Yan et al. (2020), sampling strategy Caron et al. (2018), equipartition constraints Asano et al. (2020); Caron et al. (2020), to avoid collapsing.However, it is unclear if these additions are appropriate or how such components will affect the representation quality.SeLa Asano et al. (2020) and SwAV Caron et al. (2020) formulate their pseudo-labeling process into optimization problems, i.e., optimal transport (OT) problem, and solve it iteratively with Sinkhorn-Knopp (SK) algorithm Cuturi (2013).To avoid collapse and apply the SK algorithm, they assume the equipartition of data into clusters.Mathematically, the difference to MIRA is in how to deal with the marginal entropy.SeLa and SwAV constrain the marginal entropy to maximum value–equipartition while MIRA decides it by MI regularization333Adding the equipartition constraint into Eq. 4, our problem converts to the OT problem of SwAV Caron et al. (2020)..Asano et al. (2020) argue that their pseudo-labels with the OT problem maximize the MI between labels and data indices under the equipartition constraint.However, it more resembles assuming MI maximization and then finding the cluster assignments that are optimal transport to the model prediction.In contrast, MIRA directly maximizes the MI by regularization without artificial constraints.While SwAV performs better than SeLa in most self-supervised benchmarks, we verify that MIRA improves over SwAV in various downstream tasks.In this paper, we propose Mutual Information Regularized Assignment (MIRA), a pseudo-labeling algorithm that enables clustering-based SSL without any artificial constraints or extra training techniques.MIRA is designed to follow the infomax principle Linsker (1988) and the intuition that good labels are something that can reduce most of the uncertainty about the data.Our method assigns a pseudo-label in a principled way by constructing an optimization problem.For a given training model that predicts pseudo-labels, the optimization problem finds a solution that maximizes the mutual information (MI) between the pseudo-labels and data while considering the model probability.We formulate the problem as a convex optimization problem and derive the necessary and sufficient condition of solution with the Karush-Kuhn-Tucker (KKT) condition.This solution can be achieved by fixed-point iteration that we prove the convergence.We remark that MIRA does not require any form of extra training techniques or artificial constraints, e.g., equipartition constraints.Our contributions are summarized as follows:•We propose MIRA, a simple and principled pseudo-label assignment algorithm based on mutual information.Our method does not require extra training techniques or artificial constraints.•We apply MIRA to clustering-based representation learning, showing comparable performance against the state-of-the-art methods with half of the training epochs.Specifically, MIRA achieves 75.6% top-1 accuracy on ImageNet linear evaluation with only 400 epochs of training and the best performance in 9 out of 11 datasets in transfer learning.•Representation by MIRA also consistently improves over other information-based SSL methods.Especially our method without multi-crop augmentation achieves 74.1% top-1 accuracy and outperforms BarlowTwins Zbontar et al. (2021), a baseline information maximization-based self-supervised method.				
588	paper_175	What is mutual information means in the paper?	Mutual information between pseudo-label and data without any artificial constraints.	We argue that such pseudo-labels should maximize the mutual information (MI) between themselves and data while accounting for the model probabilities \bm{P}.Let \mathcal{B}\in\{1,...,B\} and \mathcal{Y}_{\bm{W}}\in\{1,...,K\} be the random variables associated with the data index in mini-batch and labels by probability distributions \bm{W}=\{\bm{w}_{i}\}_{i=1}^{B}, respectively.Our online pseudo-label (cluster) assignment is determined by solving the following optimization problem:\displaystyle\bm{W^{*}}\displaystyle=\operatorname*{arg\,min}_{\bm{W}\subset\Delta_{K}}\frac{1}{B}\sum_{i=1}^{B}D_{\text{KL}}(\bm{w}_{i},\bm{p}_{i})-\beta\hat{I}(\mathcal{Y}_{\bm{W}};\mathcal{B}),(1)where \Delta_{K}\coloneqq\{\bm{w}\in\mathbb{R}^{K}_{+}\mid\bm{w}^{\intercal}\bm{1}_{K}=1\}, \hat{I} indicates an empirical (Monte Carlo) estimates of MI, and \beta is a trade-off parameter.The problem consists of the (1) KL divergence term that makes pseudo-labels to be based on the model probability \bm{p} and (2) MI term between the pseudo-labels and data to induce more information about data into the pseudo-labels.By combining these two terms, we provide a refined pseudo-label that take account of both the model probability and MI.To make the optimization problem tractable, we substitute the MI term \hat{I} with the mini-batch estimates of the entropy \hat{H}(\mathcal{Y}_{\bm{W}}|\mathcal{B}) and marginal entropy \hat{H}(\mathcal{Y}_{\bm{W}}) in Eq. 2. We get:\displaystyle\hat{I}(\mathcal{Y}_{\bm{W}};\mathcal{B})=\hat{H}(\mathcal{Y}_{\bm{W}})-\hat{H}(\mathcal{Y}_{\bm{W}}|\mathcal{B})=-\sum_{j=1}^{K}\bar{w}_{j}\log{\bar{w}_{j}}+\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{K}w_{ij}\log{w_{ij}},(2)\displaystyle\frac{1}{B}\sum_{i=1}^{B}D_{\text{KL}}(\bm{w}_{i},\bm{p}_{i})=-\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{K}w_{ij}\log{p_{ij}}+\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{K}w_{ij}\log{w_{ij}},(3)\displaystyle\bm{W^{*}}=\operatorname*{arg\,min}_{\bm{W}\subset\Delta_{K}}-\frac{1}{B}\sum_{i=1}^{B}\sum_{j=1}^{K}w_{ij}\log{p_{ij}}+\frac{1-\beta}{B}\sum_{i=1}^{B}\sum_{j=1}^{K}w_{ij}\log{w_{ij}}+\beta\sum_{j=1}^{K}\overline{w}_{j}\log{\overline{w}_{j}},(4)where \overline{w}_{j}=\frac{1}{B}\sum_{i=1}^{B}w_{ij} is the marginal probability of a cluster j with \bm{W}.In practice, we find the optimal point \bm{W}^{*} of the optimization problem Eq. 4 for pseudo-labeling.SeLa Asano et al. (2020) and SwAV Caron et al. (2020) formulate their pseudo-labeling process into optimization problems, i.e., optimal transport (OT) problem, and solve it iteratively with Sinkhorn-Knopp (SK) algorithm Cuturi (2013).To avoid collapse and apply the SK algorithm, they assume the equipartition of data into clusters.Mathematically, the difference to MIRA is in how to deal with the marginal entropy.SeLa and SwAV constrain the marginal entropy to maximum value–equipartition while MIRA decides it by MI regularization333Adding the equipartition constraint into Eq. 4, our problem converts to the OT problem of SwAV Caron et al. (2020)..Asano et al. (2020) argue that their pseudo-labels with the OT problem maximize the MI between labels and data indices under the equipartition constraint.However, it more resembles assuming MI maximization and then finding the cluster assignments that are optimal transport to the model prediction.In contrast, MIRA directly maximizes the MI by regularization without artificial constraints.While SwAV performs better than SeLa in most self-supervised benchmarks, we verify that MIRA improves over SwAV in various downstream tasks.In this paper, we propose Mutual Information Regularized Assignment (MIRA), a pseudo-labeling algorithm that enables clustering-based SSL without any artificial constraints or extra training techniques.MIRA is designed to follow the infomax principle Linsker (1988) and the intuition that good labels are something that can reduce most of the uncertainty about the data.Our method assigns a pseudo-label in a principled way by constructing an optimization problem.For a given training model that predicts pseudo-labels, the optimization problem finds a solution that maximizes the mutual information (MI) between the pseudo-labels and data while considering the model probability.We formulate the problem as a convex optimization problem and derive the necessary and sufficient condition of solution with the Karush-Kuhn-Tucker (KKT) condition.This solution can be achieved by fixed-point iteration that we prove the convergence.We remark that MIRA does not require any form of extra training techniques or artificial constraints, e.g., equipartition constraints.				
590	paper_175	What are EMA and multi-crop strategies?	We may check on the reffered paper.	The pseudo-code of MIRA for representation learning with Eq. 8 is provided in the Appendix.In the following experiments, we verify the effectiveness of MIRA for a representation learning purpose.We note that MIRA can integrate recently suggested self-supervised learning components, such as exponential moving average (EMA) or multi-crop (MC) augmentation strategy following the baselines Chen et al. (2021); Caron et al. (2020, 2021).For convenience, in the rest of this paper, we call the representation learning with MIRA also as MIRA.We discuss some further details as follows:				
591	paper_175	How is MIRA different to TWIST fundamentally?	TWIST's direct optimization of MI through model parameters leads to the suboptimal solution while MIRA optimizes MI between pseudo-label and data without updating model parameters.	In this paper, we propose Mutual Information Regularized Assignment (MIRA), a pseudo-labeling algorithm that enables clustering-based SSL without any artificial constraints or extra training techniques.MIRA is designed to follow the infomax principle Linsker (1988) and the intuition that good labels are something that can reduce most of the uncertainty about the data.Our method assigns a pseudo-label in a principled way by constructing an optimization problem.For a given training model that predicts pseudo-labels, the optimization problem finds a solution that maximizes the mutual information (MI) between the pseudo-labels and data while considering the model probability.We formulate the problem as a convex optimization problem and derive the necessary and sufficient condition of solution with the Karush-Kuhn-Tucker (KKT) condition.This solution can be achieved by fixed-point iteration that we prove the convergence.We remark that MIRA does not require any form of extra training techniques or artificial constraints, e.g., equipartition constraints.Information maximization is a principal approach to learn representation and to avoid collapse.DeepInfoMax Hjelm et al. (2019) propose the MI maximization between the local and global views for representation learning; the existence of negative pairs prevents training toward the trivial solution.BarlowTwins Zbontar et al. (2021) and W-MSE Ermolov et al. (2021) address the collapsing with redundancy reduction that indirectly maximizes the content information of embedding vectors.Among clustering-based approaches, IIC Ji et al. (2019) maximizes the MI between the embedding codes to enable representation learning;similar to ours, TWIST Feng et al. (2021) proposes combining the MI between the data and class prediction as a negative loss term with an augmentation invariance consistency loss.Both IIC and TWIST use the MI as a loss function and directly optimize their model parameters with gradient descent of the loss.However, the direct optimization of MI terms by updating model parameters often leads to a sub-optimal solution Feng et al. (2021); TWIST copes with this issue by appending the normalization layer before softmax and introducing an additional self-labeling stage.In contrast, MIRA addresses the difficulty of MI maximization in a principled way via explicit optimization.				
592	paper_175	Why should we focus on the self-supervised method? For example, the limited label can make a tremendous gap against the self-supervised approaches.	Self-supervised learning methods perform well in semi-supervised learning, transfer learning, and object detection.	There has been a growing interest in using a large-scale dataset to build powerful machine learning models Radford et al. (2021).Self-supervised learning (SSL), which aims to learn a useful representation without labels, is suitable for this trend; it is actively studied in the fields of natural language processing Devlin et al. (2019); Du et al. (2021) and computer vision Chen et al. (2020a); He et al. (2020).In the vision domain, recent SSL methods commonly use data augmentations and induce their visual representation to be augmentation-invariant.They have achieved state-of-the-art performance surpassing supervised representation in a variety of visual tasks, including semi-supervised learning Caron et al. (2020); Zbontar et al. (2021), transfer learning Ericsson et al. (2021), and object detection Chen et al. (2020c).				
594	paper_175	Why localization, objection detection, and image segmentation downstream tasks are underwhelming?	Clustering-based methods relies on pseudo-labels on representation learning.  Therefore, our testbed is focused on classification-based benchmark.  Object detection is not our main interest.	Meanwhile, a line of work uses clustering for un-/self-supervised representation learning.They explicitly assign pseudo-labels to embedded representation via clustering, and the model is thereby trained to predict such labels.These clustering-based methods can account for inter-data similarity; representations are encouraged to encode the semantic structure of data.Prior works Yang et al. (2016); Xie et al. (2016); Bautista et al. (2016); Hu et al. (2017) have shown encouraging results in small-scaled settings; Caron et al. (2018) show that it can also be applied to the large-scaled dataset or even to a non-curated dataset Caron et al. (2019).Recently, several works Asano et al. (2020); Caron et al. (2020); Li et al. (2021) have adopted the philosophy of augmentation invariance and achieved strong empirical results.They typically assign pseudo-labels using augmented views while predicting the labels by looking at other differently augmented views.We apply MIRA to clustering-based representation learning and verify the representation quality on several standard self-supervised learning benchmarks.We demonstrate its state-of-the-art performance on linear/k-NN evaluation, semi-supervised learning, and transfer learning benchmark.We further experiment with convergence speed, scalability, and different components of our method.				
595	paper_175	What is the role of epoch in self-supervised learning?	Our method show better result in only half of the training.	Our contributions are summarized as follows:•We propose MIRA, a simple and principled pseudo-label assignment algorithm based on mutual information.Our method does not require extra training techniques or artificial constraints.•We apply MIRA to clustering-based representation learning, showing comparable performance against the state-of-the-art methods with half of the training epochs.Specifically, MIRA achieves 75.6% top-1 accuracy on ImageNet linear evaluation with only 400 epochs of training and the best performance in 9 out of 11 datasets in transfer learning.•Representation by MIRA also consistently improves over other information-based SSL methods.Especially our method without multi-crop augmentation achieves 74.1% top-1 accuracy and outperforms BarlowTwins Zbontar et al. (2021), a baseline information maximization-based self-supervised method.				
596	paper_18	Is GLUE a benchmark for BERT or corpus for BERT?	GLUE is the benchmark dataset for BERT.	The General Language Understanding Evaluation (GLUE) benchmark Wang et al. (2019b) is a collection of 9 datasets for evaluating natural language understanding systems.666The datasets are: CoLA Warstadt et al. (2018), Stanford Sentiment Treebank (SST) Socher et al. (2013), Microsoft Research Paragraph Corpus (MRPC) Dolan and Brockett (2005), Semantic Textual Similarity Benchmark (STS) Agirre et al. (2007), Quora Question Pairs (QQP) Iyer et al. (2016), Multi-Genre NLI (MNLI) Williams et al. (2018), Question NLI (QNLI) Rajpurkar et al. (2016), Recognizing Textual Entailment (RTE) Dagan et al. (2006); Bar-Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009) and Winograd NLI (WNLI) Levesque et al. (2011).Tasks are framed as either single-sentence classification or sentence-pair classification tasks.The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.				
599	paper_18	RoBERTa is based on BERT-large or BERT base?	RoBERTa is based on BERT-large.	To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERT{}_{\textsc{large}} architecture (L=24, H=1024, A=16, 355M parameters).We pretrain for 100K steps over a comparable BookCorpus plus Wikipedia dataset as was used in Devlin et al. (2019).We pretrain our model using 1024 V100 GPUs for approximately one day.We present our results in Table 5.In the first setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets.Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERT{}_{\textsc{large}}, yet consistently outperforms both BERT{}_{\textsc{large}} and XLNet{}_{\textsc{large}}.This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.				
600	paper_18	How NSP plays a role in BERT?	NSP helps to improve the ability of distinguishing  the observed document segments come from the same or distinct documents in BERT.	In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p=0.5) or from distinct documents.In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss.				
601	paper_18	Give two examples of public BERT-style english corpora.	CC-News and OpenWebText are BERT-style english corpora.	We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:•BookCorpus Zhu et al. (2015) plus English Wikipedia. This is the original data used to train BERT. (16GB).•CC-News, which we collected from the English portion of the CommonCrawl News dataset Nagel (2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).444We use news-please Hamborg et al. (2017) to collect and extract CC-News. CC-News is similar to the RealNews dataset described in Zellers et al. (2019).•OpenWebText Gokaslan and Cohen (2019), an open-source recreation of the WebText corpus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).555The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.•Stories, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB).				
602	paper_18	What is the difference between BERT paper and RoBERTa paper’s point of views? Give an answer in NSP loss and their performance perspective.	In BERT paper, author said that removing NSP can hurt the performance of the model.  However, in RoBERTa paper, author said that removing NSP improves downstream task performance.  Therefore, point of views in terms of NSP is different between BERT and RoBERTa.	The NSP loss was hypothesized to be an important factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1.However, some recent work has questioned the necessity of the NSP loss Lample and Conneau (2019); Yang et al. (2019); Joshi et al. (2019).We next compare training without the NSP loss and training with blocks of text from a single document (doc-sentences).We find that this setting outperforms the originally published BERT{}_{\textsc{base}} results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019).It is possible that the original BERT implementation may only have removed the loss term while still retaining the segment-pair input format.				
603	paper_18	In models inserting token expression, ([CLS],x1,...,xN,[SEP],y1,...,yM,[EOS]) calculate maximum value of N + M in RoBERTa case.	It is not true.  BERT takes concatenated two sequences as input like [\mathit{CLS}],x_{1},\ldots,x_{N},[\mathit{SEP}],y_{1},\ldots,y_{M}, They calculate N+M to control maximum sequence length.  However, RoBERTa takes four sequences as input not like BERT.	BERT takes as input a concatenation of two segments (sequences of tokens), x_{1},\ldots,x_{N} and y_{1},\ldots,y_{M}.Segments usually consist of more than one natural sentence.The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [\mathit{CLS}],x_{1},\ldots,x_{N},[\mathit{SEP}],y_{1},\ldots,y_{M},[\mathit{EOS}].M and N are constrained such that M+N<T, where T is a parameter that controls the maximum sequence length during training.We modify RoBERTa for this task by concatenating each candidate answer with the corresponding question and passage.We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer.We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.				
604	paper_18	In models inserting token expression "([CLS],x1,...,xN,[SEP],y1,...,yM,[EOS])", calculate maximum value of N + M in RoBERTa case.	It is not true.  BERT takes concatenated two sequences as input like [\mathit{CLS}],x_{1},\ldots,x_{N},[\mathit{SEP}],y_{1},\ldots,y_{M}, They calculate N+M to control maximum sequence length.  However, RoBERTa takes four sequences as input not like BERT.	BERT takes as input a concatenation of two segments (sequences of tokens), x_{1},\ldots,x_{N} and y_{1},\ldots,y_{M}.Segments usually consist of more than one natural sentence.The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [\mathit{CLS}],x_{1},\ldots,x_{N},[\mathit{SEP}],y_{1},\ldots,y_{M},[\mathit{EOS}].M and N are constrained such that M+N<T, where T is a parameter that controls the maximum sequence length during training.We modify RoBERTa for this task by concatenating each candidate answer with the corresponding question and passage.We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer.We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.				
605	paper_18	How many tokens are changed to [MASK] in BERT training? Give a ratio.	80% of tokens are replaced with [MASK] during training.	A random sample of the tokens in the input sequence is selected and replaced with the special token [\mathit{MASK}]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [\mathit{MASK}], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token.				
606	paper_18	RoBERTa uses large batch size. How many times larger than BERT-large one?	RoBERTa use 32 times larger batch size than BERT because batch size of BERT and RoBERTa are 256 and 8K, respectively.	Devlin et al. (2019) originally trained BERT{}_{\textsc{base}} for 1M steps with a batch size of 256 sequences.This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K.In Table 3 we compare perplexity and end-task performance of BERT{}_{\textsc{base}} as we increase the batch size, controlling for the number of passes through the training data.We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.Large batches are also easier to parallelize via distributed data parallel training,888Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in fairseq Ott et al. (2019). and in later experiments we train with batches of 8K sequences.				
607	paper_18	Why RoBERTa uses Dynamic masking rather than Static masking?	They use dynamic masking to avoid using the same mask in iteration.	As discussed in Section 2, BERT relies on randomly masking and predicting tokens.The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask.To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training.Thus, each training sequence was seen with the same mask four times during training.				
608	paper_18	Why author said that “the data used for pretraining” have been under-emphesized? Give an evidence data on Table 4.	Author said that “the data used for pretraining” have been under-emphesized.  Because they improved model performance by using additional training data for pretraining.	In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-News, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch Paszke et al. (2017).Next, we combine this data with the three additional datasets described in Section 3.2.We train RoBERTa over the combined data with the same number of training steps as before (100K).In total, we pretrain over 160GB of text.We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.999Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work.				
609	paper_18	Why author said that they adopt a much simpler approach for SQuAD compared to past work?	Author said that they adopt a much simpler approach for SQuAD compared to past work to emphasize that they only finetune RoBERTa using the SQuAD training data, and they use the same learning rate for all layers, not like previous works.	We adopt a much simpler approach for SQuAD compared to past work.In particular, while both BERT Devlin et al. (2019) and XLNet Yang et al. (2019) augment their training data with additional QA datasets, we only finetune RoBERTa using the provided SQuAD training data.Yang et al. (2019) also employed a custom layer-wise learning rate schedule to finetune XLNet, while we use the same learning rate for all layers.				
610	paper_18	How can author said that their results illustrate the importance of previously overlooked design decisions on BERT?	They could say that the importance of previously overlooked design decisions on BERT, because they improved the performance significantly by training the model longer, with bigger batches over more data.  removing the next sentence prediction objective.  training on longer sequences.  and dynamically changing the masking pattern applied to the training data.	We carefully evaluate a number of design decisions when pretraining BERT models.We find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data.Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD.These results illustrate the importance of these previously overlooked design decisions and suggest that BERT’s pretraining objective remains competitive with recently proposed alternatives.				
611	paper_18	Why author said that it can be challenging to determine which aspects of the methods contribute the most?	It is challenging to determine which aspects of the methods contribute the most since training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.	Self-training methods such as ELMo Peters et al. (2018), GPT Radford et al. (2018), BERT Devlin et al. (2019), XLM Lample and Conneau (2019), and XLNet Yang et al. (2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.				
612	paper_18	Explain the author’s motivation to make CC-News dataset.	The motivation of making CC-News dataset is most additional datasets in previous works are not available.	BERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT Radford et al. (2019); Yang et al. (2019); Zellers et al. (2019).Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.				
614	paper_18	According to the paper, does BERT imporved by several other papers?	Baevski et al.  (2019) prove that increasing training data size can improve performance.  Moreover, (2019).  Yang et al.  (2019).  Zellers et al.  (2019) train BERT with larger and more diverse dataset than original BERT.	BERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT Radford et al. (2019); Yang et al. (2019); Zellers et al. (2019).Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.Notably You et al. (2019) train BERT with even larger batche sizes, up to 32K sequences.We leave further exploration of the limits of large batch training to future work.				
616	paper_180	Why does zero-shot evaluation has been suggested as a genuine measure for reasoning capability?	It is hard to measure reasoning capability using individual datasets because the model cannot learn how to perform general semantic reasoning.	The ability to understand natural language through commonsense reasoning is one of the core focuses in the field of natural language processing. To measure and study the different aspects of commonsense reasoning, several datasets are developed, such as SocialIQA (Sap et al., 2019b), CommonsenseQA (Talmor et al., 2018), and PhysicalIQA (Bisk et al., 2020), each requiring different type of commonsense knowledge (e.g., social, taxonomic, causal, declarative, etc) to select the correct answer. While large-scale neural systems (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019b) have shown human-level accuracy on these benchmarks, recent studies (Mitra et al., 2019) also criticize that these models solve individual datasets, rather than learning how to perform general semantic reasoning. To this end, Ma et al. (2021) suggested zero-shot evaluation as a genuine measure for the reasoning capability of the machine.				
617	paper_180	What is AdapterFusion?	AdapterFusion is one of the multi-task learning method based on attention-like mechanism.  It aggregates pre-trained adapters in a non-destructive manner mitigating catastrophic forgetting and interference between tasks.	To address these limitations, we propose a novel, modularized framework that aims to learn multiple expert models for KGs, then conduct zero-shot fusion to allow collaboration among KGs. For this purpose, we leverage AdapterFusion (Pfeiffer et al., 2021) where multiple tiny modules between Transformer blocks called adapters (Houlsby et al., 2019) can be combined after independent training, thus allowing a continual integration of the adapters without retraining the entire framework. Specifically, we treat the adapters as different KG-specific experts, and combine them using an attention-like fusion module. To improve the fusion of adapters, we suggest a KG-alignment adapter that guides to the apt expert adapters. Here, we use KGs in three different synthetic supervision training: (1) KG-specific QA datasets to train the KG-specific expert adapters, (2) a KG classification datasets to train the KG-alignment adapter, and (3) a balanced mixture of KG-specific QA datasets to train the fusion module. Our modularized method alleviates the interference between different KGs, which is the pitfall of MTL from our empirical observation, and thus combines multiple KGs into a synergetic zero-shot framework.To address this problem, AdapterFusion (Pfeiffer et al., 2021) has been proposed to fuse task specific parameters called adapters for the given target task leveraging attention-like mechanism. AdapterFusion aggregates adapters, which is trained independently for each task, in a non-destructive manner mitigating aforementioned MTL problems such as forgetting and interference between tasks. Recently, it has been used for zero-shot cross-lingual transfer framework (Pfeiffer et al., 2020c; Wang et al., 2021b), which motivates our work to transfer multi-source knowledge with less interference for zero-shot commonsense reasoning.				
619	paper_180	What is the main weak point of conventional Multi-task learning for zero-shot learning with multiple types of commonsense knowledge?	Conventional Multi-Task Learning (MTL) is known to be prone to interference between various tasks, as well as a phenomenon known as catastrophic forgetting, wherein the model struggles to retain knowledge of different types acquired during MTL.	To consider different types of reasoning, this paper extends ideas from the aforementioned zero-shot learning to the multi-source case such that it benefits from different types of commonsense knowledge on individual KGs. For example, ATOMIC (Sap et al., 2019a) focuses on social commonsense while ConceptNet (Speer et al., 2017) contains conceptual knowledge. A practical approach is multi-task learning (MTL; Caruana, 1997; Liu et al., 2019a), which learns a shared encoder for different synthetic QA datasets from multiple KGs. Despite its effectiveness, MTL scheme suffers from interference among different KGs, which results in forgetting previously learned knowledge when trained on new KG which has different kinds of knowledge (Pilault et al., 2021; Pfeiffer et al., 2021; Wang et al., 2021a; Wu et al., 2020).MTL (Liu et al., 2019a; Zhang and Yang, 2017; Caruana, 1997) learns a shared representation while aggregating knowledge across multiple learning tasks, often leading to better generalization ability of a model. However, parametric aggregation of knowledge with MTL has following limitations: (1) retraining the full model when adding new tasks (Houlsby et al., 2019; Pfeiffer et al., 2021, 2020b) (2) catastrophic forgetting and interference between tasks leading to difficulties of solving each task equally well (Pilault et al., 2021; Wu et al., 2020; Yu et al., 2020) and (3) inconsistent effect (Lourie et al., 2021). To deal with these challenges, Mixture-of-Experts (MoE) is a parameterized generalization of ensembling techniques, which has been adapted for MTL with gating network trained to optimize each task (Ma et al., 2018). However, simple linear gating networks are too shallow and thus may destruct task knowledge for commonsense reasoning.				
620	paper_180	Why is KG Modularization needed?	KG modularization is crucial for maintaining the intrinsic knowledge of each individual KG.  As the selection and alignment of an appropriate KG has been shown to have a significant impact on downstream tasks, it is important that the model is able to learn the subtle differences between each KG without any interference from other KGs.	First, we modularize the KGs to preserve their intrinsic knowledge. Considering the importance of using a suitable and well-aligned KG (Ma et al., 2019, 2021) on a downstream task, the subtle difference between each KG should be learned by the model without any interference from each other. Accordingly, we adopt the adapter module (Houlsby et al., 2019) which repurposes a pre- trained language model (PLM) to incorporate each KG as tiny modules in between Transformer blocks. Specifically, as illustrated in Figure 2 (except for green area), the adapter training strategy involves injecting new layers (parameterized by Φ) into the original PLM (parameterized by θ). The weights of the original PLM are untouched, while the new adapter layers are initialized at random. Formally, we call each adapter trained with DkQA as an expert adapter for KG k, parameterized by ΦkQA.				
621	paper_180	What are the advantages of KG modularization using adapters?	Adapter enables to store the corresponding knowledge separately without any interference.  We can parallelize the training of the adapter for all KGs.  The efficiency of adapter training allows our modularization to be more scalable.	where KG-invariant parameters θ are fixed and only KG-dependent parameters Φk QA are learned, which enables to store the corresponding knowledge separately without any interference. Further, we can parallelize the training of the adapter for all KGs. The efficiency of adapter training allows our modularization to be more scalable.				
622	paper_180	How does the author show the mitigation of interference?	Use interference ratio.	Using the interference ratio, we can precisely compare the negative effects of multi-KG models on knowledge aggregation since the only reason to get the correct samples wrong is the interference caused by learning with additional KGs. We present the interference ratio of the models on five benchmark datasets in Figure 5. This figure shows that MTL has the higher interference ratio than the competing models across all benchmarks. Our method achieves a substantially better ratio, especially when KG-C adapter is used. This demonstrates the efficacy of our framework in mitigating interference between knowledge, which is one of the major problems of MTL.				
623	paper_180	What is the difference between zero-shot fusion and original AdapterFusion?	In contrast to AdapterFusion where the focus is learning to transfer knowledge to a specific target task, our zero-shot fusion aims to generalize this transfer to any arbitrary target task.	Once the expert adapters are learned, we combine the knowledge from each expert adapter using an attention-like mechanism. We present a novel fusion strategy as shown in Figure 2, which is referred to as the zero-shot fusion. In contrast to AdapterFusion (Pfeiffer et al., 2021) where the focus islearning to transfer knowledge to a specific targettask, our zero-shot fusion aims to generalize this transfer to any arbitrary target task. Specifically, the zero-shot fusion parameters Ψ learn to combine fixed expert adapters which are parameterized by Φ_1 QA, ..., Φ K QA. In each Transformer layer l of PLM with the injected fusion layer, the zero-shot fusion parameters ΨQA consist of query, key, and value matrices, denoted by WQ_l, WK_l, and WV_l respectively. These parameters are used to learn the balancing between the representation of each expert adapters through attention-like mechanism. While fixing both the parameters θ and all expert adapters Φ_1 QA, ..., Φ_K QA, the only trainable weights ΨQA on the fusion layer learns to combine the knowledge from different K expert adapters by using the subset of {Dk QA} K k=1 by random sampling. Here, we balance the ratio between the K knowledge-driven datasets as N samples (details are in Appendix D).				
624	paper_180	Which dataset is used for fusion layer training?	a balanced mixture of KG-specific QA datasets to train the fusion module.	To address these limitations, we propose a novel, modularized framework that aims to learn multiple expert models for KGs, then conduct zero-shot fusion to allow collaboration among KGs. For this purpose, we leverage AdapterFusion (Pfeiffer et al., 2021) where multiple tiny modules between Transformer blocks called adapters (Houlsby et al., 2019) can be combined after independent training, thus allowing a continual integration of the adapters without retraining the entire framework. Specifically, we treat the adapters as different KG-specific experts, and combine them using an attention-like fusion module. To improve the fusion of adapters, we suggest a KG-alignment adapter that guides to the apt expert adapters. Here, we use KGs in three different synthetic supervision training: (1) KG-specific QA datasets to train the KG-specific expert adapters, (2) a KG classification datasets to train the KG-alignment adapter, and (3) a balanced mixture of KG-specific QA datasets to train the fusion module. Our modularized method alleviates the interference between different KGs, which is the pitfall of MTL from our empirical observation, and thus combines multiple KGs into a synergetic zero-shot framework.				
625	paper_180	Why is KG-Classifier adapter suggested?	To compensate that usage a mixture of synthetic QA for fusion training, which is not exactly a training task.	AdapterFusion uses the PLM hidden representation h^l_P LM as a query which is learned when training on a specific downstream task. In our zero-shot setting, however, we use a mixture of synthetic QA for fusion training, which is not exactly a training				
626	paper_180	Which dataset is used for KG-Classifier adapter training?	For KG-Classifier adapter training, KG classification dataset has been used.	To address these limitations, we propose a novel, modularized framework that aims to learn multiple expert models for KGs, then conduct zero-shot fusion to allow collaboration among KGs. For this purpose, we leverage AdapterFusion (Pfeiffer et al., 2021) where multiple tiny modules between Transformer blocks called adapters (Houlsby et al., 2019) can be combined after independent training, thus allowing a continual integration of the adapters without retraining the entire framework. Specifically, we treat the adapters as different KG-specific experts, and combine them using an attention-like fusion module. To improve the fusion of adapters, we suggest a KG-alignment adapter that guides to the apt expert adapters. Here, we use KGs in three different synthetic supervision training: (1) KG-specific QA datasets to train the KG-specific expert adapters, (2) a KG classification datasets to train the KG-alignment adapter, and (3) a balanced mixture of KG-specific QA datasets to train the fusion module. Our modularized method alleviates the interference between different KGs, which is the pitfall of MTL from our empirical observation, and thus combines multiple KGs into a synergetic zero-shot framework.Paragraph10 : Specifically, we propose a novel training task for KG-Classifier adapter, which requires predicting the KG for the given sample of the task. For that, given {Dk_QA} K k=1, we first transform a QA sample (Qi, Ai) into a new KG classification sample [Qi; Ai,label] where [; ] is the concatenation. Then, we obtain a new label yi ∈ {0, 1} K indicating the corresponding KG source. The samples are in Appendix E. Formally, KG classification dataset DKGC is defined as: DKGC = {([Qi ; Ai,label], yi)} M_i=1 (9) where M is the total size of {Dk QA} K k=1.				
627	paper_180	Which benchmark has been used for evaluation?	, 2019b), CommonsenseQA (CSQA) (Talmor et al. , 2018), Abductive NLI (a-NLI) (Bhagavatula et al. , 2020), PhysicalIQA (PIQA) (Bisk et al. , 2020), and WinoGrande (WG) (Sakaguchi et al. , 2020).	We evaluate our proposed framework on five question-answering benchmarks for commonsense				
628	paper_180	What does STL stand for?	Single-Task Learning (STL): The model is pre-trained on a synthetic QA dataset generated from a single KG.	Single-Task Learning (STL): The model is pre-trained on a synthetic QA dataset generated from a single KG. Specifically, we experiment two architectural choices: PLM (STLPLM) and PLM with adapters (STL-Adapter). For each architecture, there are four STL models for each of synthetic QA datasets derived from ATOMIC, ConceptNet, WikiData, and WordNet. We note that the trained STLAdapter is an expert adapter from a specific KG in our framework. The performance of each STL baseline is shown in Appendix I Table 9 and Table 10.				
629	paper_180	What is the difference in test results according to the presence or absence of adapters?	KG-C adapter improves the average accuracy of zero-shot fusion by 0.	Moreover, as an ablation, we compare the zeroshot fusion with and without KG-C adapter to explore the efficacy of the KG-C adapter. We can observe that zero-shot fusion with KG-C adapter improves the average accuracy by 0.4%, which implies that the use of KG-C adapter improves the overall performance and makes our method generalize better on most of the evaluation benchmarks.				
630	paper_180	How does KG-Classifier affect zero-shot fusion?	zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely.	Further, we explore how the KG-C adapter affects zero-shot fusion which is based on an attention-like mechanism (Pfeiffer et al., 2021) compared to zero-shot fusion without KG-C adapter. Here, while zero-shot fusion without KGC adapter simply uses the representation of PLM as a query, zero-shot fusion with KG-C adapter leverages the representation of KG-C adapter. To illustrate this strength, we visualize the attention probability of [CLS] token from each fusion layer as a representative in Figure 4. The column of the darker cell indicates the adapter that has the bigger influence on the fused representation. We can observe that zero-shot fusion with KG-C adapter fuses the knowledge from different experts with a subtle difference rather than focusing on a single expert severely. This implies that KG-C adapter enables the delicate balancing between multiple knowledge sources based on the KG-alignment awareness, which leads to performance improvements in commonsense reasoning tasks. Interestingly, both cases have the ability not to focus on the expert adapter based on WikiData, which can be seen as a redundant expert.4 This observation would benefit from the further study that explores the optimal combination of KGs by expert selection or rejection.				
631	paper_180	What is the correlation between the number of KGs and the performance when using zero-shot fusion?	Zero-shot fusion obtains relative performance improvement across most of benchmark when more KGs are utilized for training.	In Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks. In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA. Except for the above case, we can observe that as more KGs are leveraged, the color of the cell gets greener, which implies that our method gains more advantages for better performance. This demonstrates that our method enables knowledge aggregation for multiple KGs synergetically.				
632	paper_180	Why is neural integration of different KGs better than symbolic KG integration?	Rather than such symbolic KG integration with the inevitable loss of knowledge, in this work, we explore the neural KG integration leveraging the multiple KGs without additional processing and alignment information between KG and task.	However, most of existing work are either assuming the existence of the alignment information between tasks and KGs (Banerjee and Baral, 2020) or an integrated KG (Ma et al., 2021). For example, \texttt{ATOMIC}^{20}_{20} (Hwang et al., 2021), a commonsense KG which incorporates tuples from ConceptNet and ATOMIC with new relations and further crowdsourcing, combines multiple KGs into a new integrated KG, but as widely known (Ilievski et al., 2020; Hwang et al., 2021), heterogeneous schema between different KGs may limit triplets that can be integrated.111Only 172K tuples of the 3.4M tuples and 5 relations of 36 relations in ConceptNet are integrated into \texttt{ATOMIC}^{20}_{20}. Rather than such symbolic KG integration with the inevitable loss of knowledge, in this work, we explore the neural KG integration leveraging the multiple KGs without additional processing and alignment information between KG and task.				
634	paper_180	How does the author convert the triplet in KG into synthetic QA specifically?	given a triple (e^{head},r,e^{tail}) in a KG, where e^{head}, e^{tail} and r denote head/tail entity and relation respectively, we transform e^{head} and r into a natural language question Q_{i} using templates.	In our setup, we repurpose synthetic QA generation (Ma et al., 2021) for the task of knowledge-driven zero-shot learning for commonsense reasoning, i.e., we transform a KG into multiple (Q_{i},A_{i}) pairs where Q_{i} is a natural language question and A_{i}=\{A_{i,1},...,A_{i,m}\} is the set of options with m answer candidates. Specifically, given a triple (e^{head},r,e^{tail}) in a KG, where e^{head}, e^{tail} and r denote head/tail entity and relation respectively, we transform e^{head} and r into a natural language question Q_{i} using templates. For the option set A_{i}, we use the combination of the correct answer e^{tail} and m-1 distractors which are tail entities from other triples sampled randomly (Ma et al., 2021). Details are described in Appendix B.				
635	paper_180	What does MTL stand for?	Conventionally, MTL stands for multiple learning tasks.  Here, for experiment, the author call the model pre-trained on multiple synthetic QA datasets as MTL.	MTL (Liu et al., 2019a; Zhang and Yang, 2017; Caruana, 1997) learns a shared representation while aggregating knowledge across multiple learning tasks, often leading to better generalization ability of a model. However, parametric aggregation of knowledge with MTL has following limitations: (1) retraining the full model when adding new tasks (Houlsby et al., 2019; Pfeiffer et al., 2021, 2020b) (2) catastrophic forgetting and interference between tasks leading to difficulties of solving each task equally well (Pilault et al., 2021; Wu et al., 2020; Yu et al., 2020) and (3) inconsistent effect (Lourie et al., 2021). To deal with these challenges, Mixture-of-Experts (MoE) is a parameterized generalization of ensembling techniques, which has been adapted for MTL with gating network trained to optimize each task (Ma et al., 2018). However, simple linear gating networks are too shallow and thus may destruct task knowledge for commonsense reasoning.Paragraph10 : Multi-Task Learning (MTL): The model is pre-trained on multiple synthetic QA datasets, each of which is generated from a KG. We experiment with a PLM trained on all four aforementioned synthetic QA datasets. We note that the difference between STL-PLM and MTL is whether to use one synthetic QA dataset or multiple synthetic QA datasets for its training.				
636	paper_180	What can be the future work related to this paper?	In the future, our work can be extended to adapt our methods to further various multiple KGs with studies of appropriate scale for KG modularization.  In addition, based on our hypothesis that the existence of an optimal combination, we can explore the study for the optional use of modularized KG experts for the best transfer learning.	In the future, our work can be extended to adapt our methods to further various multiple KGs with studies of appropriate scale for KG modularization. In addition, based on our hypothesis that the existence of an optimal combination, we can explore the study for the optional use of modularized KG experts for the best transfer learning.				
637	paper_180	Why is there decrease of the performance of the zeor-shot fusion without ATOMIC?	In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA.	In Figure 6, while the MTL tends to show the decrease of the performance when more KGs are utilized for training, our method obtains relative performance improvement across most of benchmarks. In both framework, the slightly degraded performance of the combination of KGs without ATOMIC could be due to the strong alignment between ATOMIC and SIQA. Except for the above case, we can observe that as more KGs are leveraged, the color of the cell gets greener, which implies that our method gains more advantages for better performance. This demonstrates that our method enables knowledge aggregation for multiple KGs synergetically.				
638	paper_180	What is the main motivation of this work?	This work has been motivated from the fact that real-world QA systems require simultaneously considering different types of reasoning abilities.  Therefore, this paper target an arbitrary commonsense reasoning task where conventional approaches are not applicable to such zero-shot learning scenarios.	Inspired by this new metric, in this work, we focus on building unsupervised zero-shot multiple-choice QA systems. That is, we target an arbitrary commonsense reasoning task where conventional approaches (that rely heavily on task-specific supervision) are not applicable to such zero-shot learning scenarios. To learn QA models without expensive annotation efforts, recent works (Ma et al., 2021; Banerjee and Baral, 2020; Malaviya et al., 2020) propose to generate a synthetic QA dataset using a commonsense KG such as ATOMIC (Sap et al., 2019a) and ConceptNet (Speer et al., 2017). Such an approach mostly focuses only on one specific type of reasoning relations (e.g., if-then relation, or declarative relation), neglecting the fact that real-world QA systems require simultaneously considering different types of reasoning abilities (e.g., declarative and social, or causal and physical reasoning; Ilievski et al., 2021; Chang et al., 2021).				
641	paper_180	What are the examples of the Synthetic QA?	Q: Dana speeds on the highway.	QA from ATOMIC (Sap et al., 2019a)				
642	paper_182	How does the authors claim that the proposed method could improve the accuracy-latency tradeoff over existing SoTA CNN models?	They compared their network with state-of-the-art models in Table 6.  Table 6 shows that the baseline model achieved higher accuracy than comparisons with similar latency.	We apply the proposed NAS method with the supernet architecture described above. The depth of 5 stages is set to 3,4,7,4,11, respectively. The latency constraint is set to 2.5 ms that corresponds to the latency of EfficientNet-B1 on our target NPU, MIDAP. Table 6 compares our search results with the state-of-the-art models: EdgeTPU (Gupta and Akin, 2020), EfficientNet (Tan and Le, 2019a), Once-For-All (Cai et al., 2019). The latency of the other models is obtained by running the network on the MIDAP cycle-accurate simulator. We compare the accuracy without quantization, assuming that quantization effects will be similar to all models.As shown in Table 6, the baseline model, ours-M, found by the proposed NAS technique has higher accuracy than the other models on our target NPU; ours-M achieves more than 1.7% higher top-1 accuracy than EfficientNet-lite2 with similar latency. Moreover, it is 0.5% higher than EfficientNet-B1, even without using SE and h-swish activation function. Note that the number of parameters and the number of FLOPS in ours-M is larger than EfficientNet-B1. It implies that the complexity of the network is not a direct indicator of the end-to-end latency of the network. The end-to-end latency depends on the NPU architecture, and the proposed NAS technique could find a larger network with shorter latency by adding the latency factor to the loss function directly. The main benefit comes from different block assignment to stages.Finally, we selectively removed SE blocks from ours-XL+, resulting in ours-XL-rmSE+. We collected the activation values using randomly sampled 10K images from the training dataset and calculated the metric explained in Sec. 4.3.3.After removing SE blocks from ours-XL+ based on the metric, only about 60% of the blocks in the network have SE blocks. As a result, we could make the latency shorter, while the accuracy was slightly improved than ours-XL+. This model achieves 82.72% top-1 accuracy with only 11.66ms latency.It is much better than EfficientNet-EdgeTPU-L (Gupta and Akin, 2020) that achieves 80.62% FP32 top-1 accuracy with more than 20ms on EdgeTPU. Our architecture on MIDAP is about 2 times faster with 2.1% higher accuracy.In this work, we propose a fast NPU-aware NAS methodology extending the Single-Path NAS technique (Stamoulis et al., 2019). We modify the supernet architecture by varying the number of blocks in stages and adding mixed depthwise convolution (Tan and Le, 2019b) to the search space. By modifying the loss function to directly include the target latency estimated by a cycle-accurate simulator of the target NPU, we could find a better baseline architecture with a shorter latency than the latency constraint. Using a tight latency constraint, we can reduce the search space to find the baseline network fast. Afterward, we apply compound scaling to find a larger network than the baseline network, and add SE blocks and h-swish activation functions in the post-processing step.Through the proposed NAS methodology, we could obtain a network with 82.72% accuracy with 11.66ms latency on our target NPU, without special data augmentation in training. It dominates the existing network models on the target NPU. It confirms the importance of supernet architecture design for a given NPU and effectiveness of the three-step approach in the proposed NAS methodology: supernet design, SinglePath NAS with a tighter latency constraint, and compound scaling and post-processing.Experiments show that the proposed NAS technique could improve the accuracy-latency tradeoff over existing SoTA CNN models. Our best model achieves 82.72% top-1 accuracy on ImageNet with 11.66ms latency without any special data augmentation. Note that the latency is estimated by cycle-accurate simulation. For a fair comparison with the related work, the latency of each compared network is also estimated with the same simulator.				
643	paper_182	What is the contribution of this paper?	They modify the supernet architecture by varying the number of blocks in stages, and adds MixConv to the search space.  This enables more diverse combinations of kernel sizes and expansion ratios than original MixConv.  Moreover, they eases the search process.  As a result, they could find a better network than existing network models.  Note that their method can be used to any type of NPU.	Even though the proposed methodology can be applied to any type of NPU, the current implementation is made for an adder-tree type NPU, called MIDAP (Kanget al., 2019).It has a fully-pipelined micro-architecture that consists of separate hardware modules and memory modules for convolution, activation function, and various reduction operations. Since it enables us to make a fully static schedule of operations without resource contention in the data path, we can estimate the end-to-end latency of a CNN quite accurately analytically. Unexpected delay may incur from off-chip DRAM delay that is not fully hidden by double buffering.Figure 6 depicts our building block structure. This block starts and ends with 1×1 convolution, with N searchable superkernels in the middle. Each searchable superkernel is designed similarly to Eq. (3), while we may use different threshold values in each superkernel. The kernel sizes and expansion ratios are selected among predetermined values. If the j-th searchable superkernel chooses an expansion ratio e_{j}, the j-th kernel has e_{j} times more channels than the first 1×1 convolution. Compared with the original MixConv suggested in (Tan and Le, 2019b), the proposed building block supports more diverse combinations of kernel sizes and expansion ratios. It enhances the efficiency of search results on our target NPU (Table 5).We propose to modify the loss function to activate the latency-aware loss term only when the estimated latency is larger than the latency constraint as follows:(9)CE+\lambda_{1}\cdot log(1+\lambda_{2}\cdot ReLU((\sum L)-T))Although this is not a panacea, this modification significantly eases the search process, which will be discussed in section 5.2 with various experiments.Figure 11 depicts an example distribution of activation values produced by two different SE blocks for three different images. The authors of the original paper (Hu et al., 2018) conjectured that if such distribution from a SE block does not differ widely between image classes, the SE block is not important. Thus, after training, they obtained averaged activation values of a SE block over multiple images in the same class.They compared the distributions of the averaged values over different image classes. They observed that removing the SE blocks that have similar distributions over different image classes incurs only a marginal loss in accuracy.In this work, we propose a fast NPU-aware NAS methodology extending the Single-Path NAS technique (Stamoulis et al., 2019). We modify the supernet architecture by varying the number of blocks in stages and adding mixed depthwise convolution (Tan and Le, 2019b) to the search space. By modifying the loss function to directly include the target latency estimated by a cycle-accurate simulator of the target NPU, we could find a better baseline architecture with a shorter latency than the latency constraint. Using a tight latency constraint, we can reduce the search space to find the baseline network fast. Afterward, we apply compound scaling to find a larger network than the baseline network, and add SE blocks and h-swish activation functions in the post-processing step.Through the proposed NAS methodology, we could obtain a network with 82.72% accuracy with 11.66ms latency on our target NPU, without special data augmentation in training. It dominates the existing network models on the target NPU. It confirms the importance of supernet architecture design for a given NPU and effectiveness of the three-step approach in the proposed NAS methodology: supernet design, SinglePath NAS with a tighter latency constraint, and compound scaling and post-processing.				
644	paper_182	Why focusing on latency-aware NAS is important?	Other criteria, such as complexity of the network or the number of MAC operations, is not a proper measure of latency.  Thus targeting on latency is important.	As shown in Table 6, the baseline model, ours-M, found by the proposed NAS technique has higher accuracy than the other models on our target NPU; ours-M achieves more than 1.7% higher top-1 accuracy than EfficientNet-lite2 with similar latency. Moreover, it is 0.5% higher than EfficientNet-B1, even without using SE and h-swish activation function. Note that the number of parameters and the number of FLOPS in ours-M is larger than EfficientNet-B1. It implies that the complexity of the network is not a direct indicator of the end-to-end latency of the network. The end-to-end latency depends on the NPU architecture, and the proposed NAS technique could find a larger network with shorter latency by adding the latency factor to the loss function directly. The main benefit comes from different block assignment to stages.One of the most closely related work is the recently proposed NAS technique tailored for Google’s Edge-TPU [9]. While MBConv is widely used for GPU-aware NAS techniques, they prefer to use a single full convolution by fusing expansion layer and DWConv layer in some parts of the network, observing that the Edge-TPU runs the fused full convolution faster even though the required number of MAC (multiply-accumulate) operations is much larger. It confirms that the number of MAC operations is not a proper measure of latency, and platform-specific performance estimation is required.				
645	paper_182	How "superkernel" is different from supernet?	A superkernel is a component for searching expansion ratio and kernel sizes.  Supernet defines the largest network we can search.	Figure 6 depicts our building block structure. This block starts and ends with 1×1 convolution, with N searchable superkernels in the middle. Each searchable superkernel is designed similarly to Eq. (3), while we may use different threshold values in each superkernel. The kernel sizes and expansion ratios are selected among predetermined values. If the j-th searchable superkernel chooses an expansion ratio e_{j}, the j-th kernel has e_{j} times more channels than the first 1×1 convolution. Compared with the original MixConv suggested in (Tan and Le, 2019b), the proposed building block supports more diverse combinations of kernel sizes and expansion ratios. It enhances the efficiency of search results on our target NPU (Table 5).A superkernel has two parameters to search: expansion ratio and kernel size. To limit the search space, we choose the expansion ratio among 0, 2, 4, and 6, and the kernel size between 3 and 5 when MBConv or full convolution is used as the building block. In the case of the MixConv-based building block, we use N=3 superkenels whose expansion ratio is 0 or 2; The sum of the expansion ratio of three superkernels has the same range as the expansion ratio of a single MBConv block. To allow three superkernels to have different kernel sizes, we let one of three superkernels be able to have 7 as the kernel size.A NAS technique explores a predefined search space and estimates the performance for each candidate architecture to find an optimal one with the highest accuracy under a given latency constraint. Thus there are three factors that affect the performance of NAS, as shown in Figure 1: search space, search strategy, and performance estimation. The search space of a NAS technique is usually restricted by a supernet that defines the topology of the largest network to explore. Since the performance of a network depends on the hardware platform, the NAS technique needs to be customized to a given hardware platform. While numerous NAS techniques have been proposed with various search strategies recently, their assumed hardware platforms are mostly GPUs. In this paper, we present a customized NAS technique for an NPU, which produces a CNN architecture with a better accuracy-latency tradeoff than existing models.				
646	paper_182	Why did the authors choose MIDAP as the target NPU to experiment on?	The end-to-end latency can be estimated quite accurately, and MIDAP can efficiently support which which lower the MAC utilization in other NPUs.	Even though the proposed methodology can be applied to any type of NPU, the current implementation is made for an adder-tree type NPU, called MIDAP (Kanget al., 2019).It has a fully-pipelined micro-architecture that consists of separate hardware modules and memory modules for convolution, activation function, and various reduction operations. Since it enables us to make a fully static schedule of operations without resource contention in the data path, we can estimate the end-to-end latency of a CNN quite accurately analytically. Unexpected delay may incur from off-chip DRAM delay that is not fully hidden by double buffering.Another good feature of MIDAP is that it efficiently supports the following operations that would lower the MAC (multiply-accumulate) utilization in other NPUs that have many MAC units: pooling, DWConv, and squeeze-and-excitation (SE). For DWConv operation, it does not use an adder tree but an alternative hardware logic that consists of a set of individual accumulators connected to the multiply units. For pooling and SE operations, reduction logic is included in the pipeline.Note that MIDAP has not been implemented as a real hardware chip yet but as a virtual prototype with a cycle-accurate simulator. Thanks to the cycle-accurate simulator that considers the DRAM access contention and parametrized DRAM access delay, we could build an accurate analytical model for end-to-end latency estimation, based on the profiling result with the simulator.				
647	paper_182	Why the authors experiment on an NPU simulator, not the real hardware chip?	MIDAP can support DWConv, SE more efficiently than other NPUs.  However, MIDAP is not implemented as a real hardware chip yet, but the cycle-accurate simulator is open-sourced.	Another good feature of MIDAP is that it efficiently supports the following operations that would lower the MAC (multiply-accumulate) utilization in other NPUs that have many MAC units: pooling, DWConv, and squeeze-and-excitation (SE). For DWConv operation, it does not use an adder tree but an alternative hardware logic that consists of a set of individual accumulators connected to the multiply units. For pooling and SE operations, reduction logic is included in the pipeline.Note that MIDAP has not been implemented as a real hardware chip yet but as a virtual prototype with a cycle-accurate simulator. Thanks to the cycle-accurate simulator that considers the DRAM access contention and parametrized DRAM access delay, we could build an accurate analytical model for end-to-end latency estimation, based on the profiling result with the simulator.We evaluate the proposed NAS technique for image classification with the ImageNet dataset. The current implementation is made for MIDAP (Kanget al., 2019) that can perform DWConv and SE operations efficiently so that MBConv is preferred to full 3-D convolution as the basic building block, as explained above. Latencies on the target NPU are obtained with the cycle-accurate simulator222https://github.com/cap-lab/MidapSim.				
648	paper_182	How does the authors select SE blocks to remove?	Removing SE blocks having similar distributions over different image classes are known to incur only a marginal loss in accuracy.  Thus, for each channel c, authors calculated the standard deviation \sigma_{c} of activation values over different images.  Small value of \sigma_{c} would mean that SE block is having similar distrubution over different images.  Thus they defined the metric as the average of \sigma_{c} over all channels.  Specifically, they sample from 10K images from the training dataset, and remove until only about 60% of blocks are remained.	Figure 11 depicts an example distribution of activation values produced by two different SE blocks for three different images. The authors of the original paper (Hu et al., 2018) conjectured that if such distribution from a SE block does not differ widely between image classes, the SE block is not important. Thus, after training, they obtained averaged activation values of a SE block over multiple images in the same class.They compared the distributions of the averaged values over different image classes. They observed that removing the SE blocks that have similar distributions over different image classes incurs only a marginal loss in accuracy.Inspired by this observation, we propose to remove SE blocks selectively to minimize the additional computation cost caused by SE blocks.We obtain activation values from a SE block for each input image and measure how the distribution of activation values varies over different input images.For each channel c, we calculate the standard deviation \sigma_{c} of activation values over different images. If \sigma_{c} is small in most channels, the activation values from the SE block does not differ much over images. Conceptually, it implies that the SE block does not help to discriminate further which channel is more influential. From the engineering perspective, it means that channel-wise multiplication of a SE block is similar to constant multiplication, which can be handled by the following convolutional layer.We define a metric as the average of standard deviation values \sigma_{c} over all channels that represent the diverseness of the activation distribution over different images. If the metric value is small, we remove the SE block. For example, in Figure 11, our metric of the SE block on the left side has a value of 0.021, while the right side has a value of 0.118, more than 5x larger than the left side; The left side is a better candidate for SE block removal. When we remove SE blocks according to this metric, the accuracy is found to be similar, while the latency got shorter (Table 6).Finally, we selectively removed SE blocks from ours-XL+, resulting in ours-XL-rmSE+. We collected the activation values using randomly sampled 10K images from the training dataset and calculated the metric explained in Sec. 4.3.3.After removing SE blocks from ours-XL+ based on the metric, only about 60% of the blocks in the network have SE blocks. As a result, we could make the latency shorter, while the accuracy was slightly improved than ours-XL+. This model achieves 82.72% top-1 accuracy with only 11.66ms latency.It is much better than EfficientNet-EdgeTPU-L (Gupta and Akin, 2020) that achieves 80.62% FP32 top-1 accuracy with more than 20ms on EdgeTPU. Our architecture on MIDAP is about 2 times faster with 2.1% higher accuracy.				
649	paper_182	How does authors verify that searching in a small supernet then scaling is good tactic to search a big network?	Searching in a small supernet then scaling was better than directly searching in a big supernet.  The accuracy was similar, but the direct search needed much higher search cost.	There are two methods to find an architecture with a loose latency constraint. One is to use compound scaling that scales a small network with shorter latency, and the other is to search a network directly. To compare these two methods,we first scaled ours-M using the same scaling coefficients that we used to scale ours-M+ to ours-L+ and trained it. When conducting a direct search, we scaled the depth and width of the supernet and the input image size first and applied the proposed NAS technique for the scaled supernet. We used batch size 512 instead of 1024 during the architecture search due to the memory limitation of TPU. The comparison result is shown in Table 7 in terms of top-1 accuracy(%) and the latency on the target NPU(ms).Two results were similar while direct search needed 10 hours on TPUv3; It means that compound scaling is an effective method to find a large network fast.				
650	paper_182	Why does author experiment the quantized linear supernet design even though Radosavovic et al. already provided similar result?	The previous study shows the linear design is beneficial in terms of computational complexity, while the author shows the result in terms of latency.	Thus, we place more blocks to stages with larger width in the supernet, making the cumulative depth up to a specific stage is proportional to the width of the stage, which is similar to PyramidNet (Han et al., 2017). A recent study (Radosavovic et al., 2020) also claims that neural architectures with a linear relationship between the cumulative depth and the width tend to have higher accuracy with a similar amount of computation complexity. Our experiment shows that our modification to supernet enhances the efficiency of the search result in terms of accuracy as well as latency (Table 4).				
651	paper_182	How does authors claim that squeeze-and-excitation block removal is beneficial?	Previous work shows the potential of removing SE blocks, and authors confirms the benefit of removal with an experimental result.	Figure 11 depicts an example distribution of activation values produced by two different SE blocks for three different images. The authors of the original paper (Hu et al., 2018) conjectured that if such distribution from a SE block does not differ widely between image classes, the SE block is not important. Thus, after training, they obtained averaged activation values of a SE block over multiple images in the same class.They compared the distributions of the averaged values over different image classes. They observed that removing the SE blocks that have similar distributions over different image classes incurs only a marginal loss in accuracy.Finally, we selectively removed SE blocks from ours-XL+, resulting in ours-XL-rmSE+. We collected the activation values using randomly sampled 10K images from the training dataset and calculated the metric explained in Sec. 4.3.3.After removing SE blocks from ours-XL+ based on the metric, only about 60% of the blocks in the network have SE blocks. As a result, we could make the latency shorter, while the accuracy was slightly improved than ours-XL+. This model achieves 82.72% top-1 accuracy with only 11.66ms latency.It is much better than EfficientNet-EdgeTPU-L (Gupta and Akin, 2020) that achieves 80.62% FP32 top-1 accuracy with more than 20ms on EdgeTPU. Our architecture on MIDAP is about 2 times faster with 2.1% higher accuracy.				
652	paper_182	Is the extended search strategy beneficial? Does the gain simply come from modified search space?	The randomly searched network on the same supernet could not outperform the proposed result with Single-Path NAS.  Thus the search strategy was beneficial.	While most NAS techniques are not compared with a random search method, the authors (Li and Talwalkar, 2019) reported that a random search method is highly competitive. So we conducted an experiment to compare the proposed NAS technique with two random search methods, exploring the same search space defined by the supernet structure of ours-M.First, we designed a simple random search method that has the similar time complexity of the proposed technique. In this method, we randomly generate 15 models having a similar latency with ours-M, from the same search space. Then we train each of them for 1 epoch with cosine learning rate decay. After evaluating each of them, we choose the architecture with the topmost top-1 accuracy and fully train it. In the second method, called random selection, we randomly generate 20 models having a similar latency with ours-M and train them fully and take the architecture with the highest top-1 accuracy. Since the random selection method performs search and training simultaneously, it is slower than the proposed technique by the number of randomly generated models.Comparison results are reported in Table 6. It is confirmed that both random selection and random search are quite competitive, but noticeably inferior to ours-M in terms of accuracy.In detail, the worst case of random selection showed 0.8% lower accuracy than ours-M. The best performance obtained from 20 randomly generated models is 79.19%, still lower than the accuracy of ours-M.Note that random search and random selection show similar performance that is no smaller than the other networks. It means that the search space defined by the supernet architecture has a more significant effect on the accuracy than the search method.				
653	paper_182	How the proposed loss function is different from that of original Single-Path NAS?	Previous method needs additional search cost for hyperparameter, since they have no information for target latency.  The author's method directly includes the target latency, resulting in ease of search process.	The existing hardware-aware differentiable NAS methods mostly define some hyperparameters to balance between accuracy and latency, including SinglePath NAS, whose loss function is defined as Eq. (6). Since there is no information on the target latency in the loss function, in case there is a strict latency constraint, they have to pay additional search costs for the hyperparameters to let the final architecture have no larger latency than the constraint. In addition, this process needs to be repeated whenever the target latency is changed.In this work, we propose a fast NPU-aware NAS methodology extending the Single-Path NAS technique (Stamoulis et al., 2019). We modify the supernet architecture by varying the number of blocks in stages and adding mixed depthwise convolution (Tan and Le, 2019b) to the search space. By modifying the loss function to directly include the target latency estimated by a cycle-accurate simulator of the target NPU, we could find a better baseline architecture with a shorter latency than the latency constraint. Using a tight latency constraint, we can reduce the search space to find the baseline network fast. Afterward, we apply compound scaling to find a larger network than the baseline network, and add SE blocks and h-swish activation functions in the post-processing step.Through the proposed NAS methodology, we could obtain a network with 82.72% accuracy with 11.66ms latency on our target NPU, without special data augmentation in training. It dominates the existing network models on the target NPU. It confirms the importance of supernet architecture design for a given NPU and effectiveness of the three-step approach in the proposed NAS methodology: supernet design, SinglePath NAS with a tighter latency constraint, and compound scaling and post-processing.Although this is not a panacea, this modification significantly eases the search process, which will be discussed in section 5.2 with various experiments.				
654	paper_182	Why authors choose to extend Single-Path NAS as the search strategy, instead of famous NAS methods such as MNASNet?	The author's aim is building a fast NAS methodology.  Single-Path NAS could search a good architecture faster than existing NAS techniques.  It builds a faster NAS technique by reducing the number of trainable parameters.  Another reason is that Single-Path NAS can be efficiently extended to support MixConv.	Among diverse techniques to decrease the search cost, Single-Path NAS (Stamoulis et al., 2019) was recently proposed to find a good architecture faster than the existing differentiable NAS techniques. This technique is extended to broaden the search space by including the squeeze-and-excitation (SE) block in the search space (Stamoulis et al., 2020). Our work is grounded on the original Single-Path NAS technique.Differentiable NAS methods usually define architecture parameters to choose which convolution layer to use in the block, training each convolution layer independently. Single-Path NAS (Stamoulis et al., 2019) reduce the search cost by decreasing the number of trainable parameters by sharing the kernel weights between convolution layers.The key idea is designing an over-parameterized depthwise convolution kernel named superkernel, and letting each depthwise convolution kernel of candidate MBConvs directly inherit the weights of this superkernel.We finish this subsection by highlighting the merit of Single-Path NAS on building a MixConv-based differentiable NAS. Conventional multi-path NAS methods would have difficulties when adding inverted bottleneck convolution with MixConv to their search space. Since the number of possible choices of such blocks grows proportionally to the partition number, multi-path NAS methods would introduce a significant increase in memory requirements and the search time. On the contrary, MixConv can be efficiently supported in Single-Path NAS, as explained below.Since an NPU is much faster than a GPU, it enables us to explore the wider search space for NAS under a given latency constraint. Since there are many factors to define the search space, such as the number of layers, channels, kernel sizes, and so on, the search space grows exponentially as the allowed computation complexity grows. Hence, reducing the search space, as well as the search time, is very challenging for NPU-aware NAS techniques. While the aforementioned work for Google’s Edge TPU trains each architecture candidate from scratch to estimate the performance, it is not computationally efficient. In contrast, we adopt a fast differentiable hardware-aware One-Shot NAS, called Single-Path NAS (Stamoulis et al., 2019), in order to reduce the search time.In this work, we propose a fast NPU-aware NAS methodology extending the Single-Path NAS technique (Stamoulis et al., 2019). We modify the supernet architecture by varying the number of blocks in stages and adding mixed depthwise convolution (Tan and Le, 2019b) to the search space. By modifying the loss function to directly include the target latency estimated by a cycle-accurate simulator of the target NPU, we could find a better baseline architecture with a shorter latency than the latency constraint. Using a tight latency constraint, we can reduce the search space to find the baseline network fast. Afterward, we apply compound scaling to find a larger network than the baseline network, and add SE blocks and h-swish activation functions in the post-processing step.Through the proposed NAS methodology, we could obtain a network with 82.72% accuracy with 11.66ms latency on our target NPU, without special data augmentation in training. It dominates the existing network models on the target NPU. It confirms the importance of supernet architecture design for a given NPU and effectiveness of the three-step approach in the proposed NAS methodology: supernet design, SinglePath NAS with a tighter latency constraint, and compound scaling and post-processing.				
655	paper_182	How does equation 2 let the supernet search kernel size?	They define a trainable threshold value t, and compare the norm of the kernel weights with the threshold, to determine the kernel size.	Let \mathbf{w}_{k,e} denote the depthwise convolution kernel of candidate MBConv with kernel size k and expansion ratio e (MBConv{}_{k,e}). First, they introduce a large \mathbf{w}_{5,6}, which is the DWConv kernel of MBConv{}_{5,6}. Then, the inner core of \mathbf{w}_{5,6} can be considered as \mathbf{w}_{3,6}, a DWConv kernel of MBConv{}_{3,6}.A superkernel containing these two kernel size options can be expressed as Figure 4:(1)\mathbf{w}_{*,6}=\mathbf{w}_{3,6}+\mathbbm{1}(\rm{use\leavevmode\nobreak\ kernel\leavevmode\nobreak\ size\leavevmode\nobreak\ 5})\cdot\mathbf{w}_{5\backslash 3,6}where \mathbf{w}_{5\backslash 3,e} means the outer part, \mathbf{w}_{5,e}-\mathbf{w}_{3,e}.Next, they formulate conditions to determine the kernel size. They define a certain threshold value t and compare the norm of the kernel weights with the threshold. If the norm of a subset weight is larger than the threshold, it remains in the supernet. To this end, Eq. (1) is changed as follows:(2)\mathbf{w}_{*,6}(t_{k=5})=\mathbf{w}_{3,6}+\mathbbm{1}(\lVert\mathbf{w}_{5\backslash 3,6}\rVert^{2}>t_{k=5})\cdot\mathbf{w}_{5\backslash 3,6}The threshold value is also trainable to be automatically chosen during training. To enable back-propagation, they relax 1(x > t) to σ(x − t) when computing gradients. In addition, they optimize kernel weights and threshold values simultaneously. For a given tight search time, this method is shown to be more effective than the other methods [29].				
656	paper_182	What is 'cumulative depth up to a specific stage' ?	The total number of blocks starting from the very first bloci in the network up to the last block in a specific stage.	In this section, we will briefly review the Single-Path NAS technique and our target NPU.Before going further, we define some terminologies used in this paper, as shown in Figure 3. A neural architecture consists of stages at the top level. A stage consists of a sequence of blocks whose output feature maps have the same dimension. In the proposed supernet, a block is defined as MBConv that typically starts with 1×1 conv (expansion layer) and ends with 1×1 conv. Adopting the MixConv approach, the depthwise convolution layer consists of parallel superkernels whose kernel size will be determined during the NAS process. The width of block denotes the number of channels in the final output feature map of the block, and the width of stage is the width of the final block in the stage. We will call the total number of blocks starting from the very first block in the network up to the last block in a specific stage S, as the cumulative depth up to stage S.				
657	paper_182	How does the authors verify adding h-swish and SE is beneficial?	Previous works showed that using swish activation function instead of ReLU showed better accuracy, and h-swish shows similar impact on accuracy.  The authors verify that replacing ReLU with h-swish and adding SE improves the accuracy by around 1%.	Extensive studies have been conducted to find a better activation function than ReLU, and the swish activation function (Ramachandranet al., 2017) was found. Several neural networks (Tan and Le, 2019b; Mei et al., 2019; Tan and Le, 2019a) use swish activation function instead of ReLU to improve accuracy. Howard et al. (Howard et al., 2019) proposed a quantization-friendly version of the swish activation function called h-swish that has a similar impact on accuracy. So, we replace ReLU with h-swish (Howard et al., 2019) activation function.We improve the baseline network by adding the h-swish activation function and squeeze-and-excitation(SE) block to get the ours-M+ model. Figure 12 shows the topology of ours-M+ architecture in which the height of each block is proportional to the expansion ratio of the block. Compared with the baseline network, ours-M, we achieve around 1% accuracy boost with ours-M+, paying the cost of 16% latency increase. This model outperforms the other models, 0.5% higher accuracy and 14% faster than EfficientNet-B2. Since EfficientNet-B2 is too large to run with the default configuration on MIDAP, we increase the memory size for filter weights. To examine how SE and h-swish impact accuracy individually, we compare four combinations as displayed in Table 8. The baseline is ours-M that does not use SE and h-swish activation function. Replacing ReLU with h-swish gives a marginal improvement on accuracy while adding SE blocks improves the accuracy noticeably. Adding both SE and h-swish activation function improves the accuracy by around 1%.For accurate latency estimation, an analytical latency estimator is devised, based on a cycle-level NPU simulator that runs an entire CNN considering the memory access overhead accurately.Since the NPU assumed in this paper can execute depth-wise separable convolution (DWConv), squeeze-and-excitation (SE), and h-swish activation function efficiently, the proposed supernet prefers DWConv to regular convolution. Observing that the accuracy is improved by around 1% if SE and h-swish activation function are used, we add a post-processing phase after a CNN network is found by NAS to add SE layers and to replace ReLU to h-swish activation function.				
658	paper_182	What happens if author removes the linear supernet design and opt to use the covnentional supernet design?	Using the conventional, constant depth method would drop the accuracy.	As shown in Table 4, a supernet with linear depth outperforms a supernet with constant depth in terms of accuracy with similar latency. It confirms that this simple change of block assignment in supernet gives notable accuracy boost with the same latency constraint, without any additional optimization techniques.The number of blocks is one of the key parameters in this structure. Some recent studies [23, 24] report that the way of assigning the number of blocks in each stage has a noticeable impact on the accuracy, even with the same number of blocks in total. However, in conventional One-Shot NAS methods, each stage in the supernet has the same number of blocks [8, 17, 18, 22].				
659	paper_182	How can rely on this latency prediction model?	The latency of randomly generated models shows that the latency model is accurate.	Figure 9 shows the estimated latency and simulated latency of randomly generated 100 models on our search space. It validates the accuracy of the proposed latency model, whose mean absolute percentage error(MAPE) is about 0.16%.				
660	paper_19	What is the difference between Siamese Network and our works?	Learned distance from Siamese network can be used to solve one-shot problems.  This network can play a role as a single layer message-passing iteration of our model.	Siamese Networks Koch et al. (2015) can be interpreted asa single layer message-passing iteration of our model, and using the same initialnode embedding (5) {\bf x}_{i}^{(0)}=(\phi(x_{i}),h_{i}) , using a non-trainableedge feature\varphi({\bf x}_{i},{\bf x}_{j})=\|\phi(x_{i})-\phi(x_{j})\|~{},~{}\tilde{A}^{(0)}=\text{softmax}(-\varphi)~{},and resulting label estimation\hat{Y}_{*}=\sum_{j}\tilde{A}_{*,j}^{(0)}\langle{\bf x}_{j}^{(0)},u\rangle~{},with u selecting the label field from {\bf x}. In this model,the learning is reduced to learning image embeddings \phi(x_{i}) whoseeuclidean metric is consistent with the label similarities.Since then, great progress has been done in one-shot learning. Koch et al. (2015) presented a deep-learning model based on computing the pair-wise distance between samples using Siamese Networks, then, this learned distance can be used to solve one-shot problems by k-nearest neighbors classification. Vinyals et al. (2016) Presented an end-to-end trainable k-nearest neighbors using the cosine distance, they also introduced a contextual mechanism using an attention LSTM model that takes into account all the samples of the subset \mathcal{T} when computing the pair-wise distance between samples. Snell et al. (2017) extended the work from Vinyals et al. (2016), by using euclidean distance instead of cosine which provided significant improvements, they also build a prototype representation of each class for the few-shot learning scenario. Mehrotra & Dukkipati (2017) trained a deep residual network together with a generative model to approximate the pair-wise distance between samples.				
661	paper_19	What does Active learning means?	Active learning is training strategy which uses both labeld and unlabeld data in training as well as semi-supervised learning.	Besides few-shot learning, a related task is the ability to learn from a mixture oflabeled and unlabeled examples — semi-supervised learning, as wellas active learning, in which the learner has the option to request those missing labelsthat will be most helpful for the prediction task.Our graph-based architecture is naturally extended to these setups withminimal changes in the training design.We validate experimentally the model on few-shot image classification, matchingstate-of-the-art performance with considerably fewer parameters,and demonstrate applications to semi-supervised and active learning setups.				
662	paper_19	If there are few examples for learn, we called them few-shot learning. Guess the meaning of zero-shot learning.	Few-shot learning is learning from few examples.  Therefore, zero-shot learning is learning from no real example.	One such instance is the ability to learn from few examples, in the so-called few-shot learning tasks.Rather than relying on regularization to compensate for the lack of data, researchers have exploredways to leverage a distribution of similar tasks, inspired by human learning Lake et al. (2015).This defines a new supervised learning setup (also called ‘meta-learning’) in which the input-outputpairs are no longer given by iid samples of images and their associated labels, but by iid samples ofcollections of images and their associated label similarity.				
663	paper_19	What is the demerit of using GNN?	Demerit of GNN is high computational complexity.   Kipf & Welling (2016) used polynomials of the graph Laplacian to resolve the computational bottleneck of GNN.	Another related area of research concerns deep learning architectures on graph-structured data.The GNN was first proposed in Gori et al. (2005); Scarselli et al. (2009), as a trainable recurrent message-passingwhose fixed points could be adjusted discriminatively.Subsequent works Li et al. (2015); Sukhbaatar et al. (2016) have relaxed the model by untying the recurrent layer weights and proposed several nonlinear updates through gating mechanisms.Graph neural networks are in fact natural generalizations of convolutional networks to non-Euclidean graphs. Bruna et al. (2013); Henaff et al. (2015) proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high computational cost, and Defferrard et al. (2016); Kipf & Welling (2016) resolved the computational bottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. In particular, Kipf & Welling (2016)was the first to propose the use of GNNs on semi-supervised classification problems.We refer the reader to Bronstein et al. (2017) for an exhaustive literature review on the topic.GNNs and the analogous Neural Message Passing Models are finding application in many different domains. Battaglia et al. (2016); Chang et al. (2016) develop graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics. Duvenaud et al. (2015); Kearnes et al. (2016) study molecular fingerprints using variants of the GNN architecture, and Gilmer et al. (2017) further develop the model by combining it with set representations Vinyals et al. (2015), showing state-of-the-art results on molecular prediction.				
664	paper_19	Explain the meaning of N-Way and M-shot.	N-way and M-shot means that we sample N random classes from the dataset, and we sample M random samples from each class.	We evaluate our model by performing different q-shot, K-way experiments on both datasets. For every few-shot task \mathcal{T}, we sample K random classes from the dataset, and from each class we sample q random samples. An extra sample to classify is chosen from one of that K classes.				
665	paper_19	Few-shot learning and semi-supervised learning is the same term. Is this true?	Unlike Few-shot learning, semi-supervised learning is training strategy which uses both labeled and unlabeled examples.	Besides few-shot learning, a related task is the ability to learn from a mixture oflabeled and unlabeled examples — semi-supervised learning, as wellas active learning, in which the learner has the option to request those missing labelsthat will be most helpful for the prediction task.Our graph-based architecture is naturally extended to these setups withminimal changes in the training design.We validate experimentally the model on few-shot image classification, matchingstate-of-the-art performance with considerably fewer parameters,and demonstrate applications to semi-supervised and active learning setups.				
666	paper_19	Does previous researches, which paper mentioned, using GNN?	Previous researches such as Duvenaud et al.  (2015)and Kearnes et al.  (2016) used GNN.	Another related area of research concerns deep learning architectures on graph-structured data.The GNN was first proposed in Gori et al. (2005); Scarselli et al. (2009), as a trainable recurrent message-passingwhose fixed points could be adjusted discriminatively.Subsequent works Li et al. (2015); Sukhbaatar et al. (2016) have relaxed the model by untying the recurrent layer weights and proposed several nonlinear updates through gating mechanisms.Graph neural networks are in fact natural generalizations of convolutional networks to non-Euclidean graphs. Bruna et al. (2013); Henaff et al. (2015) proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high computational cost, and Defferrard et al. (2016); Kipf & Welling (2016) resolved the computational bottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. In particular, Kipf & Welling (2016)was the first to propose the use of GNNs on semi-supervised classification problems.We refer the reader to Bronstein et al. (2017) for an exhaustive literature review on the topic.GNNs and the analogous Neural Message Passing Models are finding application in many different domains. Battaglia et al. (2016); Chang et al. (2016) develop graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics. Duvenaud et al. (2015); Kearnes et al. (2016) study molecular fingerprints using variants of the GNN architecture, and Gilmer et al. (2017) further develop the model by combining it with set representations Vinyals et al. (2015), showing state-of-the-art results on molecular prediction.				
667	paper_19	Matching network uses BERT. Is this true?	It's not true.  Matching network uses attention mechanism, not BERT.	Matching networks Vinyals et al. (2016) use a set representation for the ensemble of images in \mathcal{T},similarly as our proposed graph neural network model, but with two important differences.First, the attention mechanism considered in this set representationis akin to the edge feature learning, with the difference that the mechanism attends always to the same node embeddings, as opposed to our stacked adjacency learning, which is closer to Vaswani et al. (2017). In other words, instead of the attention kernel in (3),matching networks consider attention mechanisms of the form \tilde{A}_{*,j}^{(k)}=\varphi({\bf x}_{*}^{(k)},{\bf x}_{j}^{(T)}),where {\bf x}_{j}^{(T)} is the encoding function for the elements of the support set, obtained with bidirectional LSTMs. In that case, the support set encoding is thus computed independently of the target image.Second, the label and image fields are treated separately throughout the model, with a final step that aggregates linearly the labels using a trained kernel. This may prevent the model to leverage complex dependencies between labels and images at intermediate stages.				
668	paper_19	Give two examples which fit in following case: “Despite these successes, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.”	There were great succeed in computer vision and speech tasks.  However, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks,thanks to improvements in optimization technology, larger datasets and streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	There were great succeed in laptop vision and speech tasks.  However, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Tortured phrases	computer vision -> laptop vision	
669	paper_19	What does Omniglot means?	Omniglot is a dataset consist of 1623 characters from 50 different alphabets.  In this dataset, 20 different people drew each character.	Omniglot is a dataset of 1623 characters from 50 different alphabets, each character/class has been drawn by 20 different people. Following Vinyals et al. (2016) implementation we split the dataset into 1200 classes for training and the remaining 423 for testing. We augmented the dataset by multiples of 90 degrees as proposed by Santoro et al. (2016).				
670	paper_2	What significance do the numbers like 51.3 and 30.0 have with respect to the One Billion Word benchmark?	As the average per-word log-probability, perplexity is a measure of how confused the language model is in predicting the next word.	The typical measure used for reporting progress in language modeling is perplexity, which is the average per-word log-probability on the holdout data set: e^{-\frac{1}{N}\sum_{i}\ln{p_{w_{i}}}}. We follow the standard procedure and sum over all the words (including the end of sentence symbol).				
671	paper_2	What exactly is "smoothing" and how does it help count-based LMs account for unseen sequences?	The paper does not discuss what smoothing is or how it helps some LMs account for unseen sequences.	Language Modeling (LM) has been a central task in NLP. The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language. Much work has been done on both parametric (e.g., log-linear models) and non-parametric approaches (e.g., count-based LMs). Count-based approaches (based on statistics of N-grams) typically add smoothing which account for unseen (yet possible) sequences, and have been quite successful. To this extent, Kneser-Ney smoothed 5-gram models (Kneser & Ney, 1995) are a fairly strong baseline which, for large amounts of training data, have challenged other parametric approaches based on Neural Networks (Bengio et al., 2006).				
672	paper_2	What is a "recurrent state space"?	Paper does not discuss what a recurrent state space is.	A crucial aspect which we discuss in detail in later sections is the size of our models. Despite the large number of parameters, we try to minimize computation as much as possible by adopting a strategy proposed in (Sak et al., 2014) of projecting a relatively big recurrent state space down so that the matrices involved remain relatively small, yet the model has large memory capacity.Unsurprisingly, size matters: when training on a very large and complex data set, fitting the training data with an LSTM is fairly challenging. Thus, the size of the LSTM layer is a very important factor that influences the results, as seen in Table 1. The best models are the largest we were able to fit into a GPU memory. Our largest model was a 2-layer LSTM with 8192+1024 dimensional recurrent state in each of the layers. Increasing the embedding and projection size also helps but causes a large increase in the number of parameters, which is less desirable. Lastly, training an RNN instead of an LSTM yields poorer results (about 5 perplexity worse) for a comparable model size.				
673	paper_2	What is the difference between a 1-d CNN and a 2-layer highway network?	The paper does not discuss the difference between 1D CNN and highway networks.	In (Kim et al., 2015), the words characters are processed by a 1-d CNN (Le Cun et al., 1990) with max-pooling across the sequence for each convolutional feature. The resulting features are fed to a 2-layer highway network (Srivastava et al., 2015b), which allows the embedding to learn semantic representations. The model was evaluated on small-scale language modeling experiments for various languages and matched the best results on the PTB data set despite having 60% fewer parameters.				
674	paper_2	What is a 128-dim correction?	It is a means of improvement over words that have different meaning but are spelled similarly.	z_{w}=h^{T}CNN(chars_{w})+h^{T}Mcorr_{w}where M is a matrix projecting a low-dimensional embedding vector corr_{w} back up to the dimensionality of the projected LSTM hidden state of h. This amounts to adding a bottleneck linear layer, and brings the CNN Softmax much closer to our best result, as can be seen in Table 1, where adding a 128-dim correction halves the gap between regular and the CNN Softmax.As described in Section 3.2, adding a “correction” word embedding term alleviates the gap between regular and CNN Softmax. Indeed, we can trade-off model size versus perplexity. For instance, by adding 100M weights (through a 128 dimensional bottleneck embedding) we achieve 35.8 perplexity (see Table 1).				
675	paper_2	What is dropout and how does it alleviate overfitting?	Dropout is a neural network component parametrized with a probability.  The paper does not discuss how it alleviates overfitting.	Following (Zaremba et al., 2014) we use dropout (Srivastava, 2013) before and after every LSTM layer. The biases of LSTM forget gate were initialized to 1.0 (Jozefowicz et al., 2015). The size of the models will be described in more detail in the following sections, and the choices of hyper-parameters will be released as open source upon publication.As shown in Table 1, using dropout improves the results. To our surprise, even relatively small models (e.g., single layer LSTM with 2048 units projected to 512 dimensional outputs) can over-fit the training set if trained long enough, eventually yielding holdout set degradation.Using dropout on non-recurrent connections largely mitigates these issues. While over-fitting still occurs, there is no more need for early stopping. For models that had 4096 or less units in the LSTM layer, we used 10% dropout probability. For larger models, 25% was significantly better. Even with such regularization, perplexities on the training set can be as much as 6 points below test.				
676	paper_2	How does IS and NCE compare in terms of model performance?	IS performs better.	p(Y=k|W)\propto_{Y}\frac{p_{d}(w_{k})}{p_{n}(w_{k})}and, following a similar argument than for NCE, if we define p(Y=k|W)=softmax(s_{\theta}(w_{k})-\log p_{n}(w_{k})) then p^{\prime}(w)=softmax(s_{\theta}(w,h)) is a good approximation of p_{d}(word). Note that the only difference between NCE and IS is that, in NCE, we define a binary classification task between true or noise words with a logistic loss, whereas in IS we define a multiclass classification problem with a Softmax and cross entropy loss. We hope that our derivation helps clarify the similarities and differences between the two. In particular, we observe that IS, as it optimizes a multiclass classification task (in contrast to solving a binary task), may be a better choice. Indeed, the updates to the logits with IS are tied whereas in NCE they are independent.Table 3 shows the test perplexities of NCE vs IS loss after a few epochs of 2048 unit LSTM with 512 projection. The IS objective significantly improves the speed and the overall performance of the model when compared to NCE.				
677	paper_2	Why doesn't character-level embeddings degrade performance compared to word-level embeddings?	The paper discusses one advantage of character-level embeddings over word-level embeddings.  There is no comprehensive discussion on why the resulting performance does not degrade.	The character-level features allow for a smoother and compact parametrization of the word embeddings. Recent efforts on small scale language modeling have used CNN character embeddings for the input embeddings (Kim et al., 2015). Although not as straightforward, we propose an extension to this idea to also reduce the number of parameters of the Softmax layer. Recall from Section 2.3 that the Softmax computes a logit as z_{w}=h^{T}e_{w} where h is a context vector and e_{w} the word embedding. Instead of building a matrix of |V|\times|h| (whose rows correspond to e_{w}), we produce e_{w} with a CNN over the characters of w as e_{w}=CNN(chars_{w}) – we call this a CNN Softmax. We used the same network architecture to dynamically generate the Softmax word embeddings without sharing the parameters with the input word-embedding sub-network. For inference, the vectors e_{w} can be precomputed, so there is no computational complexity increase w.r.t. the regular Softmax.				
678	paper_2	How does maximizing log-likelihood lead to optimizing cross-entropy between target probability distribution and the model prediction?	The paper does not discuss the detailed workings of the established relation.	Assigning probability distributions over large vocabularies is computationally challenging. For modeling language, maximizing log-likelihood of a given word sequence leads to optimizing cross-entropy between the target probability distribution (e.g., the target word we should be predicting), and our model predictions p. Generally, predictions come from a linear layer followed by a Softmax non-linearity: p(w)=\frac{\exp(z_{w})}{\sum_{w^{\prime}\in V}\exp(z_{w^{\prime}})} where z_{w} is the logit corresponding to a word w. The logit is generally computed as an inner product z_{w}=h^{T}e_{w} where h is a context vector and e_{w} is a “word embedding” for w.				
679	paper_2	How does increasing the embedding and projection size help with respect to the model?	The paper does not discuss how the positive effects are brought about.	Unsurprisingly, size matters: when training on a very large and complex data set, fitting the training data with an LSTM is fairly challenging. Thus, the size of the LSTM layer is a very important factor that influences the results, as seen in Table 1. The best models are the largest we were able to fit into a GPU memory. Our largest model was a 2-layer LSTM with 8192+1024 dimensional recurrent state in each of the layers. Increasing the embedding and projection size also helps but causes a large increase in the number of parameters, which is less desirable. Lastly, training an RNN instead of an LSTM yields poorer results (about 5 perplexity worse) for a comparable model size.				
680	paper_20	Previous RE task SOTA model was provided by Yamada et. al. at 2020. Is this true?	It is true.  LUKE Yamada et al.  (2020) is one of SOTA model in RE task.  This model extends the pretraining objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism.	Recent work on sentence-level RE can be divided into two lines.One focuses on injecting external knowledge into PLMs.Methods of such, including ERNIE Zhang et al. (2019) and KnowBERT Peters et al. (2019), take entity embedding pretrained from knowledge graphs as inputs to the Transformer.Similarly, K-Adapter Wang et al. (2020) introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model.LUKE Yamada et al. (2020) further extends the pretraining objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism.The other line of work focuses on continually pretraining PLMs on text with linked entities using relation-oriented objectives.Specifically, BERT-MTB Baldini Soares et al. (2019) proposes a matching-the-blanks objective that decides whether two relation instances share the same entities.Despite extensively studied, existing RE models still perform far from perfect.On the commonly-used benchmark TACRED Zhang et al. (2017), the SOTA F_{1} result only increases from 70.1\% (BERT{}_{\text{LARGE}}) to 72.7\% (LUKE) after applying PLMs to this task.It is unclear what building block is missing to constitute a promising RE system.				
681	paper_20	Give one example Relation Extraction question and its answer.	Relation Extraction(RE) task is task of finding relationship between two entities.	As one of the fundamental information extraction (IE) tasks,relation extraction (RE) aims at identifying the relationship(s) between two entities in a given piece of text from a pre-defined set of relationships of interest.For example, given the sentence “Bill Gates founded Microsoft together with his friend Paul Allen in 1975” and an entity pair (“Bill Gates”, “Microsoft”), the RE model is expected to predict the relation ORG:FOUNDED_BY.On this task, SOTA models based on PLMs Devlin et al. (2019); Joshi et al. (2020) have gained significant success.				
682	paper_20	What are the obstacles of RE? Does this paper solved them?	There are two obstacles of RE.  First, existing method has limit of characterization of the entities.  Second, there are many noises in human labeled data.  They improve the model performance by addressing these problems.	In this work, we discuss two obstacles that have hindered the performance of existing RE models.First, the RE task provides a structured input of both the raw texts and side information of the entities, such as entity names, spans, and types (typically provided by NER models), which are shown important to the performance of RE models Peng et al. (2020).However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities.Second, human-labeled RE datasets (e.g., TACRED), may contain a large portion of noisy or ill-defined labels, causing the model performance to be misestimated.Alt et al. (2020) relabeled the development and test set of TACRED and found that 6.62\% of labels are incorrect.Stoica et al. (2021) refined many ill-defined relation types and further re-annotated the TACRED dataset using an improved annotation strategy to ensure high-quality labels.To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentence-level RE, which leads to promising improvement of performance over existing RE models.				
683	paper_20	Is there any different way to construct RE model instead of using PLM strategy previously?	ERNIE Zhang et al.  (2019) and KnowBERT Peters et al.  (2019) construct RE model by injecting external knowledge into PLMs.  BERT-MTB Baldini Soares et al.  (2019) continually pretrain PLMs on text with linked entities using relation-oriented objectives.	Recent work on sentence-level RE can be divided into two lines.One focuses on injecting external knowledge into PLMs.Methods of such, including ERNIE Zhang et al. (2019) and KnowBERT Peters et al. (2019), take entity embedding pretrained from knowledge graphs as inputs to the Transformer.Similarly, K-Adapter Wang et al. (2020) introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model.LUKE Yamada et al. (2020) further extends the pretraining objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism.The other line of work focuses on continually pretraining PLMs on text with linked entities using relation-oriented objectives.Specifically, BERT-MTB Baldini Soares et al. (2019) proposes a matching-the-blanks objective that decides whether two relation instances share the same entities.Despite extensively studied, existing RE models still perform far from perfect.On the commonly-used benchmark TACRED Zhang et al. (2017), the SOTA F_{1} result only increases from 70.1\% (BERT{}_{\text{LARGE}}) to 72.7\% (LUKE) after applying PLMs to this task.It is unclear what building block is missing to constitute a promising RE system.				
684	paper_20	What datasets did this paper used for?	They used three datasets, the original TACRED Zhang et al.  (2017), TACREV Alt et al.  (2020), and Re-TACRED Stoica et al.  (2021).	Datasets. The datasets we have used in the experiments include three versions of TACRED: the original TACRED Zhang et al. (2017), TACREV Alt et al. (2020), and Re-TACRED Stoica et al. (2021).Alt et al. (2020) observed that the TACRED dataset contains about 6.62\% noisily-labeled instances and relabeled the development and test set.Stoica et al. (2021) further refined the label definitions in TACRED and relabeled the whole dataset.We provide the statistics of the datasets in Appendix A.In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness.Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels.We propose an improved entity representation technique, which significantly outperforms existing sentence-level RE models.Especially, our improved RE baseline achieves an F_{1} score of 91.1\% on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task.We hope the proposed techniques and analyses can benefit future research on RE.				
685	paper_20	Can improvement of NER technology improves RE technology?	Entity names, spans and types are important to the performance of RE models Peng et al.  (2020).  Since types are typically provided by NER models, improvement of NER technology can improve RE technology.	In this work, we discuss two obstacles that have hindered the performance of existing RE models.First, the RE task provides a structured input of both the raw texts and side information of the entities, such as entity names, spans, and types (typically provided by NER models), which are shown important to the performance of RE models Peng et al. (2020).However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities.Second, human-labeled RE datasets (e.g., TACRED), may contain a large portion of noisy or ill-defined labels, causing the model performance to be misestimated.Alt et al. (2020) relabeled the development and test set of TACRED and found that 6.62\% of labels are incorrect.Stoica et al. (2021) refined many ill-defined relation types and further re-annotated the TACRED dataset using an improved annotation strategy to ensure high-quality labels.To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentence-level RE, which leads to promising improvement of performance over existing RE models.				
686	paper_20	Should RE model use spanBERT instead of BERT?	SpanBERT Joshi et al.  (2020) achieved improved performance on RE than BERT.  Therefore, RE model should use spanBERT to improve model performance.	Compared methods.We compare with the following methods.PA-LSTM Zhang et al. (2017) adopts bi-directional LSTM Hochreiter and Schmidhuber (1997) and positional-aware attention Bahdanau et al. (2015) to encode the text into an embedding, which is then fed into a softmax layer to predict the relation.C-GCN Zhang et al. (2018) is a graph-based model, which feeds the pruned dependency tree of the sentence into the graph convolutional network Kipf and Welling (2017) to obtain the representation of entities.SpanBERT Joshi et al. (2020) is a PLM based on the Transformer Vaswani et al. (2017). It extends BERT Devlin et al. (2019) by incorporating a training objective of span prediction and achieves improved performance on RE.KnowBERT Peters et al. (2019) jointly trains a language model and an entity linker, which allows the subtokens to attend to entity embedding that is pretrained on knowledge bases.LUKE Yamada et al. (2020) pretrains the language model on both large text corpora and knowledge graphs. It adds frequent entities into the vocabulary and proposes an entity-aware self-attention mechanism.				
687	paper_20	Author said that they achieved to make SOTA RE models. Give an evidences for this statement.	Their improved RE baseline achieved SOTA performance on the RE-TACRED dataset with f1 score of 91.  Moreover, Using RoBERTa Liu et al.  (2019) as the backbone, they improved baseline model on TACRED and TACREV with f1 score 74. 6% and 83. 2%, respectively.  The RoBERTa model achieves 1. 9% higher f1 score than the SOTA model LUKE Yamada et al.	We first provide an analysis on different entity representation techniques. In this analysis, we use the base and large versions of BERT Devlin et al. (2019) and the large version of RoBERTa Liu et al. (2019) as the encoder.Table 1 shows the performance of the PLMs incorporated with different entity representation techniques.For each technique, we also provide an example of the processed text.We have several observations from the results.First, the typed entity marker and its variants outperform untyped entity representation techniques by a notable margin.Especially, the RoBERTa model achieves an F_{1} score of 74.6\% using the typed entity marker (punct), which is significantly higher than the SOTA result of 72.7\% by LUKE Yamada et al. (2020).This shows that representing all categories of entity information is helpful to the RE task.It also shows that keeping entity names in the input improves the performance of RE models.Second, symbols used in entity markers have an obvious impact on the performance of RE models.Although the original and punct versions of entity representation techniques represent the same categories of entity information, they do lead to a difference in model performance.Particularly, introducing new special tokens hinders the model performance drastically on RoBERTa.On RoBERTa{}_{\text{LARGE}}, the entity marker underperforms the entity marker (punct) by 0.7\%, the typed entity marker underperforms the typed entity marker (punct) by 3.6\%, while the entity mask gets a much worse result of 60.9\%.In this paper, we present a simple yet strong RE baseline that offers new SOTA performance, along with a comprehensive study to understand its prediction generalizability and robustness.Specifically, we revisit two technical problems in sentence-level RE, namely entity representation and noisy or ill-defined labels.We propose an improved entity representation technique, which significantly outperforms existing sentence-level RE models.Especially, our improved RE baseline achieves an F_{1} score of 91.1\% on the Re-TACRED dataset, showing that PLMs already achieve satisfactory performance on this task.We hope the proposed techniques and analyses can benefit future research on RE.We evaluate our model on TACRED Zhang et al. (2017), TACREV Alt et al. (2020), and Re-TACRED Stoica et al. (2021).Using RoBERTa Liu et al. (2019) as the backbone, our improved baseline model achieves an F_{1} of 74.6\% and 83.2\% on TACRED and TACREV, respectively, significantly outperforming various SOTA RE models.Particularly, our baseline model achieves an F_{1} of 91.1\% on Re-TACRED, demonstrating that PLMs can achieve much better results on RE than shown in previous work.222This work first appeared as a technical report on arXiv in Feb 2021 Zhou and Chen (2021).Since then, the proposed techniques have been incorporated into several follow-up works Chen et al. (2022); Wang et al. (2022b, a); Lu et al. (2022); Han et al. (2021); Kulkarni et al. (2022) that are published before this version of the paper.				
688	paper_20	Entity marker and entity mask are same terms. Is this true?	It's not true.  Two term has different meaning.  Entity marker is the teqnique introduces special tokens pairs to enclose the object and subject entities, whereas entity mask is the teqnique introduces new special tokens to mask the object or object entities.	•Entity mask Zhang et al. (2017). This technique introduces new special tokens [SUBJ-TYPE] or [OBJ-TYPE] to mask the subject or object entities in the original text, where TYPE is substituted with the respective entity type.This technique was originally proposed in the PA-LSTM model Zhang et al. (2017), and was later adopted by PLMs such as SpanBERT Joshi et al. (2020).Zhang et al. (2017) claim that this technique prevents the RE model from over-fitting specific entity names, leading to more generalizable inference.•Entity marker Zhang et al. (2019); Baldini Soares et al. (2019). This technique introduces special tokens pairs [E1], [/E1] and [E2], [/E2] to enclose the subject and object entities, therefore modifying the input text to the format of “[E1] subj [/E1] … [E2] obj [/E2]”333subj and obj are respectively the original token spans of subject and object entities..•Entity marker (punct) Wang et al. (2020); Zhou et al. (2021). This technique is a variant of the previous technique that encloses entity spans using punctuation.It modifies the input text to “@ subj @ … # obj #”. The main difference from the previous technique is that this one does not introduce new special tokens into the model’s reserved vocabulary.•Typed entity marker Zhong and Chen (2021). This technique further incorporates the NER types into entity markers.It introduces new special tokens \langleS:TYPE\rangle, \langle/S:TYPE\rangle, \langleO:TYPE\rangle, \langle/O:TYPE\rangle, where TYPE is the corresponding NER type given by a named entity tagger. The input text is accordingly modified to “\langleS:TYPE\rangle subj \langle/S:TYPE\rangle … \langleO:TYPE\rangle obj \langle/O:TYPE\rangle”.•Typed entity marker (punct). We propose a variant of the typed entity marker technique that marks the entity span and entity types without introducing new special tokens.This is to enclose the subject and object entities with “@” and “#”, respectively.We also represent the subject and object entity types using their label text, which is prepended to the entity spans and is enclosed by “*” for subjects or “\wedge” for objects.The modified text is “@ * subj-type * subj @ … # \wedge obj-type \wedge obj # ”, where subj-type and obj-type is the label text of NER types.				
689	paper_21	BLINK is Scalable. Is this true?	The paper shows the scalability of the proposed simple two-stage method with the experiments conducted on the zero-shot entity-linking dataset where external entity knowledge is not available, which enables the model to be used on various entity linking tasks that contain millions of possible entities to consider.  The state-of-the-art result and the extensive evaluation of the accuracy-speed trade-off support that the proposed method is efficient and scalable.	Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables Ganea and Hofmann (2017), incoming Wikipedia link popularity Yamada et al. (2016), and gold Wikipedia entity categories Gillick et al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.More specifically, we introduce a two stage approach for zero-shot linking (see Figure 1 for an overview), based on fine-tuned BERT architectures Devlin et al. (2019). In the first stage, we do retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions Humeau et al. (2019); Gillick et al. (2019).Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the mention and entity text, following Logeswaran et al. (2019). This overall approach is conceptually simple but highly effective, as we show through detailed experiments.We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:•Enriching entity representations by adding entity type and entity graph information;•Modeling coherence by jointly resolving mentions in a document;•Extending our work to other languages and other domains;•Joint models for mention detection and entity linking.				
690	paper_21	BLINK have two different versions, bi-encoding version and cross-encoding version. Is this true?	BLINK model is a two-stage method using two encoders: bi-encoder and cross-encoder.  With the qualitative analysis, the authors compared the BLINK with its bi-encoding version which uses a bi-encoder for candidate ranking instead of a cross-encoder, and showed that the cross-encoding version utilizing context information better than the bi-encoding version.  Therefore we can say that BLINK has two different versions.	As expected, the cross-encoder performs better than the bi-encoder on ranking. However, both models exceed state-of-the-art performance levels, demonstrating that the overall approach is highly effective. We observe that our model also performs well when we change the underlying Knowledgebase to full Wikipedia, and even without fine-tuning on the dataset. In Table 5 we show that our bi-encoder model is highly effective at retrieving relevant entities, where the underlying Knowledgebase is full Wikipedia.In the first example, we see that the bi-encoder mistakenly links “Ronaldo” to the Brazilian football player, while the cross-encoder is able to use context word “Juventus” to disambiguate. In the second example, the cross-encoder is able to identify from context that the sentence is describing art instead of fiction, where the bi-encoder failed. In the third example, the bi-encoder is able to find the correct entity “Ancient Greek,”; where the cross-encoder mistakenly links it to the entity “Ancient Greek philosophy,” likely because that the word “philosophers” is in context. We observe that cross-encoder is often better at utilizing context information than bi-encoder, but can sometimes make mistakes because of misleading context cues.Figure 1 shows our overall approach. The bi-encoder uses two independent BERT transformers to encode model context/mention and entity into dense vectors, and each entity candidate is scored as the dot product of these vectors. The candidates retrieved by the bi-encoder are then passed to the cross-encoder for ranking. The cross-encoder encodes context/mention and entity in one transformer, and applies an additional linear layer to compute the final score for each pair.After training our model on Wikipedia, we fine-tune the model on the TACKBP-2010 training dataset. We use the top 100 candidates retrieved by the bi-encoder as training examples for the cross-encoder, and chose hyper-parameters based on cross validation. We report accuracy results in Table 4. For ablation studies, we also report the following versions of our model:				
691	paper_21	Using cross-encoder is time-consuming but accurate. Is this true?	Since the cross-encoder has large memory consumption and compute footprint, it is time-consuming and not suitable for tasks that require fast inference.  However, it is relatively accurate compared to the bi-encoder, which is the reason the author utilized knowledge distillation so that they can obtain some accuracy gain from the cross-encoder.  Therefore, we can say that using a cross-encoder is time-consuming but accurate.	Due to its larger memory and compute footprint, we use the cross-encoder in a re-ranking stage, over a small set (≤100)\leq 100)≤ 100 ) of candidates retrieved with the bi-encoder. The cross-encoder is not suitable for retrieval or tasks that require fast inference.Finally, we do an extensive evaluation of the accuracy-speed trade-off inherent in our bi- and cross-encoder models.We show that the two stage methods scales well in a full Wikipedia setting, by linking against all the 5.9M Wikipedia entities for TACKBP-2010, while still outperforming existing model with much smaller candidate sets.We also show that bi-encoder linking is very fast with approximate nearest neighbor search (e.g. linking over 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation.We release our code and models, as well as a system to link entity mentions to all of Wikipedia (similar to TagME Ferragina and Scaiella (2011)).111Our code and models are available at https://github.com/facebookresearch/BLINK				
692	paper_21	When does cross-encoder powerful? Give an example.	Cross-encoder exerts its power for the cases requiring disambiguation with the given context.  Table 8 shows some examples where the cross-encoder can accurately identify and linked entities through the context while the bi-encoder failed. For example, the cross-encoder links "Ronaldo" to a corret person with the context of "Juventus", while the bi-encoder links to another football player that are Brazilian.  Another example shows that the cross-encoder can identify that the sentence is describing gothic art instead of gothic fiction. 	Table 8 presents some examples from our bi-encoder and cross-encoder model predictions, to provide intuition for how these two models consider context and mention for entity linking.In the first example, we see that the bi-encoder mistakenly links “Ronaldo” to the Brazilian football player, while the cross-encoder is able to use context word “Juventus” to disambiguate. In the second example, the cross-encoder is able to identify from context that the sentence is describing art instead of fiction, where the bi-encoder failed. In the third example, the bi-encoder is able to find the correct entity “Ancient Greek,”; where the cross-encoder mistakenly links it to the entity “Ancient Greek philosophy,” likely because that the word “philosophers” is in context. We observe that cross-encoder is often better at utilizing context information than bi-encoder, but can sometimes make mistakes because of misleading context cues.				
693	paper_21	They said that cross-encoder make mistakes sometimes. Give an example. This is provided on the paper.	In the third example of Table 8, the cross-encoder linked the mention to the wrong entity "Ancient Greek philosophy", which is likely because of a word "philosophers" in the context. 	Table 8 presents some examples from our bi-encoder and cross-encoder model predictions, to provide intuition for how these two models consider context and mention for entity linking.In the first example, we see that the bi-encoder mistakenly links “Ronaldo” to the Brazilian football player, while the cross-encoder is able to use context word “Juventus” to disambiguate. In the second example, the cross-encoder is able to identify from context that the sentence is describing art instead of fiction, where the bi-encoder failed. In the third example, the bi-encoder is able to find the correct entity “Ancient Greek,”; where the cross-encoder mistakenly links it to the entity “Ancient Greek philosophy,” likely because that the word “philosophers” is in context. We observe that cross-encoder is often better at utilizing context information than bi-encoder, but can sometimes make mistakes because of misleading context cues.				
694	paper_21	What TACKBP-2010 means?	TACKBP-2010 is the dataset for evaluating entity linking systems that are widely used in research in this field like "Khalife and Vazirgiannis (2018)" and "Raiman and Raiman (2018)".  This dataset is made in 2010 and contains the entities in the TAC Reference Knowledge Base which contains 818,741 entities with titles, descriptions, and other meta information.  This paper also used the TACKBP-2010 for fine-tuning the model.  While it is likely that TACKB and 2010 are from the name of the knowledge base and when the dataset was made, it is not clear what P stands for in TACKBP-2010. 	is widely used for evaluating entity linking systems Ji et al. (2010).444https://tac.nist.gov Following prior work, we measure in-KB accuracy (P@1). There are 1,074 and 1,020 annotated mention/entity pairs derived from 1,453 and 2,231 original news and web documents on training and evaluation dataset, respectively. All the entities are from the TAC Reference Knowledgebase which contains 818,741 entities with titles, descriptions and other meta info.After training our model on Wikipedia, we fine-tune the model on the TACKBP-2010 training dataset. We use the top 100 candidates retrieved by the bi-encoder as training examples for the cross-encoder, and chose hyper-parameters based on cross validation. We report accuracy results in Table 4. For ablation studies, we also report the following versions of our model:1.bi-encoder only: we use bi-encoder for candidate ranking instead of cross-encoder.2.Full Wikipedia: we use 5.9M Wikipedia articles as our entity Knowlegebase, instead of TACKBP Reference Knowledgebase.3.Full Wikipedia w/o finetune: same as above, without fine-tuning on the TACKBP-2010 training set.There are however many other cues that could potentially be added in future work. For example, Khalife and Vazirgiannis (2018) report 94.57\% precision on the TACKBP-2010 dataset. However, their method is based on the strong assumption that a gold fine-grained entity type is given for each mention (and they do not attempt to do entity type prediction). Indeed, if fine-grained entity type information is given by an oracle at test time, then Raiman and Raiman (2018) reports 98.6\% accuracy on TACKBP-2010, indicating that improving fine-grained entity type prediction would likely improve entity linking. Our results is achieved without gold fine-grained entity type information. Instead, our model learns representations of context, mention and entities based on text only.				
695	paper_21	Why author emphasized their model as “Zero-shot”?	In the zero-shot settings, the set of documents/mentions/entities from training data is not visible in test data, which means the information of the entity that should be linked at test time is not learned directly from the training set.  This setting is related to scalability, which is important for the entity linking tasks since there can be lots of possible entity candidates for each mention.  The proposed BERT-based models can deal with these settings and show their accuracy and efficiency in scale.	Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables Ganea and Hofmann (2017), incoming Wikipedia link popularity Yamada et al. (2016), and gold Wikipedia entity categories Gillick et al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use cross-encoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP.Gillick et al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale.We also study zero-shot entity linking Logeswaran et al. (2019). Here the document setup is the same, but the knowledge base is separated in training and test time. Formally, denote \mathcal{E}_{train} and \mathcal{E}_{test} to be the knowledge base in training and test, we require \mathcal{E}_{train}\cap\mathcal{E}_{test}=\emptyset.The set of text documents, mentions, and entity dictionary are separated in training and test so that the entities being linked at test time are unseen.				
696	paper_21	How we can say Wikia dataset is zero-shot dataset?	In the Wikia dataset, entities in the validation and test sets are from different domains than the train set.  This setting allows the model evaluation can be done in a zero-shot setting since the set of entities are separated in training and test so that the model can't see the entities when linked at the test time.	was constructed by Logeswaran et al. (2019) from Wikia.333https://www.wikia.com.The task is to link entity mentions in text to an entity dictionary with provided entity descriptions, in a set of domains. There are 49K, 10K, and 10K examples in the train, validation, test sets respectively.The entities in the validation and test sets are from different domains than the train set, allowing for evaluation of performance on entirely unseen entities. The entity dictionaries cover different domains and range in size from 10K to 100K entities.We also study zero-shot entity linking Logeswaran et al. (2019). Here the document setup is the same, but the knowledge base is separated in training and test time. Formally, denote \mathcal{E}_{train} and \mathcal{E}_{test} to be the knowledge base in training and test, we require \mathcal{E}_{train}\cap\mathcal{E}_{test}=\emptyset.The set of text documents, mentions, and entity dictionary are separated in training and test so that the entities being linked at test time are unseen.				
697	paper_21	How BLINK can achieved zero-shot linking?	The BLINK model used a two-stage approach for entity linking based on fine-tuned BERT architectures that first encode the mention context and entity text with the bi-encoder for the candidate retrieval and utilize the cross-encoder to score and rank them.  These pre-trained architectures are simple yet scalable and effective for entity link tasks without the help of task-specific heuristics or external knowledge.  The authors showed that BLINK can achieve state-of-the-art performance for the large-scale entity linking on the dataset with a zero-shot setup.	Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables Ganea and Hofmann (2017), incoming Wikipedia link popularity Yamada et al. (2016), and gold Wikipedia entity categories Gillick et al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.More specifically, we introduce a two stage approach for zero-shot linking (see Figure 1 for an overview), based on fine-tuned BERT architectures Devlin et al. (2019). In the first stage, we do retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions Humeau et al. (2019); Gillick et al. (2019).Each retrieved candidate is then examined more carefully with a cross-encoder that concatenates the mention and entity text, following Logeswaran et al. (2019). This overall approach is conceptually simple but highly effective, as we show through detailed experiments.We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:•Enriching entity representations by adding entity type and entity graph information;•Modeling coherence by jointly resolving mentions in a document;•Extending our work to other languages and other domains;•Joint models for mention detection and entity linking.Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use cross-encoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP.Gillick et al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale.				
698	paper_21	Why BLINK valuable?	The BLINK model can be said to be valuable since the model is simple yet scalable and effective compared to existing works.  The proposed BERT-based model can perform entity linking with large-scale and zero-shot setups, which is crucial in real-world use cases that often contain a lot of unseen entities.  BLINK also achieved a new state-of-the-art result for two zero-shot benchmarks by using only the provided text description without external knowledge, which shows the effectiveness of the proposed model.	Scale is a key challenge for entity linking; there are millions of possible entities to consider for each mention. To efficiently filter or rank the candidates, existing methods use different sources of external information, including manually curated mention tables Ganea and Hofmann (2017), incoming Wikipedia link popularity Yamada et al. (2016), and gold Wikipedia entity categories Gillick et al. (2019). In this paper, we show that BERT-based models set new state-of-the-art performance levels for large scale entity linking when used in a zero shot setup, where there is no external knowledge and a short text description provides the only information we have for each entity. We also present an extensive evaluation of the accuracy-speed trade-off inherent to large pre-trained models, and show is possible to achieve very efficient linking with modest loss of accuracy.Our two-stage approach achieves a new state-of-the-art result on TACKBP-2010, with an over 30% relative error reduction. By simply reading the provided text descriptions, we are able to outperform previous methods that included many extra cues such as entity name dictionaries and link popularity. We also improve the state of the art on existing zero-shot benchmarks, including a nearly 6 point absolute gain on the recently introduced Wikia corpus Logeswaran et al. (2019) and more than 7 point absolute gain on WikilinksNED Unseen-Mentions Onoe and Durrett (2019).We proposed a conceptually simple, scalable, and highly effective two stage approach for entity linking. We show that our BERT-based model outperforms IR methods for entity retrieval, and achieved new state-of-the-art results on recently introduced zero-shot entity linking dataset, WikilinksNED Unseen-Mentions dataset, and the more established TACKBP-2010 benchmark, without any task-specific heuristics or external entity knowledge. We present evaluations of the accuracy-speed trade-off inherent to large pre-trained models, and show that it is possible to achieve efficient linking with modest loss of accuracy.Finally, we show that knowledge distillation can further improve bi-encoder model performance.Future work includes:•Enriching entity representations by adding entity type and entity graph information;•Modeling coherence by jointly resolving mentions in a document;•Extending our work to other languages and other domains;•Joint models for mention detection and entity linking.Two recent results are most closely related to our work. Logeswaran et al. (2019) proposed the zero-shot entity linking task. They use cross-encoders for entity ranking, but rely on traditional IR-techniques for candidate generation and did not evaluate on large scale benchmarks such as TACKBP.Gillick et al. (2019) show that dense embeddings work well for candidate generation, but they did not do pre-training and included external category labels in their bi-encoder architectures, limiting their linking to entities in Wikipedia. Our approach can be seen as generalizing both of these lines of work, and showing for the first time that pre-trained zero-shot architectures are both highly accurate and computationally efficient at scale.				
702	paper_22	What is maximum spanning tree problem?	They lead to a maximum spanning tree problem in their global approach by proposing bidirectional connections between mentions.  However, we cannot know maximum spanning tree only with this information.	To solve EL in the general case, evenwhen the first mention does not have the correct entity, we propose bidirectional connections between mentions, thus leading to a maximum spanning tree problem in our Global approach.Here we define a score for a (sub)tree t, noted as \Phi_{\mathrm{tr}}(t):\Phi_{\mathrm{tr}}(t)=\sum_{(i,j)\in t}\Phi_{\mathrm{cl}}(u_{i},u_{j}),(7)where u_{i} and u_{j} are two connected nodes (i.e., root, candidate entities or spans) in t.For a ground truth cluster c\in C (with C being the set of all such clusters), with its set444For a single cluster annotation, indeed it is possible that multiple correct trees can be drawn. of correct subtree representations \mathcal{T}_{c}, we model the cluster’s likelihood with its subtree scores. We minimize the negative log-likelihood \mathcal{L} of all clusters:\displaystyle\mathcal{L}\displaystyle=-\log\frac{\prod_{c\in C}\sum_{t\in\mathcal{T}_{c}}\exp\big{(}\Phi_{\mathrm{tr}}(t)\big{)}}{\sum_{t\in\mathcal{T}_{\textit{all}}}\exp\big{(}\Phi_{\mathrm{tr}}(t)\big{)}}.(8)Naively enumerating all possible spanning trees (\mathcal{T}_{\textit{all}} or \mathcal{T}_{c}) implied by this equation is infeasible, since their number is exponentially large.We use the adapted Kirchhoff’s Matrix Tree Theorem(MTT; Koo et al. (2007); Tutte (1984))to solve this:the sum of the weights of the spanning trees in a directed graph rooted in r is equal to the determinant of the Laplacian matrix of the graph with the row and column corresponding to r removed (i.e., the minor of the Laplacian with respect to r). This way, eq. (8) can be rewritten as\displaystyle\mathcal{L}\displaystyle=-\log\frac{\prod_{c\in C}{\det\Big{(}\mathbf{\hat{L}}_{c}\big{(}\mathbf{\Phi_{\mathrm{cl}}}\big{)}\Big{)}}}{\det\Big{(}\mathbf{L}_{r}\big{(}\mathbf{\Phi_{\mathrm{cl}}}\big{)}\Big{)}},(9)where \mathbf{\Phi_{\mathrm{cl}}} is the weighted adjacency matrix of the graph, and \mathbf{L}_{r} is the minor of the Laplacian with respect to the root node r. An entry in the Laplacian matrix iscalculatedas\displaystyle\medmath{L_{i,j}=\begin{cases}\sum\limits_{k}\exp(\Phi_{\mathrm{cl}}(u_{k},u_{j}))&\text{if $i=j$}\\-\exp(\Phi_{\mathrm{cl}}(u_{i},u_{j}))&\text{otherwise}\end{cases}},(10)Similarly, \mathbf{\hat{L}}_{c} is a modified Laplacian matrix where the first row is replaced with the root r selection scores \Phi_{\mathrm{cl}}(r,u_{j}).For clarity, Appendix A presents a toy example with detailed steps to calculate the loss in eq. (9).				
703	paper_22	What is the difference between the local model and the global model?	Local model optimize the marginalized probability of the correct antecedents for each given span whereas global model overcomes inherent limitation by using bidirectional connections between mentions.	Our first approach (Local in Fig. 1(a)) is motivated by current state-of-the-artcoreference resolution models(Joshi et al., 2019; Wu et al., 2020)that predict a single antecedent for each span to resolve.We extend this architecture by also considering entity links as potential antecendents:in the example of Fig. 1, the mention “Alliance” can be either connected to its antecedent mention “NATO” or to any of its candidate links (Alliance or Alliance,_Ohio).While straightforward,this approach cannot solvecases where the first coreferenced mention does not include the correct entity in its candidate list(e.g., if the order of “NATO” and “Alliance” mentions in Fig. 1 would be reversed).We therefor propose a second approach, Global, which by construction overcomes this inherent limitation by using bidirectional connections between mentions.Because that implies cycles could be formed, we resort to solving a maximum spanning tree problem.Mentions that refer to the same entity form a cluster, represented as a subtree rooted by the single entity they link to.To encode the overall document’s clusters in a single spanning tree, we introduce a virtual root node(see Fig. 1(b)).222Coreference clusters without a linked entity, i.e., a NIL cluster, have a link of a mention directly to the root.We propose twomethods for joint coreference and EL.The first, Local, is motivated by end-to-end span-based coreference resolution models(Lee et al., 2017, 2018)that optimize the marginalized probability of the correct antecedents for each given span.We extend this local marginalization to include the span’s candidate entity links. Formally, themodeledprobability ofy(text span or candidate entity)being the antecedent of span s_{i} is:P_{\mathrm{cl}}(y|s_{i})=\dfrac{\exp\big{(}\Phi_{\mathrm{cl}}(s_{i},y)\big{)}}{\sum_{y^{\prime}\in\mathcal{Y}(s_{i})}\exp\big{(}\Phi_{\mathrm{cl}}(s_{i},y^{\prime})\big{)}},(2)where \mathcal{Y}(s_{i}) is the set of antecedent spans unified with the candidate entities for s_{i}. For antecedent spans \{s_{j}:j<i\} the score \Phi_{\mathrm{cl}} is defined as:\displaystyle\medmath{\Phi_{\mathrm{cl}}(s_{i},s_{j})=\Phi_{\mathrm{p}}(s_{i})+\Phi_{\mathrm{p}}(s_{j})+\Phi_{\mathrm{c}}(s_{i},s_{j})},(3)\displaystyle\medmath{\Phi_{\mathrm{c}}(s_{i},s_{j})=\mathrm{FFNN}_{C}([\textbf{g}_{i};\textbf{g}_{j};\textbf{g}_{i}\odot\textbf{g}_{j};\boldsymbol{\varphi}_{i,j}])},(4)where \boldsymbol{\varphi}_{i,j} is an embedding encoding the distance333Measured in number of spans, after pruning. between spans s_{i} and s_{j}.Similarly, for a particular candidate entity e_{j}, the score \Phi_{\mathrm{cl}} is:\displaystyle\Phi_{\mathrm{cl}}(s_{i},e_{j})=\Phi_{\mathrm{p}}(s_{i})+\Phi_{{\ell}}(s_{i},e_{j}),(5)\displaystyle\Phi_{\ell}(s_{i},e_{j})=\mathrm{FFNN}_{L}([\textbf{g}_{i};\textbf{e}_{j}]).(6)An example graph of mentions and entities with edges for which aforementioned scores \Phi_{\mathrm{cl}} would be calculated is sketched in Fig. 1(a).While simple, this approach failsto correctly solve EL when the correct entity is only present in the candidate lists of mention spans occurring later in the text (sinceearliermentions have no access to it).				
704	paper_22	What datasets provide important corner cases, which was mentioned by paper?	They considered DWIE (Zaporojets et al. , 2021) and AIDA (Hoffart et al. , 2011).  They only extend AIDA dataset by adding missing mention links becaue it does not contain coreference information.  Therefore, DWIE contains important corner cases.	We considered two datasets to evaluate our proposed models: DWIE (Zaporojets et al., 2021) and AIDA (Hoffart et al., 2011).Since AIDA essentially does not contain coreference information, we had to extend it by(i) adding missing mention links in order to make annotations consistent on the coreference cluster level, and(ii) annotating NIL coreference clusters.We note this extended dataset as AIDA{}^{+}. See Table 1 for the details.				
705	paper_22	Why author build their model jointly?	They build their model jointly to make model can use the correct candidates from other mentions in the cluster.	In order to tackle research question (Q3), we study the accuracy of our models on the important corner case that involves mentions without correct entity in their candidate lists.This is illustrated in Table 4, which focuses on such mentionsin clusters where at least one mention contains the correct entity in its candidate list.As expected, theStandalone model cannot link such mentions, as it is limited to the localcandidate list.In contrast, both our joint approaches can solve some of these cases by using the correct candidates from other mentions in the cluster, with a superior performance of our Global model compared to the Local one.				
706	paper_22	Give a situation that global model is necessary.	Global model is necessary when it should use correct candidates from other mentions in the cluster.	In order to tackle research question (Q3), we study the accuracy of our models on the important corner case that involves mentions without correct entity in their candidate lists.This is illustrated in Table 4, which focuses on such mentionsin clusters where at least one mention contains the correct entity in its candidate list.As expected, theStandalone model cannot link such mentions, as it is limited to the localcandidate list.In contrast, both our joint approaches can solve some of these cases by using the correct candidates from other mentions in the cluster, with a superior performance of our Global model compared to the Local one.				
707	paper_22	What is the difference between AIDA and AIDA+?	AIDA+ is extended AIDA by adding missing mention links.	We considered two datasets to evaluate our proposed models: DWIE (Zaporojets et al., 2021) and AIDA (Hoffart et al., 2011).Since AIDA essentially does not contain coreference information, we had to extend it by(i) adding missing mention links in order to make annotations consistent on the coreference cluster level, and(ii) annotating NIL coreference clusters.We note this extended dataset as AIDA{}^{+}. See Table 1 for the details.				
708	paper_22	Why author explained their model as end-to-end?	They explained their model as end-to-end since the model can jointly predict the mentions, entity links and coreference relations between them.	Our model takes as input(i) the full document text, and(ii) an alias table with entity candidates for each of the possible spans.Our end-to-end approach allows to jointly predict the mentions, entity links and coreference relations between them.				
709	paper_24	What are different types of categories in the FashionMNIST dataset?	categories are men , women , kids and neutral.	We use the front look thumbnail images of 70,000 unique products to build Fashion-MNIST. Those products come from different gender groups: men, women, kids and neutral. In particular, white-color products are not included in the dataset as they have low contrast to the background. The thumbnails (51\times 73) are then fed into the following conversion pipeline, which is visualized in Figure 1.				
710	paper_24	What is the distribution of images in the training and testing set of FashionMNIST dataset?	Training set has 6,000 example from each class.	Finally, the dataset is divided into a training and a test set. The training set receives a randomly-selected 6,000 examples from each class. Images and labels are stored in the same file format as the MNIST data set, which is designed for storing vectors and multidimensional matrices. The result files are listed in Table 1. We sort examples by their labels while storing, resulting in smaller label files after compression comparing to the MNIST. It is also easier to retrieve examples with a certain class label. The data shuffling job is therefore left to the algorithm developer.				
712	paper_24	Why is MNIST so popular?	The popularity is related to size which allows researchers to check and prototype their model.	The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.				
713	paper_24	Among MNIST and FashionMNIST, which dataset poses more challenging classification task?	MNIST provides more challenging classification task.	This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.				
714	paper_24	What is the issue with EMNIST dataset?	To be able to use it seamlessly one needs to not only extend the deep learning framework’s MNIST helpers but also change the underlying deep neural network to classify these extra classes.	We also looked at the EMNIST dataset provided by Cohen et al. (2017), an extended version of MNIST that extends the number of classes by introducing uppercase and lowercase characters. However, to be able to use it seamlessly one needs to not only extend the deep learning framework’s MNIST helpers, but also change the underlying deep neural network to classify these extra classes.				
715	paper_24	What was the process of creating FashionMNIST?	It is based on Zolando's website Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.  front and back looks, details, looks with model, and an outfit.	Fashion-MNIST is based on the assortment on Zalando’s website222Zalando is the Europe’s largest online fashion platform. http://www.zalando.com. Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762\times 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.For the class labels, we use the silhouette code of the product. The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando. Each product contains only one silhouette code. Table 2 gives a summary of all class labels in Fashion-MNIST with examples for each class.				
716	paper_24	What is Zalando?	Zalando’s website222Zalando is the Europe’s largest online fashion platform.	Fashion-MNIST is based on the assortment on Zalando’s website222Zalando is the Europe’s largest online fashion platform. http://www.zalando.com. Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762\times 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.				
718	paper_24	Which are the classification models used by the authors for benchmarking on the dataset?	all machine learning libraries (e.  scikit-learn) and deep learning frameworks (e.	The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.	None			
720	paper_24	How does the performance of ML algorithms on the FashionMNIST dataset compare to those on real world fashion images?	Images and labels are stored in the same file format as the MNIST data set, which is designed for storing vectors and multidimensional matrices.  The original picture has a light-gray background (hexadecimal color: #fdfdfd) and is stored in 762 imes 1000 JPEG format.	Fashion-MNIST is based on the assortment on Zalando’s website222Zalando is the Europe’s largest online fashion platform. http://www.zalando.com. Every fashion product on Zalando has a set of pictures shot by professional photographers, demonstrating different aspects of the product, i.e. front and back looks, details, looks with model and in an outfit. The original picture has a light-gray background (hexadecimal color: #fdfdfd) and stored in 762\times 1000 JPEG format. For efficiently serving different frontend components, the original picture is resampled with multiple resolutions, e.g. large, medium, small, thumbnail and tiny.Finally, the dataset is divided into a training and a test set. The training set receives a randomly-selected 6,000 examples from each class. Images and labels are stored in the same file format as the MNIST data set, which is designed for storing vectors and multidimensional matrices. The result files are listed in Table 1. We sort examples by their labels while storing, resulting in smaller label files after compression comparing to the MNIST. It is also easier to retrieve examples with a certain class label. The data shuffling job is therefore left to the algorithm developer.				
722	paper_24	What type of ML models have been successful on the FashionMNIST dataset?	the author talks about using various ML and DL models and getting the accuracy as well, but exactly which models are used has not been specified.	The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70,000 grayscale images in the size of 28\times 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. (2013); Ciregan et al. (2012).				
723	paper_24	What is the most widely used dataset in the deep learning comminity?	MNIST dataset is widely used in DL.	The MNIST dataset comprising of 10-class handwritten digits, was first introduced by LeCun et al. (1998) in 1998. At that time one could not have foreseen the stellar rise of deep learning techniques and their performance. Despite the fact that today deep learning can do so much the simple MNIST dataset has become the most widely used testbed in deep learning, surpassing CIFAR-10 (Krizhevsky and Hinton, 2009) and ImageNet (Deng et al., 2009) in its popularity via Google trends111https://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNet. Despite its simplicity its usage does not seem to be decreasing despite calls for it in the deep learning community.The reason MNIST is so popular has to do with its size, allowing deep learning researchers to quickly check and prototype their algorithms. This is also complemented by the fact that all machine learning libraries (e.g. scikit-learn) and deep learning frameworks (e.g. Tensorflow, Pytorch) provide helper functions and convenient examples that use MNIST out of the box.				
725	paper_24	What is the purpose of FashionMNIST?	The purpose of dataset is a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithms and create good benchmark.	This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70,000 grayscale images in the size of 28\times 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. (2013); Ciregan et al. (2012).	The purpose of dataset is a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking AI calculation and create good benchmark.	Tortured phrases	machine learning algorithm -> AI calculation	
726	paper_24	What is the MNIST dataset?	The MNIST dataset comprising of 10-class handwritten digits, was first introduced by LeCun et al.	The MNIST dataset comprising of 10-class handwritten digits, was first introduced by LeCun et al. (1998) in 1998. At that time one could not have foreseen the stellar rise of deep learning techniques and their performance. Despite the fact that today deep learning can do so much the simple MNIST dataset has become the most widely used testbed in deep learning, surpassing CIFAR-10 (Krizhevsky and Hinton, 2009) and ImageNet (Deng et al., 2009) in its popularity via Google trends111https://trends.google.com/trends/explore?date=all&q=mnist,CIFAR,ImageNet. Despite its simplicity its usage does not seem to be decreasing despite calls for it in the deep learning community.This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.				
727	paper_24	How do we get the silhoutte code for the class labels?	The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando.	For the class labels, we use the silhouette code of the product. The silhouette code is manually labeled by the in-house fashion experts and reviewed by a separate team at Zalando. Each product contains only one silhouette code. Table 2 gives a summary of all class labels in Fashion-MNIST with examples for each class.				
728	paper_29	Why did the author choose to use the standard COCO metrics for the comparison of Mask R-CNN to the state of the art on the COCO dataset ?	The standard COCO metrics were used for the comparison of Mask R-CNN to the state of the art on the COCO dataset, but the reason is not strongly discussed int paragraphs.	We perform a thorough comparison of Mask R-CNN to the state of the art along with comprehensive ablations on the COCO dataset [28]. We report the standard COCO metrics including AP (averaged over IoU thresholds), AP{}_{50}, AP{}_{75}, and AP{}_{S}, AP{}_{M}, AP{}_{L} (AP at different scales). Unless noted, AP is evaluating using mask IoU. As in previous work [5, 27], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k val images (minival). We also report results on test-dev [28].				
729	paper_29	What is the goal of this work ?	Vision community has rapidly improved object detection and semantic segmentation results over a short period of time.  In significant part, these gains have been driven by powerful basic systems, such as the Fast/Faster R-CNN and Fully Convolutional Network (FCN) frameworks.  The purpose of this effort is to provide a comparable enabling framework for instance segmentation.	The vision community has rapidly improved object detection and semantic segmentation results over a short period of time. In large part, these advances have been driven by powerful baseline systems, such as the Fast/Faster R-CNN [12, 36] and Fully Convolutional Network (FCN) [30] frameworks for object detection and semantic segmentation, respectively. These methods are conceptually intuitive and offer flexibility and robustness, together with fast training and inference time. Our goal in this work is to develop a comparably enabling framework for instance segmentation.				
730	paper_29	What is instance segmentation ?	Instance segmentation is a new type of computer vision task that aims to solve the problem of how to represent all objects in an image.  It combines elements from the classical computer vision tasks of object detection and localizing each object using a bounding box, and semantic segmentation.	Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection. Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.	Instance segmentation is a new type of pc imaginative and presicent task that aims to solve the problem of how to represent all objects in an image.  It combines elements from the classical computer vision tasks of object detection and localizing each object using a bounding box, and semantic segmentation.	Tortured phrases	computer vision -> pc imaginative and presicent	
731	paper_29	What are the outputs of Mask-RNN	Output of Mask -RNN are class label , bounding box and object mask.	Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask. Mask R-CNN is thus a natural and intuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.				
732	paper_29	What are the stage the Mask R-CNN consists of ??	Two stages are RPN and  In the second stage, in addition to predicting the class and box offset, Mask R-CNN also produces a binary mask for each RoI.	Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN). In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI. This is in contrast to most recent systems, where classification depends on mask predictions (e.g. [33, 10, 26]). Our approach follows the spirit of Fast R-CNN [12] that applies bounding-box classification and regression in parallel (which turned out to largely simplify the multi-stage pipeline of original R-CNN [13]).				
733	paper_29	What is the loss used during training of Faster R-CNN ?	For an RoI associated with ground-truth class k, L_{mask} is only defined on the k-th mask, where L is the average binary cross-entropy loss.  The classification loss L Cls and bounding-box loss L_Box .	Formally, during training, we define a multi-task loss on each sampled RoI as L=L_{cls}+L_{box}+L_{mask}. The classification loss L_{cls} and bounding-box loss L_{box} are identical as those defined in [12]. The mask branch has a Km^{2}-dimensional output for each RoI, which encodes K binary masks of resolution m\times m, one for each of the K classes. To this we apply a per-pixel sigmoid, and define L_{mask} as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, L_{mask} is only defined on the k-th mask (other mask outputs do not contribute to the loss).				
734	paper_29	What are the consequences of using class labels and box layouts ?	Using class labels and box layouts make collapsing into short output vector by fully connected layers inevitable.	A mask encodes an input object’s spatial layout. Thus, unlike class labels or box offsets that are inevitably collapsed into short output vectors by fully-connected (fc) layers, extracting the spatial structure of masks can be addressed naturally by the pixel-to-pixel correspondence provided by convolutions.				
735	paper_29	What are the hyperparameters used for inference ?	various hyperparameter are:learning rate of 0. 02 ,bounding-box NMS with a threshold of 0. 5 , floating-number mask output is  resized to the RoI size, and binarized at a threshold of 0.	At test time, the proposal number is 300 for the C4 backbone (as in [36]) and 1000 for FPN (as in [27]). We run the box prediction branch on these proposals, followed by non-maximum suppression [14]. The mask branch is then applied to the highest scoring 100 detection boxes. Although this differs from the parallel computation used in training, it speeds up inference and improves accuracy (due to the use of fewer, more accurate RoIs). The mask branch can predict K masks per RoI, but we only use the k-th mask, where k is the predicted class by the classification branch. The m\timesm floating-number mask output is then resized to the RoI size, and binarized at a threshold of 0.5.Models are trained on all COCO trainval35k images that contain annotated keypoints. To reduce overfitting, as this training set is smaller, we train using image scales randomly sampled from [640, 800] pixels; inference is on a single scale of 800 pixels. We train for 90k iterations, starting from a learning rate of 0.02 and reducing it by 10 at 60k and 80k iterations. We use bounding-box NMS with a threshold of 0.5. Other details are identical as in §3.1.				
737	paper_29	What is RoIPool used for in relation to Mask R-CNN ??	RoIPool performs coarse spatial quantization for feature extraction.  Quantization introduces misalignments between the RoI and the extracted features.  While this may not impact classification, it has a large negative effect on predicting pixel-accurate masks.	RoIPool [12] is a standard operation for extracting a small feature map (e.g., 7\times7) from each RoI. RoIPool first quantizes a floating-number RoI to the discrete granularity of the feature map, this quantized RoI is then subdivided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling). Quantization is performed, e.g., on a continuous coordinate x by computing [x/16], where 16 is a feature map stride and [\cdot] is rounding; likewise, quantization is performed when dividing into bins (e.g., 7\times7). These quantizations introduce misalignments between the RoI and the extracted features. While this may not impact classification, which is robust to small translations, it has a large negative effect on predicting pixel-accurate masks.In principle Mask R-CNN is an intuitive extension of Faster R-CNN, yet constructing the mask branch properly is critical for good results. Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is most evident in how RoIPool [18, 12], the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations. Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation.				
738	paper_29	What metrics should be used for comparison of Mask R-CNN to the state of the art on the COCO dataset ?	Metrics used for comparison are AP , multi-scale train/test, horizontal flip test, and online hard example mining (OHEM).	We perform a thorough comparison of Mask R-CNN to the state of the art along with comprehensive ablations on the COCO dataset [28]. We report the standard COCO metrics including AP (averaged over IoU thresholds), AP{}_{50}, AP{}_{75}, and AP{}_{S}, AP{}_{M}, AP{}_{L} (AP at different scales). Unless noted, AP is evaluating using mask IoU. As in previous work [5, 27], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k val images (minival). We also report results on test-dev [28].We compare Mask R-CNN to the state-of-the-art methods in instance segmentation in Table 1. All instantiations of our model outperform baseline variants of previous state-of-the-art models. This includes MNC [10] and FCIS [26], the winners of the COCO 2015 and 2016 segmentation challenges, respectively. Without bells and whistles, Mask R-CNN with ResNet-101-FPN backbone outperforms FCIS+++ [26], which includes multi-scale train/test, horizontal flip test, and online hard example mining (OHEM) [38]. While outside the scope of this work, we expect many such improvements to be applicable to ours.				
739	paper_29	What is the reason behind the Mask R-CNN outperforming all the winner of COCO 2015 - 2016 ??	The reason is that Faster R-CNN.  has two outputs for each candidate object, a class label, and a bounding-box offset.  to this, we add a third branch that outputs the object mask.  But the additional mask output is distinct from the class and box outputs, requiring extraction of a much finer spatial layout of an object.  This model performs better than the model presented due to RoIAlign but is 0. 9 points box AP lower than Mask R- CNN.	Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask. Mask R-CNN is thus a natural and intuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.In principle Mask R-CNN is an intuitive extension of Faster R-CNN, yet constructing the mask branch properly is critical for good results. Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is most evident in how RoIPool [18, 12], the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations. Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation.				
741	paper_29	Is it better to use class specific or class agnostic masks in general ?	Both  class specific or class agnostic masks in general is nearly as effective.	Our default instantiation predicts class-specific masks, i.e., one m\timesm mask per class. Interestingly, Mask R-CNN with class-agnostic masks (i.e., predicting a single m\timesm output regardless of class) is nearly as effective: it has 29.7 mask AP vs. 30.3 for the class-specific counterpart on ResNet-50-C4. This further highlights the division of labor in our approach which largely decouples classification and segmentation.				
743	paper_29	How is Mask R-CNN used to estimate human poses ?	A keypoint's position is represented as a one-hot mask, then adopt Mask R-CNN to predict K masks, one for each of K keypoint types (e. , left shoulder, right elbow) Each keypoint is seen as a single-hot binary mask, with minimum modification that may be done to identify instance-specific poses.	Our framework can easily be extended to human pose estimation. We model a keypoint’s location as a one-hot mask, and adopt Mask R-CNN to predict K masks, one for each of K keypoint types (e.g., left shoulder, right elbow). This task helps demonstrate the flexibility of Mask R-CNN.Finally, we showcase the generality of our framework via the task of human pose estimation on the COCO keypoint dataset [28]. By viewing each keypoint as a one-hot binary mask, with minimal modification Mask R-CNN can be applied to detect instance-specific poses. Mask R-CNN surpasses the winner of the 2016 COCO keypoint competition, and at the same time runs at 5 fps. Mask R-CNN, therefore, can be seen more broadly as a flexible framework for instance-level recognition and can be readily extended to more complex tasks.				
746	paper_29	What is the difference between Mask R-CNN and Faster R-CNN ?	Mask R-CNN has pixel-to-pixel alignment whereas Faster R-CNN doesn't.	Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask. Mask R-CNN is thus a natural and intuitive idea. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. Next, we introduce the key elements of Mask R-CNN, including pixel-to-pixel alignment, which is the main missing piece of Fast/Faster R-CNN.Our method, called Mask R-CNN, extends Faster R-CNN [36] by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and bounding box regression (Figure 1). The mask branch is a small FCN applied to each RoI, predicting a segmentation mask in a pixel-to-pixel manner. Mask R-CNN is simple to implement and train given the Faster R-CNN framework, which facilitates a wide range of flexible architecture designs. Additionally, the mask branch only adds a small computational overhead, enabling a fast system and rapid experimentation.In principle Mask R-CNN is an intuitive extension of Faster R-CNN, yet constructing the mask branch properly is critical for good results. Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is most evident in how RoIPool [18, 12], the de facto core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, we propose a simple, quantization-free layer, called RoIAlign, that faithfully preserves exact spatial locations. Despite being a seemingly minor change, RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation.				
747	paper_29	How can we solve the chllenges of image segmentation ?	In this paper, It is shown that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.  We use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances.  Given this, one might expect a complex method to be required to achieve good results.	Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection. Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.Most recently, Li et al. [26] combined the segment proposal system in [8] and object detection system in [11] for “fully convolutional instance segmentation” (FCIS). The common idea in [8, 11, 26] is to predict a set of position-sensitive output channels fully convolutionally. These channels simultaneously address object classes, boxes, and masks, making the system fast. But FCIS exhibits systematic errors on overlapping instances and creates spurious edges (Figure 6), showing that it is challenged by the fundamental difficulties of segmenting instances.Mask R-CNN outputs are visualized in Figures 2 and 5. Mask R-CNN achieves good results even under challenging conditions. In Figure 6 we compare our Mask R-CNN baseline and FCIS+++ [26]. FCIS+++ exhibits systematic artifacts on overlapping instances, suggesting that it is challenged by the fundamental difficulty of instance segmentation. Mask R-CNN shows no such artifacts.				
748	paper_3	Was the GPT3 model finetuned on Self-Instruct also finetuned only on 50k instances?	Based on the introduction, it appears as though the authors may have finetuned their GPT3-Self Instruct model with 82k samples.	To evaluate SELF-INSTRUCT empirically, we run this framework on GPT3 (Brown et al., 2020), which is a vanilla LM (§4). The iterative SELF-INSTRUCT process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs. We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (§4.2). On this resulting data, we build GPT3SELF-INST by fine-tuning GPT3 (i.e., the same model used for generating the instructional data). We evaluate GPT3SELF-INST in comparison to various other models on both typical NLP tasks included in SUPER-NATURALINSTRUCTIONS (Wang et al., 2022), and a set of new instructions that are created for novel usage of instruction-following models (§5). The SUPERNI results indicate that GPT3SELF-INST outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of InstructGPT001. Moreover, our human evaluation on the newly-created instruction set shows that GPT3SELF-INST demonstrates a broad range of instruction-following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind InstructGPT001.				
749	paper_3	Why is it crucial for the pipeline to identify whether the instruction represents a classification task? How are classification tasks particularly distinct or special?	The main reason why this is a crucial step is because the authors’ pipeline uses a different approach for classification tasks.  For non-classification tasks, the authors first prompt a language model to come up with the input fields require, then provide sample inputs, for which the language model generates outputs.  However, for classification tasks, the authors first generate the list of classes, and then require the model to provide an example for that instruction for each class.  They do this because the first approach, used for non-classification instructions, does not work well for unbalanced classes.  This step, of identifying classification tasks, is important since it is not possible to use the same generation technique for both classification and non-classification tasks effectively with the same generation method.	Our pipeline for generating the instruction data consists of four steps: 1) instruction generation, 2) identifying whether the instruction represents a classification task or not, 3) instance generation with the input-first or the output-first approach, and 4) filtering low-quality data.Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.333More concretely, we regard tasks that have a limited and small output label space as classification tasks.We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table 7.Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table 8.However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose an Output-first Approach for classification tasks, where we first gener-ate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in Table 9.4 We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.				
750	paper_3	What are the existing public datasets that contain instructions for tuning large language models?	PromptSource and Super-NaturalInstructions (also called "super-NI" in short) are two existing public datasets that contain instructions for tuning large language models. 	The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource (Bach et al., 2022) and Super-NaturalInstructions (Wang et al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T0 Bach et al. (2022); Sanh et al. (2022) and Tk-Instruct Wang et al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	PromptSource and Super-NaturalInstructions (also called "super-NI" in short) are two existing public datasets that contain instructions for tuning enormous language models. 	Tortured phrases	large language models -> enormous language models	
751	paper_3	What is the similarities and differences between the NaturalInstructions dataset and the SuperNaturalInstructions dataset?	This paper only mentions and discusses the SuperNaturalInstructions (or SuperNI) dataset, and does not explicitly discuss the NaturalInstructions dataset.  However, based on the naming, it is possible that both the Supe- NaturalInstructiuons and NaturalInstructions datasets are datasets that contain annotated instructional data to help large language models (LLMs) perform a wider range of specialized tasks.	Despite the comprehensiveness of SuperNI in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. Table 4 presents a small portion of the 252 tasks. The whole test set will be available upon request.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	This paper mentions and discusses the SuperNaturalInstructions (or SuperNI) dataset, and also  explicitly discuss the NaturalInstructions dataset.	Change concept	This paper mentions and discusses... and also	This paper only mentions and discusses the SuperNaturalInstructions (or SuperNI) dataset, and does not explicitly discuss the NaturalInstructions dataset.
752	paper_3	If LMs have been found to be biased and limited in creativity, how would LMs be able to create instructions that cover a greater diversity of tasks compared to humans who might be able to imagine possible tasks?	LMs might be able to create instructions covering a greater diversity of tasks since they are trained on a large corpus of material that encompasses the work of many humans.  Additionally, the authors' proposed approach, Self-Instruct, uses a bootstrapping phase where humans provide the first set of instructions, and a LM uses those as examples to generate more instructions.  Approaches such as these, combining the efforts of a human and a language model, might be one way to ensure LMs create a wider array of tasks.  However, the authors do acknowledge that LMs are prone to be biased towards commonly occurring sequences, at the cost of rarer sequences, meaning that this is an open research question.	Annotating large-scale instruction data can be challenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the labeled instances for each task.In this section, we detail our process for Self-Instruct, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself and then conducting instruction tuning with this generated data in order to align the language model to follow instructions better. This pipeline is depicted in Figure 1.Self-Instruct is based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context. This provides us with a way to grow the instruction data from a small set of seed human-written instructions.We propose to generate a diverse set of instructions in a bootstrapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors. For every step, we sample 8 task instructions from this pool as in-context examples.Of the 8 instructions, 6 are from the human-written tasks,and 2 are from the model-generated tasks in previous steps to promote diversity.The prompting template is shown in Table 6.To encourage diversity, a new instruction is added to the task pool only when its ROUGE-L overlap with any existing instruction is less than 0.7.We also exclude instructions that contain some specific keywords (e.g., images, pictures, graphs) that usually can not be processed by language models. When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.Despite the comprehensiveness of SuperNI in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. Table 4 presents a small portion of the 252 tasks. The whole test set will be available upon request.Self-Instruct depends on LMs, and it will inherit all the limitations that carry over with LMs.As recent studies have shown Razeghi et al. (2022); Kandpal et al. (2022), tail phenomena pose a serious challenge to the success of LMs. In other words, LMs’ largest gains correspond to the frequent uses of languages (head of the language use distribution), and there are minimal gains in the low-frequency contexts.Similarly, in the context of this work, it would not be surprising if the majority of the gains by Self-Instruct are skewed towardtasks or instructions that present more frequently in the pre-training corpus.As a consequence, the approach might show brittleness with respect to uncommon and creative instructions.				
753	paper_3	What type of new tasks was Self-Instruct able to generate which were not seen in previous human-created instruction datasets?	The paper mentions that their model is used to generate 52,000 instructions and 82,000 input-output pairs, for a wide range of tasks, which the authors have made publicly available.  It might be possible to find more information on the specific tasks that were created by looking at this synthetic dataset, but the paper itself does not contain concrete examples.	A series of recent works Zhou et al. (2022b); Ye et al. (2022); Singh et al. (2022); Honovich et al. (2022) generate instructions of a task given a few examples. While Self-Instruct also involves instruction generation, a major difference in our case is it is task-agnostic; we generate new tasks (instructions along with instances) from scratch.Annotating large-scale instruction data can be challenging for humans because it requires 1) creativity to come up with novel tasks and 2) expertise for writing the labeled instances for each task.In this section, we detail our process for Self-Instruct, which refers to the pipeline of generating tasks with a vanilla pretrained language model itself and then conducting instruction tuning with this generated data in order to align the language model to follow instructions better. This pipeline is depicted in Figure 1.To evaluate Self-Instruct empirically, we run this framework on GPT3 Brown et al. (2020), which is a vanilla LM (§4).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (§4.2).On this resulting data, we build GPT3{}_{\textsc{Self-Inst}} by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3{}_{\textsc{Self-Inst}} in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions Wang et al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (§5).The SuperNI results indicate thatGPT3{}_{\textsc{Self-Inst}} outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of \text{InstructGPT}_{\text{001}}. Moreover, our human evaluation on the newly-created instruction set shows that GPT3{}_{\textsc{Self-Inst}} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind \text{InstructGPT}_{\text{001}}.In summary, our contributions are: (1) Self-Instruct, a method for inducing instruction-following capabilitywith minimal human-labeled data;(2) We demonstrate its effectiveness via extensive instruction-tuning experiments;(3) We release a large synthetic dataset of 52K instructions and a set of manually-written novel tasks for building and evaluating future instruction-following models.				
754	paper_3	Beyond correctness, why did the authors not evaluate the actual quality, meaning, or usefulness of the generated instructions?	One reason to explain why the authors did not perform more comprehensive quality evaluation of the generated outputs is the difficulty in judging the output of the model.  Some tasks cannot be quickly verified by the average human (one example the authors provide for this is converting first-order logic into natural language - a task that only experts with the appropriate domain knowledge can perform).  However, despite this challenge, the authors do perform some analysis to gauge the overall quality of the generated samples.  They randomly select 200 instructions, and for each sample they examine the quality of one instance within each instruction.  Doing this evaluation reveals that their approach performs much better than a vanilla GPT3 (i.  a bare-bones GPT3 with nothing else).	So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table 2 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table 10 and Table 11 respectively.Evaluating models’ performance on this evaluation set of diverse tasks is extremely challenging because different tasks require different expertise. Indeed, many of these tasks cannot be measured by automatic metrics or even be judged by normal crowdworkers (e.g., writing a program, or converting first-order logic into natural language). To get a more faithful evaluation, we asked the authors of the instructions to judge model predictions. The evaluators were asked to rate the output based on whether it accurately and effectively completes the task. We implemented a four-level rating system for categorizing the quality of the models’ outputs, defined as follows:Figure 5 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \text{InstructGPT}_{\text{001}} (c.f. footnote 1), GPT3{}_{\textsc{Self-Inst}} is quite close in the performance—if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\textsc{Self-Inst}} is only 5% behind \text{InstructGPT}_{\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \text{InstructGPT}_{\text{002}} & \text{InstructGPT}_{\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et al. (2022).				
755	paper_3	How are T0 and Tk-INSTRUCT different?	T0 and Tk-Instruct are two related models, proposed in papers published in 2022.  T0 was proposed by Bach et al.  and Sanh et al, while Tk-Instruct was proposed by Wang et al.  Both these approaches used a large T5-model, with billions of parameters in their proposed approach, and both approaches were also trained using instructional datasets (PromptSource, Super-NaturalInstructions) that were annotated manually by humans.  The fact that they were published in the same year, use the same datasets, and probably try to achieve similar goals indicates that, either: (1) the work for both of these approaches was done concurrently (perhaps by different groups of authors) or (2) one work was inspired from the other and might have been an extension of that.  More details about either of these, including the differences are not discussed in detail in this paper, but can probably be found by referring the the respective papers mentioned for each of these.	The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource (Bach et al., 2022) and Super-NaturalInstructions (Wang et al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T0 Bach et al. (2022); Sanh et al. (2022) and Tk-Instruct Wang et al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.T0 and Tk-Instruct are two instruction-tuned models proposed in Sanh et al. (2022) and Wang et al. (2022) respectively, and are demonstrated to be able to follow instructions for many NLP tasks.Both of these models are finetuned from the T5 Raffel et al. (2020) checkpoints and are publicly available888https://huggingface.co/bigscience/T0999https://huggingface.co/allenai/tk-instruct-11b-def. For both of these models, we use their largest version with 11B parameters.				
756	paper_3	Was the performance difference between Self-Instruct training and SuperNI training significant?	While it does appear as though there is a measurable performance improvement from SuperNI to Self-Instruct, quantifying the impact and magnitude of that improvement is not straightforward.  Evaluations with ROGUE-L scores find that the absolute difference between both these methods is not very high, though additional information and context may be needed to judge the meaning of the absolute difference between these numbers.  The authors do claim that they outperform T0 or SuperNI by a large margin, which is strong evidence to suggest that the difference was indeed significant, but such claims must be taken with some grains of salt since authors are usually incentivized to show their models are the best.	To evaluate Self-Instruct empirically, we run this framework on GPT3 Brown et al. (2020), which is a vanilla LM (§4).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (§4.2).On this resulting data, we build GPT3{}_{\textsc{Self-Inst}} by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3{}_{\textsc{Self-Inst}} in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions Wang et al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (§5).The SuperNI results indicate thatGPT3{}_{\textsc{Self-Inst}} outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of \text{InstructGPT}_{\text{001}}. Moreover, our human evaluation on the newly-created instruction set shows that GPT3{}_{\textsc{Self-Inst}} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind \text{InstructGPT}_{\text{001}}.Additionally, to compare Self-Instruct training with other publicly available instruction tuning data, we further finetune GPT3 model with data from PromptSource and SuperNI, which are used to train the T0 and Tk-Instruct models. We call them T0 training and SuperNI training for short, respectively.To save the training budget, we sampled 50K instances (but covering all their instructions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from Wang et al. (2022) and our early experiments, reducing the number of instances per task does not degrade the model’s generalization performance to unseen tasks.We make the following observations from the results in Table 3.Self-Instructboosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation.Compared with other models that are not specifically trained for SuperNI, GPT3{}_{\textsc{Self-Inst}} achieves better performance than T0 or the GPT3 finetuned on the T0 training set, which takes tremendous human labeling efforts. Notably, GPT3{}_{\textsc{Self-Inst}} also nearly matches the performance of \text{InstructGPT}_{\text{001}}, which is trained with private user data and human-annotated labels.Models trained on SuperNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that Self-Instruct still brings in additional gains when combined with the SuperNI training set, proving its value as complementary data.Figure 5 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \text{InstructGPT}_{\text{001}} (c.f. footnote 1), GPT3{}_{\textsc{Self-Inst}} is quite close in the performance—if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\textsc{Self-Inst}} is only 5% behind \text{InstructGPT}_{\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \text{InstructGPT}_{\text{002}} & \text{InstructGPT}_{\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et al. (2022).We introduce Self-Instruct, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on Super-NaturalInstructions. This performance is on par with \text{InstructGPT}_{\text{001}}, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind \text{InstructGPT}_{\text{001}}. We hope Self-Instruct can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models.				
757	paper_3	If performance can be attributed to instruction style and formatting, why did the authors not test baselines that were not fine-tuned but had few-shot examples?	It is unclear why the authors did not test their baselines with few shot examples.  They acknowledge this as a problem, but they justify their position by pointing out that using both SuperNI and their approach together leads to increased performance gains - indicating that their approach (Self-Instruct) and SuperNI are complementary and can be used jointly to achieve higher performance than any one method being used on its' own.	Models trained on SuperNI training set still achieve better performance on its evaluation set, which we attribute to the similar instruction style and formatting. However, we show that Self-Instruct still brings in additional gains when combined with the SuperNI training set, proving its value as complementary data.				
758	paper_3	What is the PromptSource dataset and what kind of data does it include?	The PromptSource dataset is a dataset of human-written instructions for various tasks.  It probably includes instructions (and examples) for various sorts of tasks.  The current paper, which focuses on attempting to automate this dataset-generation process, does not discuss details on the kind of tasks the PromptSource dataset includes, but details on the dataset could probably be found in the paper Bach et al. , 2022.	The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource (Bach et al., 2022) and Super-NaturalInstructions (Wang et al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T0 Bach et al. (2022); Sanh et al. (2022) and Tk-Instruct Wang et al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.Additionally, to compare Self-Instruct training with other publicly available instruction tuning data, we further finetune GPT3 model with data from PromptSource and SuperNI, which are used to train the T0 and Tk-Instruct models. We call them T0 training and SuperNI training for short, respectively.To save the training budget, we sampled 50K instances (but covering all their instructions) for each dataset, which has a comparable size to the instruction data we generated. Based on the findings from Wang et al. (2022) and our early experiments, reducing the number of instances per task does not degrade the model’s generalization performance to unseen tasks.				
759	paper_3	How are the results of the input-first and output-first approach different?	If by results, you are referring to the outputs of these approaches, then the final output will look very similar - the output for each instance will consist of a tuple of an (input, output) where input and output follow the instructions for a certain task.  However, the order in which this output is generated will differ -- for example, in "input first" approach, the input is generated first, while in the output first case, the language model is conditioned to provide the required output.  On the other hand, if, by results, you are referring to "performance" of both of these approaches, the authors mention that the input first approach performs very poorly on classification instances, which is why they proposed the alternative approach of output-first generation for classification tasks. 	Because we need two different approaches for classification and non-classification tasks, we next identify whether the generated instruction represents a classification task or not.333More concretely, we regard tasks that have a limited and small output label space as classification tasks.We prompt vanilla GPT3 few-shot to determine this, using 12 classification instructions and 19 non-classification instructions from the seed tasks. The prompting template is shown in Table 7.Given the instructions and their task type, we generate instances for each instruction independently. This is challenging because it requires the model to understand what the target task is, based on the instruction, figure out what additional input fields are needed and generate them, and finally complete the task by producing the output. We found that pretrained language models can achieve this to a large extent when prompted with instruction-input-output in-context examples from other tasks.A natural way to do this is the Input-first Approach, where we can ask a language model to come up with the input fields first based on the instruction, and then produce the corresponding output. This generation order is similar to how models are used to respond to instruction and input, but here with in-context examples from other tasks. The prompting template is shown in Table 8.However, we found that this approach can generate inputs biased toward one label, especially for classification tasks (e.g., for grammar error detection, it usually generates grammatical input). Therefore, we additionally propose anOutput-first Approach for classification tasks, where we first generate the possible class labels, and then condition the input generation on each class label. The prompting template is shown in Table 9.444In this work, we use a fixed set of seed tasks for prompting the instance generation, and thus only generate a small number of instances per task in one round. Future work can use randomly sampled tasks to prompt the model to generate a larger number of instances in multiple rounds. We apply the output-first approach to the classification tasks identified in the former step, and the input-first approach to the remaining non-classification tasks.				
760	paper_3	Who were the authors of the instructions?	The authors of the paper created the first set of 175 instructions themselves.  After that, they used an iterative bootstrapping process in which they used GPT3 to create more tasks (and instructions).  In the end, they ended up with a dataset of 52k instructions.	The instruction data we want to generate contains a set of instructions \{I_{t}\}, each of which defines a task t in natural language. Each task has one or more input-output instances (X_{t},Y_{t}).A model M is expected to produce the output y, given the task instruction I_{t} and the instance input x: M(I_{t},x)=y,\;\mbox{for}\ (x,y)\in(X_{t},Y_{t}).Note that the instruction and instance input does not have a strict boundary in many cases. For example, “write an essay about school safety” can be a valid instruction that we expect models to respond to directly, while it can also be formulated as “write an essay about the following topic” as the instruction, and “school safety” as an instance input. To encourage the diversity of the data format, we allow such instructions that do not require additional input (i.e., x is empty).Self-Instruct is based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context. This provides us with a way to grow the instruction data from a small set of seed human-written instructions.We propose to generate a diverse set of instructions in a bootstrapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors. For every step, we sample 8 task instructions from this pool as in-context examples.Of the 8 instructions, 6 are from the human-written tasks,and 2 are from the model-generated tasks in previous steps to promote diversity.The prompting template is shown in Table 6.So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table 2 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table 10 and Table 11 respectively.In summary, our contributions are: (1) Self-Instruct, a method for inducing instruction-following capabilitywith minimal human-labeled data;(2) We demonstrate its effectiveness via extensive instruction-tuning experiments;(3) We release a large synthetic dataset of 52K instructions and a set of manually-written novel tasks for building and evaluating future instruction-following models.				
761	paper_3	How did the authors judge that the generated instructions were "meaningful"?	The authors judged a generated instruction as meaningful by seeing if it described a valid task.  Out of the 200 randomly sampled instructions, they found 92% of them described a valid task.	So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table 2 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table 10 and Table 11 respectively.				
762	paper_3	Why are the gains of instruction-tuning higher for larger models?	The authors theorize that their approach probably works better for larger models due to it's dependence on language models' inductive biases, but they offer no extra explanation or detail to clarify or explain their stance.	Because of Self-Instruct’s dependence on the inductive biases extracted from LMs, it might work best for larger models.If true, this may create barriers to access for those who may not have large computing resources.We hope future studies will carefully study the gains as a function of model size or various other parameters.It is worthwhile to note that instruction-tuning with human annotation also suffers from a similar limitation: gains of instruction-tuning are higher for larger model Wei et al. (2022).				
763	paper_3	How did they finetune GPT3?	The authors fine-tuned the GPT3 model via an OpenAI API.  The model, which is also called "da vinci", was dine-tuned by the authors for 2 epochs with a prompt loss weight set to zero. 	In this section, we apply our method for inducing instruction data to GPT3 as a case study. We use the largest GPT3 language model (“davinci” engine) accessed through the OpenAI API555https://openai.com/api/. The parameters for making queries are described in Appendix A.1. Here we present an overview of the generated data.With the instruction-generated instruction data, we conduct instruction tuning for the GPT3 model itself (the “davinci” engine). As we described in §3.3, we use various templates to concatenate the instruction and input, and train the model to generate the output. This finetuning is done through the OpenAI finetuning API777https://beta.openai.com/docs/guides/fine-tuning. We use the default hyper-parameters, except that we set the prompt loss weight to 0, and we train the model for 2 epochs. We refer the readers to Appendix A.2 for additional finetuning details. The resulting model is denoted as GPT3{}_{\textsc{Self-Inst}}.				
764	paper_3	What if one of the generated questions was poor in quality? Wouldn't this result in errors being propagated through the dataset to generate a high number of low quality instructions?	However, the authors do evaluate and compare the quality of generated samples and find that most samples are of high quality.  Even out of those samples that are not fully correct, most of them are atleast partly correct.  However, this is a concern with iterative algorithms such as this, a problem probably noted by the authors as an open research question.	Self-Instruct is based on a finding that large pretrained language models can be prompted to generate new and novel instructions when presented with some existing instructions in the context. This provides us with a way to grow the instruction data from a small set of seed human-written instructions.We propose to generate a diverse set of instructions in a bootstrapping fashion. We initiate the task pool with 175 tasks (1 instruction and 1 instance for each task) written by our authors. For every step, we sample 8 task instructions from this pool as in-context examples.Of the 8 instructions, 6 are from the human-written tasks,and 2 are from the model-generated tasks in previous steps to promote diversity.The prompting template is shown in Table 6.So far, we have shown the quantity and diversity of the generated data, but its quality remains uncertain. To investigate this, we randomly sample 200 instructions and randomly select 1 instance per instruction. We asked an expert annotator (co-author of this work) to label whether each instance is correct or not, in terms of the instruction, the instance input, and the instance output.Evaluation results in Table 2 show that most of the generated instructions are meaningful, while the generated instances may contain more noise (to a reasonable extent). However, we found that even though the generations may contain errors, most of them are still in the correct format or even partially correct, which can provide useful guidance for training models to follow instructions. We listed a number of good generations and bad generations in Table 10 and Table 11 respectively.A point of concern for the authors is the unintended consequences of this iterative algorithm, such as the amplification of problematic social biases (stereotypes or slurs about genders, races, etc.).Relatedly, one observed challenge in this process is the algorithm’s difficulty in producing balanced labels, which reflected models’ prior biases.We hope future work will hash out such details to better understand the pros and cons of the approach.				
765	paper_3	Why do the authors claim that human feedback may be less important when their experiments showed that InstructGPT, which had human-generated data, outperformed their model without human-generated data?	The authors claim that human feedback might not be essential since their model is able to almost meet the performance of InstructGPT despite not having access to private human-generated training data or manual annotations.  They claim that their model's success, of almost reaching InstructGPT performance with only a 5% gap is a strong indication that human data, while useful is not necessarily essential for teaching models how to follow instructions.  Additionally, they point out that their work is merely a beginning step in research in this field - while numerous studies have successfully used human annotations to improve performance, studies that attempt to remove the human requirement have not been as explored.  Also, the authors do acknowledge that the truth is somewhere in between the two extremes of (1) human instructional data is essential, or (2) such data is largely optional, and similar results can be achieved without it.	To evaluate Self-Instruct empirically, we run this framework on GPT3 Brown et al. (2020), which is a vanilla LM (§4).The iterative Self-Instruct process on this model leads to about 52k instructions, paired with about 82K instance inputs and target outputs.We observe that the resulting data provides a diverse range of creative tasks and over 50% of them have less than 0.3 ROUGE-L overlaps with the seed instructions (§4.2).On this resulting data, we build GPT3{}_{\textsc{Self-Inst}} by fine-tuning GPT3 (i.e., the same model used for generating the instructional data).We evaluate GPT3{}_{\textsc{Self-Inst}} in comparison to various other models on both typical NLP tasks included in Super-NaturalInstructions Wang et al. (2022), and a set of new instructions that are created for novel usage of instruction-following models (§5).The SuperNI results indicate thatGPT3{}_{\textsc{Self-Inst}} outperforms GPT3 (the original model) by a large margin (+33.1%) and nearly matches the performance of \text{InstructGPT}_{\text{001}}. Moreover, our human evaluation on the newly-created instruction set shows that GPT3{}_{\textsc{Self-Inst}} demonstrates a broad range of instruction following ability, outperforming models trained on other publicly available instruction datasets and leaving only a 5% gap behind \text{InstructGPT}_{\text{001}}.We evaluate \text{InstructGPT}_{\text{}} Ouyang et al. (2022),which is developed by OpenAI based on GPT3 to follow human instructions better and has been found by the community to have impressive zero-shot abilities.There are various generations of these models,where newer ones use more expansive data or algorithmic novelties101010https://beta.openai.com/docs/model-index-for-researchers.For our SuperNI experiments in §5.3, we only compare with their text-davinci-001 engine, because their newer engines are trained with the latest user data and are likely to already see the SuperNI evaluation set. For our human evaluation of these models on newly written instructions, we include their 001, 002 and 003 engines for completeness.We make the following observations from the results in Table 3.Self-Instructboosts the instruction-following ability of GPT3 by a large margin. The vanilla GPT3 model basically cannot follow human instructions at all. Upon manual analysis, we find that it usually generates irrelevant and repetitive text, and does not know when to stop generation.Compared with other models that are not specifically trained for SuperNI, GPT3{}_{\textsc{Self-Inst}} achieves better performance than T0 or the GPT3 finetuned on the T0 training set, which takes tremendous human labeling efforts. Notably, GPT3{}_{\textsc{Self-Inst}} also nearly matches the performance of \text{InstructGPT}_{\text{001}}, which is trained with private user data and human-annotated labels.Figure 5 provides the performance of GPT3 model and its instruction-tuned counterparts on this newly written instruction set. As anticipated, the vanilla GPT3 language model is largely unable to respond to instructions, and all instruction-tuned models demonstrate comparatively higher performance,Nonetheless, GPT3{}_{\textsc{Self-Inst}} (i.e., GPT3 model fine-tuned with Self-Instruct) outperforms those counterparts trained on T0 or SuperNI by a large margin, demonstrating the value of the generated data despite the noise.Compared with \text{InstructGPT}_{\text{001}} (c.f. footnote 1), GPT3{}_{\textsc{Self-Inst}} is quite close in the performance—if we count acceptable response with minor imperfections (Rating-3) as valid, GPT3{}_{\textsc{Self-Inst}} is only 5% behind \text{InstructGPT}_{\text{001}}. Lastly, our evaluation confirms the impressive instruction-following ability of \text{InstructGPT}_{\text{002}} & \text{InstructGPT}_{\text{003}} models. Although there are many factors behind this success, we conjecture that future work can largely benefit from improving the quality of our generated data by using human annotators or training a reward model to select better generations, similar to the algorithm used in Ouyang et al. (2022).A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.(H_{1})Human feedback is a necessary and indispensable aspect of instruction-tuning as LMs need to learn about issues that were not quite learned during pre-training.(H_{2})Human feedback is an optional aspect of instruction-tuning as LMs are already quite familiar with instructions from their pre-training. Observing the human feedback is merely a lightweight process for aligning their pre-training distribution/objective which might be replaceable with a different process.While the reality probably lies somewhere in between these two extremes, we conjecturethat it is closer to H_{2}, particularly for larger models.This intuition, that LMs already know much about language instructions, is a key motivation for Self-Instruct and is also supported by its empirical success.We introduce Self-Instruct, a task-agnostic method to improve the instruction-following capabilities of language models via its own generation of instruction data (instruction, input, and output samples) and bootstrapping with it. Our method conducts instruction-tuning of the original model on the pruned subset of generated samples. On experimenting with vanilla GPT3, we observe a 33% absolute improvement over the original model on Super-NaturalInstructions. This performance is on par with \text{InstructGPT}_{\text{001}}, which is trained with private user data and expensive human annotations. Furthermore, we curate a set of expert-written instructions for novel tasks. Human evaluation on this set shows that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind \text{InstructGPT}_{\text{001}}. We hope Self-Instruct can serve as the first step to align pretrained language models to follow human instructions, and future work can build on top of this data to improve instruction-following models.Additionally, despite the remarkable performance of models like \text{InstructGPT}_{\text{}} Ouyang et al. (2022), their construction process remains quite opaque.In particular, the role of data has remained understudied due to limited transparency and data released by major corporate entities behind these key models.Addressing such challenges necessitates the creation of a large-scale, public dataset covering a broad range of tasks.				
766	paper_3	What is the "verb-noun structure" and what does it show?	The "verb-noun structure" of a sentence is created by identifying the primary verb (action word) in a sentence and identify the corresponding noun that action is being performed on.  The authors use this structure to identify what kinds of tasks (i.  verbs/action words) are used along with specific kinds of nouns. 	To study what types of instructions are generated and how diverse they are, we identify the verb-noun structure in the generated instructions. We use the Berkeley Neural Parser666https://parser.kitaev.io/ (Kitaev and Klein, 2018; Kitaev et al., 2019) to parse the instructions, and then extract the verb that is closest to the root of the parse tree as well as its first direct noun object. 26,559 out of the 52,445 instructions contain such structure; other instructions usually contain more complex clauses (e.g., “Classify whether this tweet contains political content or not.”) or are framed as questions (e.g., “Which of these statements are true?”).We plot the top 20 most common root verbs and their top 4direct noun objects in Figure 4, which accounts for 14% of the entire set. Overall, we see quite diverse intents and textual formats in these instructions.				
767	paper_30	What is the motivation behind learning disentangled representations of data ?	The motivation behind learning disentangled representations of data is a desire to achieve interpretability particularly the decomposability of latent representations to admit intuitive explanations.  Another motivation of learning disentangled representations is to generalize framework of decomposition.	An oft-stated motivation for learning disentangled representations of data with deep generative models is a desire to achieve interpretability [5, 10]—particularly the decomposability [see §3.2.1 in 33] of latent representations to admit intuitive explanations.Most work has focused on capturing purely independent factors of variation [10, 7, 16, 25, 4, 57, 3, 8, 17, 15, 59], typically evaluating this using purpose-built, synthetic data [15, 17, 25], whose generative factors are independent by construction.In this work, we explored and analysed the fundamental characteristics of learning disentangled representations, and showed how these can be generalised to a more general framework of decomposition [33].We characterised the learning of decomposed latent representation with vaes in terms of the control of two factors:i) overlap in the latent space between encodings of different datapoints, andii) regularisation of the aggregate encoding distribution to the given prior, which encodes the structure one would wish for the latent space to have.				
768	paper_30	What is the aim of using VAEs ?	The aim to use VAEs is to explore their decomposition capability to fulfill latent encoding of overlapping data and aggregate encoding of this data conforming to desired structure.  The approaches that explore disentanglement in the context of  VAEs aims to achieve independence between the dimensions of the aggregate encoding.	Of particular relevance to this work are approaches that explore disentanglement in the context of vaes [17, 3, 51, 25, 8, 16].Here one aims to achieve independence between the dimensions of the aggregate encoding, typically defined asq_{\phi}(\bm{z})\triangleq\operatorname{{}\mathbb{E}}_{p_{\mathcal{D}}(\bm{x})}\left[q(\bm{z}|\bm{x})\right]\approx\frac{1}{n}\sum_{i}^{n}q(\bm{z}|\bm{x}_{i}).The significance of q_{\phi}(\bm{z}) is that it is the marginal distribution induced on the latents by sampling a datapoint and then using the encoder to sample an encoding given that datapoint. It can thus informally be thought of as the pushforward distribution for “sampling” representations in the latent space.We characterise the decomposition of latent spaces in vaes to be the fulfilment of two factors (as shown in Figure 1):a.An “appropriate” level of overlap in the latent space—ensuring that the range of latent values capable of encoding a particular datapoint is neither too small, nor too large.This is, in general, dictated by the level of stochasticity in the encoder: the noisier the encoding process is, the higher the number of datapoints which can plausibly give rise to a particular encoding.b.The aggregate encoding q_{{\phi}}\left(\bm{z}\right) matching the prior p\left(\bm{z}\right), where the latter expresses the desired dependency structure between latents.We posit a generalisation of disentanglement in vaes—decomposing their latent representations—that can help avoid such pitfalls.We characterise decomposition in vaes as the fulfilment of two factors:a) the latent encodings of data having an appropriate level of overlap, andb) the aggregate encoding of data conforming to a desired structure, represented through the prior.We emphasize that neither of these factors is sufficient in isolation: withoutan appropriate level of overlap, encodings can degrade to a lookup tablewhere the latents convey little information about data, and without the aggregate encoding of data following a desired structure, the encodings do not decompose as desired.				
770	paper_30	What is Disentanglement ?	Disentanglement refers to independence among features in a representation.  The idea stems back to traditional methods such as ICA and conventional autoencoders.  Disentanglement implicitly makes a choice of decomposition: that the latent features are independent of one another.  Much of the prior work in the field has either implicitly or explicitly presumed a slightly more ambitious definition of disentanglement than considered above: that it is a measure of how well one captures true factors of variation (which happen to be independent by construction for synthetic data), rather than just independent factors.  The commonly assumed notion of disentanglement is quite restrictive for complex models where the true generative factors are not independent, very large in number, or where it cannot be reasonably assumed that there is a well-defined set of “true” generative factors.	Much of the prior work in the field has either implicitly or explicitly presumed a slightly more ambitious definition of disentanglement than considered above: that it is a measure of how well one captures true factors of variation (which happen to be independent by construction for synthetic data), rather than just independent factors.After all, if we wish for our learned representations to be interpretable, it is necessary for the latent variables to take on clear-cut meaning.The commonly assumed notion of disentanglement is quite restrictive for complex models where the true generative factors are not independent, very large in number, or where it cannot be reasonably assumed that there is a well-defined set of “true” generative factors (as will be the case for many, if not most, real datasets).To this end, we introduce a generalization of disentanglement, decomposition, which at a high-level can be thought of as imposing a desired structure on the learned representations.This permits disentanglement as a special case, for which the desired structure is that q_{{\phi}}\left(\bm{z}\right) factors along its dimensions.Disentanglement implicitly makes a choice of decomposition: that the latent features are independent of one another.We make this explicit and exploit it to both provide improvement to disentanglement through judicious choices of structure in the prior, and to introduce a more general framework flexible enough to capture alternate, more complex, notions of decomposition such as sparsity, clustering, hierarchical structuring, or independent subspaces.Disentanglement, as typically employed in literature, refers to independence among features in a representation [5, 15, 18].Conceptually, however, it has a long history, far longer than we could reasonably do justice here, and is far from specific to vaes.The idea stems back to traditional methods such as ICA [58, 23] and conventional autoencoders [50], through to a range of modern approaches employing deep learning [47, 36, 9, 37, 1, 19, 11].				
771	paper_30	Why was decomposition introduced ?	Decomposition can be thought of as imposing a desired structure on the learned representations at a high level.  It is used to introduce a generalization of disentanglement.  In VAEs, it is used for two factors: : a) the latent encodings of data having an appropriate level of overlap, and b) the aggregate encoding of data conforming to a desired structure, represented through the prior.	The commonly assumed notion of disentanglement is quite restrictive for complex models where the true generative factors are not independent, very large in number, or where it cannot be reasonably assumed that there is a well-defined set of “true” generative factors (as will be the case for many, if not most, real datasets).To this end, we introduce a generalization of disentanglement, decomposition, which at a high-level can be thought of as imposing a desired structure on the learned representations.This permits disentanglement as a special case, for which the desired structure is that q_{{\phi}}\left(\bm{z}\right) factors along its dimensions.We posit a generalisation of disentanglement in vaes—decomposing their latent representations—that can help avoid such pitfalls.We characterise decomposition in vaes as the fulfilment of two factors:a) the latent encodings of data having an appropriate level of overlap, andb) the aggregate encoding of data conforming to a desired structure, represented through the prior.We emphasize that neither of these factors is sufficient in isolation: withoutan appropriate level of overlap, encodings can degrade to a lookup tablewhere the latents convey little information about data, and without the aggregate encoding of data following a desired structure, the encodings do not decompose as desired.				
772	paper_30	What are the factors of fullfilement for the decomposition of latent spaces in VAEs ?	The decomposition in vaes as the fulfilment of two factors: a) the latent encodings of data having an appropriate level of overlap—ensuring that the range of latent values capable of encoding a particular datapoint is neither too small, nor too large. This is, in general, dictated by the level of stochasticity in the encoder: the noisier the encoding process is, the higher the number of datapoints which can plausibly give rise to a particular encoding, and b) the aggregate encoding of data conforming to a desired structure, represented through the prior.	We characterise the decomposition of latent spaces in vaes to be the fulfilment of two factors (as shown in Figure 1):a.An “appropriate” level of overlap in the latent space—ensuring that the range of latent values capable of encoding a particular datapoint is neither too small, nor too large.This is, in general, dictated by the level of stochasticity in the encoder: the noisier the encoding process is, the higher the number of datapoints which can plausibly give rise to a particular encoding.b.The aggregate encoding q_{{\phi}}\left(\bm{z}\right) matching the prior p\left(\bm{z}\right), where the latter expresses the desired dependency structure between latents.We posit a generalisation of disentanglement in vaes—decomposing their latent representations—that can help avoid such pitfalls.We characterise decomposition in vaes as the fulfilment of two factors:a) the latent encodings of data having an appropriate level of overlap, andb) the aggregate encoding of data conforming to a desired structure, represented through the prior.We emphasize that neither of these factors is sufficient in isolation: withoutan appropriate level of overlap, encodings can degrade to a lookup tablewhere the latents convey little information about data, and without the aggregate encoding of data following a desired structure, the encodings do not decompose as desired.				
773	paper_30	What is the effect of having larger values of β ?	Large \beta is not universally beneficial for disentanglement, as the level of overlap can be increased too far.  Increasing \beta can reinforce existing inductive biases, wherein mean-field assumptions encourage representations which reduce dependence between the latent dimensions.  Increasing \beta increases the level of overlap in q_{{\phi}}\left(\bm{z}\right), as a consequence of increasing the encoder variance for individual datapoints.  When \beta is too large, the encoding of a datapoint loses meaning.  overly large values of \beta actually cause a mismatch between q_{{\phi}}\left(\bm{z}\right) and p\left(\bm{z}\right) (see top right of Figure 3).	Taken together, this implies that the \beta-vae’s ability to encourage disentanglement is predominantly through direct control over the level of overlap.It places no other direct constraint on the latents to disentangle (although in some cases, the annealed prior may inadvertently encourage better disentanglement), but instead helps avoid the pitfalls of inappropriate overlap.Amongst other things, this explains why large \beta is not universally beneficial for disentanglement, as the level of overlap can be increased too far.It should be noted, however, that the value of \beta can indirectly influence the level of disentanglement when using a mean-field assumption for the encoder distribution (i.e. restricting S_{\phi}(x) to be diagonal).As noted by Stühmer et al. [52], Rolinek et al. [49], increasing \beta can reinforce existing inductive biases, wherein mean-field assumptions encourage representations which reduce dependence between the latent dimensions [55].We see in Figure 3 that increasing \beta increases the level of overlap in q_{{\phi}}\left(\bm{z}\right), as a consequence of increasing the encoder variance for individual datapoints.When \beta is too large, the encoding of a datapoint loses meaning.Also, as a single datapoint encodes to a Gaussian distribution, q_{{\phi}}\left(\bm{z}|\bm{x}\right) is unable to match p\left(\bm{z}\right) exactly.Because q_{{\phi}}\left(\bm{z}|\bm{x}\right)\rightarrow q_{{\phi}}\left(\bm{z}\right) when \beta\rightarrow\infty, this in turn means thatoverly large values of \beta actually cause a mismatch between q_{{\phi}}\left(\bm{z}\right) and p\left(\bm{z}\right) (see top right of Figure 3).Increasing \alpha, instead always improved the match between q_{{\phi}}\left(\bm{z}\right) and p\left(\bm{z}\right).Here, the finiteness of the dataset and the choice of divergence results in an increase in overlap with increasing \alpha, but only up to the level required for a non-negligible overlap between the nearby datapoints: large values of \alpha did not cause the encodings to collapse to a mode.				
774	paper_30	Does β-VAE's objective control overlap ?	The \beta-vae objective to show that its contribution to disentangling is primarily through direct control of the level of overlap between encodings of the data, expressed by maximising the entropy of the encoding distribution.  \beta-vae’s ability to encourage disentanglement is predominantly through direct control over the level of overlap.  Hence, β-VAE's objective control overlap.	Taken together, this implies that the \beta-vae’s ability to encourage disentanglement is predominantly through direct control over the level of overlap.It places no other direct constraint on the latents to disentangle (although in some cases, the annealed prior may inadvertently encourage better disentanglement), but instead helps avoid the pitfalls of inappropriate overlap.Amongst other things, this explains why large \beta is not universally beneficial for disentanglement, as the level of overlap can be increased too far.This shows that the \beta-vae objective does not directly encourage latent variables to take on meaningful representations when using the standard choice of an isotropic Gaussian prior.In fact, on its own, it encourages latent representations which match the true generative factors no more than it encourages any arbitrary rotation of these factors, with such rotations capable of exhibiting strong correlations between latents.This view is further supported by our empirical results (see Figure 2), where we did not observe any gains in disentanglement (using the metric from Kim and Mnih [25]) from increasing \beta>0 with an isotropic Gaussian prior trained on the 2D Shapes dataset [38].It may also go some way to explaining the extremely high levels of variation we found in the disentanglement-metric scores between different random seeds at train time.Given the characterisation set out above, we now develop an objective that incorporates the effect of both factors (a) and (b).Our analysis of the \beta-vae tells us that its objective allows direct control over the level of overlap, i.e. factor Item a.To incorporate direct control over the regularisation Item b between the marginal posterior and the prior, we add a divergence term \mathbb{D}(q_{{\phi}}\left(z\right),p(\bm{z})), yielding\displaystyle\begin{split}\mathcal{L}_{\alpha,\beta}&(\bm{x})=\operatorname{{}\mathbb{E}}_{q_{{\phi}}\left(\bm{z}\mid\bm{x}\right)}\left[\log p_{{\theta}}\left(\bm{x}\mid\bm{z}\right)\right]\\&-\beta~{}\operatorname{\scalebox{0.95}{\text{KL}}}\left(q_{{\phi}}\left(\bm{z}\mid\bm{x}\right)\,\|\;p(\bm{z})\right)-\alpha~{}\mathbb{D}(q_{{\phi}}\left(\bm{z}\right),p(\bm{z}))\end{split}(7)allowing control over how much factors (a) and (b) are enforced, through appropriate setting of \beta and \alpha respectively.Connecting prior work on disentanglement to this framework, we analysed the \beta-vae objective to show that its contribution to disentangling is primarily through direct control of the level of overlap between encodings of the data, expressed by maximising the entropy of the encoding distribution.In the commonly encountered case of assuming an isotropic Gaussian prior and an independent Gaussian posterior, we showed that control of overlap is the only effect of the \beta-vae.Motivated by this observation, we developed an alternate objective for the elbo that allows control of the two factors of decomposability through an additional regularisation term.We then conducted empirical evaluations using this objective, targeting alternate forms of decompositions such as clustering and sparsity, and observed the effect of varying the extent of regularisation to the prior on the quality of the resulting clustering and sparseness of the learnt embeddings.The results indicate that we were successful in attaining those decompositions.To connect our framework with existing approaches for encouraging disentanglement, we provide a theoretical analysis of the \beta-vae [17, 3, 2], and show that it typically only allows control of latent overlap, the first decomposition factor.We show that it can be interpreted, up to a constant offset, as the standard vae objective with its prior annealed as p_{{\theta}}\left(\bm{z}\right)^{\beta} and an additional maximum entropy regularization of the encoder that increases the stochasticity of the encodings.Specialising this result for the typical choice of a Gaussian encoder and isotropic Gaussian prior indicates that the \beta-vae, up to a scaling of the latent space, is equivalent to the vae plus a regulariser encouraging higher encoder variance.Moreover, this objective is invariant to rotations of the learned latent representation, meaning that it does not, on its own, encourage the latent variables to take on meaningful representations any more than an arbitrary rotation of them.				
776	paper_30	How controlling the variance specifically affects the level of overlap?	Controlling the variance is effective means of achieving the desired overlap behaviour.  Increasing variance increases level of overlap.	We see in Figure 3 that increasing \beta increases the level of overlap in q_{{\phi}}\left(\bm{z}\right), as a consequence of increasing the encoder variance for individual datapoints.When \beta is too large, the encoding of a datapoint loses meaning.Also, as a single datapoint encodes to a Gaussian distribution, q_{{\phi}}\left(\bm{z}|\bm{x}\right) is unable to match p\left(\bm{z}\right) exactly.Because q_{{\phi}}\left(\bm{z}|\bm{x}\right)\rightarrow q_{{\phi}}\left(\bm{z}\right) when \beta\rightarrow\infty, this in turn means thatoverly large values of \beta actually cause a mismatch between q_{{\phi}}\left(\bm{z}\right) and p\left(\bm{z}\right) (see top right of Figure 3).Increasing \alpha, instead always improved the match between q_{{\phi}}\left(\bm{z}\right) and p\left(\bm{z}\right).Here, the finiteness of the dataset and the choice of divergence results in an increase in overlap with increasing \alpha, but only up to the level required for a non-negligible overlap between the nearby datapoints: large values of \alpha did not cause the encodings to collapse to a mode.However, when the encoder is unimodal with fixed form (in particularly the tail behaviour is fixed) and the prior is well-characterised by Euclidean distances, then these factors have a substantially reduced ability to vary for a given I(\bm{x};\bm{z}), which subsequently becomes a good characterisation of the level of overlap.When q_{{\phi}}\left(\bm{z}|\bm{x}\right) is Gaussian, controlling the variance of q_{{\phi}}\left(\bm{z}|\bm{x}\right) (with a fixed q_{{\phi}}\left(\bm{z}\right)) should similarly provide an effective means of achieving the desired overlap behaviour.As this is the most common use case, we leave the development of more a general definition of overlap to future work, simply noting that this is an important consideration when using flexible encoder distributions.				
777	paper_30	How does the form of the encoding distribution affect the ability to uncover true generative factors in VAEs?	The exact form of the encoding distribution affect the ability to uncover true generative factors in VAEs.  If we restrict the encoder variance to be isotropic and then use a two dimensional prior where one latent dimension has a much larger variance than the other. It will be possible to store more information in the prior dimension with higher variance.  Consequently, that dimension is more likely to correspond to an important factor of the generative process than the other.  However it cannot be concluded that form of encoding is a true factor of variation in generative process and choosing encoding distribution arbitrarily is a good choice.	The exact form of the encoding distribution is also important here.For example, imagine we restrict the encoder variance to be isotropic and then use a two dimensional prior where one latent dimension has a much larger variance than the other.It will be possible to store more information in the prior dimension with higher variance (as we can spread points out more relative to the encoder variance).Consequently, that dimension is more likely to correspond to an important factor of the generative process than the other.Of course, this does not imply that this is a true factor of variation in the generative process, but neither is the meaning that can be attributed to each dimension completely arbitrary.				
779	paper_30	How does the β-VAE objective contribute to disentangling in this work?	Tthe β-VAE objective to show that its contribution to disentangling is primarily through direct control of the level of overlap between encodings of the data, expressed by maximising the entropy of the encoding distribution.	Taken together, this implies that the \beta-vae’s ability to encourage disentanglement is predominantly through direct control over the level of overlap.It places no other direct constraint on the latents to disentangle (although in some cases, the annealed prior may inadvertently encourage better disentanglement), but instead helps avoid the pitfalls of inappropriate overlap.Amongst other things, this explains why large \beta is not universally beneficial for disentanglement, as the level of overlap can be increased too far.Connecting prior work on disentanglement to this framework, we analysed the \beta-vae objective to show that its contribution to disentangling is primarily through direct control of the level of overlap between encodings of the data, expressed by maximising the entropy of the encoding distribution.In the commonly encountered case of assuming an isotropic Gaussian prior and an independent Gaussian posterior, we showed that control of overlap is the only effect of the \beta-vae.Motivated by this observation, we developed an alternate objective for the elbo that allows control of the two factors of decomposability through an additional regularisation term.We then conducted empirical evaluations using this objective, targeting alternate forms of decompositions such as clustering and sparsity, and observed the effect of varying the extent of regularisation to the prior on the quality of the resulting clustering and sparseness of the learnt embeddings.The results indicate that we were successful in attaining those decompositions.To connect our framework with existing approaches for encouraging disentanglement, we provide a theoretical analysis of the \beta-vae [17, 3, 2], and show that it typically only allows control of latent overlap, the first decomposition factor.We show that it can be interpreted, up to a constant offset, as the standard vae objective with its prior annealed as p_{{\theta}}\left(\bm{z}\right)^{\beta} and an additional maximum entropy regularization of the encoder that increases the stochasticity of the encodings.Specialising this result for the typical choice of a Gaussian encoder and isotropic Gaussian prior indicates that the \beta-vae, up to a scaling of the latent space, is equivalent to the vae plus a regulariser encouraging higher encoder variance.Moreover, this objective is invariant to rotations of the learned latent representation, meaning that it does not, on its own, encourage the latent variables to take on meaningful representations any more than an arbitrary rotation of them.				
782	paper_31	What is the ImageNet classification dataset and how is it used in the experiments?	ImageNet 2012 classification dataset [36] that consists of 1000 classes.  The model are trained on the 1. 28 million training images, and evaluated on the 50k validation images.  We also obtain a final result on the 100k test images, reported by the test server.  We evaluate both top-1 and top-5 error rates.	We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evaluated on the 50k validation images. We also obtain a final result on the 100k test images, reported by the test server. We evaluate both top-1 and top-5 error rates.				
783	paper_31	What are identity shortcuts and projection shortcuts, and how are they used in the experiments?	The identity and projection shortcut are not defined in this section.  Identity shortcuts help in training, do not increase the complexity of the bottleneck architectures and solve degradation problem.  projection shortcuts are not essential for addressing the degradation problem so these are not considered.	Identity vs. Projection Shortcuts.We have shown that parameter-free, identity shortcuts help with training. Next we investigate projection shortcuts (Eqn.(2)).In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free (the same as Table 2 and Fig. 4 right); (B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections.Table 3 shows that all three options are considerably better than the plain counterpart.B is slightly better than A. We argue that this is because the zero-padded dimensions in A indeed have no residual learning. C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts. But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes. Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.				
784	paper_31	What is the motivation behind using residual learning in deep neural networks?	[Residual learning is used to solve degradation problem in deep neural networks.	["This reformulation is motivated ……….. zero to approach identity mappings."]				
786	paper_31	What is the purpose of using shortcut connections in residual learning?	[The purpose of using shortcut connections in residual learning is to ease the comparison between plain and residual networks.	["The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).."]				
787	paper_31	What is the purpose of the experiments described?	[Using imageNet/CIFAR-10  data set , classification is evaluated for plain/residual networks and compared with state of the art.	["section 4"]				
788	paper_31	How do the authors define plain networks and residual networks?	[Plain network do not use shortcut connections whereas in residual networks, shortcutconnection is added to each pair of filters.	["Next we evaluate 18-layer and 34- layer residual nets (ResNets)……. plain counterparts.."]				
790	paper_31	What are the main findings of the experiments with respect to plain networks and residual networks?	[Residual network reduce more training error than plain networks.  ResNet eases the optimization by providing faster convergence at the early stage when net is not deep than plain networks.	[" Second, compared to its plain counterpart, the 34-layer ResNet reduces the top-1 error by 3.5% ………. providing faster convergence at the early stage."]				
791	paper_31	How does the depth of the residual networks affect their performance in the experiments?	[The increased depth of Residual network improves performance of this network, lower training error and make it generalizable to data.	[" We have three major observations from Table 2 and Fig. 4. First, the situation is reversed with residual learning ……. from increased depth.."]				
793	paper_31	How do shortcut connections, or identity mapping, contribute to the effectiveness of the deep residual learning framework?	The use of identity mapping/shortcut connections can skip one or more layers from network without introducing any extra parameter or computational complexity.  The entire network can be implemented in same manner.	"The formulation of F(x)+ x can be realized ....without modifying the solvers."				
794	paper_31	What is the importance of normalization in deep networks and how does it contribute to the effectiveness of the deep residual learning framework?	Normalization is used in initialization and intermediate layers.  It contributes in effectiveness of deep residual learning framework with convergence for SGD.	"Driven by the significance of depth…..with backpropagation [22]."				
795	paper_31	How does the use of shortcut connections in this paper compare to previous practices and theories, such as those used in MLPs and highway networks?	The shortcut connections presented in this paper are parameter free and always learn residual functions.  All information is always passed through additional residual functions to be learned.  It also demonstrate the accuracy gain with increased depths unlike highway network.  The highway network uses shortcut connections with gating functions which are data dependent and have parameters.  It uses non-residual functions when identity shortcut is closed.  MLP uses shortcut connections with an additional linear layer from input to output for training.	Shortcut Connections.Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time.An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output [34, 49]. In [44, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients. The papers of [39, 38, 31, 47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections. In [44], an “inception” layer is composed of a shortcut branch and a few deeper branches.Concurrent with our work, “highway networks” [42, 43] present shortcut connections with gating functions [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free. When a gated shortcut is “closed” (approaching zero), the layers in highway networks represent non-residual functions. On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned. In addition, highway networks have not demonstrated accuracy gains with extremely increased depth (e.g., over 100 layers).				
796	paper_31	How does the use of identity shortcuts in this paper, which are parameter-free and always pass information through, differ from gated shortcuts used in highway networks that can be closed and block information?	The identity shortcuts presented in this paper differ from gated shortcuts because it don’t depend on data and always learn residual functions.	"Concurrent with our work, “highway networks” …….depth (e.g., over 100 layers)."				
797	paper_31	In what ways has the use of residual representations and shortcut connections improved the accuracy of deep neural networks in image recognition tasks?	The residual representation presents a powerful shallow representation for image retrieval tasks.	"In image recognition….than encoding original vectors."				
798	paper_31	How does the effectiveness of the residual representation method used in this paper compare to other methods such as Multigrid and hierarchical basis preconditioning in solving Partial Differential Equations?	Multigrid method reformulates the system as sub problems at multiple scales, where each sub problem is responsible for the residual solution between a coarser and a finer scale.  Hierarchical basis preconditioning relies on variables that represent residual vectors between two scales.	"In low-level vision and computer…………..or preconditioning can simplify the optimization."				
799	paper_32	What are the different approaches that have been used in face recognition technology over the years?	Starting from the 1990s to the 2000s, holistic approaches were the most prominent direction in face recognition, Later on, local-feature-based face recognition was introduced.  In the 2010s, shallow learning-based-local-descriptors were used.  In 2014, the DeepFace, a deep learning-based model, was invented.  And ever since, the state-of-the-art techniques were from deep learning-based approaches.	Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What’s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, “shallow” methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that “shallow” methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:We present the development of face processing methods in chronological order in Fig. 12. As we can see from the figure, most papers attempted to perform face processing by autoencoder model in 2014 and 2015; while 3D model played an important role in 2016. GAN [40] has drawn substantial attention from the deep learning and computer vision community since it was first proposed by Goodfellow et al. It can be used in different fields and was also introduced into face processing in 2017. GAN can be used to perform “one-to-many augmentation” and “many-to-one normalization”, and it broke the limit that face synthesis should be done under supervised way. Although GAN has not been widely used in face processing for training and recognition, it has great latent capacity for preprocessing, for example, Dual-Agent GANs (DA-GAN) [56] won the 1st places on verification and identification tracks in the NIST IJB-A 2017 FR competitions.				
800	paper_32	How has deep learning improved the accuracy of face recognition systems compared to traditional methods?	The traditional methods (i. e holistic approaches, local-feature-based methods, shallow learning) were the approaches used before the boom of deep learning based techniques.  They could achieve an accuracy of 95%, while the human accuracy was 97.  The rapid progress of deep learning methods quickly equaled human performance (DeepFace 97. 35%) and later on surpassed them with 99.  These methods could achieve such numbers by using bigger and bigger datasets and new architectures.	In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What’s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, “shallow” methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that “shallow” methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable. Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving. We present these mainstream architectures of deep FR in Fig. 9. In 2014, DeepFace [20] was the first to use a nine-layer CNN with several locally connected layers. With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet [38] used a large private dataset to train a GoogleNet. It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%. In the same year, VGGface [37] designed a procedure to collect a large-scale dataset from the Internet. It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet. VGGface obtains an accuracy of 98.95%. In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin. It boosts the achieves to 99.42% on LFW. In the end of 2017, a new large-scale face dataset, namely VGGface2 [39], was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession. Cao et al. first trained a SENet with MS-celeb-1M dataset [45] and then fine-tuned the model with VGGface2 [39], and achieved the SOTA performance on the IJB-A [41] and IJB-B [42].In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:				
801	paper_32	In what areas has face recognition technology been commonly used?	Face recognition is widely used in the military, finance, public security, and daily life.	Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.•Ubiquitous face recognition across applications and scenes. Deep face recognition has been successfully applied on many user-cooperated applications, but the ubiquitous recognition applications in everywhere are still an ambitious goal. In practice, it is difficult to collect and label sufficient samples for innumerable scenes in real world. One promising solution is to first learn a general model and then transfer it to an application-specific scene. While deep domain adaptation [145] has recently been applied to reduce the algorithm bias on different scenes [148], different races [173], general solution to transfer face recognition is largely open.				
802	paper_32	What are the three main modules of a face recognition system?	The 3 main modules are: face detection, facial landmark detector, and FR module.	Before a face image is fed to an FR module, face anti-spoofing, which recognizes whether the face is live or spoofed, is applied to avoid different types of attacks. Then, recognition can be performed. As shown in Fig. 3(c), an FR module consists of face processing, deep feature extraction and face matching, and it can be described as follows:As mentioned in [32], there are three modules needed for FR system, as shown in Fig. 3. First, a face detector is used to localize faces in images or videos. Second, with the facial landmark detector, the faces are aligned to normalized canonical coordinates. Third, the FR module is implemented with these aligned face images. We only focus on the FR module throughout the remainder of this paper.				
803	paper_32	How is deep learning used in the process of feature extraction in face recognition systems?	The deep learning model does feature extraction by processing the image through many layers and giving an encoding of the face that can be used to solve different FR tasks.  The early layers of a deep learning model tend to represent simple textures that continuously evolve into facial structures in the later layers.	For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a “zero-shot” learning task. Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest. Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set. For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 10^{6}-10^{7} IDs [38, 20].But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning [22]. Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2. It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience. The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes. In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye. In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction. Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of “one-to-many augmentation” to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.	The profound learning model does feature extraction by processing the image through many layers and giving an encoding of the face that can be used to solve different FR tasks.  The early layers of a deep learning model tend to represent simple textures that continuously evolve into facial structures in the later layers.	Tortured phrases	 (deep learning -> profound learning)	
804	paper_32	What are some challenges that researchers have encountered when generating 3D face images from 2D images?	The paper mainly talks about research on 3D face data reconstruction in terms of "one-to-many augmentation" methods.  Also, attempts to enlarge 3D face datasets with the same method are mentioned.  However, no challenges or specific issues during the process of 3D face reconstruction are included in the paper. 	3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network’s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of “one-to-many augmentation” to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.				
805	paper_32	How has the quality and diversity of generated 3D face images improved over time, and what advances have contributed to these improvements?	The paper only talks about the line of work on 3D image reconstruction, in other words, the methods and approaches to reconstruct 3D face images.  However, the quality and improvements that were made are not mentioned explicitly, but only referenced.	3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network’s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.				
806	paper_32	What are the main differences between the large-scale training datasets MS-Celeb-1M, VGGface2, and Megaface in terms of depth versus breadth, long tail distribution, and data engineering practices? 	In terms of depth vs breadth, VGGFace2 stands as the dataset with the most depth among the 3.  Although it contains a smaller number of subjects, it includes a large number of images per subject.  It lets models focus on intra-class variations such as lighting, age, pose, etc.  On the other hand, MS-Celeb-1M and Megface are datasets of breadth, in other words, they contain a lot of subjects but not particularly many images per subject.  They let models cover sufficiently different appearances of different people.  In terms of long tail distribution, VGGFace2 uses the head part of the distribution, Megaface uses the entire distribution to have as many images as possible, and different challenges of MS-Celeb-1M use the central or long tail part of the distribution.  In terms of data engineering, the Megaface and MS-Celeb-1M datasets are usually filtered and cleaned to increase the performance of trained models.  However, the leaders in this aspect are industry companies that hold computational capacity and data.	Depth v.s. breadth. These large training sets are expanded from depth or breadth. VGGface2 provides a large-scale training dataset of depth, which have limited number of subjects but many images for each subjects. The depth of dataset enforces the trained model to address a wide range intra-class variations, such as lighting, age, and pose. In contrast, MS-Celeb-1M and Mageface (Challenge 2) offers large-scale training datasets of breadth, which contains many subject but limited images for each subjects. The breadth of dataset ensures the trained model to cover the sufficiently variable appearance of various people. Cao et al. [39] conducted a systematic studies on model training using VGGface2 and MS-Celeb-1M, and found an optimal model by first training on MS-Celeb-1M (breadth) and then fine-tuning on VGGface2 (depth).Long tail distribution. The utilization of long tail distribution is different among datasets. For example, in Challenge 2 of MS-Celeb-1M, the novel set specially uses the tailed data to study low-shot learning; central part of the long tail distribution is used by the Challenge 1 of MS-Celeb-1M and images’ number is approximately limited to 100 for each celebrity; VGGface and VGGface2 only use the head part to construct deep databases; Megaface utilizes the whole distribution to contain as many images as possible, the minimal number of images is 3 per person and the maximum is 2469.Data engineering. Several popular benchmarks, such as LFW unrestricted protocol, Megaface Challenge 1, MS-Celeb-1M Challenge 1&2, explicitly encourage researchers to collect and clean a large-scale data set for enhancing the capability of deep neural network. Although data engineering is a valuable problem to computer vision researchers, this protocol is more incline to the industry participants. As evidence, the leaderboards of these experiments are mostly occupied by the companies holding invincible hardwares and data scales. This phenomenon may not be beneficial for developments of new models in academic community.				
807	paper_32	What are some methods that have been proposed to address the security vulnerabilities of deep face recognition systems, such as presentation attacks and adversarial attacks?	Despite some defense systems for face spoofing that use two-stream CNN, classification with CNN, and LSTM, there is still a presentation attack with a 3D model that can crack them.  Also, since the root cause of adversarial perturbations is unclear, methods like detecting and removing vulnerable layers are insufficient.  In order to mitigate all different types of FR attacks, continued research in this direction is necessary.	With the success of FR techniques, various types of attacks, such as face spoofing and adversarial perturbations, are becoming large threats. Face spoofing involves presenting a fake face to the biometric sensor using a printed photograph, worn mask, or even an image displayed on another electronic device. In order to defense this type of attack, several methods are proposed [211, 273, 274, 275, 276, 277, 278, 279]. Atoum et al. [211] proposed a novel two-stream CNN in which the local features discriminate the spoof patches that are independent of the spatial face areas, and holistic depth maps ensure that the input live sample has a face-like depth. Yang et al. [273] trained a CNN using both a single frame and multiple frames with five scales as input, and using the live/spoof label as the output. Taken the sequence of video frames as input, Xu et al. [274] applied LSTM units on top of CNN to obtain end-to-end features to recognize spoofing faces which leveraged the local and dense property from convolution operation and learned the temporal structure using LSTM units. Li et al. [275] and Patel et al. [276] fine-tuned their networks from a pretrained model by training sets of real and fake images. Jourabloo et al. [277] proposed to inversely decompose a spoof face into the live face and the spoof noise pattern. Adversarial perturbation is the other type of attack which can be defined as the addition of a minimal vector r such that with addition of this vector into the input image x, i.e. (x+r), the deep learning models misclassifies the input while people will not. Recently, more and more work has begun to focus on solving this perturbation of FR. Goswami et al. [280] proposed to detect adversarial samples by characterizing abnormal filter response behavior in the hidden layers and increase the network’s robustness by removing the most problematic filters. Goel et al. [281] provided an open source implementation of adversarial detection and mitigation algorithms. Despite of progresses of anti-attack algorithms, attack methods are updated as well and remind us the need to further increase security and robustness in FR systems, for example, Mai et al. [282] proposed a neighborly de-convolutional neural network (NbNet) to reconstruct a fake face using the stolen deep templates.•Security issues. Presentation attack [289], adversarial attack [280, 281, 290], template attack [291] and digital manipulation attack [292, 293] are developing to threaten the security of deep face recognition systems. 1) Presentation attack with 3D silicone mask, which exhibits skin-like appearance and facial motion, challenges current anti-sproofing methods [294]. 2) Although adversarial perturbation detection and mitigation methods are recently proposed [280][281], the root cause of adversarial vulnerability is unclear and thus new types of adversarial attacks are still upgraded continuously [295, 296]. 3) The stolen deep feature template can be used to recover its facial appearance, and how to generate cancelable template without loss of accuracy is another important issue. 4) Digital manipulation attack, made feasible by GANs, can generate entirely or partially modified photorealistic faces by expression swap, identity swap, attribute manipulation and entire face synthesis, which remains a main challenge for the security of deep FR.				
808	paper_32	What are some common methods used in facial recognition and how do they compare in terms of effectiveness and challenges?	There are broadly 4 methods that are used in FR.  Holistic methods were the first-ever attempt to solve the FR problem.  But they were too primitive and could not account for uncontrolled facial changes that did not fit its assumptions.  Then, there are local feature-based methods that try to extract invariant properties with local filtering.  However, although better than holistic methods, these are also short of complexity and capacity to address the vastness of facial appearances.  The first learning-based methods also lacked the robustness to address the non-linearity and complexity of FR.  Also, the efforts that were made in this direction were too scattered and there were no traditional methods that could address the FR problem entirely.  Afterward, deep learning based methods were introduced which surpassed the human ability in FR.  Unfortunately, these methods are prone to adversarial noises and need large datasets.  In particular, designing bigger and bigger datasets is becoming a privacy issue, that is yet to be handled by deep FR models.	Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms. The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly. What’s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise. There was no any integrated technique to address these unconstrained challenges integrally. As a result, with continuous efforts of more than a decade, “shallow” methods only improved the accuracy of the LFW benchmark to about 95% [15], which indicates that “shallow” methods are insufficient to extract stable identity feature invariant to real-world changes. Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:Despite the high accuracy in the LFW [23] and Megaface [44, 164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes. In this section, we present several special algorithms of FR.•Privacy-preserving face recognition. With the leakage of biological data, privacy concerns are raising nowadays. Facial images can predict not only demographic information such as gender, age, or race, but even the genetic information [297]. Recently, the pioneer works such as Semi-Adversarial Networks [298, 299, 285] have explored to generate a recognizable biometric templates that can hidden some of the private information presented in the facial images. Further research on the principles of visual cryptography, signal mixing and image perturbation to protect users’ privacy on stored face templates are essential for addressing public concern on privacy.•Understanding deep face recognition. Deep face recognition systems are now believed to surpass human performance in most scenarios [300]. There are also some interesting attempts to apply deep models to assist human operators for face verification [183][300]. Despite this progress, many fundamental questions are still open, such as what is the “identity capacity” of a deep representation [301]? Why deep neural networks, rather than humans, are easily fooled by adversarial samples? While bigger and bigger training dataset by itself cannot solve this problem, deeper understanding on these questions may help us to build robust applications in real world. Recently, a new benchmark called TALFW has been proposed to explore this issue [93].				
809	paper_32	How does the contrastive loss function work in deep face recognition?	In Euclidean space, contrastive loss, pulls together positive pairs and pushes apart negative pairs: \begin{split}\mathcal{L}=&y_{ij}max\left(0,\left\|f(x_{i})-f(x_{j})\right\|_{2}-\epsilon^{+}\right)\\&+(1-y_{ij})max\left(0,\epsilon^{-}-\left\|f(x_{i})-f(x_{j})\right\|_{2}\right)\end{split}(2)where y_{ij}=1 means x_{i} and x_{j} are matching samples and y_{ij}=0 means non-matching samples.  f(\cdot) is the feature embedding, \epsilon^{+} and \epsilon^{-} control the margins of the matching and non-matching pairs respectively.  However, the problem is that the margin parameters are difficult to choose.	Euclidean-distance-based loss is a metric learning method [118, 119] that embeds images into Euclidean space in which intra-variance is reduced and inter-variance is enlarged. The contrastive loss and the triplet loss are the commonly used loss functions. The contrastive loss [35, 21, 36, 61, 120] requires face image pairs, and then pulls together positive pairs and pushes apart negative pairs.\begin{split}\mathcal{L}=&y_{ij}max\left(0,\left\|f(x_{i})-f(x_{j})\right\|_{2}-\epsilon^{+}\right)\\&+(1-y_{ij})max\left(0,\epsilon^{-}-\left\|f(x_{i})-f(x_{j})\right\|_{2}\right)\end{split}(2)where y_{ij}=1 means x_{i} and x_{j} are matching samples and y_{ij}=0 means non-matching samples. f(\cdot) is the feature embedding, \epsilon^{+} and \epsilon^{-} control the margins of the matching and non-matching pairs respectively. DeepID2 [21] combined the face identification (softmax) and verification (contrastive loss) supervisory signals to learn a discriminative representation, and joint Bayesian (JB) was applied to obtain a robust embedding space. Extending from DeepID2 [21], DeepID2+ [35] increased the dimension of hidden representations and added supervision to early convolutional layers. DeepID3 [36] further introduced VGGNet and GoogleNet to their work. However, the main problem with the contrastive loss is that the margin parameters are often difficult to choose.Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them. Along with FaceNet [38] proposed by Google, Triplet loss [38, 37, 81, 80, 58, 60] was introduced into FR. It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity. FaceNet made \left\|f(x_{i}^{a})-f(x_{i}^{p})\right\|_{2}^{2}+\alpha<-\left\|f(x_{i}^{a})-f(x_{i}^{n})\right\|_{2}^{2} using hard triplet face samples, where x_{i}^{a}, x_{i}^{p} and x_{i}^{n} are the anchor, positive and negative samples, respectively, \alpha is a margin and f(\cdot) represents a nonlinear transformation embedding an image into a feature space. Inspired by FaceNet [38], TPE [81] and TSE [80] learned a linear projection W to construct triplet loss. Other methods optimize deep models using both triplet loss and softmax loss [59, 58, 60, 121]. They first train networks with softmax and then fine-tune them with triplet loss.				
810	paper_32	How do angular/cosine-margin-based loss functions improve the separability of learned features in deep face recognition? 	Angular/cosine-margin-based loss allows the separation of learned features with larger angular/cosine distance.  When the bias is removed and the weights are normalized in softmax, the outcome only depends on the angle between the weight and the features.  Based on the prior that the human face lies on a manifold, the angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypersphere manifold.	In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples. Angular/cosine-margin-based loss [104, 84, 105, 106, 108] is proposed to make learned features potentially separable with a larger angular/cosine distance. The decision boundary in softmax loss is \left(W_{1}-W_{2}\right)x+b_{1}-b_{2}=0, where x is feature vector, W_{i} and b_{i} are weights and bias in softmax loss, respectively. Liu et al. [104] reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss. They constrain b_{1}=b_{2}=0, so the decision boundaries for class 1 and class 2 become \left\|x\right\|\left(\left\|W_{1}\right\|cos\left(m\theta_{1}\right)-\left\|W_{2}\right\|cos\left(\theta_{2}\right)\right)=0 and \left\|x\right\|\left(\left\|W_{1}\right\|\left\|W_{2}\right\|cos\left(\theta_{1}\right)-cos\left(m\theta_{2}\right)\right)=0, respectively, where m is a positive integer introducing an angular margin, and \theta_{i} is the angle between W_{i} and x. Due to the non-monotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity. The loss function is defined as follows:\mathcal{L}_{i}=-log\left(\frac{e^{\left\|W_{yi}\right\|\left\|x_{i}\right\|\varphi(\theta_{yi})}}{e^{\left\|W_{yi}\right\|\left\|x_{i}\right\|\varphi(\theta_{yi})+\sum_{j\neq y_{i}}e^{\left\|W_{yi}\right\|\left\|x_{i}\right\|cos(\theta_{j})}}}\right)(4)where\varphi(\theta)=(-1)^{k}cos(m\theta)-2k,\theta\in\left[\frac{k\pi}{m},\frac{(k+1)\pi}{m}\right](5)Considering that L-Softmax is difficult to converge, it is always combined with softmax loss to facilitate and ensure the convergence. Therefore, the loss function is changed into: f_{y_{i}}=\frac{\lambda\left\|W_{y_{i}}\right\|\left\|x_{i}\right\|cos(\theta_{y_{i}})+\left\|W_{y_{i}}\right\|\left\|x_{i}\right\|\varphi(\theta_{y_{i}})}{1+\lambda}, where \lambda is a dynamic hyper-parameter. Based on L-Softmax, A-Softmax loss [84] further normalized the weight W by L2 norm (\left\|W\right\|=1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6). Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss. To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace [106] and CosFace [105], AMS loss [107] respectively introduced an additive angular/cosine margin cos(\theta+m) and cos\theta-m. They are extremely easy to implement without tricky hyper-parameters \lambda, and are more clear and able to converge without the softmax supervision. The decision boundaries under the binary classification case are given in Table V. Based on large margin, FairLoss [122] and AdaptiveFace [123] further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data. Compared to Euclidean-distance-based loss, angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold. However, Wang et al. [124] showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.				
811	paper_32	How does normalizing the features and weights in the softmax loss function improve the performance of deep face recognition systems? 	Normalizing the weights only can help angular/cosine-margin-based loss to make the learned features more discriminative, whereas normalizing only the learned features can help overcome the bias to the sample distribution of the softmax.  Since L2-norms of learned features with softmax loss were observed to be reflective of the quality of the face, making all the features have the same L2-norm may help to give similar attention to all different qualities of samples.  Lastly, normalizing both the weights and features has become a common method since it was proven necessary by Wang et al.  It is difficult to give more specific reasons why normalizing helps softmax loss, as most of the explanations come from referenced papers.	In 2017, in addition to reformulating softmax loss into an angular/cosine-margin-based loss as mentioned above, some works tries to normalize the features and weights in loss functions to improve the model performance, which can be written as follows:\hat{W}=\frac{W}{\left\|W\right\|},\hat{x}=\alpha\frac{x}{\left\|x\right\|}(6)where \alpha is a scaling parameter, x is the learned feature vector, W is weight of last fully connected layer. Scaling x to a fixed radius \alpha is important, as Wang et al. [110] proved that normalizing both features and weights to 1 will make the softmax loss become trapped at a very high value on the training set. After that, the loss function, e.g. softmax, can be performed using the normalized features and weights. Some papers [84, 108] first normalized the weights only and then added angular/cosine margin into loss functions to make the learned features be discriminative. In contrast, some works, such as [109, 111], adopted feature normalization only to overcome the bias to the sample distribution of the softmax. Based on the observation of [125] that the L2-norm of features learned using the softmax loss is informative of the quality of the face, L2-softmax [109] enforced all the features to have the same L2-norm by feature normalization such that similar attention is given to good quality frontal faces and blurry faces with extreme pose. Rather than scaling x to the parameter \alpha, Hasnat et al. [111] normalized features with \hat{x}=\frac{x-\mu}{\sqrt{\sigma^{2}}}, where \mu and \sigma^{2} are the mean and variance. Ring loss [117] encouraged the norm of samples being value R (a learned parameter) rather than explicit enforcing through a hard normalization operation. Moreover, normalizing both features and weights [110, 112, 115, 105, 106] has become a common strategy. Wang et al. [110] explained the necessity of this normalization operation from both analytic and geometric perspectives. After normalizing features and weights, CoCo loss [112] optimized the cosine distance among data features, and Hasnat et al. [115] used the von Mises-Fisher (vMF) mixture model as the theoretical basis to develop a novel vMF mixture loss and its corresponding vMF deep features.				
812	paper_32	What are some common metrics used to evaluate the performance of face recognition systems?	Common metrics that are used to evaluate the accuracy of FR models are: ROC, Acc.  for face verification.  rank-N and CMC curve for closed-set face identification, and DET curve for open-set face identification.  Also, the metrics for the complexity and size of FR models are important.  Lastly, the metrics that measure the age/gender/racial bias of the FR models are becoming necessary.	•We present a comparison and analysis on public available databases that are of vital importance for both model training and testing. Major FR benchmarks, such as LFW [23], IJB-A/B/C [41, 42, 43], Megaface [44], and MS-Celeb-1M [45], are reviewed and compared, in term of the four aspects: training methodology, evaluation tasks and metrics, and recognition scenes, which provides an useful reference for training and testing deep FR.In order to evaluate whether our deep models can solve the different problems of FR in real life, many testing datasets are designed to evaluate the models in different tasks, i.e. face verification, close-set face identification and open-set face identification. In either task, a set of known subjects is initially enrolled in the system (the gallery), and during testing, a new subject (the probe) is presented. Face verification computes one-to-one similarity between the gallery and probe to determine whether the two images are of the same subject, whereas face identification computes one-to-many similarity to determine the specific identity of a probe face. When the probe appears in the gallery identities, this is referred to as closed-set identification; when the probes include those who are not in the gallery, this is open-set identification.Face verification is relevant to access control systems, re-identification, and application independent evaluations of FR algorithms. It is classically measured using the receiver operating characteristic (ROC) and estimated mean accuracy (Acc). At a given threshold (the independent variable), ROC analysis measures the true accept rate (TAR), which is the fraction of genuine comparisons that correctly exceed the threshold, and the false accept rate (FAR), which is the fraction of impostor comparisons that incorrectly exceed the threshold. And Acc is a simplified metric introduced by LFW [23], which represents the percentage of correct classifications. With the development of deep FR, more accurate recognitions are required. Customers concern more about the TAR when FAR is kept in a very low rate in most security certification scenario. PaSC [179] reports TAR at a FAR of 10^{-2}; IJB-A [41] evaluates TAR at a FAR of 10^{-3}; Megaface [44, 164] focuses on TAR@10^{-6}FAR; especially, in MS-celeb-1M challenge 3 [163], TAR@10^{-9}FAR is reported.Close-set face identification is relevant to user driven searches (e.g., forensic identification), rank-N and cumulative match characteristic (CMC) is commonly used metrics in this scenario. Rank-N is based on what percentage of probe searches return the probe’s gallery mate within the top k rank-ordered results. The CMC curve reports the percentage of probes identified within a given rank (the independent variable). IJB-A/B/C [41, 42, 43] concern on the rank-1 and rank-5 recognition rate. The MegaFace challenge [44, 164] systematically evaluates rank-1 recognition rate function of increasing number of gallery distractors (going from 10 to 1 Million), the results of the SOTA evaluated on MegaFace challenge are listed in Table IX. Rather than rank-N and CMC, MS-Celeb-1M [45] further applies a precision-coverage curve to measure identification performance under a variable threshold t. The probe is rejected when its confidence score is lower than t. The algorithms are compared in term of what fraction of passed probes, i.e. coverage, with a high recognition precision, e.g. 95% or 99%, the results of the SOTA evaluated on MS-Celeb-1M challenge are listed in Table X.•Remaining challenges defined by non-saturated benchmark datasets. Three current major datasets, namely, MegaFace [44, 164] , MS-Celeb-1M [45] and IJB-A/B/C [41, 42, 43], are corresponding to large-scale FR with a very large number of candidates, low/one-shot FR and large pose-variance FR which will be the focus of research in the future. Although the SOTA algorithms can be over 99.9 percent accurate on LFW [23] and Megaface [44, 164] databases, fundamental challenges such as matching faces cross ages [181], poses [188], sensors, or styles still remain. For both datasets and algorithms, it is necessary to measure and address the racial/gender/age biases of deep FR in future research.•Pursuit of extreme accuracy and efficiency. Many killer-applications, such as watch-list surveillance or financial identity verification, require high matching accuracy at very low alarm rate, e.g. 10^{-9}. It is still a big challenge even with deep learning on massive training data. Meanwhile, deploying deep face recognition on mobile devices pursues the minimum size of feature representation and compressed deep network. It is of great significance for both industry and academic to explore this extreme face-recognition performance beyond human imagination. It is also exciting to constantly push the performance limits of the algorithm after it has already surpassed human.Open-set face identification is relevant to high throughput face search systems (e.g., de-duplication, watch list identification), where the recognition system should reject unknown/unseen subjects (probes who do not present in gallery) at test time. At present, there are very few databases covering the task of open-set FR. IJB-A/B/C [41], [42], [43] benchmarks introduce a decision error tradeoff (DET) curve to characterize the the false negative identification rate (FNIR) as function of the false positive identification rate (FPIR). FPIR measures what fraction of comparisons between probe templates and non-mate gallery templates result in a match score exceeding T . At the same time, FNIR measures what fraction of probe searches will fail to match a mated gallery template above a score of T . The algorithms are compared in term of the FNIR at a low FPIR, e.g. 1% or 10%, the results of the SOTA evaluated on IJB-A dataset as listed in Table XI.				
813	paper_32	Can the methods of "one-to-many augmentation" like data augmentation and 3D face reconstruction effectively improve the performance of deep FR algorithms in terms of accuracy and diversity of training data?	In terms of accuracy, the paper mentions a set of work done on assembled multi-input networks that used "one-to-many augmentation" methods to expand their dataset and achieve better results compared to individual networks.  In terms of diversity, all data augmentation, 3D face reconstruction, autoencoders, and especially GANs were found to be effective in generating faces in certain poses, angles, with different expressions, etc. 	Collecting a large database is extremely expensive and time consuming. The methods of “one-to-many augmentation” can mitigate the challenges of data collection, and they can be used to augment not only training data but also the gallery of test data. we categorized them into four classes: data augmentation, 3D model, autoencoder model and GAN model.Data augmentation. Common data augmentation methods consist of photometric transformations [75, 22] and geometric transformations, such as oversampling (multiple patches obtained by cropping at different scales) [22], mirroring [153], and rotating [154] the images. Recently, data augmentation has been widely used in deep FR algorithms [58, 59, 60, 35, 21, 36, 61, 62]. for example, Sun et al. [21] cropped 400 face patches varying in positions, scales, and color channels and mirrored the images. Liu et al. [58] generated seven overlapped image patches centered at different landmarks on the face region and trained them with seven CNNs with the same structure.3D model. 3D face reconstruction is also a way to enrich the diversity of training data. They utilize 3D structure information to model the transformation between poses. 3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles. There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR. In [47], Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data. Masi et al. [48] used generic 3D faces and rendered fixed views to reduce much of the computational effort. Richardson et al. [49] employed an iterative 3D CNN by using a secondary input channel to represent the previous network’s output as an image for reconstructing a 3D face as shown in Fig. 13. Dou et al. [51] used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction. Tran et al. [53] directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture. An et al. [156] synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.2) Assembled Networks : Multi-input networks. In “one-to-many augmentation”, multiple images with variety are generated from one image in order to augment training data. Taken these multiple images as input, multiple networks are also assembled together to extract and combine features of different type of inputs, which can outperform an individual network. In [58], [59], [60], [99], [34], [21], [35], assembled networks are built after different face patches are cropped, and then different types of patches are fed into different sub-networks for representation extraction. By combining the results of subnetworks, the performance can be improved. Other papers [96], [95], [98] used assembled networks to recognize images.Autoencoder model. Rather than reconstructing 3D models from a 2D image and projecting it back into 2D images of different poses, autoencoder models can generate 2D target images directly. Taken a face image and a pose code encoding a target pose as input, an encoder first learns pose-invariant face representation, and then a decoder generates a face image with the same identity viewed at the target pose by using the pose-invariant representation and the pose code. For example, given the target pose codes, multi-view perceptron (MVP) [55] trained some deterministic hidden neurons to learn pose-invariant face representations, and simultaneously trained some random hidden neurons to capture pose features, then a decoder generated the target images by combining pose-invariant representations with pose features. As shown in Fig. 14, Yim et al. [157] and Qian et al. [158] introduced an auxiliary CNN to generate better images viewed at the target poses. First, an autoencoder generated the desired pose image, then the auxiliary CNN reconstructed the original input image back from the generated target image, which guarantees that the generated image is identity-preserving. In [65], two groups of units are embedded between encoder and decoder. The identity units remain unchanged and the rotation of images is achieved by taking actions to pose units at each time step.GAN model. In GAN models, a generator aims to fool a discriminator through generating images that resemble the real images, while the discriminator aims to discriminate the generated samples from the real ones. By this minimax game between generator and discriminator, GAN can successfully generate photo-realistic images with different poses. After using a 3D model to generate profile face images, DA-GAN [56] refined the images by a GAN, which combines prior knowledge of the data distribution and knowledge of faces (pose and identity perception loss). CVAE-GAN [159] combined a variational auto-encoder with a GAN for augmenting data, and took advantages of both statistic and pairwise feature matching to make the training process converge faster and more stably. In addition to synthesizing diverse faces from noise, some papers also explore to disentangle the identity and variation, and synthesize new faces by exchanging identity and variation from different people. In CG-GAN [160], a generator directly resolves each representation of input image into a variation code and an identity code and regroups these codes for cross-generating, simultaneously, a discriminator ensures the reality of generated images. Bao et al. [161] extracted identity representation of one input image and attribute representation of any other input face image, then synthesized new faces by recombining these representations. This work shows superior performance in generating realistic and identity preserving face images, even for identities outside the training dataset. Unlike previous methods that treat classifier as a spectator, FaceID-GAN [162] proposed a three-player GAN where the classifier cooperates together with the discriminator to compete with the generator from two different aspects, i.e. facial identity and image quality respectively.				
814	paper_32	How has the evolution of network architectures in deep face recognition systems, such as the transition from AlexNet to ResNet and SENet, impacted the performance of these systems? 	As deep FR models followed the footsteps of deep object classification network architectures the performance got better, training got more controllable, and models got deeper.  It started with DeepFace which was based on AlexNet that achieved 97. 35% on the LFW benchmark.  Then came the FaceNet based on GoogleNet which achieved 99.  VGGFace with a procedure to collect the large-scale dataset on the web and using the VGGNet architecture reached 98.  SphereFace used ResNet to achieve 99. 42% accuracy.  After the new VGGFace2 dataset was introduced Cao et al.  trained a SENet-based architecture to achieve the SOTA for several datasets.	Mainstream architectures. The commonly used network architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly. We present the most influential architectures of deep object classification and deep face recognition in chronological order 111The time we present is when the paper was published. in Fig. 8. With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable. Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving. We present these mainstream architectures of deep FR in Fig. 9. In 2014, DeepFace [20] was the first to use a nine-layer CNN with several locally connected layers. With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet [38] used a large private dataset to train a GoogleNet. It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%. In the same year, VGGface [37] designed a procedure to collect a large-scale dataset from the Internet. It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet. VGGface obtains an accuracy of 98.95%. In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin. It boosts the achieves to 99.42% on LFW. In the end of 2017, a new large-scale face dataset, namely VGGface2 [39], was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession. Cao et al. first trained a SENet with MS-celeb-1M dataset [45] and then fine-tuned the model with VGGface2 [39], and achieved the SOTA performance on the IJB-A [41] and IJB-B [42].In 2014, DeepFace [20] achieved the SOTA accuracy on the famous LFW benchmark [23], approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images. Inspired by this work, research focus has shifted to deep-learning-based approaches, and the accuracy was dramatically boosted to above 99.80% in just three years. Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols. Therefore, it is of great significance to review the breakthrough and rapid development process in recent years. There have been several surveys on FR [24, 25, 26, 27, 28] and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR [29], 3D FR [28], pose-invariant FR [30][31]. Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays. This survey focuses only on recognition problem, and one can refer to Ranjan et al. [32] for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. [33] for a survey of face alignment. Specifically, the major contributions of this survey are as follows:				
815	paper_32	How do feature-based methods work in face recognition?	The only feature-based method that is mentioned in the paper is the local-feature-based methods from the 2000s of Gabor and LBP that tried local filtering to extract invariant properties.  But they were too rigid and lacked distinctiveness and compactness.	Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life. FR has been a long-standing research topic in the CVPR community. In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach [1]. The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted. The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace [2][3][4], manifold [5][6][7], and sparse representation [8][9][10][11]. This idea dominated the FR community in the 1990s and 2000s. However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions. In the early 2000s, this problem gave rise to local-feature-based FR. Gabor [12] and LBP [13], as well as their multilevel and high-dimensional extensions [14][15][16], achieved robust performance through some invariant properties of local filtering. Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness. In the early 2010s, learning-based local descriptors were introduced to the FR community [17][18][19], in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness. However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.				
816	paper_32	Can adaptive-architecture networks be used in other tasks besides face recognition?	Adaptive-architecture networks have been successfully applied to various tasks like image classification, semantic segmentation, and more.	Adaptive-architecture networks. Considering that designing architectures manually by human experts are time-consuming and error-prone processes, there is growing interest in adaptive-architecture networks which can find well-performing architectures, e.g. the type of operation every layer executes (pooling, convolution, etc) and hyper-parameters associated with the operation (number of filters, kernel size and strides for a convolutional layer, etc), according to the specific requirements of training and testing data. Currently, neural architecture search (NAS) [130] is one of the promising methodologies, which has outperformed manually designed architectures on some tasks such as image classification [131] or semantic segmentation [132]. Zhu et al. [88] integrated NAS technology into face recognition. They used reinforcement learning [133] algorithm (policy gradient) to guide the controller network to train the optimal child architecture. Besides NAS, there are some other explorations to learn optimal architectures adaptively. For example, conditional convolutional neural network (c-CNN) [89] dynamically activated sets of kernels according to modalities of samples; Han et al. [90] proposed a novel contrastive convolution consisted of a trunk CNN and a kernel generator, which is beneficial owing to its dynamistic generation of contrastive kernels based on the pair of faces being compared.				
817	paper_32	What are the main loss functions that have been explored for improving deep FR methods and how have they evolved over time?	There are 3 categories of loss functions for FR: Euclidean-distance-based loss, angular/cosine-margin-based loss, and softmax loss variations.  Initially, cross-entropy softmax loss was used, then some models tried using Euclidean-distance-based loss functions which started from contrastive loss and triplet loss.  However, due to their instability, the center loss and its variants (range loss, center-invariant loss) were introduced.  With a better understanding of loss functions for FR angular/cosine-margin-based loss functions were used.  It began with a reformulation of a softmax loss called L-Softmax, later A-Softmax appeared which adopted the L-Softmax idea but tried normalizing the weights.  Afterward, there were several improvements such as ArcFace, CosFace, and AMS which facilitated the convergence, while Fairloss and AdaptiveFace dealt with unbalanced data.  Lastly, there are different variations of softmax that try to normalize the L2-norms (L2-softmax, Ring loss), the weights, the features, or both weights and features (CoCo loss and vMF mixture loss).	Inheriting from the object classification network such as AlexNet, the initial Deepface [20] and DeepID [34] adopted cross-entropy based softmax loss for feature learning. After that, people realized that the softmax loss is not sufficient by itself to learn discriminative features, and more researchers began to explore novel loss functions for enhanced generalization ability. This becomes the hottest research topic in deep FR research, as illustrated in Fig. 5. Before 2017, Euclidean-distance-based loss played an important role; In 2017, angular/cosine-margin-based loss as well as feature and weight normalization became popular. It should be noted that, although some loss functions share the similar basic idea, the new one is usually designed to facilitate the training procedure by easier parameter or sample selection.•A systematic review on the evolution of the network architectures and loss functions for deep FR is provided. Various loss functions are categorized into Euclidean-distance-based loss, angular/cosine-margin-based loss and softmax loss and its variations. Both the mainstream network architectures, such as Deepface [20], DeepID series [34, 35, 21, 36], VGGFace [37], FaceNet [38], and VGGFace2 [39], and other architectures designed for FR are covered.In this paper, we provide a comprehensive survey of deep FR from both data and algorithm aspects. For algorithms, mainstream and special network architectures are presented. Meanwhile, we categorize loss functions into Euclidean-distance-based loss, angular/cosine-margin-based loss and variable softmax loss. For data, we summarize some commonly used datasets. Moreover, the methods of face processing are introduced and categorized as “one-to-many augmentation” and “many-to-one normalization”. Finally, the special scenes of deep FR, including video FR, 3D FR and cross-age FR, are briefly introduced.1) Euclidean-distance-based Loss : Euclidean-distance-based loss is a metric learning method [118], [119] that embeds images into Euclidean space in which intra-variance is reduced and inter-variance is enlarged. The contrastive loss and the triplet loss are the commonly used loss functions. The contrastive loss [35], [21], [36], [61], [120] requires face image pairs, and then pulls together positive pairs and pushes apart negative pairs. L =yij max (0, ‖f (xi) − f (xj )‖2 − +) + (1 − yij )max (0, − − ‖f (xi) − f (xj )‖2 ) (2) where yij = 1 means xi and xj are matching samples and yij = 0 means non-matching samples. f (·) is the feature embedding, + and − control the margins of the matching and non-matching pairs respectively. DeepID2 [21] combined the face identification (softmax) and verification (contrastive loss) supervisory signals to learn a discriminative representation, and joint Bayesian (JB) was applied to obtain a robust embedding space. Extending from DeepID2 [21], DeepID2+ [35] increased the dimension of hidden representations and added supervision to early convolutional layers. DeepID3 [36] further introduced VGGNet and GoogleNet to their work. However, the main problem with the contrastive loss is that the margin parameters are often difficult to choose.Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them. Along with FaceNet [38] proposed by Google, Triplet loss [38], [37], [81], [80], [58], [60] was introduced into FR. It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity. FaceNet made ‖f (xa i ) − f (xp i )‖2 2 + α < − ‖f (xa i ) − f (xn i )‖2 2 using hard triplet face samples, where xa i , xp i and xn i are the anchor, positive and negative samples, respectively, α is a margin and f (·) represents a nonlinear transformation embedding an image into a feature space. Inspired by FaceNet [38], TPE [81] and TSE [80] learned a linear projection W to construct triplet loss. Other methods optimize deep models using both triplet loss and softmax loss [59], [58], [60], [121]. They first train networks with softmax and then fine-tune them with triplet loss.However, the contrastive loss and triplet loss occasionally encounter training instability due to the selection of effective training samples, some paper begun to explore simple alternatives. Center loss [101] and its variants [82], [116], [102] are good choices for reducing intra-variance. The center loss [101] learned a center for each class and penalized the distances between the deep features and their corresponding class centers. This loss can be defined as follows: LC = 1 2 m∑ i=1 ‖xi − cyi ‖2 2 (3) where xi denotes the i-th deep feature belonging to the yi-th class and cyi denotes the yi-th class center of deep features. To handle the long-tailed data, a range loss [82], which is a variant of center loss, is used to minimize k greatest range’s harmonic mean values in one class and maximize the shortest interclass distance within one batch. Wu et al. [102] proposed a center-invariant loss that penalizes the difference between each center of classes. Deng et al. [116] selected the farthest intraclass samples and the nearest inter-class samples to compute a margin loss. However, the center loss and its variants suffer from massive GPU memory consumption on the classification layer, and prefer balanced and sufficient training data for each identity.2) Angular/cosine-margin-based Loss : In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples. Angular/cosine-margin-based loss [104], [84], [105], [106], [108] is proposed to make learned features potentially separable with a larger angular/cosine distance. The decision boundary in softmax loss is (W1 − W2) x + b1 − b2 = 0, where x is feature vector, Wi and bi are weights and bias in softmax loss, respectively. Liu et al. [104] reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss. They constrain b1 = b2 = 0, so the decision boundaries for class 1 and class 2 become ‖x‖ (‖W1‖ cos (mθ1) − ‖W2‖ cos (θ2)) = 0 and ‖x‖ (‖W1‖ ‖W2‖ cos (θ1) − cos (mθ2)) = 0, respectively, where m is a positive integer introducing an angular margin, and θi is the angle between Wi and x. Due to the nonmonotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity. The loss function is defined as follows: Li = −log ( e‖Wyi‖‖xi‖φ(θyi) e‖Wyi‖‖xi‖φ(θyi)+∑ j6 =yi e‖Wyi‖‖xi‖cos(θj ) ) (4) where φ(θ) = (−1)kcos(mθ) − 2k, θ ∈ [ kπ m , (k + 1)π m ] (5) Considering that L-Softmax is difficult to converge, it is always combined with softmax loss to facilitate and ensure the convergence. Therefore, the loss function is changed into: fyi = λ‖Wyi ‖‖xi‖cos(θyi )+‖Wyi ‖‖xi‖φ(θyi ) 1+λ , where λ is a dynamic hyper-parameter. Based on L-Softmax, A-Softmax loss [84] further normalized the weight W by L2 norm (‖W ‖ = 1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6). Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss. To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace [106] and CosFace [105], AMS loss [107] respectively introduced an additive angular/cosine margin cos(θ + m) and cosθ − m. They are extremely easy to implement without tricky hyperparameters λ, and are more clear and able to converge without the softmax supervision. The decision boundaries under the binary classification case are given in Table V. Based on large margin, FairLoss [122] and AdaptiveFace [123] further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data. Compared to Euclidean-distance-based loss, angular/cosine-margin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold. However, Wang et al. [124] showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.3) Softmax Loss and its Variations : In 2017, in addition to reformulating softmax loss into an angular/cosine-margin-based loss as mentioned above, some works tries to normalize the features and weights in loss functions to improve the model performance, which can be written as follows: ˆW = W ‖W ‖ , ˆx = α x ‖x‖ (6) where α is a scaling parameter, x is the learned feature vector, W is weight of last fully connected layer. Scaling x to a fixed radius α is important, as Wang et al. [110] proved that normalizing both features and weights to 1 will make the softmax loss become trapped at a very high value on the training set. After that, the loss function, e.g. softmax, can be performed using the normalized features and weights.Some papers [84], [108] first normalized the weights only and then added angular/cosine margin into loss functions to make the learned features be discriminative. In contrast, some works, such as [109], [111], adopted feature normalization only to overcome the bias to the sample distribution of the softmax. Based on the observation of [125] that the L2-norm of features learned using the softmax loss is informative of the quality of the face, L2-softmax [109] enforced all the features to have the same L2-norm by feature normalization such that similar attention is given to good quality frontal faces and blurry faces with extreme pose. Rather than scaling x to the parameter α, Hasnat et al. [111] normalized features with ˆx = x−μ√σ2 , where μ and σ2 are the mean and variance. Ring loss [117] encouraged the norm of samples being value R (a learned parameter) rather than explicit enforcing through a hard normalization operation. Moreover, normalizing both features and weights [110], [112], [115], [105], [106] has become a common strategy. Wang et al. [110] explained the necessity of this normalization operation from both analytic and geometric perspectives. After normalizing features and weights, CoCo loss [112] optimized the cosine distance among data features, and Hasnat et al. [115] used the von Mises-Fisher (vMF) mixture model as the theoretical basis to develop a novel vMF mixture loss and its corresponding vMF deep features.				
818	paper_32	What are some of the specific challenges that FR models face in real-world applications and how have researchers attempted to address these challenges through the design of specialized algorithms? 	Cross-pose FR is still a challenging problem for existing algorithms and over 10% decrease in accuracy was observed in frontal-frontal to frontal-profile verification.  Techniques like DREAM and PIM were employed to perform frontalization in the deep face and learn pose-invariant representations.  Cross-age FR is also a natural problem as facial appearance changes over time.  There were attempts to synthesize images from the same age group with a generative probabilistic model and conditional GANs were used to generate an identity-preserved face with a target age.  Further, local manifold adaptation (LMA) and pyramidal adversarial discriminator approaches were tried to deal with the imperfect preservation of identities of GAN-synthesized images.  Alternatively, decomposing the identity and age from each other was another direction.  Latent identity analysis (LIA) and decomposing in a spherical coordinate system are some methods from that direction.  Lastly, CNN fine-tuning, siamese deep network, feature extraction, and deep learning with CNN were some of the notable approaches.  Makeup FR is another real-world problem that needs a solution, as makeup can drastically change the appearance of the subject.  Bi-level adversarial network (BLAN) was used to generate non makeup images from makeup images.  Fine-tuning the triplet network with a small makeup dataset was another try.  In particular, facial disguise is a big issue for FR as people can either want to hide their identity or impersonate another one.  Identity hiding increases intra-class variation, while impersonation decreases inter-class distinction.  Using DCNN and finding the transformation matrix with PCA for face disguise, fine-tuning models with disguised faces, hard example mining, and learning the representation of images in colors, shapes, and textures are some of the attempts to solve the issue.  NIR-VIS FR is needed to match the NIS images, (near-infrared spectrum) that usually come from surveillance contexts to VIS (visible light spectrum) images, as most of the available datasets contain VIS images.  Transferring from VIS to NIR with fine-tuning, transforming NIR images to VIS with CNN, using the siamese network for each VIS and NIR respectively, dividing the network into NIR, VIS, and NIR-VIS layers to learn modality-invariant features, embedding cross-spectral face hallucination and discriminative features, and low-rank relevance and cross-modal ranking are some of the methods that were used to solve the issue.  Low-resolution FR needs addressing, although deep models are mostly robust to such cases.  Mapping low and high-resolution faces into the same space with CNN, using face semantic information and local structural constraints to restore the shape and detail of the images are notable approaches in this direction.  Photo-sketch FR can help find suspects effectively.  Approaches usually either use transfer learning to directly match photos to sketches or perform image-to-image translation (image to sketch or sketch to an image).  For the first type, training with images of faces and fine-tuning with sketches is one of the attempts.  For the second type, branched fully convolution network (BFCN) and lately, GAN architectures were used to translate the image to sketch or vice versa.  In many real-world scenarios low-shot FR is needed where only a few data points are available.  Researchers tried to either synthesize more data or learn more meaningful features.  3D models, GANs, data augmentation, hybrid classifiers, and normalization are some of the attempts that were found useful.  Using not only a single image but a set of data as the smallest unit matches many of the biometric scenarios.  There are 2 types of methods in set/template-based FR, either processing all the data in the set separately to find the matching score by combining the individual scores with a certain function or doing feature pooling which generates a single representation of the set and compares only them.  Additionally, a deep heterogeneous feature fusion network and actor-critic reinforcement learning are some of the alternative attempts to deal with sets/templates.  Video FR is also a complex problem consisting of combining the data across frames and handling individual frames with blur, pose variations, and occlusions.  A neural aggregation network (NAN), combining metric and adversarial learning is some of the attempts to aggregate the frames.  To deal with bad frames: deep reinforcement learning, learning blur-robust representations, and reconstruction of frames with CNN was tried.  3D FR is underdeveloped due to a lack of good datasets.  Despite attempts to enlarge such datasets with 3D reconstruction from 2D images, using 2D CNN, and using 3-channel inputs, the direction is still open for exploration.  Partial Face Recognition is emerging in several real-world scenarios where a decision should be made with only a part of the face available.  Dividing the aligned image into multi-scale patches and Dynamic Feature Matching (DFM) are some of the approaches for Partial FR.  Applying FR in mobile devices is an important problem that needs a solution under stricter conditions.  Deep models like MobiFace and the multi-batch method are some of the work in this direction.  However, the light networks and compressing methods as in image classification still need exploration in the FR context.  Face Anti-attack systems are needed to defend from face spoofing, adversarial perturbations, etc.  For face spoofing, ensuring face-like depth with two-stream CNN, classification with CNN, and LSTM for sequences of frames were tried to resolve the issue.  In terms of adversarial perturbation, detecting abnormal layers of the network to increase the robustness of the model was an idea.  However, the attack methods evolve as well, thus continued work in this direction is necessary.  Highly biased FR datasets impose fairness issues on FR models.  Thus, debiasing attempts are made by unbalanced training, attribute removal, and domain adaptation.  Unbalanced training, for example, RL-RBN,  tries to remove the bias of the model by regularization (i. e adjusting the objective function).  The attribute-removal method tries to learn attribute-invariant representations by removing demographic information.  Lastly, domain adaptation attempts to learn domain-invariant representations to avoid any domain bias.	Despite the high accuracy in the LFW [23] and Megaface [44, 164] benchmarks, the performance of FR models still hardly meets the requirements in real-world application. A conjecture in industry is made that results of generic deep models can be improved simply by collecting big datasets of the target scene. However, this holds only to a certain degree. More and more concerns on privacy may make the collection and human-annotation of face data become illegal in the future. Therefore, significant efforts have been paid to design excellent algorithms to address the specific problems with limited data in these realistic scenes. In this section, we present several special algorithms of FR.With the emergence of mobile phones, tablets and augmented reality, FR has been applied in mobile devices. Due to computational limitations, the recognition tasks in these devices need to be carried out in a light but timely fashion. MobiFace [87] required efficient memory and low cost operators by adopting fast downsampling and bottleneck residual block, and achieves99.7% on LFW database and 91.3% on Megaface database. Tadmor et al. [263] proposed a multibatch method that first generates signatures for a minibatch of k face images and then constructs an unbiased estimate of the full gradient by relying on all k^{2}-k pairs from the minibatch. As mentioned in Section 3.2.1, light-weight deep networks [126, 127, 128, 129] perform excellently in the fundamental tasks of image classification and deserve further attention in FR tasks. Moreover, some well-known compressed networks such as Pruning [264, 265, 266], BinaryNets [267, 268, 269, 270], Mimic Networks [271, 272], also have potential to be introduced into FR.1) Cross-Pose Face Recognition: As [182] shows that many existing algorithms suffer a decrease of over 10% from frontal-frontal to frontal-profile verification, cross-pose FR is still an extremely challenging scene. In addition to the aforementioned methods, including “one-to-many augmentation”, “many-to-one normalization” and assembled networks (Section IV and III-B.2), there are some other algorithms designed for cross-pose FR. Considering the extra burden of above methods, Cao et al. [215] attempted to perform frontalization in the deep feature space rather than the image space. A deep residual equivariant mapping (DREAM) block dynamically added residuals to an input representation to transform a profile face to a frontal image. Chen et al. [216] proposed to combine feature extraction with multi-view subspace learning to simultaneously make features be more pose-robust and discriminative. Pose Invariant Model (PIM) [217] jointly performed face frontalization and learned pose invariant representations end-to-end to allow them to mutually boost each other, and further introduced unsupervised cross-domain adversarial training and a learning to learn strategy to provide high-fidelity frontal reference face images.2) Cross-Age Face Recognition: Cross-age FR is extremely challenging due to the changes in facial appearance by the aging process over time. One direct approach is to synthesize the desired image with target age such that the recognition can be performed in the same age group. A generative probabilistic model was used by [218] to model the facial aging process at each short-term stage. The identity-preserved conditional generative adversarial networks (IPCGANs) [219] framework utilized a conditional-GAN to generate a face in which an identity-preserved module preserved the identity information and an age classifier forced the generated face with the target age. Antipov et al. [220] proposed to age faces by GAN, but the synthetic faces cannot be directly used for face verification due to its imperfect preservation of identities. Then, they used a local manifold adaptation (LMA) approach [221] to solve the problem of [220]. In [222], high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales to generate more lifelike facial details. An alternative to address the cross-age problem is to decompose aging and identity components separately and extract age-invariant representations. Wen et al. [192] developed a latent identity analysis (LIA) layer to separate these two components, as shown in Fig. 22. In [193], age-invariant features were obtained by subtracting age-specific factors from the representations with the help of the age estimation task. In [124], face features are decomposed in the spherical coordinate system, in which the identity-related components are represented with angular coordinates and the age-related information is encoded with radial coordinate. Additionally, there are other methods designed for cross-age FR. For example, Bianco ett al. [223] and El et al. [224] fine-tuned the CNN to transfer knowledge across age. Wang et al. [225] proposed a siamese deep network to perform multi-task learning of FR and age estimation. Li et al. [226] integrated feature extraction and metric learning via a deep CNN.3) Makeup Face Recognition: Makeup is widely used by the public today, but it also brings challenges for FR due to significant facial appearance changes. The research on matching makeup and nonmakeup face images is receiving increasing attention. Li et al. [208] generated nonmakeup images from makeup ones by a bi-level adversarial network (BLAN) and then used the synthesized nonmakeup images for verification as shown in Fig. 23. Sun et al. [227] pretrained a triplet network on videos and fine-tuned it on a small makeup datasets. Specially, facial disguise [214], [228], [229] is a challenging research topic in makeup face recognition. By using disguise accessories such as wigs, beard, hats, mustache, and heavy makeup, disguise introduces two variations: (i) when a person wants to obfuscate his/her own identity, and (ii) another individual impersonates someone else’s identity. Obfuscation increases intra-class variations whereas impersonation reduces the inter-class dissimilarity, thereby affecting face recognition/verification task. To address this issue, a variety of methods are proposed. Zhang et al. [230] first trained two DCNNs for generic face recognition and then used Principal Components Analysis (PCA) to find the transformation matrix for disguised face recognition adaptation. Kohli et al. [231] finetuned models using disguised faces. Smirnov et al. [232] proposed a hard example mining method benefitted from class-wise (Doppelganger Mining [233]) and example-wise mining to learn useful deep embeddings for disguised face recognition. Suri et al. [234] learned the representations of images in terms of colors, shapes, and textures (COST) using an unsupervised dictionary learning method, and utilized the combination of COST features and CNN features to perform recognition.1) NIR-VIS Face Recognition: Due to the excellent performance of the near-infrared spectrum (NIS) images under low-light scenarios, NIS images are widely applied in surveillance systems. Because most enrolled databases consist of visible light (VIS) spectrum images, how to recognize a NIR face from a gallery of VIS images has been a hot topic. Saxena et al. [235] and Liu et al. [236] transferred the VIS deep networks to the NIR domain by fine-tuning. Lezama et al. [237] used a VIS CNN to recognize NIR faces by transforming NIR images to VIS faces through cross-spectral hallucination and restoring a low-rank structure for features through low-rank embedding. Reale et al. [198] trained a VISNet (for visible images) and a NIRNet (for near-infrared images), and coupled their output features by creating a siamese network. He et al. [238], [239] divided the high layer of the network into a NIR layer, a VIS layer and a NIR-VIS shared layer, then, a modality-invariant feature can be learned by the NIR-VIS shared layer. Song et al. [240] embedded cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In [196], the low-rank relevance and cross-modal ranking were used to alleviate the semantic gap.2) Low-Resolution Face Recognition: Although deep networks are robust to low resolution to a great extent, there are still a few studies focused on promoting the performance of low-resolution FR. For example, Zangeneh et al. [241] proposed a CNN with a two-branch architecture (a super-resolution network and a feature extraction network) to map the high and low-resolution face images into a common space where the intra-person distance is smaller than the interperson distance. Shen et al. [242] exploited the face semantic information and local structural constraints to better restore the shape and detail of face images. In addition, they optimized the network with perceptual and adversarial losses to produce photo-realistic results.3) Photo-Sketch Face Recognition: The photo-sketch FR may help law enforcement to quickly identify suspects. The commonly used methods can be categorized as two classes. One is to utilize transfer learning to directly match photos to sketches. Deep networks are first trained using a large face database of photos and are then fine-tuned using small sketch database [243], [244]. The other is to use the image-to-image translation, where the photo can be transformed to a sketch or the sketch to a photo; then, FR can be performed in one domain. Zhang et al. [200] developed a fully convolutional network with generative loss and a discriminative regularizer to transform photos to sketches. Zhang et al. [245] utilized a branched fully convolutional neural network (BFCN) to generate a structure-preserved sketch and a texture-preserved sketch, and then they fused them together via a probabilistic method. Recently, GANs have achieved impressive results in image generation. Yi et al. [246], Kim et al. [247] and Zhu et al. [248] used two generators, GA and GB , to generate sketches from photos and photos from sketches, respectively (Fig. 24). Based on [248], Wang et al. [202] proposed a multi-adversarial network to avoid artifacts by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork. Similar to photo-sketch FR, photocaricature FR is one kind of heterogenous FR scenes which is challenging and important to understanding of face perception. Huo et al. [213] built a large dataset of caricatures and photos, and provided several evaluation protocols and their baseline performances for comparison.1) Low-Shot Face Recognition: For many practical applications, such as surveillance and security, the FR system should recognize persons with a very limited number of training samples or even with only one sample. The methods of low-shot learning can be categorized as 1) synthesizing training data and 2) learning more powerful features. Hong et al. [249] generated images in various poses using a 3D face model and adopted deep domain adaptation to handle other variations, such as blur, occlusion, and expression (Fig. 25). Choe et al. [250] used data augmentation methods and a GAN for pose transition and attribute boosting to increase the size of the training dataset. Wu et al. [176] proposed a framework with hybrid classifiers using a CNN and a nearest neighbor (NN) model. Guo et al. [143] made the norms of the weight vectors of the one-shot classes and the normal classes aligned to address the data imbalance problem. Cheng et al. [137] proposed an enforced softmax that contains optimal dropout, selective attenuation, L2 normalization and model-level optimization. Yin et al. [251] augmented feature space of low-shot classes by transferring the principal components from regular to low-shot classes to encourage the variance of low-shot classes to mimic that of regular classes.2) Set/Template-Based Face Recognition: Different from traditional image-to-image recognition, set-to-set recognition takes a set (heterogeneous contents containing both images and videos) as the smallest unit of representation. This kind of setting does reflect the real-world biometric scenarios, thereby attracting a lot of attention. After learning face representations of media in each set, two strategies are generally adopted to perform set-to-set matching. One is to use these representations to perform pair-wise similarity comparison of two sets and aggregate the results into a single and final score by max score pooling [96], average score pooling [252] and its variations [253], [254]. The other strategy is feature pooling [96], [103], [81] which first aggregates face representations into a single representation for each set and then performs a comparison between two sets. In addition to the commonly used strategies, there are also some novel methods proposed for set/template-based FR. For example, Hayat et al. [255] proposed a deep heterogeneous feature fusion network to exploit the features’ complementary information generated by different CNNs. Liu et al. [256] introduced the actor-critic reinforcement learning for set-based FR. They casted the inner-set dependency modeling to a Markov decision process in the latent space, and trained a dependency-aware attention control agent to make attention control for each image in each step.3) Video Face Recognition: There are two key issues in video FR: one is to integrate the information across different frames together to build a representation of the video face, and the other is to handle video frames with severe blur, pose variations, and occlusions. For frame aggregation, Yang et al. [83] proposed a neural aggregation network (NAN) in which the aggregation module, consisting of two attention blocks driven by a memory, produces a 128-dimensional vector representation (Fig. 26). Rao et al. [187] aggregated raw video frames directly by combining the idea of metric learning and adversarial learning. For dealing with bad frames, Rao et al. [185] discarded the bad frames by treating this operation as a Markov decision process and trained the attention model through a deep reinforcement learning framework. Ding et al. [257] artificially blurred clear images for training to learn blur-robust face representations. Parchami et al. [258] used a CNN to reconstruct a lower-quality video into a high-quality face.1) 3D Face Recognition: 3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of “one-to-many augmentation” to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize aces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.2) Partial Face Recognition: Partial FR, in which only arbitrary-size face patches are presented, has become an emerging problem with increasing requirements of identification from CCTV cameras and embedded vision systems in mobile devices, robots and smart home facilities. He et al. [261] divided the aligned face image into several multi-scale patches, and the dissimilarity between two partial face images is calculated as the weighted L2 distance between corresponding patches. Dynamic feature matching (DFM) [262] utilized a sliding window of the same size as the probe feature maps to decompose the gallery feature maps into several gallery sub-feature maps, and the similarity-guided constraint imposed on sparse representation classification (SRC) provides an alignment-free matching.4) Face Anti-attack: With the success of FR techniques, various types of attacks, such as face spoofing and adversarial perturbations, are becoming large threats. Face spoofing involves presenting a fake face to the biometric sensor using a printed photograph, worn mask, or even an image displayed on another electronic device. In order to defense this type of attack, several methods are proposed [211], [273], [274], [275], [276], [277], [278], [279]. Atoum et al. [211] proposed a novel two-stream CNN in which the local features discriminate the spoof patches that are independent of the spatial face areas, and holistic depth maps ensure that the input live sample has a face-like depth. Yang et al. [273] trained a CNN using both a single frame and multiple frames with five scales as input, and using the live/spoof label as the output. Taken the sequence of video frames as input, Xu et al. [274] applied LSTM units on top of CNN to obtain end-to-end features to recognize spoofing faces which leveraged the local and dense property from convolution operation and learned the temporal structure using LSTM units. Li et al. [275] and Patel et al. [276] fine-tuned their networks from a pretrained model by training sets of real and fake images. Jourabloo et al. [277] proposed to inversely decompose a spoof face into the live face and the spoof noise pattern. Adversarial perturbation is the other type of attack which can be defined as the addition of a minimal vector r such that with addition of this vector into the input image x, i.e. (x + r), the deep learning models misclassifies the input while people will not. Recently, more and more work has begun to focus on solving this perturbation of FR. Goswami et al. [280] proposed to detect adversarial samples by characterizing abnormal filter response behavior in the hidden layers and increase the network’s robustness by removing the most problematic filters. Goel et al. [281] provided an open source implementation of adversarial detection and mitigation algorithms. Despite of progresses of anti-attack algorithms, attack methods are updated as well and remind us the need to further increase security and robustness in FR systems, for example, Mai et al. [282] proposed a neighborly deconvolutional neural network (NbNet) to reconstruct a fake face using the stolen deep templates.5) Debiasing face recognition: As described in Section V-A, existing datasets are highly biased in terms of the distribution of demographic cohorts, which may dramatically impact the fairness of deep models. To address this issue, there are some works that seek to introduce fairness into face recognition and mitigate demographic bias, e,g. unbalanced-training [283], attribute removal [284], [285], [286] and domain adaptation [173], [287], [147]. 1) Unbalanced-training methods mitigate the bias via model regularization, taking into consideration of the fairness goal in the overall model objective function. For example, RL-RBN [283] formulated the process of finding the optimal margins for non-Caucasians as a Markov decision process and employed deep Q-learning to learn policies based on large margin loss. 2) Attribute removal methods confound or remove demographic information of faces to learn attribute-invariant representations. For example, Alvi et al. [284] applied a confusion loss to make a classifier fail to distinguish attributes of examples so that multiple spurious variations are removed from the feature representation. SensitiveNets [288] proposed to introduce sensitive information into triplet loss. They minimized the sensitive information, while maintaining distances between positive and negative embeddings. 3) Domain adaptation methods propose to investigate data bias problem from a domain adaptation point of view and attempt to design domain-invariant feature representations to mitigate bias across domains. IMAN [173] simultaneously aligned global distribution to decrease race gap at domain-level, and learned the discriminative target representations at cluster level. Kan [147] directly converted the Caucasian data to non-Caucasian domain in the image space with the help of sparse reconstruction coefficients learnt in the common subspace.				
819	paper_35	Is the segmented training data 2d or 3d ?	V-Net is trained on the 3D MRI prostate volumes.	Segmentation is a highly relevant task in medical image analysis.Automatic delineation of organs and structures of interest is often necessary to perform tasks such as visual augmentation [10], computer assisted diagnosis [12], interventions [20] and extraction of quantitative indices from images [1].In particular, since diagnostic and interventional imagery often consists of 3D images, being able to perform volumetric segmentations by taking into account the whole volume content at once, has a particular relevance.In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. Moreover, MRI volumes are often affected by artefacts and distortions due to field inhomogeneity. Prostate segmentation is nevertheless an important task having clinical relevance both during diagnosis, where the volume of the prostate needs to be assessed [13], and during treatment planning, where the estimate of the anatomical boundary needs to be accurate [4, 20].Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\times 128\times 64 voxels and a spatial resolution of 1\times 1\times 1.5 millimeters.We trained our method on 50 MRI volumes, and the relative manual ground truth annotation, obtained from the ”PROMISE2012” challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations.We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.Fully convolutional network trained end-to-end were so far applied only to 2D images both in computer vision [11, 8] and microscopy image analysis [14]. These models, which served as an inspiration for our work, employed different network architectures and were trained to predict a segmentation mask, delineating the structures of interest, for the whole image. In [11] a pre-trained VGG network architecture [15] was used in conjunction with its mirrored, de-convolutional, equivalent to segment RGB images by leveraging the descriptive power of the features extracted by the innermost layer. In [8] three fully convolutional deep neural networks, pre-trained on a classification task, were refined to produce segmentations while in [14] a brand new CNN model, especially tailored to tackle biomedical image analysis problems in 2D, was proposed.In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.				
820	paper_35	What is the difference between foreground and background voxels?	At the output of the V-Net foreground voxels represents the score for the anatomy and background voxels represents score for not having the anatomy at a region.	The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\times 1\times 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\times 5\times 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.The network predictions, which consist of two volumes having the same resolution as the original input data, are processed through a soft-max layer which outputs the probability of each voxel to belong to foreground and to background. In medical volumes such as the ones we are processing in this work, it is not uncommon that the anatomy of interest occupies only a very small region of the scan. This often causes the learning process to get trapped in local minima of the loss function yielding a network whose predictions are strongly biased towards background. As a result the foreground region is often missing or only partially detected. Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise. The dice coefficient D between two binary volumes can be written asD=\frac{2\sum_{i}^{N}p_{i}g_{i}}{\sum_{i}^{N}p_{i}^{2}+\sum_{i}^{N}g_{i}^{2}}A Previously unseen MRI volume can be segmented by processing it in a feed-forward manner through the network. The output of the last convolutional layer, after soft-max, consists of a probability map for background and foreground. The voxels having higher probability (>0.5) to belong to the foreground than to the background are considered part of the anatomy.We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.				
821	paper_35	Does it have anything to do with the nature and complexity of data we are working with ?	In general MRI 3D volume data is complex.  Prostate anomaly segmentation also makes the data in consideration unique.	We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\times 128\times 64 voxels and a spatial resolution of 1\times 1\times 1.5 millimeters.Annotated medical volumes are not easy to obtain due to the fact that one or more experts are required to manually trace a reliable ground truth annotation and that there is a cost associated with their acquisition. In this work we found necessary to augment the original training dataset in order to obtain robustness and increased precision on the test dataset.We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].				
822	paper_35	Going deep through network layers makes it harder to remember shallower local information, wouldn't that make segmentation harder?	Since non anatomy part has a much larger spatial support than the anatomy and As we move down the layers in a CNN the receptive field of the features increases therefore proposed CNN would work fine for the local information.	Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.Downsampling allows us to reduce the size of the signal presented as input and to increase the receptive field of the features being computed in subsequent network layers. Each of the stages of the left part of the network, computes a number of features which is two times higher than the one of the previous layer.				
823	paper_35	Why it is needed to have a two channel volumetric segmentation in the output?	Two channel volumetric segmentation is used at the output to perform binary classification of foreground and background classes using soft-max.  Each volume represent the logits for each class at each pixel location.	Segmentation is a highly relevant task in medical image analysis.Automatic delineation of organs and structures of interest is often necessary to perform tasks such as visual augmentation [10], computer assisted diagnosis [12], interventions [20] and extraction of quantitative indices from images [1].In particular, since diagnostic and interventional imagery often consists of 3D images, being able to perform volumetric segmentations by taking into account the whole volume content at once, has a particular relevance.In this work, we aim to segment prostate MRI volumes. This is a challenging task due to the wide range of appearance the prostate can assume in different scans due to deformations and variations of the intensity distribution. Moreover, MRI volumes are often affected by artefacts and distortions due to field inhomogeneity. Prostate segmentation is nevertheless an important task having clinical relevance both during diagnosis, where the volume of the prostate needs to be assessed [13], and during treatment planning, where the estimate of the anatomical boundary needs to be accurate [4, 20].The right portion of the network extracts features and expands the spatial support of the lower resolution feature maps in order to gather and assemble the necessary information to output a two channel volumetric segmentation. The two features maps computed by the very last convolutional layer, having 1\times 1\times 1 kernel size and producing outputs of the same size as the input volume, are converted to probabilistic segmentations of the foreground and background regions by applying soft-max voxelwise.After each stage of the right portion of the CNN, a de-convolution operation is employed in order increase the size of the inputs (Figure 3) followed by one to three convolutional layers involving half the number of 5\times 5\times 5 kernels employed in the previous layer. Similar to the left part of the network, also in this case we resort to learn residual functions in the convolutional stages.The network predictions, which consist of two volumes having the same resolution as the original input data, are processed through a soft-max layer which outputs the probability of each voxel to belong to foreground and to background. In medical volumes such as the ones we are processing in this work, it is not uncommon that the anatomy of interest occupies only a very small region of the scan. This often causes the learning process to get trapped in local minima of the loss function yielding a network whose predictions are strongly biased towards background. As a result the foreground region is often missing or only partially detected. Several previous approaches resorted to loss functions based on sample re-weighting where foreground regions are given more importance than background ones during learning. In this work we propose a novel objective function based on dice coefficient, which is a quantity ranging between 0 and 1 which we aim to maximise. The dice coefficient D between two binary volumes can be written asD=\frac{2\sum_{i}^{N}p_{i}g_{i}}{\sum_{i}^{N}p_{i}^{2}+\sum_{i}^{N}g_{i}^{2}}A Previously unseen MRI volume can be segmented by processing it in a feed-forward manner through the network. The output of the last convolutional layer, after soft-max, consists of a probability map for background and foreground. The voxels having higher probability (>0.5) to belong to the foreground than to the background are considered part of the anatomy.				
825	paper_35	Do we really no longer need hand crafted feature in the ML life cycle?	Deep Neural Networks such as CNNs are much better option than the handcrafted features for computer vision problem such as segmentation.	Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.	Profound Neural Networks such as CNNs are much better option than the handcrafted features for computer vision problem such as segmentation.	Tortured phrases	 (Deep Neural Network -> Profound Neural Network)	
826	paper_35	Can we enclose all appearances of prostate MRI volumes?	Due to the training/testing and augmentation on the diverse set of prostate scans all appearances of prostate can be encoded with V-Net.	Our CNN is trained end-to-end on a dataset of prostate scans in MRI. An example of the typical content of such volumes is shown in Figure 1. All the volumes processed by the network have fixed size of 128\times 128\times 64 voxels and a spatial resolution of 1\times 1\times 1.5 millimeters.During every training iteration, we fed as input to the network randomly deformed versions of the training images by using a dense deformation field obtained through a 2\times 2\times 2 grid of control-points and B-spline interpolation. This augmentation has been performed ”on-the-fly”, prior to each optimisation iteration, in order to alleviate the otherwise excessive storage requirements. Additionally we vary the intensity distribution of the data by adapting, using histogram matching, the intensity distributions of the training volumes used in each iteration, to the ones of other randomly chosen scans belonging to the dataset.We trained our method on 50 MRI volumes, and the relative manual ground truth annotation, obtained from the ”PROMISE2012” challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations.We tested V-Net on 30 MRI volumes depicting prostate whose ground truth annotation was secret. All the results reported in this section of the paper were obtained directly from the organisers of the challenge after submitting the segmentation obtained through our approach. The test set was representative of the clinical variability encountered in prostate scans in real clinical settings [7].In this work we present our approach to medical image segmentation that leverages the power of a fully convolutional neural networks, trained end-to-end, to process MRI volumes.Differently from other recent approaches we refrain from processing the input volumes slice-wise and we propose to use volumetric convolutions instead. We propose a novel objective function based on Dice coefficient maximisation, that we optimise during training.We demonstrate fast and accurate results on prostate MRI test volumes and we provide direct comparison with other methods which were evaluated on the same test data 111Detailed results available on http://promise12.grand-challenge.org/results/.				
827	paper_35	How did authors claim that their approach overcome the problems that Ultrasound made to earlier approaches?	V-Net solves the problem of patch based CNNs for ultrasound by using a 3D image volume.	We trained our method on 50 MRI volumes, and the relative manual ground truth annotation, obtained from the ”PROMISE2012” challenge dataset [7]. This dataset contains medical data acquired in different hospitals, using different equipment and different acquisition protocols. The data in this dataset is representative of the clinical variability and challenges encountered in clinical settings. As previously stated we massively augmented this dataset through random transformation performed in each training iteration, for each mini-batch fed to the network. The mini-batches used in our implementation contained two volumes each, mainly due to the high memory requirement of the model during training. We used a momentum of 0.99 and a initial learning rate of 0.0001 which decreases by one order of magnitude every 25K iterations.CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.				
831	paper_35	Does the phrase "data with larger spatial support than the typical size of the anatomy" refer to feature maps with a larger number of channels than the input map at the deepest layer, or does it refer to something else?	As we move deeper the network will capture more features.	We report in Table 1 the receptive fields of each network layer, showing the fact that the innermost portion of our CNN already captures the content of the whole input volume. We believe that this characteristic is important during segmentation of poorly visible anatomy: the features computed in the deepest layer perceive the whole anatomy of interest at once, since they are computed from data having a spatial support much larger than the typical size of the anatomy we seek to delineate, and therefore impose global constraints.				
832	paper_35	Does performing augmentation "on-the-fly" prior to each optimization iteration slow down the learning process?	On the fly data augmentation will decrease the storage requirements and will increase the speed of each training iteration.	During every training iteration, we fed as input to the network randomly deformed versions of the training images by using a dense deformation field obtained through a 2\times 2\times 2 grid of control-points and B-spline interpolation. This augmentation has been performed ”on-the-fly”, prior to each optimisation iteration, in order to alleviate the otherwise excessive storage requirements. Additionally we vary the intensity distribution of the data by adapting, using histogram matching, the intensity distributions of the training volumes used in each iteration, to the ones of other randomly chosen scans belonging to the dataset.				
835	paper_35	What is volumetric neural network?	A volumetric neural network works on the input of 3D volumes and uses 3D convolution filters.	We presented and approach based on a volumetric convolutional neural network that performs segmentation of MRI prostate volumes in a fast and accurate manner. We introduced a novel objective function that we optimise during training based on the Dice overlap coefficient between the predicted segmentation and the ground truth annotation. Our Dice loss layer does not need sample re-weighting when the amount of background and foreground pixels is strongly unbalanced and is indicated for binary segmentation tasks. Although we inspired our architecture to the one proposed in [14], we divided it into stages that learn residuals and, as empirically observed, improve both results and convergence time. Future works will aim at segmenting volumes containing multiple regions in other modalities such as ultrasound and at higher resolutions by splitting the network over multiple GPUs.				
836	paper_36	What are differences between  “warping error”, the “Rand error” and the “pixel error” ?	If both queries and documents are short, fine-granular interaction is not required.  : it uses BERT to produce a single embedding vector for the query and another for the document, extracted from BERT’s [CLS] contextualized embedding and expanded through a linear layer to dimension 4096 (which equals N_{q}\times 128=32\times 128).  Relevance is estimated as the inner product of the query’s and the document’s embeddings, which we found to perform better than cosine similarity for single-vector re-ranking.	The u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382.				
837	paper_36	Is Data Augmentation always sufficient to support performance in the segmentation task?	Performance of microscopy image segmentation task can be improved by using elastic deformation based segmentation.	Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images.We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB).We provide the full Caffe[6]-based implementation and the trained networks444U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. We are sure that the u-net architecture can be applied easily to many more tasks.As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.				
838	paper_36	Is fast processing the only metric that we consider in segmentation?	Other than the processing time, performance of the segmentation task is also measure by computing the warping error, Rand error and the pixel error from thresholded segmentation map and also by IOU metric.	We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14].Here we achieve an average IOU (“intersection over union”) of 92%, which is significantly better than the second best algorithm with 83% (see Table 2).				
839	paper_36	Can we try to re-generate those unreachable images using recent methods to enhance medical tasks?	unreachable images can be generated by using elastic deformation based augmentation methods.	Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images.We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.				
841	paper_36	Why does using larger patches reduce localization accuracy?	Larger patches reduce localization accuracy because they require more max-pooling layers.	Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches.Secondly, there is a trade-off between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11, 4] proposed a classifier output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time.				
842	paper_36	What would happen to the model performance if we just use connections from earlier layers of contracting path while going deeper without upsampling to perform localization?	Model need to match the size of expansion path with the contacting path at each stage.  Otherwise the localization performance would suffer.	In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network” [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information.One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.				
843	paper_36	What happen to the pixels with no content in the segmentation map?	There are no pixels without any content because for the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.	One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.	There are no pixels without any content because for the pixels in the border region of the image, the missing context is extrapolated by mirroring the info picture.	Tortured phrases	input image -> info picture	
845	paper_36	What is the contribution that augmentation data with deformation adds to the overall performance?	U-Net achieves good performance, less training time and less memory by using deformation based data augmentation.	Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images.We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.The u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382.The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB).We provide the full Caffe[6]-based implementation and the trained networks444U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. We are sure that the u-net architecture can be applied easily to many more tasks.As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.				
846	paper_36	Is convolution and up-convolution considered as the transform and its inverse?	Ye up-convolution operation upsamples the feature resolution back to original and also reduces the number of feature channels.	The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.				
848	paper_36	Which gives better performance: using more than one image in the batch or larger input tiles with only one image in the batch ?	According to the experiments in the paper use of large tiles instead of large size is preferred which reduces the overhead and maximize the use of GPU memory.	The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [6].Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.				
849	paper_36	What is meant by "differential interference contrast"?	Differential interference contrast (DIC)  is a microscopy technique which can be used to record HeLa cells.	The second data set “DIC-HeLa”333Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with 46%.				
851	paper_36	How can we learn the network to be invariant to gray value variations?	Data augmentation and drop-out layer can make the network invariant to gray value variations.	Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images.We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.				
852	paper_36	What is bicubic interpolation ?	Bicubic interpolation is a method to estimate missing pixels in a grid.	Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images.We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.				
853	paper_36	What is electron microscopic recordings used for ?	Electron microscopic recordings can be used to highlight neuronal structures.	We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14].				
854	paper_36	Can we use U-Net architecture in self-driving car and providing a segmentation map for the scene around?	The paper only discuss the application of U-Net for the segmentation of biomedical images.	The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB).We provide the full Caffe[6]-based implementation and the trained networks444U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. We are sure that the u-net architecture can be applied easily to many more tasks.				
855	paper_37	What performance metric does IOU measure?	The IOU measures the overlap between the ground-truth RoI and the RoI detected by the model.	With pre-computed region proposals, it is easy to end-to-end train the R-FCN architecture. Following [6], our loss function defined on each RoI is the summation of the cross-entropy loss and the box regression loss: L(s,t(x,y,w,h)) = Lcls(sc∗ ) + λ[c∗ > 0]Lreg(t, t∗). Here c∗ is the RoI’s ground-truth label (c∗ = 0 means background). Lcls(sc∗ ) = − log(sc∗) is the cross-entropy loss for classification, Lreg is the bounding box regression loss as defined in [6], and t∗ represents the ground truth box. [c∗ > 0] is an indicator which equals to 1 if the argument is true and 0 otherwise. We set the balance weight λ = 1 as in [6]. We define positive examples as the RoIs that have intersection-over-union (IoU) overlap with a ground-truth box of at least 0.5, and negative otherwise.				
856	paper_37	Is Faster R-CNN +++ another different architecture or just some refinement for the known models?	Faster R-CNN+++ is a refined model of Faster R-CNN.  Faster R-CNN+++ uses iterative box regression, context and multi-scale training to refine the original model.	Table 5 shows more comparisons. Following the multi-scale training in [8], we resize the image in each training iteration such that the scale is randomly sampled from {400,500,600,700,800} pixels. We still test a single scale of 600 pixels, so add no test-time cost. The mAP is 80.5%. In addition, we train our model on the MS COCO [13] trainval set and then fine-tune it on the PASCAL VOC set. R-FCN achieves 83.6% mAP (Table 5), close to the “Faster R-CNN +++” system in [9] that uses ResNet-101 as well. We note that our competitive result is obtained at a test speed of 0.17 seconds per image, 20\times faster than Faster R-CNN +++ that takes 3.36 seconds as it further incorporates iterative box regression, context, and multi-scale testing [9].These comparisons are also observed on the PASCAL VOC 2012 test set (Table 5).				
857	paper_37	Why was R-FCN unable to converge using only one score map?	R-FCN depends on the score maps of R-FCN for its output as the score maps are used for the rod pooling layer.  Therefore, when the k = 1 and there is only one score map, RoIs don't actually capture any spatial information and therefore the model fails to learn the task.	In this paper, we develop a framework called Region-based Fully Convolutional Network (R-FCN) for object detection. Our network consists of shared, fully convolutional architectures as is the case of FCN [15]. To incorporate translation variance into FCN, we construct a set of position-sensitive score maps by using a bank of specialized convolutional layers as the FCN output. Each of these score maps encodes the position information with respect to a relative spatial position (e.g., “to the left of an object”). On top of this FCN, we append a position-sensitive RoI pooling layer that shepherds information from these score maps, with no weight (convolutional/fc) layers following. The entire architecture is learned end-to-end. All learnable layers are convolutional and shared on the entire image, yet encode spatial information required for object detection. Figure 1 illustrates the key idea and Table 1 compares the methodologies among region-based detectors.The importance of position-sensitivity is further demonstrated by setting k=1, for which R-FCN is unable to converge. In this degraded case, no spatial information can be explicitly captured within an RoI. Moreover, we report that naïve Faster R-CNN is able to converge if its RoI pooling output resolution is 1\times 1, but the mAP further drops by a large margin to 61.7% (Table 2).Given the proposal regions (RoIs), the R-FCN architecture is designed to classify the RoIs into object categories and background. In R-FCN, all learnable weight layers are convolutional and are computed on the entire image. The last convolutional layer produces a bank of k^{2} position-sensitive score maps for each category, and thus has a k^{2}(C+1)-channel output layer with C object categories (+1 for background). The bank of k^{2} score maps correspond to a k\times k spatial grid describing relative positions. For example, with k\times k=3\times 3, the 9 score maps encode the cases of {top-left, top-center, top-right, …, bottom-right} of an object category.				
858	paper_37	What was Faster R-CNN developed to overcome in the Fast R-CNN?	Faster R-CNN uses a RPN component to predict bounding boxes of objects it detects instead of sliding windows, which is what Fast R-CNN uses.  From the names of the two models, it can be inferred that the change in methodology allowed Faster R-CNN to be more computationally efficient while still performing well.	There have been object detectors that can be thought of as “fully convolutional” models. OverFeat [21] detects objects by sliding multi-scale windows on the shared convolutional feature maps; similarly, in Fast R-CNN [6] and [12], sliding windows that replace region proposals are investigated. In these cases, one can recast a sliding window of a single scale as a single convolutional layer. The RPN component in Faster R-CNN [18] is a fully convolutional detector that predicts bounding boxes with respect to reference boxes (anchors) of multiple sizes. The original RPN is class-agnostic in [18], but its class-specific counterpart is applicable (see also [14]) as we evaluate in the following.				
859	paper_37	What is the hole algorithm?	The authors do not explain exactly what the hole algorithm is.  It is implied that the hole algorithm is a trick that is used to try to improve performance by changing the stride and filters of convolutional layers.	À trous and stride. Our fully convolutional architecture enjoys the benefits of the network modifications that are widely used by FCNs for semantic segmentation [15, 2]. Particularly, we reduce ResNet-101’s effective stride from 32 pixels to 16 pixels, increasing the score map resolution. All layers before and on the conv4 stage [9] (stride=16) are unchanged; the stride=2 operations in the first conv5 block is modified to have stride=1, and all convolutional filters on the conv5 stage are modified by the “hole algorithm” [15, 2] (“Algorithme à trous” [16]) to compensate for the reduced stride. For fair comparisons, the RPN is computed on top of the conv4 stage (that are shared with R-FCN), as is the case in [9] with Faster R-CNN, so the RPN is not affected by the à trous trick. The following table shows the ablation results of R-FCN (k\times k=7\times 7, no hard example mining). The à trous trick improves mAP by 2.6 points.				
860	paper_37	What is the effect of changing stride of the convolution?	By reducing the stride of the convolution with the hole algorithm, the authors were able to improve mAP by 2. 6 points.	À trous and stride. Our fully convolutional architecture enjoys the benefits of the network modifications that are widely used by FCNs for semantic segmentation [15, 2]. Particularly, we reduce ResNet-101’s effective stride from 32 pixels to 16 pixels, increasing the score map resolution. All layers before and on the conv4 stage [9] (stride=16) are unchanged; the stride=2 operations in the first conv5 block is modified to have stride=1, and all convolutional filters on the conv5 stage are modified by the “hole algorithm” [15, 2] (“Algorithme à trous” [16]) to compensate for the reduced stride. For fair comparisons, the RPN is computed on top of the conv4 stage (that are shared with R-FCN), as is the case in [9] with Faster R-CNN, so the RPN is not affected by the à trous trick. The following table shows the ablation results of R-FCN (k\times k=7\times 7, no hard example mining). The à trous trick improves mAP by 2.6 points.R-FCN with ResNet-101 on:conv4, stride=16conv5, stride=32conv5, à trous, stride=16mAP (%) on VOC 07 test72.574.076.6				
861	paper_37	What does "non-maximum suppression" mean?	The authors do not explain exactly what non-maximum suppression is.  It is implied that it is a common post-processing method that culls RoIs that have low IoU scores.	Inference. As illustrated in Figure 2, the feature maps shared between RPN and R-FCN are computed (on an image with a single scale of 600). Then the RPN part proposes RoIs, on which the R-FCN part evaluates category-wise scores and regresses bounding boxes.During inference we evaluate 300 RoIs as in [18] for fair comparisons. The results are post-processed by non-maximum suppression (NMS) using a threshold of 0.3 IoU [7], as standard practice.				
862	paper_37	How can highest-loss examples be selected?	Losses are calculated individually for each RoI, then sorted.  Then, only the losses of the RoIs chosen are used for backpropagation.	It is easy for our method to adopt online hard example mining (OHEM) [22] during training. Our negligible per-RoI computation enables nearly cost-free example mining. Assuming N proposals per image, in the forward pass, we evaluate the loss of all N proposals. Then we sort all RoIs (positive and negative) by loss and select B RoIs that have the highest loss. Backpropagation [11] is performed based on the selected examples. Because our per-RoI computation is negligible, the forward time is nearly not affected by N, in contrast to OHEM Fast R-CNN in [22] that may double training time. We provide comprehensive timing statistics in Table 5 in the next section.				
863	paper_37	What does " online hard example mining" mean?	The authors do not explain exactly what OHEM is.  From the provided explanation after OHEM, it seems that it involves only using specific outputs that satisfy a certain condition instead of all outputs for optimization during training.	It is easy for our method to adopt online hard example mining (OHEM) [22] during training. Our negligible per-RoI computation enables nearly cost-free example mining. Assuming N proposals per image, in the forward pass, we evaluate the loss of all N proposals. Then we sort all RoIs (positive and negative) by loss and select B RoIs that have the highest loss. Backpropagation [11] is performed based on the selected examples. Because our per-RoI computation is negligible, the forward time is nearly not affected by N, in contrast to OHEM Fast R-CNN in [22] that may double training time. We provide comprehensive timing statistics in Table 5 in the next section.				
864	paper_37	What are the evidences that region-wise computations are of low cost ?	As seen in Table 5, Faster R-CNN is 2. 5 times slower than R-FCN when mining for 300 RoIs, and 6 times slower when mining for 2000 RoIs, proving that region-wise computations are of low cost.	Next we compare with standard “Faster R-CNN + ResNet-101” [9] which is the strongest competitor and the top-performer on the PASCAL VOC, MS COCO, and ImageNet benchmarks. We use k\times k=7\times 7 in the following.Table 5 shows the comparisons. Faster R-CNN evaluates a 10-layer subnetwork for each region to achieve good accuracy, but R-FCN has negligible per-region cost.With 300 RoIs at test time, Faster R-CNN takes 0.42s per image, 2.5\times slower than our R-FCN that takes 0.17s per image (on a K40 GPU; this number is 0.11s on a Titan X GPU). R-FCN also trains faster than Faster R-CNN.Moreover, hard example mining [22] adds no cost to R-FCN training (Table 5).It is feasible to train R-FCN when mining from 2000 RoIs, in which case Faster R-CNN is 6\times slower (2.9s vs. 0.46s). But experiments show that mining from a larger set of candidates (e.g., 2000) has no benefit (Table 5). So we use 300 RoIs for both training and inference in other parts of this paper.				
865	paper_37	What is the purpose of bounding box regression?	Bounding box regression is needed to find the coordinates of the bounding boxes of the objects within the RoI.	We further address bounding box regression [7, 6] in a similar way. Aside from the above k^{2}(C+1)-d convolutional layer, we append a sibling 4k^{2}-d convolutional layer for bounding box regression. The position-sensitive RoI pooling is performed on this bank of 4k^{2} maps, producing a 4k^{2}-d vector for each RoI. Then it is aggregated into a 4-d vector by average voting. This 4-d vector parameterizes a bounding box as t=(t_{x},t_{y},t_{w},t_{h}) following the parameterization in [6]. We note that we perform class-agnostic bounding box regression for simplicity, but the class-specific counterpart (i.e., with a 4k^{2}C-d output layer) is applicable.				
866	paper_37	What is the difference between (x,y) possible pairs and number of pixels n in each bin?	The number of possible pairs (x,y) is n, as (x,y) iterates through each pixel in the bin.	Position-sensitive score maps & Position-sensitive RoI pooling. To explicitly encode position information into each RoI, we divide each RoI rectangle into k\times k bins by a regular grid. For an RoI rectangle of a size w\times h, a bin is of a size \approx\frac{w}{k}\times\frac{h}{k} [8, 6].In our method, the last convolutional layer is constructed to produce k^{2} score maps for each category. Inside the (i,j)-th bin (0\leq i,j\leq k-1), we define a position-sensitive RoI pooling operation that pools only over the (i,j)-th score map:r_{c}(i,j\leavevmode\nobreak\ |\leavevmode\nobreak\ \Theta)=\sum_{(x,y)\in\text{bin}(i,j)}z_{i,j,c}(x+x_{0},y+y_{0}\leavevmode\nobreak\ |\leavevmode\nobreak\ \Theta)/n.(1)Here r_{c}(i,j) is the pooled response in the (i,j)-th bin for the c-th category, z_{i,j,c} is one score map out of the k^{2}(C+1) score maps, (x_{0},y_{0}) denotes the top-left corner of an RoI, n is the number of pixels in the bin, and \Theta denotes all learnable parameters of the network. The (i,j)-th bin spans \lfloor i\frac{w}{k}\rfloor\leq x<\lceil(i+1)\frac{w}{k}\rceil and \lfloor j\frac{h}{k}\rfloor\leq y<\lceil(j+1)\frac{h}{k}\rceil.The operation of Eqn.(1) is illustrated in Figure 1, where a color represents a pair of (i,j).Eqn.(1) performs average pooling (as we use throughout this paper), but max pooling can be conducted as well.				
867	paper_37	Does each channel maps the a response from a different position of the image ?	The k^2 channels for each category C of the final convolutional layer each map to cells within a spatial grid that correspond to a position relative to an object.	Visualization.In Figure 4 and 4 we visualize the position-sensitive score maps learned by R-FCN when k\times k=3\times 3. These specialized maps are expected to be strongly activated at a specific relative position of an object. For example, the “top-center-sensitive” score map exhibits high scores roughly near the top-center position of an object.If a candidate box precisely overlaps with a true object (Figure 4), most of the k^{2} bins in the RoI are strongly activated, and their voting leads to a high score. On the contrary, if a candidate box does not correctly overlaps with a true object (Figure 4), some of the k^{2} bins in the RoI are not activated, and the voting score is low.Given the proposal regions (RoIs), the R-FCN architecture is designed to classify the RoIs into object categories and background. In R-FCN, all learnable weight layers are convolutional and are computed on the entire image. The last convolutional layer produces a bank of k^{2} position-sensitive score maps for each category, and thus has a k^{2}(C+1)-channel output layer with C object categories (+1 for background). The bank of k^{2} score maps correspond to a k\times k spatial grid describing relative positions. For example, with k\times k=3\times 3, the 9 score maps encode the cases of {top-left, top-center, top-right, …, bottom-right} of an object category.				
868	paper_37	What are the benefits of selective pooling?	Selective pooling allows the model to learn position-sensitive score maps, which contain information crucial for learning.  This can be seen from the k = 1 case, meaning there is only one score map and no spatial information is learned, where the model fails to converge.  Selective pooling also changes the architecture in a way that there is no need for additional layers after the final RoI layer, which greatly decreases computation time.	The concept of position-sensitive score maps is partially inspired by [3] that develops FCNs for instance-level semantic segmentation.We further introduce the position-sensitive RoI pooling layer that shepherds learning of the score maps for object detection. There is no learnable layer after the RoI layer, enabling nearly cost-free region-wise computation and speeding up both training and inference.The importance of position-sensitivity is further demonstrated by setting k=1, for which R-FCN is unable to converge. In this degraded case, no spatial information can be explicitly captured within an RoI. Moreover, we report that naïve Faster R-CNN is able to converge if its RoI pooling output resolution is 1\times 1, but the mAP further drops by a large margin to 61.7% (Table 2).R-FCN ends with a position-sensitive RoI pooling layer. This layer aggregates the outputs of the last convolutional layer and generates scores for each RoI. Unlike [8, 6], our position-sensitive RoI layer conducts selective pooling, and each of the k\times k bin aggregates responses from only one score map out of the bank of k\times k score maps. With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps.Figure 1 illustrates this idea. Figure 4 and 4 visualize an example. The details are introduced as follows.				
869	paper_37	What's the reason for sharing features between R-FCN and RPN	The feature maps contain information from the input image.  The RPN uses those features to find RoIs, while the R-FCN uses those features to detect objects.	Inference. As illustrated in Figure 2, the feature maps shared between RPN and R-FCN are computed (on an image with a single scale of 600). Then the RPN part proposes RoIs, on which the R-FCN part evaluates category-wise scores and regresses bounding boxes.During inference we evaluate 300 RoIs as in [18] for fair comparisons. The results are post-processed by non-maximum suppression (NMS) using a threshold of 0.3 IoU [7], as standard practice.				
871	paper_37	What do authors mean by saying that RoI layer was added "unnaturally" in the ResNet  ?	Old object detection networks used two subnetworks, one being a convolutional subnetwork with a pooling layer, and another being fully connected layers.  The pooling layer served as a RoI pooling layer.  The authors imply that natural intuition for creating a fully convolutional network would be to get rid of the fully connected layers and just keep using the final pooling layer as the only layer in the RoI subnetwork.  But in ResNet, the RoI subnetwork is actually in the middle of the network, which the authors deem to be unnatural.	A prevalent family [8, 6, 18] of deep networks for object detection can be divided into two subnetworks by the Region-of-Interest (RoI) pooling layer [6]: (i) a shared, “fully convolutional” subnetwork independent of RoIs, and (ii) an RoI-wise subnetwork that does not share computation. This decomposition [8] was historically resulted from the pioneering classification architectures, such as AlexNet [10] and VGG Nets [23], that consist of two subnetworks by design — a convolutional subnetwork ending with a spatial pooling layer, followed by several fully-connected (fc) layers. Thus the (last) spatial pooling layer in image classification networks is naturally turned into the RoI pooling layer in object detection networks [8, 6, 18].But recent state-of-the-art image classification networks such as Residual Nets (ResNets) [9] and GoogLeNets [24, 26] are by design fully convolutional111Only the last layer is fully-connected, which is removed and replaced when fine-tuning for object detection.. By analogy, it appears natural to use all convolutional layers to construct the shared, convolutional subnetwork in the object detection architecture, leaving the RoI-wise subnetwork no hidden layer. However, as empirically investigated in this work, this naïve solution turns out to have considerably inferior detection accuracy that does not match the network’s superior classification accuracy. To remedy this issue, in the ResNet paper [9] the RoI pooling layer of the Faster R-CNN detector [18] is unnaturally inserted between two sets of convolutional layers — this creates a deeper RoI-wise subnetwork that improves accuracy, at the cost of lower speed due to the unshared per-RoI computation.				
872	paper_37	How can one explain the statement "inferior detection accuracy that does not match the network’s superior classification accuracy" mentioned by the authors ?	The authors show in Table 2 that a naive implementation of Faster R-CNN, which used the methodology that the authors claimed to have inferior detection accuracy, actually does have inferior detection accuracy.	But recent state-of-the-art image classification networks such as Residual Nets (ResNets) [9] and GoogLeNets [24, 26] are by design fully convolutional111Only the last layer is fully-connected, which is removed and replaced when fine-tuning for object detection.. By analogy, it appears natural to use all convolutional layers to construct the shared, convolutional subnetwork in the object detection architecture, leaving the RoI-wise subnetwork no hidden layer. However, as empirically investigated in this work, this naïve solution turns out to have considerably inferior detection accuracy that does not match the network’s superior classification accuracy. To remedy this issue, in the ResNet paper [9] the RoI pooling layer of the Faster R-CNN detector [18] is unnaturally inserted between two sets of convolutional layers — this creates a deeper RoI-wise subnetwork that improves accuracy, at the cost of lower speed due to the unshared per-RoI computation.Analysis. Table 2 shows the results. We note that the standard (not naïve) Faster R-CNN in the ResNet paper [9] achieves 76.4% mAP with ResNet-101 (see also Table 5), which inserts the RoI pooling layer between conv4 and conv5 [9]. As a comparison, the naïve Faster R-CNN (that applies RoI pooling after conv5) has a drastically lower mAP of 68.9% (Table 2). This comparison empirically justifies the importance of respecting spatial information by inserting RoI pooling between layers for the Faster R-CNN system. Similar observations are reported in [19].Naïve Faster R-CNN. As discussed in the introduction, one may use all convolutional layers in ResNet-101 to compute the shared feature maps, and adopt RoI pooling after the last convolutional layer (after conv5). An inexpensive 21-class fc layer is evaluated on each RoI (so this variant is “almost” fully convolutional). The à trous trick is used for fair comparisons.				
873	paper_39	Will these embeddings be based on measuring similarities between features of new faces and features extracted from faces which the model was trained on ?	FaceNet embeddings can be used to measure similarity between new faces and trained faces.	In this paper we present a unified system for face verification (is this thesame person), recognition (who is this person) and clustering (find commonpeople among these faces). Our method is based on learning a Euclideanembedding per image using a deep convolutional network. The network is trainedsuch that the squared L2 distances in the embedding space directly correspond toface similarity: faces of the same person have small distances and faces ofdistinct people have large distances.				
874	paper_39	What does "online triplet mining method" mean?	Online triplet mining method generates two matching face thumbnails and a non-matching face thumbnail from the training data.	In contrast to these approaches, FaceNet directly trains its output to be acompact 128-D embedding using a triplet-based loss function based onLMNN [19]. Our triplets consist of two matchingface thumbnails and a non-matching face thumbnail and the loss aims to separatethe positive pair from the negative by a distance margin. The thumbnails aretight crops of the face area, no 2D or 3D alignment, other than scale andtranslation is performed.				
875	paper_39	How could we recognize previously unseen person using k-NN?	k-NN classification can be used to recognize the unseen faces by computing the distance between the FaceNet embeddings.	Once this embedding has been produced, then the aforementioned tasks becomestraight-forward: face verification simply involves thresholding the distancebetween the two embeddings; recognition becomes a k-NN classification problem;and clustering can be achieved using off-the-shelf techniques such as k-meansor agglomerative clustering.				
876	paper_39	What does curriculum learning aim at ?	Curriculum learning is a method for good triplets selection.	Choosing which triplets to use turnsout to be very important for achieving good performance and, inspired bycurriculum learning [1], we present a novel onlinenegative exemplar mining strategy which ensures consistently increasingdifficulty of triplets as the network trains. To improveclustering accuracy, we also explore hard-positive mining techniques whichencourage spherical clusters for the embeddings of a single person.				
877	paper_39	What do hard-positive mining techniques mean?	Hard-positive mining techniques use spherical clusters for the embeddings of a single person.	Choosing which triplets to use turnsout to be very important for achieving good performance and, inspired bycurriculum learning [1], we present a novel onlinenegative exemplar mining strategy which ensures consistently increasingdifficulty of triplets as the network trains. To improveclustering accuracy, we also explore hard-positive mining techniques whichencourage spherical clusters for the embeddings of a single person.				
878	paper_39	Does the inception model run different convolutions in parallel on cropped portions of the original images or on the same image?	Inception model runs convolutions in parallel but it is not clear from paper that it runs on patches or complete image.	In this paper we explore two different deep network architectures that havebeen recently used to great success in the computer vision community. Both aredeep convolutional networks [8, 11]. The firstarchitecture is based on the Zeiler&Fergus [22] model whichconsists of multiple interleaved layers of convolutions, non-linearactivations, local response normalizations, and max pooling layers. Weadditionally add several 1{\times}1{\times}d convolution layers inspired bythe work of [9]. The second architecture is based on theInception model of Szegedy et al. which was recently used as thewinning approach for ImageNet 2014 [16]. These networks usemixed layers that run several different convolutional and pooling layers inparallel and concatenate their responses. We have found that these models canreduce the number of parameters by up to 20 times and have the potential toreduce the number of FLOPS required for comparable performance.				
881	paper_4	Why did the authors finetune the VAE on language understanding tasks?	The authors might be performing finetuning of the pretrained model and classifier weights to perform better on low resource language understanding tasks.  However, the authors do explore two different methods - a feature-based method and a fine tune method, and it is not obvious if one is radically better than the other.	Due to the regularization term L R , OPTIMUS can organize sentences in the way specified by the prior distribution. For basic VAEs, a smooth feature space is learned, which is specifically beneficial for better generalization when the number of task-specific labeled data is low. To have a fair comparison, we follow the BERT paper, where the hidden feature of [CLS] is used as the sentence-level representation. In this way, the linear classifiers for both models have the same number of trainable parameters. Though the latent vector z is typically used as sentence-level representation in VAE literature, we argue that the KL regularization applied on z has a large impact on the preceding layer feature h [CLS] . Specifically, h [CLS] is fed into an linear classifier W C ∈ R K×H, where K is the number of classes, with objective − log(softmax(h [CLS] W C >  )). Two schemes are used: (i) Fine-tuning, where both the pre-trained model and the classifier are updated; (ii) Feature-based, where pre-trained model weights are frozen to provide embeddings for the classifier update.				
882	paper_4	How does training pre-training a latent space in Optimus lead to higher performance for dialog generation? Is it because the whole dialog can be encoded in the latent space?	There is no information about dialog generation specifically, to explain if this outperformance by OPTIMUS can be attributed specifically to being able to encode the entire dialog in latent space.	Dialog response generation The open-domain dialog response generation task is considered: generating responses z given a dialog history c. Following (Gao et al, 2019a), we embed the history and response in a joint latent space as z_{S2S} and z_{ae}, respectively. A fusion regularization is used to match the responses to the context. We consider Dailydialog (Li et al, 2017) used in (Gu et al, 2019), which has 13,118 daily conversations. Each utterance is processed as the response of previous 10 context utterances from both speakers. The baseline methods are described in Appendix. We measure the performance using Bleu (Chen and Cherry, 2014), and compute the precision, recall and Fl in Table 4. OPTIMUS shows higher Bleu scores than all existing baselines.				
883	paper_4	Are there any similar approaches to OPTIMUS, but on bigger and more modern architectures (e.g., GPT-J, T5)?	This paper does not mention any existing approaches similar to OPTIMUS which use larger models such as GPT-J or T-5.  The authors do mention in multiple places that using VAEs (which is what OPTIMUS is) is not very common in the field, and that existing attempts to use VAEs for language modelling typically use smaller models that are not very deep.  Both these pieces of information suggest that work using models such as GPT-J or T5 for similar VAE-based approaches does not exist, but it is not possible to say that with certainty from the contents of this paper alone.	While deep generative models (DGMs) such as VAEs are theoretically attractive due to its principle nature, it is now rarely used by practitioners in the modern pre-trained language modeling era where BERT/GPT dominate with strong empirical performance. That’s why this paper makes a timely contribution to making DGMs practical for NLP. We hope that this paper will help renew interest in DGMs for this purpose.Hence, we deliberately keep a simple model, believing that the first pre-trained big VAE model itself and its implications are novel: it helps the community to recognize the importance of DGMs in the pre-training era, and revisit DGMs to make it more practical.Indeed, Optimus is uniquely positioned to learn a smooth latent space to organize sentences, which can enable guided language generation compared with GPT-2, and yield better generalization in low-resource language understanding tasks than BERT.Variational Autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) provide a tractable method to train latent-variable generative models. In NLP, latent variables may assume the role of higher-level sentence representations, which govern a lower-level word-by-word generation process, thus facilitating controlled text generation (Bowman et al., 2016; Hu et al., 2017). By representing sentences in a low-dimensional latent space, VAEs allow easy manipulation of sentences using the corresponding compact vector representations, such as feature regularization specified by prior distributions, and guided sentence generation with interpretable vector operators. Despite the attractive theoretical strengths, the current language VAEs are often built with shallow network architectures, such as two-layer LSTMs (Hochreiter and Schmidhuber, 1997). This limits the model’s capacity and leads to sub-optimal performance.				
884	paper_4	How is the Optimus pre-training objectives and its information bottleneck approach any different to those of traditional VAEs (used for image generation)?	The paper explains how information theoretic principles can be used to measure the predictive power of a model and its' compactness (a measure of how complex the learned representations are)and represent it as a tradeoff.  They explain how they manage to inject conditioning vectors into GPT without having to pretrain it again specifically for this and also discuss how they combine GPT and BERT.  However, the authors do not specifically discuss the differences between their VAE by comparing it with VAEs used for image generation.  Presumably, this might be because the authors intended to create a simple VAE model, as a proof of concept that such models can work well for language tasks as well, which is why might not be making extensive customisations to tailor VAEs for language tasks.	From an information theory perspective, information bottleneck (IB) provides a principled approach to find the trade-off between predictive power and complexity (compactness) when summarizing observed data in learned representations. We show that our Optimus pre-training objectives effectively practice the IB principle as follows.Along the way to build the first big VAE language model, there are several technical contributions/implications that are novel: (\textup{\it i}) Latent vector injection: this work demonstrates two schemes to discuss how to effectively inject conditioning vectors into GPT-2 without re-training it.(\textup{\it ii}) The design idea to combine BERT/GPT-2 serves as a practical recipe to inspire people to integrate and reuse existing PLMs for larger and complex models.(\textup{\it iii}) Pre-training on massive datasets itself is an effective approach to reduce KL vanishing, as demonstrated by the state of-the-art performance on four VAE language modeling datasets. (\textup{\it iv}) The proof of VAE objective from the lens of IB, showing that VAE is a principled approach to balance the compactness and usability of learned representations.(\textup{\it v}) Improved performance on several language tasks shows the importance and necessity of pre-training a latent space.				
885	paper_4	How are WordPiece Embeddings and Byte Pair Encoding tokenization different, and why do BERT and GPT-2 use them respectively?	BERT and GPT2 are different kinds of models, which is why they might be using different kinds of encoding schemes.  BERT is primarily an encoder model, while GPT-like models are generative models that autoregressively predict the next token based on the series of tokens seen so far.  This primary difference in class of models might explain why BERT uses WPE while GPT uses BPE tokenization.  More details on why these specific tokenization schemes are used for each model can not be found in this paper.	PLMs can generally play two different roles:(\textup{\it i})a generic encoder such as BERT Devlin et al. (2019) to provide contextualized representations for language understanding tasks, and(\textup{\it ii})a powerful decoder such as GPT-2 Radford et al. (2019) to generate text sequences in an auto-regressive manner. In a bid to combine language understanding and generation tasks in one unified framework, several model variants have been proposed, including UniLM Dong et al. (2019), BART Lewis et al. (2019), and T5 Raffel et al. (2019). Although significant performance improvement has been reported on a wide range of NLP tasks, these models lack of explicit modeling of structures in a compact latent space, rendering it difficult to control language generation/representation from an abstract level.Two technical questions remain, when pre-training Optimus from BERT & GPT-2:(\textup{\it i}) How to represent sentences, since the two PLMs employ different tokenization schemes?(\textup{\it ii}) How to adapt a pre-trained GPT-2 to arbitrary conditional input without re-training the model again? Controllable GPT-2 models have been studied in Keskar et al. (2019); Zellers et al. (2019); Peng et al. (2020a, b) when prescribed control codes/tokens are provided, but it is still unknown how to ground GPT-2 to arbitrary conditional inputs.In BERT, WordPiece Embeddings (WPE) is used for tokenization (vocabulary size is 28996 for the cased version). In GPT-2, the modified Byte Pair Encoding (BPE) Radford et al. (2019) is used for tokenization (vocabulary size is 50260). A given token is represented as {\boldsymbol{h}}_{\texttt{Emb}}, by summing the corresponding token, position and segment embeddings 333Optimus does not require segment embeddings, but we remain it due to BERT initialization..For a sentence, we present it in both types of tokenization: the input of encoder is WPE, and the output of decoder is BPE to compute the reconstruction loss.However, the only source of variation in NLMs, GPT2 and GPT3 is modeled in the conditionals at every step: the text generation process only depends on previous word tokens, and there is limited capacity for the generation to be guided by the higher-level structures that are likely presented in natural language, such as tense, topics or sentiment.				
886	paper_4	If the same information from the latent vector is being added during decoding, why did the Memory scheme yield higher performance?	The authors theorize that the memory scheme is better since the latent information is accessible to every layer in the neural network, instead of being available to only two layers (input, output) in the embedding approach.	Similar to BERT, the first token of every sentence is always a special classification token ([CLS]). The last-layer hidden state {\boldsymbol{h}}_{\texttt{[CLS]}}\in\mathbb{R}^{H} corresponding to this token is used as the sentence-level representation. It further constructs the latent representation \boldsymbol{z}={{\bf W}}_{\text{E}}{\boldsymbol{h}}_{\texttt{[CLS]}}, where \boldsymbol{z}\in\mathbb{R}^{P} is a P-dimensional vector and {{\bf W}}_{\text{E}}\in\mathbb{R}^{P\times H} is the weight matrix. To facilitate \boldsymbol{z} in GPT-2 decoding without re-training the weights, we consider two schemes, illustrated in Figure 2:•Memory: \boldsymbol{z} plays the role of an additional memory vector {\boldsymbol{h}}_{\texttt{Mem}} for GPT2 to attend. Specifically, {\boldsymbol{h}}_{\texttt{Mem}}={{\bf W}}_{\text{M}}\boldsymbol{z}, where {{\bf W}}_{\text{M}}\in\mathbb{R}^{LH\times P} is the weight matrix. {\boldsymbol{h}}_{\texttt{Mem}}\in\mathbb{R}^{LH} is separated into L vectors of length H, each of which is attended by GPT-2 in one layer.•Embedding: \boldsymbol{z} is added on the original embedding layer, and directly used in every decoding step. The new embedding representation is {\boldsymbol{h}}_{\texttt{Emb}}^{\prime}={\boldsymbol{h}}_{\texttt{Emb}}+{{\bf W}}_{\text{D}}\boldsymbol{z}, where {{\bf W}}_{\text{D}}\in\mathbb{R}^{H\times P}.We study their empirical performance in Section B.1 of Appendix, and observe that Memory is significantly more effective than Embedding, and the integration of both schemes yields slightly better results. We hypothesize that the reason why Memory is superior is because it allows the decoder to attend the latent information at every layer ofthe network directly, while the Embedding method only allows the decoder to see the latentinformation at the input and output layer.In our experiments, we use the integration scheme by default.In summary, the encoder parameters \boldsymbol{\phi}=\{\boldsymbol{\phi}_{\text{BERT}},{{\bf W}}_{\text{E}}\}, and decoder parameters \boldsymbol{\theta}=\{\boldsymbol{\theta}_{\text{GPT-2}},{{\bf W}}_{\text{M}},{{\bf W}}_{\text{D}}\}.				
887	paper_4	Why does annealing the value of beta and, as a consequence, decrease the KL regularization during training cause the decoder to make greater use of z?	While the authors explained in detail the method in which they cyclically annealed the value of beta while training their VAE and that KL regularization impacts features on the previous layer, this paper does not delve into the reasons why annealing the value of beta causes the decoder to make greater use of z.  This is possibly because annealing beta as described in the paper is a widely used practice while training models such as these, which is why the authors may not have chosen to explain this assuming that these are broadly known pieces of information.	To reduce this issue, we follow the intuition that if the encoder is providing useful information from the beginning of decoder training, the decoder is more likely to make use of \boldsymbol{z} Fu et al. (2019); He et al. (2019). Specifically, we use the cyclical schedule to anneal \beta for 10 periods Fu et al. (2019).Within one period, there are three consecutive stages: Training AE (\beta=0) for 0.5 proportion, annealing \beta from 0 to 1 for 0.25 proportion, and fixing \beta=1 for 0.25 proportion. When \beta>0, we use the KL thresholding scheme Li et al. (2019); Kingma et al. (2016), and replace the KL term \mathcal{L}_{R} in (6) with a hinge loss term that maxes each component of the original KL with a constant \lambda:ℒR′=∑imax[λ,KL(qϕ(zi|𝒙)||p(zi))]\displaystyle\mathcal{L}_{R}^{\prime}=\sum_{i}\max[\lambda,\mbox{KL}(q_{\boldsymbol{\phi}}(z_{i}|\boldsymbol{x})||p(z_{i}))]caligraphic_L start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_max [ italic_λ , KL ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_x ) | | italic_p ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ](9)Here, z_{i} denotes the ith dimension of \boldsymbol{z}. Usingthe thresholding objective causes learning to give up driving down KL for dimensions of \boldsymbol{z} that are already beneath the target compression rate.Due to the regularization term \mathcal{L}_{R}, Optimus can organize sentences in the way specified by the prior distribution. For basic VAEs, a smooth feature space is learned, which is specifically beneficial for better generalization when the number of task-specific labeled data is low.To have a fair comparison, we follow the BERT paper, where the hidden feature of [CLS] is used as thesentence-level representation. In this way, the linear classifiers for both models have the same number of trainableparameters. Though the latent vector \boldsymbol{z} is typically used as sentence-level representation in VAE literature, we argue that the KL regularization applied on \boldsymbol{z} has a large impact on the preceding layer feature {\boldsymbol{h}}_{\texttt{[CLS]}}.Specifically, {\boldsymbol{h}}_{\texttt{[CLS]}} is fed into an linear classifier {{\bf W}}_{\text{C}}\in\mathbb{R}^{K\times H}, where K is the number of classes, with objective -\log(\text{softmax}({\boldsymbol{h}}_{\texttt{[CLS]}}{{\bf W}}_{\text{C}}^{\top})). Two schemes are used:(\textup{\it i}) Fine-tuning, where both the pre-trained model and the classifier are updated;(\textup{\it ii}) Feature-based, where pre-trained model weights are frozen to provide embeddings for the classifier update.				
888	paper_4	Why does higher performance than hierarchical VAE (hVAE) show that it is important to pre-train a latent space? Did hVAE not pre-train a latent space or was the approach different?	The authors mention that their outperformance of existing VAE methods might be due to the efficacy of their pretraining method which is able to solve the KL vanishing issue sufficiently, but it is unclear if hVAE faced similar problems or not.	The results are shown in Table 1. Various \lambda values are used, we observe a trade-off between language modeling and representation learning, controlled by \lambda. Compared with existing VAE methods, Optimus achieve significantly lower perplexity, and higher MI/AU. This indicates that our pre-training method is an effective approach to reduce KL vanishing issue and training VAEs, especially given the fact that we only fine-tune on these datasets for one epoch. Optimus achieves lower perplexity compared with GPT-2 on three out of four datasets. Intuitively, this is because the model can leverage the prior language knowledge encoded in \boldsymbol{z}. This gap is larger, when the sentences in the dataset exhibit common regularities, such as \mathtt{SNLI}, where the prior plays a more important/effective role in this scenario.Though the form of our model is simple, Optimus shows stronger empirical performance than sophisticated models that are particularly designed for long-text, such as hVAE in Shen et al. (2019). For example, the KL and PPL ofOptimus (15.09 and 22.79) are much better than hVAE (6.8 and 45.8) on Yelp dataset. This verifies the importance of pre-training a latent space.The full experimental results are shown in Table 8, 9, 10 and 11 of Appendix.				
889	paper_4	What does "Active Units" mean and how is it measured?	“Active units” is a measurement metric used by the authors to measure the learning capacity of their model.  Additional information on how it is measured is not available in this paper, presumably because it is a widely known unit of measurement in the field.	There are two types of metrics to evaluate language VAEs.(\textup{\it i}) Generation capability: we use perplexity (PPL). Note that NLM and GPT-2 has exactly PPL, while VAEs does not. Following He et al. (2019), we use the importance weighted bound in Burda et al. (2015) to approximate \log p(\boldsymbol{x}), and report PPL.(\textup{\it ii}) Representation learning capability: Active units (AU) of \boldsymbol{z} and its Mutual Information (MI) with \boldsymbol{x}.We report the full results with ELBO, KL and Reconstruction in Appendix, but note that higher ELBO does not necessarily yield better language modeling.				
890	paper_4	How do the specific contributions of this work make the construction of deep generative models, like VAEs, for language modeling more practical?	The main thesis of this work is around the idea that large VAE models for language tasks can work effectively, and the authors attempt to provide initial evidence for this by implementing a large model which they named OPTIMUS.  The first major contribution the authors make is in showing how the KL vanishing issue is addressed in the pretraining phase.  Next, the authors explain how conditioning vectors can be injected into GPT without the need for retraining, which brings down the cost and barrier to entry to develop models such as these.  Finally, the authors also discuss how to combine multiple pretrained language models (PLMs) such as BERT and GPT, which have very different input formats (i.  tokenization schemes).	Variational Autoencoders (VAEs) Kingma and Welling (2013); Rezende et al. (2014) provide a tractable method to train latent-variable generative models. In NLP, latent variables may assume the role ofhigher-level sentence representations, which govern a lower-level word-by-word generation process, thus facilitating controlled text generation Bowman et al. (2016); Hu et al. (2017). By representing sentences in a low-dimensional latent space, VAEs allow easy manipulation of sentences using the corresponding compact vector representations, such as feature regularization specified by prior distributions, and guided sentence generation with interpretable vector operators.Despite the attractive theoretical strengths, the current language VAEs are often built with shallow network architectures, such as two-layer LSTMs Hochreiter and Schmidhuber (1997). This limits the model’s capacity and leads to sub-optimal performance.Along the way to build the first big VAE language model, there are several technical contributions/implications that are novel: (\textup{\it i}) Latent vector injection: this work demonstrates two schemes to discuss how to effectively inject conditioning vectors into GPT-2 without re-training it.(\textup{\it ii}) The design idea to combine BERT/GPT-2 serves as a practical recipe to inspire people to integrate and reuse existing PLMs for larger and complex models.(\textup{\it iii}) Pre-training on massive datasets itself is an effective approach to reduce KL vanishing, as demonstrated by the state of-the-art performance on four VAE language modeling datasets. (\textup{\it iv}) The proof of VAE objective from the lens of IB, showing that VAE is a principled approach to balance the compactness and usability of learned representations.(\textup{\it v}) Improved performance on several language tasks shows the importance and necessity of pre-training a latent space.While deep generative models (DGMs) such as VAEs are theoretically attractive due to its principle nature, it is now rarely used by practitioners in the modern pre-trained language modeling era where BERT/GPT dominate with strong empirical performance. That’s why this paper makes a timely contribution to making DGMs practical for NLP. We hope that this paper will help renew interest in DGMs for this purpose.Hence, we deliberately keep a simple model, believing that the first pre-trained big VAE model itself and its implications are novel: it helps the community to recognize the importance of DGMs in the pre-training era, and revisit DGMs to make it more practical.Indeed, Optimus is uniquely positioned to learn a smooth latent space to organize sentences, which can enable guided language generation compared with GPT-2, and yield better generalization in low-resource language understanding tasks than BERT.All these efforts utilize simple LSTM Hochreiter and Schmidhuber (1997) and shallow Transformer Vaswani et al. (2017) architectures, thus with limited capacity. Our paper is the first big VAE model at the same scale of recent PLMs such as BERT and GPT-2. More importantly, we show that pre-training a meaningful latent space on a large text corpus can largely reduce the KL vanishing issue, and lead to new state-of-the-art performance.				
891	paper_4	How is using a conditional GAN to produce a latent vector from a label different to using the encoder in OPTIMUS with just the label as its input?	The conditional GAN generates a latent vector which is then passed to OPTIMUS' decoder, which produces the output.  It is unclear if merely passing a label to OPTIMUS' encoder would be sufficient to generate a useful latent space encoding since the OPTIMUS encoder has not been trained with this objective in mind.	The short \mathtt{Yelp} dataset collected in Shen et al. (2017) is used. It contains 444K training sentences, and we use separated datasets of 10K sentences for validation/testing, respectively. The goal is to generate text reviews given the positive/negative sentiment. We fine-tune Optimus using the VAE objective on the dataset, then freeze backbone weights. A conditional GAN Mirza and Osindero (2014) is trained on the fixed latent space. The generation process is to first produce a latent vector \boldsymbol{z}_{y} based on a given label y using conditional GAN, then generate sentences conditioned on \boldsymbol{z}_{y} using the decoder.The baselines are described in Appendix. G-score computes the geometric mean of Accuracy and Bleu, measuring the comprehensive quality of both content and style.Self-Bleu measures the diversity of the generated sentences. The results are shown in Table 6, Optimus achieves the best performance on all metrics.This verifies the importance of learning a smooth and meaningful latent space.The conditional generated sentences are shown in Appendix.				
892	paper_4	What metric did the authors use to measure generalizability on low-resource language understanding tasks?	The main performance metric the authors use to measure generalizability on low resource is the GLUE benchmark.  More broadly, the authors explain that their model is suitable for low resource settings to begin with since their model can be specialized at low cost (through feature based approaches) and can function with very little labelled data.	We further consider the GLUE benchmark Wang et al. (2019), which consists of nine datasets for general language understanding.Following the finetuning schedule in Devlin et al. (2019), we use learning rate [2,3,4,5]\times 10^{-5} and train the model for 3 epochs. We select the best performance among different runs. We show the results on the validation set in Table 7.With the feature-based scheme, Optimus yields higher performance than BERT, especially on the large datasets such as MNLI, QQP and QNLI. When the full models are fine-tuned, the two methods perform quite similarly.In summary, the scenarios that Optimus fit the low-resource settings are two-fold: (1) The required computing resource is low: the feature-based approach only updates the classifier, whosecomputing requirement is much lower than full-model fine-tuning; (2) The number of required labelled data is low: when labelled data is rare, Optimus adapts better.The results confirm that Optimus can maintain and exploit the structures learned in pre-training, and presents a more general representation that can be adapted to new tasks more easily than BERT – feature-based adaption is much faster and easier to perform than fine-tuning.				
893	paper_4	What does "smooth feature regularization" mean?	The authors do not explicitly define what "smooth" means anywhere in the paper, though possible meanings could be interpolated from the author's statements in the papers.  The authors mention that the regularization term in Optimus is what helps a basic VAE learn a smooth feature space.  They also use t-SNE to visualize learned features, which indicates that "smooth" in this context just means cleaner, free-flowing boundaries of the latent space.  However, additional information is required to formally define smoothness, which does not seem to be available in the paper. 	•AE. Only \mathcal{L}_{E} is considered (\beta=0), while the Gaussian sampling in q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) remains. In other words, the regularization is removed, and a point-estimate is likely to be learned to represent the text sequence’s latent feature.Note our reconstruction is on sentence-level, while other PLMs Devlin et al. (2019); Yang et al. (2019) employ masked LM loss, performing token-level reconstruction. •VAE. The full VAE objective is considered (\beta>0). It tends to learn a smooth latent space due to \mathcal{L}_{R}.Due to the regularization term \mathcal{L}_{R}, Optimus can organize sentences in the way specified by the prior distribution. For basic VAEs, a smooth feature space is learned, which is specifically beneficial for better generalization when the number of task-specific labeled data is low.To have a fair comparison, we follow the BERT paper, where the hidden feature of [CLS] is used as thesentence-level representation. In this way, the linear classifiers for both models have the same number of trainableparameters. Though the latent vector \boldsymbol{z} is typically used as sentence-level representation in VAE literature, we argue that the KL regularization applied on \boldsymbol{z} has a large impact on the preceding layer feature {\boldsymbol{h}}_{\texttt{[CLS]}}.Specifically, {\boldsymbol{h}}_{\texttt{[CLS]}} is fed into an linear classifier {{\bf W}}_{\text{C}}\in\mathbb{R}^{K\times H}, where K is the number of classes, with objective -\log(\text{softmax}({\boldsymbol{h}}_{\texttt{[CLS]}}{{\bf W}}_{\text{C}}^{\top})). Two schemes are used:(\textup{\it i}) Fine-tuning, where both the pre-trained model and the classifier are updated;(\textup{\it ii}) Feature-based, where pre-trained model weights are frozen to provide embeddings for the classifier update.We use tSNE Maaten and Hinton (2008) to visualize the learned feature on a 2D map. The validation set of Yelp is used to extract the latent features.Compared with BERT, Optimus learns a smoother space and more structured latent patterns, which explains why Optimus can yield better classification performance and faster adaptation.				
894	paper_4	What is "KL vanishing" with respect to VAEs?	The authors do not explicitly explain the KL vanishing problem in detail, but they do cite a recent work, Bowman et al.  (2016), that probably contains more detailed information on this problem.  Additionally, the authors explain that KL regularization is a problem that specifically happens with Variational Autoencoders only (i.  regular AEs do not seem to have this problem).  When explaining how VAEs can be considered to be equivalent to AEs with KL regularization, we see the main difference between AEs and VAEs is the extra KL term added to VAEs.  Putting all this information together, one could conclude that "KL vanishing" refers to this KL term in a VAE becoming zero during training.  The authors list multiple methods (annealing, specialized decoder architectures, auxiliary loss functions, etc) that people in the field have used to prevent this from happening. 	There is an alternative interpretation of the ELBO: the VAE objective can be viewed as a regularized version of the autoencoder (AE) Goodfellow et al. (2016).It is thus natural to extend the negative of \mathcal{L}_{\text{ELBO}} in (3) by introducing a hyper-parameter \beta to control the strength of regularization:\displaystyle\mathcal{L}_{\beta}\displaystyle=\mathcal{L}_{E}+\beta\mathcal{L}_{R},~{}~{}\text{with}(4)\displaystyle\mathcal{L}_{E}\displaystyle=-\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}\big{[}\log p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z})\big{]}(5)\displaystyle\mathcal{L}_{R}=KL(qϕ(𝒛|𝒙)||p(𝒛))\displaystyle=\mbox{KL}(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})||p(\boldsymbol{z}))= KL ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) | | italic_p ( bold_italic_z ) )(6)where \mathcal{L}_{E} is the reconstruction error (or negative log-likelihood (NLL)), and \mathcal{L}_{R} is a KL regularizer.The cost function \mathcal{L}_{\beta} provides a unified perspective for understanding various autoencoder variants and training methods. We consider two types of latent space with the following objectives:We train the model parameters \{\boldsymbol{\phi},\boldsymbol{\theta}\} using two objectives: AE and VAE, discussed in Section 4.1. Pre-training AE using  (5) is straightforward. However, pre-training VAE can be challenging due to the notorious KL vanishing issue Bowman et al. (2016), where(\textup{\it i})an encoder that produces posteriors almost identical to the Gaussian prior for all sentences (rather than a more interesting posterior); and(\textup{\it ii})a decoder that completely ignores \boldsymbol{z} in (2), and a learned model that reduces to a simpler NLM.Along the way to build the first big VAE language model, there are several technical contributions/implications that are novel: (\textup{\it i}) Latent vector injection: this work demonstrates two schemes to discuss how to effectively inject conditioning vectors into GPT-2 without re-training it.(\textup{\it ii}) The design idea to combine BERT/GPT-2 serves as a practical recipe to inspire people to integrate and reuse existing PLMs for larger and complex models.(\textup{\it iii}) Pre-training on massive datasets itself is an effective approach to reduce KL vanishing, as demonstrated by the state of-the-art performance on four VAE language modeling datasets. (\textup{\it iv}) The proof of VAE objective from the lens of IB, showing that VAE is a principled approach to balance the compactness and usability of learned representations.(\textup{\it v}) Improved performance on several language tasks shows the importance and necessity of pre-training a latent space.Language VAEs have inspired new applications in NLP, via exploiting many interestingproperties of the model’s latent space Bowman et al. (2016); Kim et al. (2018b).Its modeling capacity and empirical performance is somewhat limited, partially due to the KL vanishing issue described in Section 4.3. Several attempts have been made to alleviate this issue, including different KL annealing/thresholding schemes Bowman et al. (2016); Fu et al. (2019); Higgins et al. (2017); Li et al. (2019), decoder architectures Yang et al. (2017); Dieng et al. (2018), auxiliary loss Zhao et al. (2017), semi-amortized inference Kim et al. (2018a), aggressive encoder training schedule He et al. (2019), batch normalized inference Zhu et al. (2020) and flexible posterior Fang et al. (2019). Subramanian et al. (2018) have shown some promise that general encoder can benefit language generation. Transformers Vaswani et al. (2017) are recently considered in VAEs for classification Gururangan et al. (2019) and storytelling Wang and Wan (2019). Pre-training VAEs has been recently considered in conditional text generation to amortize the training of decoders and to allow easy adaptation in new generation tasks Duan et al. (2019).				
897	paper_4	Is the regularization here intended to make sure that the prior distribution is similar to a given distribution like a Gaussian distribution?	Additionally, authors discuss how the degree of regularization can be controlled through a parameter, beta.	There is an alternative interpretation of the ELBO: the VAE objective can be viewed as a regularized version of the autoencoder (AE) Goodfellow et al. (2016).It is thus natural to extend the negative of \mathcal{L}_{\text{ELBO}} in (3) by introducing a hyper-parameter \beta to control the strength of regularization:\displaystyle\mathcal{L}_{\beta}\displaystyle=\mathcal{L}_{E}+\beta\mathcal{L}_{R},~{}~{}\text{with}(4)\displaystyle\mathcal{L}_{E}\displaystyle=-\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}\big{[}\log p_{\boldsymbol{\theta}}(\boldsymbol{x}|\boldsymbol{z})\big{]}(5)\displaystyle\mathcal{L}_{R}=KL(qϕ(𝒛|𝒙)||p(𝒛))\displaystyle=\mbox{KL}(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})||p(\boldsymbol{z}))= KL ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_italic_z | bold_italic_x ) | | italic_p ( bold_italic_z ) )(6)where \mathcal{L}_{E} is the reconstruction error (or negative log-likelihood (NLL)), and \mathcal{L}_{R} is a KL regularizer.The cost function \mathcal{L}_{\beta} provides a unified perspective for understanding various autoencoder variants and training methods. We consider two types of latent space with the following objectives:Due to the regularization term \mathcal{L}_{R}, Optimus can organize sentences in the way specified by the prior distribution. For basic VAEs, a smooth feature space is learned, which is specifically beneficial for better generalization when the number of task-specific labeled data is low.To have a fair comparison, we follow the BERT paper, where the hidden feature of [CLS] is used as thesentence-level representation. In this way, the linear classifiers for both models have the same number of trainableparameters. Though the latent vector \boldsymbol{z} is typically used as sentence-level representation in VAE literature, we argue that the KL regularization applied on \boldsymbol{z} has a large impact on the preceding layer feature {\boldsymbol{h}}_{\texttt{[CLS]}}.Specifically, {\boldsymbol{h}}_{\texttt{[CLS]}} is fed into an linear classifier {{\bf W}}_{\text{C}}\in\mathbb{R}^{K\times H}, where K is the number of classes, with objective -\log(\text{softmax}({\boldsymbol{h}}_{\texttt{[CLS]}}{{\bf W}}_{\text{C}}^{\top})). Two schemes are used:(\textup{\it i}) Fine-tuning, where both the pre-trained model and the classifier are updated;(\textup{\it ii}) Feature-based, where pre-trained model weights are frozen to provide embeddings for the classifier update.				
899	paper_41	What are benefits of using learnable parameters for capturing positional information rather than using sines and cosines to capture these positions?	The two choices of Positional encoding are learned and fixed.  In the experiments the two versions produced nearly identical results.  The fixed sinusoidal positional encoding has the advantage that it can handle the sequence lengths longer than the ones encountered during training.	Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_{\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (JonasFaceNet2017, ).We also experimented with using learned positional embeddings (JonasFaceNet2017, ) instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.				
900	paper_41	Can't we use parallelization with RNN layers approach with any possible way?	Because hidden state of each input position depends on previous hidden state therefore RNN can not be parallelized.  Whereas Transformer due to attention layers are highly parallel.	Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_{t}, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved significant improvements in computational efficiency through factorization tricks (Kuchaiev2017Factorization, ) and conditional computation (shazeer2017outrageously, ), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.				
901	paper_41	How could restricting self attention to some window with size r be useful with long term dependencies?	Restricting self attention to some window with size r does improve computational performance but its effect on long term dependencies have not been explored in the paper.	As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.				
902	paper_41	How would be the results and performance considering accuracy and losses while using window-with-size r self-attention approach with shorter sequences?	Window-with-size r self-attention approach is only recommended to only improve computational performance for tasks involving very long sequences.	As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece (wu2016google, ) and byte-pair (sennrich2015neural, ) representations.To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.				
903	paper_41	why would we need to increase learning rate for the first few training steps while we initially use Adam?	In adam optimizer learning rate is linearly increased ay startup for the purpose of warmup during training.	We used the Adam optimizer (kingma2014adam, ) with \beta_{1}=0.9, \beta_{2}=0.98 and \epsilon=10^{-9}. We varied the learning rate over the course of training, according to the formula:This corresponds to increasing the learning rate linearly for the first warmup\_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup\_steps=4000.				
904	paper_41	What are the results and performance with no using of label smoothing ?	After applying label smoothing regularization perplexity decreases but the accuracy and BLEU score does improve.  This means if we do not apply label smoothing accuracy and BLEU score would be decreased.	During training, we employed label smoothing of value \epsilon_{ls}=0.1 (DBLP:journals/corr/SzegedyVISW15, ). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.				
905	paper_41	Can we generalize applying transformers to translate from any language to another rather than English, like German-to-Arabic for example?.	Since the Transformer performed great on   English-to-French and English-to-German translation tasks and can be trained significantly faster than architectures based on recurrent or convolutional layers therefore it can be hoped that it can be used for any language other than English.	On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_{drop}=0.1, instead of 0.3.For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.				
907	paper_41	what is the definition of BLEU?	BLEU score is the metric to compute performance of the language translation task.   On the WMT 2014 English-to-German translation task, big transformer model establishes a new state-of-the-art BLEU score of 28.  BLUE score also drops with single head or too many heads.	On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_{drop}=0.1, instead of 0.3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.				
908	paper_41	Does Conditional Computation mentioned by the authors mean to perform operations depending on the need to perform them?	Paper only mention the advantages of conditional computation that is to improve computational efficiency and model performance.	Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_{t}, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.Recent work has achieved significant improvements in computational efficiency through factorization tricks (Kuchaiev2017Factorization, ) and conditional computation (shazeer2017outrageously, ), while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.				
909	paper_41	Do conv Nets succeed in sequence modelling in general?	Conv Nets can compute the hidden state of the sequence data in parallel for all input and output positions.  However conv nets are still more expensive than the recurrent networks.	A single convolutional layer with kernel width k<n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log_{k}(n)) in the case of dilated convolutions (NalBytenet2017, ), increasing the length of the longest paths between any two positions in the network.Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions (xception2016, ), however, decrease the complexity considerably, to O(k\cdot n\cdot d+n\cdot d^{2}). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU (extendedngpu, ), ByteNet (NalBytenet2017, ) and ConvS2S (JonasFaceNet2017, ), all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions (hochreiter2001gradient, ). In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.				
910	paper_41	How could reducing the number of operations into constant result in decreasing resolution ?	The resolution  is decreased due to averaging in the attention position with Multi-Head Attention.	Instead of performing a single attention function with d_{\text{model}}-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d_{k}, d_{k} and d_{v} dimensions, respectively.On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d_{v}-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU (extendedngpu, ), ByteNet (NalBytenet2017, ) and ConvS2S (JonasFaceNet2017, ), all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions (hochreiter2001gradient, ). In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.				
911	paper_41	Is it crucial to use 6 layers in the encoder? if it is free to change, Does increasing those layers need more data to avoid overfitting or just would take longer time to converge?	For translation tasks the result shows that 6 layers are the optimal number of layers.	The encoder is composed of a stack of N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection (he2016deep, ) around each of the two sub-layers, followed by layer normalization layernorm2016 . That is, the output of each sub-layer is \mathrm{LayerNorm}(x+\mathrm{Sublayer}(x)), where \mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\text{model}}=512.The decoder is also composed of a stack of N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.				
912	paper_41	Why -while having data of next positions in training dataset- is it important to modify the self-attention sub-layer in the decoder stack to ensure that the predictions for position i can depend only on the known outputs at positions less than ?	Self attention layer in transformer is modified in decoder stack to attend only the past predictions to preserve the auto-regressive property in the language models.	The decoder is also composed of a stack of N=6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.The Transformer uses multi-head attention in three different ways:•In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (wu2016google, ; bahdanau2014neural, ; JonasFaceNet2017, ).•The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -\infty) all values in the input of the softmax which correspond to illegal connections. See Figure 2.				
913	paper_41	What is Attention Function?How is it calculated?	Attention function relates different positions of the sequence to get the overall representation of the sequence.  It can be computer by additive attention method or the dot-product method.  Attention have been successfully applied in various NLP tasks such as reading comprehensions and summarizations.	An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.The two most commonly used attention functions are additive attention (bahdanau2014neural, ), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \frac{1}{\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations (cheng2016long, ; decomposableAttnModel, ; paulus2017deep, ; lin2017structured, ).				
914	paper_41	What are similarities and differences between Key, Value, and Query?	Key, Value and Query are all vectors, All are used to compute attention.  Queries are mapped to a pair of key and value during attention computations.	An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.				
915	paper_41	What is the Difference Between Additive and Multiplicative Attention?	Dot product attention is calculated using optimized matrix multiplication  operations whereas Additive attention is computed by compatibility function using a feed-forward network with a single hidden layer.  Multiplicative Attention is much faster and more space-efficient than the additive attention.	The two most commonly used attention functions are additive attention (bahdanau2014neural, ), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \frac{1}{\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of d_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_{k} (DBLP:journals/corr/BritzGLL17, ). We suspect that for large values of d_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q\cdot k=\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 0 and variance d_{k}.. To counteract this effect, we scale the dot products by \frac{1}{\sqrt{d_{k}}}.	Dot product attention is calculated using optimized matrix multiplication  operations whereas Additive attention is computed by compatibility function using a feed-forward network with a single unseen layer.  Multiplicative Attention is much faster and more space-efficient than the additive attention.	Tortured phrases	hidden layer -> unseen layer	
916	paper_41	Shouldn't the inputs in the decoder updated while training after initially setting them to -inf ?	Transformer decoder generates an output sequence (y_{1},. ,y_{m}) of symbols one element at a time by using the encoder information.	Most competitive neural sequence transduction models have an encoder-decoder structure (cho2014learning, ; bahdanau2014neural, ; sutskever14, ). Here, the encoder maps an input sequence of symbol representations (x_{1},...,x_{n}) to a sequence of continuous representations \mathbf{z}=(z_{1},...,z_{n}). Given \mathbf{z}, the decoder then generates an output sequence (y_{1},...,y_{m}) of symbols one element at a time. At each step the model is auto-regressive (graves2013generating, ), consuming the previously generated symbols as additional input when generating the next.				
917	paper_41	How should positional information be interpreted or captured with sines and cosines?	Positional encoding can be generated using sinusoidal function whose wavelengths form a geometric progression which can encode relative positions.  Advantage of the sinusoidal positional encoding is that it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.	where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\pi to 10000\cdot 2\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}.We also experimented with using learned positional embeddings (JonasFaceNet2017, ) instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.				
918	paper_41	Do we care in general for resolution while modelling language?	As the transformers are used for language modeling there is a resolution problem due to averaging in attention weights.  This problem is reduced by using the multiheaded attention.	The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.				
919	paper_41	Does the statement "At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next" implies that we can use attention in handling time series forecasting?	Attention can be used for sequence modeling and can be used to build encoder decoder models which can handle time series forecasting.	In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.Most competitive neural sequence transduction models have an encoder-decoder structure (cho2014learning, ; bahdanau2014neural, ; sutskever14, ). Here, the encoder maps an input sequence of symbol representations (x_{1},...,x_{n}) to a sequence of continuous representations \mathbf{z}=(z_{1},...,z_{n}). Given \mathbf{z}, the decoder then generates an output sequence (y_{1},...,y_{m}) of symbols one element at a time. At each step the model is auto-regressive (graves2013generating, ), consuming the previously generated symbols as additional input when generating the next.				
920	paper_41	What is space meant in "space-efficient in practice" ?Does it mean space of search for solutions? or space in memory?	Dot product attention is much faster and more space-efficient than the additive attention.  Here the space efficient is referring to the less memory space.	The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.				
921	paper_41	Is task-independent sentence representations same thing as embedding?	Task-independent sentence representations learns text embedding and can be implemented efficiently using self-attention.	Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].				
922	paper_42	What are the two factors the Mobilenet hyperparameters will affect?	MobileNet architecture introduces two hyper-parameters.   width multiplier and resolution multiplier.	This paper describes an efficient network architecture and a set of two hyper-parameters in order to build very small, low latency models that can be easily matched to the design requirements for mobile and embedded vision applications. Section 2 reviews prior work in building small models. Section 3 describes the MobileNet architecture and two hyper-parameters width multiplier and resolution multiplier to define smaller and more efficient MobileNets. Section 4 describes experiments on ImageNet as well a variety of different applications and use cases. Section 5 closes with a summary and conclusion.Although the base MobileNet architecture is already small and low latency, many times a specific use case or application may require the model to be smaller and faster. In order to construct these smaller and less computationally expensive models we introduce a very simple parameter \alpha called width multiplier. The role of the width multiplier \alpha is to thin a network uniformly at each layer. For a given layer and width multiplier \alpha, the number of input channels M becomes \alpha M and the number of output channels N becomes \alpha N.In this section we first investigate the effects of depthwise convolutions as well as the choice of shrinking by reducing the width of the network rather than the number of layers. We then show the trade offs of reducing the network based on the two hyper-parameters: width multiplier and resolution multiplier and compare results to a number of popular models. We then investigate MobileNets applied to a number of different applications.In this section we first describe the core layers that MobileNet is built on which are depthwise separable filters. We then describe the MobileNet network structure and conclude with descriptions of the two model shrinking hyper-parameters width multiplier and resolution multiplier.				
923	paper_42	What are the fields that needs to be carried out in a timely fashion and on a computationally limited platform?	For robotics, self-driving cars and AR, the recognition task needs to be carried timely and with less computational cost.	Convolutional neural networks have become ubiquitous in computer vision ever since AlexNet [19] popularized deep convolutional neural networks by winning the ImageNet Challenge: ILSVRC 2012 [24]. The general trend has been to make deeper and more complicated networks in order to achieve higher accuracy [27, 31, 29, 8]. However, these advances to improve accuracy are not necessarily making networks more efficient with respect to size and speed. In many real world applications such as robotics, self-driving car and augmented reality, the recognition tasks need to be carried out in a timely fashion on a computationally limited platform.				
924	paper_42	What are MobileNets are primarily built on and what is it main goal?	MobileNets are built primarily on depthwise separable convolutions, a specialized method which reduces the computational cost.  The main goal for MobileNets to design an efficient architecture is to reduce latency while maintaining state of the art accuracy.	This paper describes an efficient network architecture and a set of two hyper-parameters in order to build very small, low latency models that can be easily matched to the design requirements for mobile and embedded vision applications. Section 2 reviews prior work in building small models. Section 3 describes the MobileNet architecture and two hyper-parameters width multiplier and resolution multiplier to define smaller and more efficient MobileNets. Section 4 describes experiments on ImageNet as well a variety of different applications and use cases. Section 5 closes with a summary and conclusion.There has been rising interest in building small and efficient neural networks in the recent literature, e.g. [16, 34, 12, 36, 22]. Many different approaches can be generally categorized into either compressing pretrained networks or training small networks directly. This paper proposes a class of network architectures that allows a model developer to specifically choose a small network that matches the resource restrictions (latency, size) for their application. MobileNets primarily focus on optimizing for latency but also yield small networks. Many papers on small networks focus only on size but do not consider speed.MobileNets are built primarily from depthwise separable convolutions initially introduced in [26] and subsequently used in Inception models [13] to reduce the computation in the first few layers. Flattened networks [16] build a network out of fully factorized convolutions and showed the potential of extremely factorized networks. Independent of this current paper, Factorized Networks[34] introduces a similar factorized convolution as well as the use of topological connections. Subsequently, the Xception network [3] demonstrated how to scale up depthwise separable filters to out perform Inception V3 networks. Another small network is Squeezenet [12] which uses a bottleneck approach to design a very small network. Other reduced computation networks include structured transform networks [28] and deep fried convnets [37].				
925	paper_42	What is distillation and why it is used?	Distillation is a knowledge transfer technique for deep networks which is used for compute efficient model design.	Another use-case for MobileNet is compressing large systems with unknown or esoteric training procedures. In a face attribute classification task, we demonstrate a synergistic relationship between MobileNet and distillation [9], a knowledge transfer technique for deep networks. We seek to reduce a large face attribute classifier with 75 million parameters and 1600 million Mult-Adds. The classifier is trained on a multi-attribute dataset similar to YFCC100M [32].We distill a face attribute classifier using the MobileNet architecture. Distillation [9] works by training the classifier to emulate the outputs of a larger model222The emulation quality is measured by averaging the per-attribute cross-entropy over all attributes. instead of the ground-truth labels, hence enabling training from large (and potentially infinite) unlabeled datasets. Marrying the scalability of distillation training and the parsimonious parameterization of MobileNet, the end system not only requires no regularization (e.g. weight-decay and early-stopping), but also demonstrates enhanced performances. It is evident from Tab. 12 that the MobileNet-based classifier is resilient to aggressive model shrinking: it achieves a similar mean average precision across attributes (mean AP) as the in-house while consuming only 1\% the Multi-Adds.				
926	paper_42	What is a depthwise separable convolution means?	Depthwise separable convolution is made up of two layers: depthwise convolutions and pointwise convolutions where depthwise convolutions apply a single filter per each input channel and a Pointwise convolution creates a linear combination of the output of the depthwise layer.	The standard convolution operation has the effect of filtering features based on theconvolutional kernels and combining features in order to produce a new representation.The filtering and combination steps can be split into two steps via the use offactorized convolutions called depthwise separable convolutions for substantial reduction in computational cost.Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1\times 1 convolution is needed in order to generate these new features.The combination of depthwise convolution and 1\times 1 (pointwise) convolution is called depthwise separable convolution which was originally introduced in [26].The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\times 1 pointwise convolution 2(c).				
927	paper_42	Describe how mobile net use depthwise separable convolution to reduce computation and the model size	MobileNets use depthwise convolution with one filter per input channel.  The pointwise convolution then combines the depthwise convolution outputs with a 1\times 1 convolution.  This factorization greatly reduces computation and model size.	The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\times 1 pointwise convolution 2(c).				
928	paper_42	What is the computational cost of the standard convolutions and what does it depends on?	Standard convolutions have the computational cost of: D_{K}\cdot D_{K}\cdot M\cdot N\cdot D_{F}\cdot D_{F} where the computational cost depends multiplicatively on the number of input channels M, the number of output channels N the kernel size D_{k}\times D_{k} and the feature map size D_{F}\times D_{F}.	D_{K}\cdot D_{K}\cdot M\cdot N\cdot D_{F}\cdot D_{F}(2)where the computational cost depends multiplicatively on the number of input channels M, the number of output channels N the kernel size D_{k}\times D_{k} and the feature map size D_{F}\times D_{F}. MobileNet models address each of these terms and their interactions. First it uses depthwise separable convolutions to break the interaction between the number of output channels and the size of the kernel.				
929	paper_42	What are the layers of depthwise separable convolution and discuss the function of each of them.	Depthwise separable convolutions have two layers—depthwise and pointwise.  Depthwise convolutions apply one filter per input channel (input depth).  The depthwise layer output is linearly combined using pointwise convolution which is a 1\times 1 convolution.	Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1\times 1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1\times 1 convolution to combine the outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This factorization has the effect of drastically reducing computation and model size. Figure 2 shows how a standard convolution 2(a) is factorized into a depthwise convolution 2(b) and a 1\times 1 pointwise convolution 2(c).				
930	paper_42	What types of non-linearities is used for both layers of the depthwise separable convolution?	MobileNet layers use batchnorm and ReLU nonlinearities.	Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.				
931	paper_42	What is the cost function of the depthwise convolution?	D_{K}\cdot D_{K}\cdot M\cdot D_{F}\cdot D_{F}+M\cdot N\cdot D_{F}\cdot D_{F} is the cost function for depthwise separable convolution.	D_{K}\cdot D_{K}\cdot M\cdot D_{F}\cdot D_{F}+M\cdot N\cdot D_{F}\cdot D_{F}(5)which is the sum of the depthwise and 1\times 1 pointwise convolutions.The computational cost of a depthwise separable convolution with width multiplier \alpha is:D_{K}\cdot D_{K}\cdot\alpha M\cdot D_{F}\cdot D_{F}+\alpha M\cdot\alpha N\cdot D_{F}\cdot D_{F}(6)where \alpha\in(0,1] with typical settings of 1, 0.75, 0.5 and 0.25. \alpha=1 is the baseline MobileNet and \alpha<1are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly \alpha^{2}. Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier \alpha and resolution multiplier \rho:D_{K}\cdot D_{K}\cdot\alpha M\cdot\rho D_{F}\cdot\rho D_{F}+\alpha M\cdot\alpha N\cdot\rho D_{F}\cdot\rho D_{F}(7)where \rho\in(0,1] which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. \rho=1 is the baseline MobileNet and \rho<1are reduced computation MobileNets. Resolution multiplier has the effect of reducing computational cost by \rho^{2}.				
932	paper_42	Why an output layer of 1*1 convolution was added at the end of the architecture?	1\times 1 convolution is used to compute linear combination of depthwise convolutions.	Depthwise separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions.We use depthwise convolutions to apply a single filter per each input channel (input depth). Pointwise convolution, asimple 1\times 1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets use bothbatchnorm and ReLU nonlinearities for both layers.Depthwise convolution is extremely efficient relative to standard convolution. However it only filters input channels, it does not combine them to create new features. So an additional layer that computes a linear combination of the output of depthwise convolution via 1\times 1 convolution is needed in order to generate these new features.				
933	paper_42	How much the computational complexity was reduced when using depthwise separable convolution?	3\times 3 depthwise separable convolutions use 8–9 times less computation than standard convolutions.	MobileNet uses 3\times 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy as seen in Section 4.				
934	paper_42	Does all the layers of the MobileNet use depthwise separable convolution?	The first layer of MobileNet is a full convolution, and the rest are depthwise separable convolutions.	The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.As an example we can look at a typical layer in MobileNet and see how depthwise separable convolutions, width multiplier and resolution multiplier reduce the cost and parameters. Table 3 shows the computation and number of parameters for a layer as architecture shrinking methods are sequentially applied to the layer. The first row shows the Mult-Adds and parameters for a full convolutional layer with an input feature map of size 14\times 14\times 512 with a kernel K of size 3\times 3\times 512\times 512. We will look in detail in the next section at the trade offs between resources and accuracy.				
935	paper_42	How many layers does the MobileNet has?	MobileNet has 28 layers.	The MobileNet structure is built on depthwise separable convolutions as mentioned in the previous section except for the first layer which is a full convolution. By defining the network in such simple terms we are able to easily explore network topologies to find a good network. The MobileNet architecture is defined in Table 1. All layers are followed by a batchnorm [13] and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Figure 3 contrasts a layer with regular convolutions, batchnorm and ReLU nonlinearity to the factorized layer with depthwise convolution, 1\times 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.				
936	paper_42	How the MobileNet model was trained and why it was different than training the large networks?	Authors use less regularization and data augmentation for MobileNets because of less overfitting.  Authors avoid side heads and label smoothing when training and limit the size of small crops to reduce image distortions Also, depthwise filters needed little or no weight decay.  While all these things are common practices for large datasets, this is redundant for MobileNets.	MobileNet models were trained in TensorFlow [1] using RMSprop [33] with asynchronous gradient descent similar to Inception V3 [31]. However, contrary to training large models we use less regularization and data augmentation techniques because small models have less trouble with overfitting. When training MobileNets we do not use side heads or label smoothing and additionally reduce the amount image of distortions by limiting the size of small crops that are used in large Inception training [31]. Additionally, we found that it was important to put very little or no weight decay (l2 regularization) on the depthwise filters since their are so few parameters in them. For the ImageNet benchmarks in the next section all models were trained with same training parameters regardless of the size of the model.				
937	paper_42	What is the goal of using width multiplier and how it is used ?	Width multiplier reduces computational cost and parameters by defining a new,untrained & reduced structure.	Although the base MobileNet architecture is already small and low latency, many times a specific use case or application may require the model to be smaller and faster. In order to construct these smaller and less computationally expensive models we introduce a very simple parameter \alpha called width multiplier. The role of the width multiplier \alpha is to thin a network uniformly at each layer. For a given layer and width multiplier \alpha, the number of input channels M becomes \alpha M and the number of output channels N becomes \alpha N.The computational cost of a depthwise separable convolution with width multiplier \alpha is:D_{K}\cdot D_{K}\cdot\alpha M\cdot D_{F}\cdot D_{F}+\alpha M\cdot\alpha N\cdot D_{F}\cdot D_{F}(6)where \alpha\in(0,1] with typical settings of 1, 0.75, 0.5 and 0.25. \alpha=1 is the baseline MobileNet and \alpha<1are reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly \alpha^{2}. Width multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.				
938	paper_42	Compare between the results of mobile net and full convolutions on the ImageNet dataset	Depthwise separable convolutions reduced ImageNet accuracy by 1±% while saving considerable multiplication addition and model parameters.	First we show results for MobileNet with depthwise separable convolutions compared to a model built with full convolutions. In Table 4 we see that using depthwise separable convolutions compared to full convolutions only reduces accuracy by 1\% on ImageNet was saving tremendously on mult-adds and parameters.				
939	paper_42	What characteristics did MobileNet showed better performance when compared to other models.	MobileNets showed better performance at reducing model size, computational complexity and latency while maintaining comparable accuracy when compared with the other models.	We re-train PlaNet using the MobileNet architecture on the same data. While the full PlaNet model based on the Inception V3 architecture [31] has 52 million parameters and 5.74 billion mult-adds. The MobileNet model has only 13 million parameters with the usual 3 million for the body and 10 million for the final layer and 0.58 Million mult-adds. As shown in Tab. 11, the MobileNet version delivers only slightly decreased performance compared to PlaNet despite being much more compact. Moreover, it still outperforms Im2GPS by a large margin.MobileNet can also be deployed as an effective base network in modern object detection systems.We report results for MobileNet trained for object detection on COCO data based on the recent work that won the 2016 COCO challenge [10].In table 13, MobileNet is compared to VGG and Inception V2 [13] under both Faster-RCNN [23] and SSD [21] framework.In our experiments, SSD is evaluated with 300 input resolution (SSD 300) and Faster-RCNN is compared with both 300 and 600 input resolution (Faster-RCNN 300, Faster-RCNN 600).The Faster-RCNN model evaluates 300 RPN proposal boxes per image. The models are trained on COCO train+val excluding 8k minival images and evaluated on minival.For both frameworks, MobileNet achieves comparable results to other networks with only a fraction of computational complexity and model size.We proposed a new model architecture called MobileNets based on depthwise separable convolutions. We investigated some of the important design decisions leading to an efficient model. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics. We concluded by demonstrating MobileNet’s effectiveness when applied to a wide variety of tasks. As a next step to help adoption and exploration of MobileNets, we plan on releasing models in Tensor Flow.				
940	paper_43	What are the reasons for the remarkable progress in the image recognition task?	The authors cite the increases in the number of well-constructed large-scale datasets and the usage of CNNs as the main reasons for the progress in the field of image recognition.  Well-constructed large-scale datasets facilitate better learning, and the CNN architecture is appropriate for learning informative features from images.	Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.	The authors cite the increases in the number of well-constructed large-scale datasets and the usage of CNNs as the main reasons for the progress in the field of photo reputation.  Well-constructed large-scale datasets facilitate better learning, and the CNN architecture is appropriate for learning informative features from images.	Tortured phrases	image recognition -> photo reputation	
941	paper_43	Give examples of two techniques of employing CNNs to medical image	There are three main techniques that are used to apply CNNs to tasks involving medical images: 1) training from scratch, 2) using pre-trained CNNs as feature extractors, then using those features with hand-crafted features, and 3) performing unsupervised pre-training then using CNN for fine-tuning.  An example of the "training from scratch" technique for employing CNNs to medical images is a CNN that was trained from scratch for LN detection.  An example of the "CNN fine-tuning" technique is a CNN pre-trained on ImageNet that was used for X-ray and CT images for chest pathology identification and detection. 	There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the “CNN from scratch” [13, 14, 15, 16, 17]; 2) using “off-the-shelf CNN” features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for Chest X-rays [10] and CT lung nodule identification [9, 12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18, 19, 20, 21].A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the “curse-of-dimensionality” issue in [22], in order to acquire a sufficient number of training image samples.Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10, 9, 12].They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.Until the detection aggregation approach [22], [41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83% recall on 3FP per patient [22], using a “Cifar-10” CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.	There are three main techniques that are used to apply CNNs to tasks involving medical images: 1) training from scratch, 2) using pre-skilled CNNs as feature extractors, then using those features with hand-crafted features, and 3) performing unsupervised pre-training then using CNN for fine-tuning.	Tortured phrases	pre-trained -> pre-skilled	There are three main techniques that are used to apply CNNs to tasks involving medical images: 1) training from scratch, 2) using pre-skilled CNNs as feature extractors, then using those features with hand-crafted features, and 3) performing unsupervised pre-training then using CNN for fine-tuning.
942	paper_43	What are the range of the number of parameters that for the models used in the study?	The number of parameters range from 5 thousand to 160 million for the models in this study.	In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems.Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5, 33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered “AlexNet-CNN” [4], a shallower “Cifar-CNN” [22], and a much deeper version of “GoogLeNet-CNN” [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34, 35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively.				
943	paper_43	What is the number of images and classes does the ImageNet dataset have?	ImageNet has more than 1. 2 million images and about 1000 classes.	Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.This success has revived the interest in CNNs [3] in computer vision.ImageNet consists of 1.2 million 256\times 256 images belonging to 1000 categories.At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model.More details about the ImageNet dataset will be discussed in Sec. III-B.AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters.AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.ImageNet [1] has more than 1.2 million 256\times 256 images categorized under 1000 object class categories.There are more than 1000 training images per class.The database is organized according to the WordNet [55] hierarchy, which currently contains only nouns in 1000 object categories.The image-object labels are obtained largely through crowd-sourcing, e.g., Amazon Mechanical Turk, and human inspection.Some examples of object categories in ImageNet are “sea snake”, “sandwich”, “vase”, “leopard”, etc.ImageNet is currently the largest image dataset among other standard datasets for visual recognition.Indeed, the Caltech101, Caltech256 and Cifar10 dataset merely contain 60000 32\times 32 images and 10 object classes.Furthermore, due to the large number (1000+) of object classes, the objects belonging to each ImageNet class category can be occluded, partial and small, relative to those in the previous public image datasets.This significant intra-class variation poses greater challenges to any data-driven learning system that builds a classifier to fit given data and generalize to unseen data.For comparison, some example images of Cifar10 dataset and ImageNet images in the “tennis ball” class category are shown in Figure 7.The ImageNet dataset is publicly available, and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) has become the standard benchmark for large-scale object recognition.				
944	paper_43	What are the reasons that there is no large-scale annotated medical image dataset such as the ImageNet?	The authors say that no such dataset exists because data acquisition and annotation in the medical image field is hard and costly.	Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.				
945	paper_43	Give two examples of conventional image descriptors that can be used for object detection and segmentation in medical image analysis.	Two examples of conventional image descriptors for object detection and segmentation in the medical image field are scale-invariant feature transform (SIFT) and histogram of oriented gradients (HOG).	Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10, 9, 12].They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.				
946	paper_43	What are the CNN architectures that were explored in this paper?	The paper uses AlexNet, CifarNet, and GoogLeNet with various numbers of parameters.	We mainly explore three convolutional neural network architectures (CifarNet [5, 22], AlexNet [4] and GoogLeNet [33]) with different model training parameter values.The current deep learning models [22, 52, 53] in medical image tasks are at least 2\sim 5 orders of magnitude smaller than even AlexNet [4].More complex CNN models [22, 52] have only about 150K or 15K parameters.Roth et al. [22] adopt the CNN architecture tailored to the Cifar-10 dataset [5] and operate on image windows of 32\times 32\times 3 pixels for lymph node detection, while the simplest CNN in [54] has only one convolutional, pooling, and FC layer, respectively.We use CifarNet [5] as used in [22] as a baseline for the LN detection.AlexNet [4] and GoogLeNet [33] are also modified to evaluate these state-of-the-art CNN architecture from ImageNet classification task [2] to our CADe problems and datasets.A simplified illustration of three CNN architectures exploited is shown in Figure 5.CifarNet always takes 32\times 32\times 3 image patches as input while AlexNet and GoogLeNet are originally designed for the fixed image dimension of 256\times 256\times 3 pixels.We also reduced the filter size, stride and pooling parameters of AlexNet and GoogLeNet to accommodate a smaller input size of 64\times 64\times 3 pixels.We do so to produce and evaluate “simplified” AlexNet and GoogLeNet versions that are better suited to the smaller scale training datasets common in CADe problems.Throughout the paper, we refer to the models as CifarNet (32x32) or CifarNet (dropping 32x32); AlexNet (256x256) or AlexNet-H (high resolution); AlexNet (64x64) or AlexNet-L (low resolution); GoogLeNet (256x256) or GoogLeNet-H and GoogLeNet (64x64) or GoogLeNet-L (dropping 3 since all image inputs are three channels).The AlexNet architecture was published in [4], achieved significantly improved performance over the other non-deep learning methods for ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012.This success has revived the interest in CNNs [3] in computer vision.ImageNet consists of 1.2 million 256\times 256 images belonging to 1000 categories.At times, the objects in the image are small and obscure, and thus pose more challenges for learning a successful classification model.More details about the ImageNet dataset will be discussed in Sec. III-B.AlexNet has five convolution layers, three pooling layers, and two fully-connected layers with approximately 60 million free parameters.AlexNet is our default CNN architecture for evaluation and analysis in the remainder of the paper.In this section, we evaluate and compare the performances of nine CNN model configurations (CifarNet, AlexNet-ImNet,AlexNet-RI-H, AlexNet-TL-H, AlexNet-RI-L, GoogLeNet-RI-H, GoogLeNet-TL-H, GoogLeNet-RI-L and combined) on two important CADe problems using publicly available datasets [22, 41, 37].In this paper, we exploit three important, but previously under-studied factors of employing deep convolutional neural networks to computer-aided detection problems.Particularly, we explore and evaluate different CNN architectures varying in width (ranging from 5 thousand to 160 million parameters) and depth (various numbers of layers), describe the effects of varying dataset scale and spatial image context on performance, and discuss when and why transfer learning from pre-trained ImageNet CNN models can be valuable. We further verify our hypothesis by inheriting and adapting rich hierarchical image features [5, 33] from the large-scale ImageNet dataset for computer aided diagnosis (CAD). We also explore CNN architectures of the most studied seven-layered “AlexNet-CNN” [4], a shallower “Cifar-CNN” [22], and a much deeper version of “GoogLeNet-CNN” [33] (with our modifications on CNN structures). This study is partially motivated by recent studies [34, 35] in computer vision. The thorough quantitative analysis and evaluation on deep CNN [34] or sparsity image coding methods [35] elucidate the emerging techniques of the time and provide useful suggestions for their future stages of development, respectively.In this work, we mainly focus on AlexNet and GoogLeNet. AlexNet is the first notably successful CNN architecture on the ImageNet challenge and has rekindled significant research interests on CNN. GoogLeNet is the state-of-the-art deep model, which has outperformed other notable models, such as AlexNet, OverFeat, and VGGNet [67, 68] in various computer vision benchmarks. Likewise, a reasonable assumption is that OverFeat and VGGNet may generate quantitative performance results ranked between AlexNet’s and GoogLeNet’s. For completeness, we include the Overfeat and VGGNet in the following evaluations, to bolster our hypothesis.				
947	paper_43	What are the computer-aided detection problems studied in this paper?	The paper studies thoraco-abdominal lymph node detection and interstitial lung disease classification.	Two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification are studied in this work. On mediastinal LN detection, we surpass all currently reported results.We obtain 86\% sensitivity on 3 false positives (FP) per patient, versus the prior state-of-art sensitivities of 78\% [36] (stacked shallow learning) and 70\% [22] (CNN), as prior state-of-the-art. For the first time, ILD classification results under the patient-level five-fold cross-validation protocol (CV5) are investigated and reported. The ILD dataset [37] contains 905 annotated image slices with 120 patients and 6 ILD labels. Such sparsely annotated datasets are generally difficult for CNN learning, due to the paucity of labeled instances.In this paper, we exploit and extensively evaluate three important, previously under-studied factors on deep convolutional neural networks (CNN) architecture, dataset characteristics, and transfer learning.We evaluate CNN performance on two different computer-aided diagnosis applications: thoraco-abdominal lymph node detection and interstitial lung disease classification.The empirical evaluation, CNN model visualization, CNN performance analysis, and conclusive insights can be generalized to the design of high performance CAD systems for other medical imaging tasks.Until the detection aggregation approach [22, 41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83\% recall on 3FP per patient [22], using a “Cifar-10” CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.				
948	paper_43	What was the model that achieved the best performance on abdominal LN Detection?	The best performing model for abdominal LN detection was a Cifar-10 CNN.	Until the detection aggregation approach [22, 41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83\% recall on 3FP per patient [22], using a “Cifar-10” CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.				
949	paper_43	What are the reasons and goals behind image sampling ?	Image sampling is useful for data augmentation to create larger datasets for training, and can also be used to balance the number of data per class during the data augmentation process.	There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the “CNN from scratch” [13, 14, 15, 16, 17]; 2) using “off-the-shelf CNN” features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for Chest X-rays [10] and CT lung nodule identification [9, 12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18, 19, 20, 21].A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the “curse-of-dimensionality” issue in [22], in order to acquire a sufficient number of training image samples.Medical datasets are often “biased”, in that the number of healthy samples is much larger than the number of diseased instances, or that the numbers of images per class are uneven. In ILD dataset, the number of fibrosis samples is about 3.5 times greater than the number of emphysema samples. The number of non-LNs is 3\sim 4 times greater than the number of LNs in lymph node detection. Different sampling or resampling rates are routinely applied to both ILD and LN detection to balance the data sample number or scale per class, as in[22]. We refer this as “Equal Prior”. If we use the same sampling rate, that will lead to a “Biased Prior” across different classes.Evaluation protocols and details are critical to deriving significant empirical findings [34].Our experimental results suggest that different CNN architectures and dataset re-sampling protocols are critical for the LN detection tasks where the amount of labeled training data is sufficient and spatial contexts are local.Since LN images are more flexible than ILD images with respect to resampling and reformatting, LN datasets may be more readily augmented by such image transformations. As a result, LN datasets contain more training and testing data instances (due to data auugmentation) than ILD datasets. They nonetheless remain less comprehensive than natural image datasets, such as ImageNet.Fine-tuning ImageNet-trained models for ILD classification is clearly advantageous and yields early promising results, when the amount of labeled training data is highly insufficient and multi-class categorization is used, as opposed to the LN dataset’s binary class categorization.Another significant finding is that CNNs trained from scratch or fine-tuned from ImageNet models consistently outperform CNNs that merely use off-the-shelf CNN features, in both the LN and ILD classification problems. We further analyze, via CNN activation visualizations, when and why transfer learning from non-medical to medical images in CADe problems can be valuable.				
950	paper_43	What are the difference between lymph nodes and the hear or liver?	Lymph nodes have no predetermined orientation relative to the human anatomy, while the heart and liver do.	Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products.Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.				
951	paper_43	What is the goal behind transforming axial, and coronal, and sagittal representations to RGB ?	The goal behind transforming the axial, coronal and sagittal representations to RGB is to help the learning process of transfer learning models that were pre-trained on ImageNet.	Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products.Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.	The goal behind transforming the axial, coronal and sagittal representations to RGB is to help the learning process of transfer learning models that were pre-educated on ImageNet.	Tortured phrases	pre-trained -> pre-educated	
952	paper_43	How many images did the dataset consist of and the number of unique patients ?	The ILD dataset has 905 image slices from 120 patients.	Interstitial Lung Disease Dataset. We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Figure 3).At the slice level, the objective is to classify the status of “presence/absence” of any of the six ILD classes for an input axial CT slice [40].Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38, 39]), can be useful for large-scale patient screening.For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations.After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates.For patch-based classification, we sampled up to 100 patches of size 64\times 64 from each ROI.This dataset is divided into five folds with disjoint patient subsets.The average number of CT slices (training instances) per fold is small, as shown in Table I.Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.				
953	paper_43	What are the six classes of the data used for training ?	The six classes are healthy, emphysema, ground glass, fibrosis, micronodules, and consolidation.	Interstitial Lung Disease Dataset. We utilize the publicly available dataset of [37]. It contains 905 image slices from 120 patients, with six lung tissue types annotations containing at least one of the following: healthy (NM), emphysema (EM), ground glass (GG), fibrosis (FB), micronodules (MN) and consolidation (CD) (Figure 3). At the slice level, the objective is to classify the status of “presence/absence” of any of the six ILD classes for an input axial CT slice [40]. Characterizing an arbitrary CT slice against any possible ILD type, without any manual ROI (in contrast to [38], [39]), can be useful for large-scale patient screening. For slice-level ILD classification, we sampled the slices 12 times with random translations and rotations. After this, we balanced the numbers of CT slice samples for the six classes by randomly sampling several instances at various rates. For patch-based classification, we sampled up to 100 patches of size 64×64 from each ROI. This dataset is divided into five folds with disjoint patient subsets. The average number of CT slices (training instances) per fold is small, as shown in Table I. Slice-level ILD classification is a very challenging task where CNN models need to learn from very small numbers of training examples and predict ILD labels on unseen patients.				
954	paper_43	How did the authors leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet to be able to use it on the medical dataset ?	The authors transformed every gray-scale axial CT image using the three CT windows of lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400.  -950HU], then encoded the transformed images into RGB images.	To leverage the CNN architectures designed for color images and to transfer CNN parameters pre-trained on ImageNet, we transform all gray-scale axial CT slice images via three CT window ranges: lung window range [-1400, -200HU], high-attenuation range [-160, 240HU], and low-attenuation range [-1400; -950HU]. We then encode the transformed images into RGB channels (to be aligned with the input channels of CNN models [4], [33] pre-trained from natural image datasets [1]). The low-attenuation CT window is useful for visualizing certain texture patterns of lung diseases (especially emphysema). The usage of different CT attenuation channels improves classification results over the usage of a single CT windowing channel, as demonstrated in [40]. More importantly, these CT windowing processes do not depend on the lung segmentation, which instead is directly defined in the CT HU space. Figure 4 shows a representative example of lung, high-attenuation, and low-attenuation CT windowing for an axis lung CT slice.				
955	paper_43	How does CNN model learns to ignore areas that appear in both healthy and diseased lungs?	The model learns very small weights in the filters for such areas.	As observed in [40], lung segmentation is crucial to holistic slice-level ILD classification. We empirically compare performance in two scenarios with a rough lung segmentation111This can be achieved by segmenting the lung using simple label-fusion methods [48]. In the first case, we overlay the target image slice with the average lung mask among the training folds. In the second, we perform simple morphology operations to obtain the lung boundary. In order to retain information from the inside of the lung, we apply Gaussian smoothing to the regions outside of the lung boundary. There is no significant difference between two setups. Due to the high precision of CNN based image processing, highly accurate lung segmentation is not necessary . The localization of ILD regions within the lung is simultaneously learned through selectively weighted CNN reception fields in the deepest convolutional layers during the classification based CNN training [49, 50].Some areas outside of the lung appear in both healthy or diseased images. CNN training learns to ignore them by setting very small filter weights around the corresponding regions (Figure 13). This observation is validated by [40].				
956	paper_43	What was the goal behind reducing the filter size and stride of the ALexNet and GoogLeNet ?	The authors reduced the filter size and stride of the two models because the input size used was smaller than what the original models were trained on.	We use CifarNet [5] as used in [22] as a baseline for the LN detection.AlexNet [4] and GoogLeNet [33] are also modified to evaluate these state-of-the-art CNN architecture from ImageNet classification task [2] to our CADe problems and datasets.A simplified illustration of three CNN architectures exploited is shown in Figure 5.CifarNet always takes 32\times 32\times 3 image patches as input while AlexNet and GoogLeNet are originally designed for the fixed image dimension of 256\times 256\times 3 pixels.We also reduced the filter size, stride and pooling parameters of AlexNet and GoogLeNet to accommodate a smaller input size of 64\times 64\times 3 pixels.We do so to produce and evaluate “simplified” AlexNet and GoogLeNet versions that are better suited to the smaller scale training datasets common in CADe problems.Throughout the paper, we refer to the models as CifarNet (32x32) or CifarNet (dropping 32x32); AlexNet (256x256) or AlexNet-H (high resolution); AlexNet (64x64) or AlexNet-L (low resolution); GoogLeNet (256x256) or GoogLeNet-H and GoogLeNet (64x64) or GoogLeNet-L (dropping 3 since all image inputs are three channels).				
957	paper_43	What is CifarNet?	CifarNet was a CNN model that was used for the object recognition task using the Cifar10 dataset.	CifarNet, introduced in [5], was the state-of-the-art model for object recognition on the Cifar10 dataset, which consists of 32\times 32 images of 10 object classes.The objects are normally centered in the images.Some example images and class categories from the Cifar10 dataset are shown in Figure 7.CifarNet has three convolution layers, three pooling layers, and one fully-connected layer.This CNN architecture, also used in [22] has about 0.15 million free parameters.We adopt it as a baseline model for the LN detection.				
958	paper_43	How was the CNN parameters initialized?	Parameters were initialized by sampling from random Gaussian distributions.	When learned from scratch, all the parameters of CNN models are initialized with random Gaussian distributions and trained for 30 epochs with the mini-batch size of 50 image instances.Training convergence can be observed within 30 epochs. The other hyperparameters are momentum: 0.9; weight decay: 0.0005; (base) learning rate: 0.01, decreased by a factor of 10 at every 10 epochs. We use the Caffe framework [56] and NVidia K40 GPUs to train the CNNs.				
959	paper_43	What are the models that yielded the least competitive detection accuracy results on the Thoracoabdominal Lymph Node Detection?	CifarNet, AlexNet-ImNet and GoogLeNet-RI-H were the models that had the worst results.	Results for lymph node detection in the mediastinum and abdomen are reported in Table II.FROC curves are illustrated in Figure 8.The area-under-the-FROC-curve (AUC) and true positive rate (TPR, recall or sensitivity) at three false positives per patient (TPR/3FP) are used as performance metrics.Of the nine investigated CNN models, CifarNet, AlexNet-ImNet and GoogLeNet-RI-H generally yielded the least competitive detection accuracy results.Our LN datasets are significantly more complex (i.e., display much larger within-class appearance variations), especially due to the extracted fields-of-view (FOVs) of (35mm-128mm) compared to (30mm-45mm) in [22], where CifarNet is also employed.In this experiment, CifarNet is under-trained with respect to our enhanced LN datasets, due to its limited input resolution and parameter complexity.The inferior performance of AlexNet-ImNet implies that using the pre-trained ImageNet CNNs alone as “off-the-shelf” deep image feature extractors may not be optimal or adequate for mediastinal and abdominal LN detection tasks.To complement “off-the-shelf” CNN features, [10, 9, 12] all add and integrate various other hand-crafted image features as hybrid inputs for the final CADe classification.				
960	paper_43	Why did the GoogLENet-RI-H performs poorly in the Thoracoabdominal Lymph Node Detection task?	The model suffers from over-fitting, as it is a very complex model but it does not have enough training data for training.	GoogLeNet-RI-H performs poorly, as it is susceptible to over-fitting. No sufficient data samples are available to train GoogLeNet-RI-H with random initialization.Indeed, due to GoogLeNet-RI-H’s complexity and 22-layer depth, million-image datasets may be required to properly train this model.However, GoogLeNet-TL-H significantly improves upon GoogLeNet-RI-H (0.81 versus 0.61 TPR/3FP in mediastinum; 0.70 versus 0.48 TPR/3FP in abdomen). This indicates that transfer learning offers a much better initialization of CNN parameters than random initialization. Likewise, AlexNet-TL-H consistently outperforms AlexNet-RI-H, though by smaller margins (0.81 versus 0.79 TPR/3FP in mediastinum; 0.69 versus 0.67 TPR/3FP in abdomen). This is also consistent with the findings reported for ILD detection in Table III and Figure 11.				
961	paper_43	What is the best performing CNN model in the abdominal LN detection?	The best performing model is GoogLeNet-TL.	Many of our CNN models achieve notably better (FROC-AUC and TPR/3FP) results than the previous state-of-the-art models [36] for mediastinal LN detection: GoogLeNet-RI-L obtains an AUC=0.95 and 0.85 TPR/3FP, versus AUC=0.92 and 0.70 TPR/3FP [22] and 0.78 TPR/3FP [36] which uses stacked shallow learning.This difference lies in the fact that annotated lymph node segmentation masks are required to learn a mid-level semantic boundary detector [36], whereas CNN approaches only need LN locations for training [22]. In abdominal LN detection, [22] obtains the best trade-off between its CNN model complexity and sampled data configuration. Our best performing CNN model is GoogLeNet-TL (256x256) which obtains an AUC=0.92 and 0.70 TPR/3FP.				
962	paper_43	What is the difference between five-fold cross validation and leave-one-patient out?	LOO performs better than five-fold cross validation.	To investigate the performance difference between five-fold cross-validation (CV) in Sec. IV-B and leave-one-patient-out (LOO) validation, this experiment is performed under the LOO protocol. By comparing results in Table III (CV-5) to those in Table VI (LOO), one can see that LOO’s quantitative performances are remarkably better than CV-5’s. For example, in ILD slice-level classification, the accuracy level drastically increases from 0.46 to 0.867 using AlexNet-TL, and from 0.57 to 0.902 for GoogLeNet-TL.				
963	paper_43	How is the original ILD images were reconstructed ?	A process consisting of deconvolution, back-engendering with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images.	The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figure 2 (b,c), are shown in Figure 13 (a,b). The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6x6) reception fields in AlexNet [4]. Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes. Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Figure 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figure 2 (b) and (c)) better than AlexNet-ImNet. The filters shown in Figure 13 that better localize regions on the input images (Figure 2 (b) and (c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).	A process consisting of deconvolution, back-propagation with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images.	Tortured phrases	back-propagation -> back-engendering	
964	paper_43	Was Transfer learning beneficial on the CADe process?	Transfer learning was shown to be beneficial in the paper's experiments, as seen by the differences in performance between AlexNet-TL/GoogLeNet-TL and their non-transfer learning counterparts.	While it is a more practical CADe scheme, slice-level CNN learning [40] is very challenging, as it is restricted to only 905 CT image slices with tagged ILD labels. We only benchmark the slice-level ILD classification results in this section. Even with the help of data augmentation (described in Sec. II), the classification accuracy of GoogLeNet-TL from Table III is only 0.57. However, transfer learning from ImageNet pre-trained model is consistently beneficial, as evidenced by AlexNet-TL (0.46) versus AlexNet-RI (0.44), and GoogLeNet-TL (0.57) versus GoogLeNet-RI (0.41). It especially prevents GoogLeNet from over-fitting on the limited CADe datasets. Finally, when the cross-validation is conducted by randomly splitting the set of all 905 CT axial slices into five folds, markedly higher F-scores are obtained (Slice-Random in Table IV). This further validates the claim that the dataset poorly generalizes ILDs for different patients. Figure 10 shows examples of misclassified ILD patches (in axial view), with their ground truth labels and inaccurately classified labels.•Deep CNN architectures with 8, even 22 layers [4, 33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been 2\sim 5 orders of magnitude smaller.•The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).•Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].•Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.•Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10, 9, 12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.				
965	paper_43	How the CADe problem can be improved?	They can be improved by either training CNNs from scratch and fine-tuning them or using hand-crafted features.	•Deep CNN architectures with 8, even 22 layers [4, 33], can be useful even for CADe problems where the available training datasets are limited. Previously, CNN models used in medical image analysis applications have often been 2\sim 5 orders of magnitude smaller.•The trade-off between using better learning models and using more training data [51] should be carefully considered when searching for an optimal solution to any CADe problem (e.g., mediastinal and abdominal LN detection).•Limited datasets can be a bottleneck to further advancement of CADe. Building progressively growing (in scale), well annotated datasets is at least as crucial as developing new algorithms. This has been accomplished, for instance, in the field of computer vision. The well-known scene recognition problem has made tremendous progress, thanks to the steady and continuous development of Scene-15, MIT Indoor-67, SUN-397 and Place datasets [58].•Transfer learning from the large scale annotated natural image datasets (ImageNet) to CADe problems has been consistently beneficial in our experiments. This sheds some light on cross-dataset CNN learning in the medical image domain, e.g., the union of the ILD [37] and LTRC datasets [64], as suggested in this paper.•Finally, applications of off-the-shelf deep CNN image features to CADe problems can be improved by either exploring the performance-complementary properties of hand-crafted features [10, 9, 12], or by training CNNs from scratch and better fine-tuning CNNs on the target medical image dataset, as evaluated in this paper.				
966	paper_44	What is the used model inspired from?	It was inspired by VGG-net.	We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.				
968	paper_44	What is the issue that come with very deep networks?	Training is difficult for very deep networks as they have many parameters and it takes a long time for them to converge.  Also, since convolutional layers shrink feature maps, having too deep a network could be bad.	One problem with using a very deep network to predict dense outputs is that the size of the feature map gets reduced every time convolution operations are applied. For example, when an input of size (n+1)\times(n+1) is applied to a network with receptive field size n\times n, the output image is 1\times 1.High Learning Rates for Very Deep NetworksTraining deep models can fail to converge in realistic limit of time. SRCNN [6] fails to show superior performance with more than three weight layers. While there can be various reasons, one possibility is that they stopped their training procedure before networks converged. Their learning rate 10^{-5} is too small for a network to converge within a week on a common GPU. Looking at Fig. 9 of [6], it is not easy to say their deeper networks have converged and their performances were saturated. While more training will eventually resolve the issue, but increasing depth to 20 does not seems practical with SRCNN.Multi-Scale While very deep models can boost performance, more parameters are now needed to define a network. Typically, one network is created for each scale factor. Considering that fractional scale factors are often used, we need an economical way to store and retrieve networks.In this work, we have presented a super-resolution method using very deep networks. Training a very deep network is hard due to a slow convergence rate. We use residual-learning and extremely high learning rates to optimize a very deep network fast. Convergence speed is maximized and we use gradient clipping to ensure the training stability. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.				
969	paper_44	What is single image super-resolution?	Single image super-resolution is the task of generating a high-resolution image from a low-resolution one.	We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand.	Single image super-resolution is the task of generating a high-goal image from a low-resolution one.	Tortured phrases	 (high-resolution -> high-goal)	
970	paper_44	What are the applications of SISR?	SISR is used in computer vision applications such as surveillance imaging and medical image to enhance low-resolution images.	We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand.				
971	paper_44	Give examples of learning methods that are used in mapping from LR to HR?	Neighbor embedding, sparse coding, random forests and CNN have been used to map from LR to HR.	Currently, learning methods are widely used to model a mapping from LR to HR patches. Neighbor embedding [4, 15] methods interpolate the patch subspace. Sparse coding [25, 26, 21, 22] methods use a learned compact dictionary based on sparse signal representation. Lately, random forest [18] and convolutional neural network (CNN) [6] have also been used with large improvements in accuracy.Among them, Dong et al. [6] has demonstrated that a CNN can be used to learn a mapping from LR to HR in an end-to-end manner. Their method, termed SRCNN, does not require any engineered features that are typically necessary in other methods [25, 26, 21, 22] and shows the state-of-the-art performance.				
972	paper_44	What are the limitations of the SRCNN in the SISR task?	It requires context from small image regions, it converges too slowly during training, and the network only works for one set scale.	Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.While SRCNN successfully introduced a deep learning technique into the super-resolution (SR) problem, we find its limitations in three aspects: first, it relies on the context of small image regions; second, training converges too slowly; third, the network only works for a single scale.We provide quantitative and qualitative comparisons. Compared methods are A+ [22], RFL[18], SelfEx [11] and SRCNN [5]. In Table 3, we provide a summary of quantitative evaluation on several datasets. Our methods outperform all previous methods in these datasets. Moreover, our methods are relatively fast. The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al [6] in their paper based on a GPU implementation.				
973	paper_44	How did the authors speed up the training?	The authors used residual learning, extremely high learning rates, and adjustable gradient clipping.	It is a basic rule of thumb to make learning rate high to boost training. But simply setting learning rate high can also lead to vanishing/exploding gradients [2]. For the reason, we suggest an adjustable gradient clipping for maximal boost in speed while suppressing exploding gradients.For maximal speed of convergence, we clip the gradients to [-\frac{\theta}{\gamma},\frac{\theta}{\gamma}], where \gamma denotes the current learning rate. We find the adjustable gradient clipping makes our convergence procedure extremely fast. Our 20-layer network training is done within 4 hours whereas 3-layer SRCNN takes several days to train.Convergence We suggest a way to speed-up the training: residual-learning CNN and extremely high learning rates. As LR image and HR image share the same information to a large extent, explicitly modelling the residual image, which is the difference between HR and LR images, is advantageous. We propose a network structure for efficient learning when input and output are highly correlated. Moreover, our initial learning rate is 10^{4} times higher than that of SRCNN [6]. This is enabled by residual-learning and gradient clipping.				
974	paper_44	What is the goal behind using a single model SR approach?	Existing methods are only trained for a single scale, so adapting them to other scales requires retraining.  However, this would be impractical, so having a single model that accepts multiple scales would fix the problem.	Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.However, preparing many individual machines for all possible scenarios to cope with multiple scales is inefficient and impractical.In this work, we design and train a single network to handle multiple scale SR problem efficiently. This turns out to work very well. Our single machine is compared favorably to a single-scale expert for the given sub-task. For three scales factors (×2,3,4\times 2,3,4× 2 , 3 , 4), we can reduce the number of parameters by three-fold.Scale Factor We propose a single-model SR approach. Scales are typically user-specified and can be arbitrary including fractions. For example, one might need smooth zoom-in in an image viewer or resizing to a specific dimension. Training and storing many scale-dependent models in preparation for all possible scenarios is impractical. We find a single convolutional network is sufficient for multi-scale-factor super-resolution.Contribution In summary, in this work, we propose a highly accurate SR method based on a very deep convolutional network. Very deep networks converge too slowly if small learning rates are used. Boosting convergence rate with high learning rates lead to exploding gradients and we resolve the issue with residual-learning and gradient clipping. In addition, we extend our work to cope with multi-scale SR problem in a single network. Our method is relatively accurate and fast in comparison to state-of-the-art methods as illustrated in Figure 1.				
975	paper_44	What is the goal behind constructing individual single scale SR system?	Since SRCNN is only trained for a single scale, we need to train individual single scale SRCNNs to deal with multiple scales.	Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.				
976	paper_44	What are the layers of the Model SRCNN?	SRCNN has three layers in the following order: patch extraction/representation, non-linear mapping, and reconstruction.	ModelSRCNN consists of three layers: patch extraction/representation, non-linear mapping and reconstruction. Filters of spatial sizes 9\times 9, 1\times 1, and 5\times 5 were used respectively.				
977	paper_44	What is the difference between the learning rate of the SRCNN and the learning rate of the proposed model?	Not only does SRCNN uses different learning rates for its layers, while the proposed model uses the same learning rate for all of its layers, but the proposed model's initial learning rate is also 10000 times greater than SRCNN's.	In addition to the aforementioned issues, there are some minor differences. Our output image has the same size as the input image by padding zeros every layer during training whereas output from SRCNN is smaller than the input. Finally, we simply use the same learning rates for all layers while SRCNN uses different learning rates for different layers in order to achieve stable convergence.Convergence We suggest a way to speed-up the training: residual-learning CNN and extremely high learning rates. As LR image and HR image share the same information to a large extent, explicitly modelling the residual image, which is the difference between HR and LR images, is advantageous. We propose a network structure for efficient learning when input and output are highly correlated. Moreover, our initial learning rate is 10^{4} times higher than that of SRCNN [6]. This is enabled by residual-learning and gradient clipping.				
978	paper_44	What are the reasons of using residual learning ?	Residual learning solves the vanishing/exploding gradients problem, allowing for the model to converge faster and perform better.	Residual-Learning In SRCNN, the exact copy of the input has to go through all layers until it reaches the output layer. With many weight layers, this becomes an end-to-end relation requiring very long-term memory. For this reason, the vanishing/exploding gradients problem [2] can be critical. We can solve this problem simply with residual-learning.Second, we show that our residual-learning network converges much faster than the standard CNN. Moreover, our network gives a significant boost in performance.First, we find that this residual network converges much faster. Two networks are compared experimentally: the residual network and the standard non-residual network. We use depth 10 (weight layers) and scale factor 2. Performance curves for various learning rates are shown in Figure 4. All use the same learning rate scheduling mechanism that has been mentioned above.Second, at convergence, the residual network shows superior performance. In Figure 4, residual networks give higher PSNR when training is done.Another remark is that if small learning rates are used, networks do not converge in the given number of epochs. If initial learning rate 0.1 is used, PSNR of a residual-learning network reaches 36.90 within 10 epochs. But if 0.001 is used instead, the network never reaches the same level of performance (its performance is 36.52 after 80 epochs). In a similar manner, residual and non-residual networks show dramatic performance gaps after 10 epochs (36.90 vs. 27.42 for rate 0.1).				
979	paper_44	How the loss was calculated in the proposed model ?	The loss is calculated as the Euclidean distance between the reconstructed image and the ground truth image.	As the input and output images are largely similar, we define a residual image {\bf r}={\bf y}-{\bf x}, where most values are likely to be zero or small. We want to predict this residual image. The loss function now becomes \frac{1}{2}||\mathbf{r}-f(\mathbf{x})||^{2}, where f(\bf{x}) is the network prediction.In networks, this is reflected in the loss layer as follows.Our loss layer takes three inputs: residual estimate, network input (ILR image) and ground truth HR image. The loss is computed as the Euclidean distance between the reconstructed image (the sum of network input and output) and ground truth.				
980	paper_44	What was the momentum value used?	Momentum was set to 0.	Training is carried out by optimizing the regression objective using mini-batch gradient descent based on back-propagation (LeCun et al. [14]). We set the momentum parameter to 0.9. The training is regularized by weight decay (L_{2} penalty multiplied by0.0001).We provide parameters used to train our final model. We use a network of depth 20. Training uses batches of size 64. Momentum and weight decay parameters are set to 0.9 and 0.0001, respectively.				
981	paper_44	What is gradient clipping?	Gradient clipping is a technique often used in training RNNs that alters hyperparameters such that the gradients are always within a certain range.	Adjustable Gradient ClippingGradient clipping is a technique that is often used in training recurrent neural networks [17]. But, to our knowledge, its usage is limited in training CNNs. While there exist many ways to limit gradients, one of the common strategies is to clip individual gradients to the predefined range[-\theta,\theta].With clipping, gradients are in a certain range. With stochastic gradient descent commonly used for training, learning rate is multiplied to adjust the step size. If high learning rate is used, it is likely that \theta is tuned to be small to avoid exploding gradients in a high learning rate regime. But as learning rate is annealed to get smaller, the effective gradient (gradient multiplied by learning rate) approaches zero and training can take exponentially many iterations to converge if learning rate is decreased geometrically.				
982	paper_44	Compare the training time of the proposed network to the SRCNN ?	The proposed method is much faster than SRCNN by a scale of 10 or more.	For maximal speed of convergence, we clip the gradients to [-\frac{\theta}{\gamma},\frac{\theta}{\gamma}], where \gamma denotes the current learning rate. We find the adjustable gradient clipping makes our convergence procedure extremely fast. Our 20-layer network training is done within 4 hours whereas 3-layer SRCNN takes several days to train.We provide quantitative and qualitative comparisons. Compared methods are A+ [22], RFL[18], SelfEx [11] and SRCNN [5]. In Table 3, we provide a summary of quantitative evaluation on several datasets. Our methods outperform all previous methods in these datasets. Moreover, our methods are relatively fast. The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al [6] in their paper based on a GPU implementation.				
983	paper_44	What was the goal behind train a multi-scale model ?	Single-scale models can only be used for one scale, so a new model must be trained for new scales.  However, this would take too long, so a multi-scale model that uses the same parameters no matter the scale would reduce model capacity and make training more efficient.	Scale As in most existing SR methods, SRCNN is trained for a single scale factor and is supposed to work only with the specified scale. Thus, if a new scale is on demand, a new model has to be trained. To cope with multiple scale SR (possibly including fractional factors), we need to construct individual single scale SR system for each scale of interest.However, preparing many individual machines for all possible scenarios to cope with multiple scales is inefficient and impractical.In this work, we design and train a single network to handle multiple scale SR problem efficiently. This turns out to work very well. Our single machine is compared favorably to a single-scale expert for the given sub-task. For three scales factors (×2,3,4\times 2,3,4× 2 , 3 , 4), we can reduce the number of parameters by three-fold.For this reason, we also train a multi-scale model. With this approach, parameters are shared across all predefined scale factors. Training a multi-scale model is straightforward. Training datasets for several specified scales are combined into one big dataset.Third, we show that our method with a single network performs as well as a method using multiple networks trained for each scale. We can effectively reduce model capacity (the number of parameters) of multi-network approaches.Scale Factor We propose a single-model SR approach. Scales are typically user-specified and can be arbitrary including fractions. For example, one might need smooth zoom-in in an image viewer or resizing to a specific dimension. Training and storing many scale-dependent models in preparation for all possible scenarios is impractical. We find a single convolutional network is sufficient for multi-scale-factor super-resolution.				
984	paper_44	Difference between data preparation for the proposed model and the SRCNN?	The proposed model's input size is the same as the receptive field size, and images were divided with no overlap.	Data preparation is similar to SRCNN [5] with some differences. Input patch size is now equal to the size of the receptive field and images are divided into sub-images with no overlap. A mini-batch consists of 64 sub-images, where sub-images from different scales can be in the same batch.				
985	paper_44	What are the main three properties of the model studied in this paper?	They study the deepness of their model, the multi-scale property and how well it performs, and residual learning for faster convergence.	In this section, we study three properties of our proposed method. First, we show that large depth is necessary for the task of SR. A very deep network utilizes more contextual information in an image and models complex functions with many nonlinear layers. We experimentally verify that deeper networks give better performances than shallow ones.Third, we show that our method with a single network performs as well as a method using multiple networks trained for each scale. We can effectively reduce model capacity (the number of parameters) of multi-network approaches.Second, we show that our residual-learning network converges much faster than the standard CNN. Moreover, our network gives a significant boost in performance.				
986	paper_44	How did the authors imporved the accuarcy by making the model deeper but decreased the training time simultaneously?	They changed the model architecture to multi-scale to reduce model capacity and used residual learning with high learning rates to increase training speed.	Second, we show that our residual-learning network converges much faster than the standard CNN. Moreover, our network gives a significant boost in performance.Third, we show that our method with a single network performs as well as a method using multiple networks trained for each scale. We can effectively reduce model capacity (the number of parameters) of multi-network approaches.In this work, we have presented a super-resolution method using very deep networks. Training a very deep network is hard due to a slow convergence rate. We use residual-learning and extremely high learning rates to optimize a very deep network fast. Convergence speed is maximized and we use gradient clipping to ensure the training stability. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.				
987	paper_49	What is the key difference between inductive and transductive learning?	General inductive transfer learning is, given a static source task and any target task where the source task and target task are not equal, to improve the performance of the target task.  This occurs by fine-tuning a model that has been pretrained on other datasets.	Inductive transfer learning has had a large impact on computer vision (CV). Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets Sharif Razavian et al. (2014); Long et al. (2015a); He et al. (2016); Huang et al. (2017).We are interested in the most general inductive transfer learning setting for NLP Pan and Yang (2010): Given a static source task \mathcal{T}_{S} and any target task \mathcal{T}_{T} with \mathcal{T}_{S}\neq\mathcal{T}_{T}, we would like to improve performance on \mathcal{T}_{T}. Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies Linzen et al. (2016), hierarchical relations Gulordava et al. (2018), and sentiment Radford et al. (2017). In contrast to tasks like MT McCann et al. (2017) and entailment Conneau et al. (2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target task, which we show significantly improves performance (see Section 5). Moreover, language modeling already is a key component of existing tasks such as MT and dialogue modeling. Formally, language modeling induces a hypothesis space \mathcal{H} that should be useful for many other NLP tasks Vapnik and Kotz (1982); Baxter (2000).While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer Blitzer et al. (2007). For inductive transfer, fine-tuning pretrained word embeddings Mikolov et al. (2013), a simple transfer technique that only targets a model’s first layer, has had a large impact in practice and is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers Peters et al. (2017); McCann et al. (2017); Peters et al. (2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.				
988	paper_49	What is catastrophic forgetting?	Catastrophic forgetting involved increasing error as a model start to overfit and knowledge captured through pretraining is lost.	Fine-tuning the target classifier is the most critical part of the transfer learning method. Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting). Besides discriminative fine-tuning and triangular learning rates, we propose gradual unfreezing for fine-tuning the classifier.We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.On all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on IMDb. The error then increases as the model starts to overfit and knowledge captured through pretraining is lost. In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning rate schedule.				
989	paper_49	What does "MT domains" mean? 	MT stands for Machine Translation.	Hypercolumns In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns (Hariharan et al., 2015) in CV2 and is used by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau et al. (2017), and McCann et al. (2017) who use language modeling, paraphrasing, entailment, and Machine Translation (MT) respectively for pretraining. Specifically, Peters et al. (2018) require engineered custom architectures, while we show state-of-the-art performance with the same basic architecture across a range of tasks. In CV, hypercolumns have been nearly entirely superseded by end-to-end fine-tuning (Long et al., 2015a).				
990	paper_49	What metrics are used to compare the performance of ULMFiT against existing approaches?	For consistency, the authors reported all results as error rates where lower is better.	In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC-6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10\% of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping.Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures (Peters et al., 2018), multiple forms of attention (McCann et al., 2017) and sophisticated embedding schemes (Johnson and Zhang, 2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real world datasets: Its documents are generally a few paragraphs long—similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.				
991	paper_49	Does the proposed approach in the paper require a labelled dataset?	ULMFiT first pretrains a language model on a large general-domain corpus, and does not require any additional in-domain documents or labels for this.  Then, the method will fine-tune the model on a target task using novel techniques.  In fine-tuning, labelled examples and, if available, other task data are used.	One of the main benefits of transfer learning is being able to train a model for a task with a small number of labels. We evaluate ULMFiT on different numbers of labeled examples in two settings: only labeled examples are used for LM fine-tuning (‘supervised’); and all task data is available and can be used to fine-tune the LM (‘semi-supervised’). We compare ULMFiT to training from scratch—which is necessary for hypercolumn-based approaches. We split off balanced fractions of the training data, keep the validation set fixed, and use the same hyperparameters as before. We show the results in Figure 3.We propose Universal Language Model Finetuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.				
992	paper_49	How well do the proposed model in this paper and the model in Dai and Lee (2015) generalize to documents of varying lengths?	This work’s method was evaluated in six widely-studied datasets which varied in document length and experimental results demonstrated that the method outperformed existing approaches in all of these datasets.  This indicates that this work’s method generalizes to documents of varying lengths.  Also compared to Dai and Le, this method achieved a lower error rate on IMDb showing that it also generalizes better to document lengths reflective of the real world.	We propose Universal Language Model Fine-tuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches Johnson and Zhang (2017); McCann et al. (2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures Peters et al. (2018), multiple forms of attention McCann et al. (2017) and sophisticated embedding schemes Johnson and Zhang (2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real-world datasets: Its documents are generally a few paragraphs long—similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecture—with the same hyperparameters and no additions other than tuned dropout hyperparameters—outperforms highly engineered models and transfer learning approaches on six widely studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10×10\times10 × and—given 50k unlabeled examples—with 100×100\times100 × more data.				
993	paper_49	Are batch normalisation and dropout used for the same reasons?	This work follows standard practices for CV classifiers by using batch normalization and dropout in each block.  Dropout is used to reduce the risk of overfitting which can decrease performance.	Finally, for fine-tuning the classifier, we augment the pretrained language model with two additional linear blocks.Following standard practice for CV classifiers, each block uses batch normalization Ioffe and Szegedy (2015) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer. Note that the parameters in these task-specific classifier layers are the only ones that are learned from scratch. The first linear layer takes as the input the pooled last hidden layer states.In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout999To avoid overfitting, we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier. with the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM reaches surprisingly good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout runs the risk of overfitting, which decreases performance.				
994	paper_49	What is the average number of words in a Wikipedia article from the Wikitext-103 dataset?	The Wikitext-103 dataset consists of 28,595 preprocessed Wikipedia articles and 103 million words.  Therefore, the approximate average number of words per article in this dataset is 3,602.	An ImageNet-like corpus for language should be large and capture general properties of language. We pretrain the language model on Wikitext-103 Merity et al. (2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words. Pretraining is most beneficial for tasks with small datasets and enables generalization even with 100 labeled examples. We leave the exploration of more diverse pretraining corpora to future work, but expect that they would boost performance. While this stage is the most expensive, it only needs to be performed once and improves performance and convergence of downstream models.				
995	paper_49	Out of all the classification datasets used in the experiments of this paper, what is the ratio of number of samples in the largest to the smallest dataset?	The statistics for each dataset and task are found in Table  1.  The ratio of the number of samples in the largest to smallest dataset could be calculated from these statistics.	We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches Johnson and Zhang (2017); McCann et al. (2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1.				
996	paper_49	Why does the proposed model need advanced preprocessing and feature generation to function well?	The proposed model is universal meaning that it does not require custom feature engineering or preprocessing.  For example, the pre-processing is taken from earlier work and only adds special tokens to capture relevant aspects for classification.	We use the same pre-processing as in earlier work Johnson and Zhang (2017); McCann et al. (2017). In addition, to allow the language model to capture aspects that might be relevant for classification, we add special tokens for upper-case words, elongation, and repetition.We propose Universal Language Model Finetuning (ULMFiT), which pretrains a language model (LM) on a large general-domain corpus and fine-tunes it on the target task using novel techniques. The method is universal in the sense that it meets these practical criteria: 1) It works across tasks varying in document size, number, and label type; 2) it uses a single architecture and training process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels				
997	paper_49	Do the authors perform hyperparameter tuning for each dataset independently?	The authors did not perform hyperparameter tuning for each dataset.  They used the same set of hyperparameters across tasks, and they tuned the parameters on the IMDb validation set.	We are interested in a model that performs robustly across a diverse set of tasks. To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the IMDb validation set. We use the AWD-LSTM language model (Merity et al., 2017a) with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70. We apply dropout of 0.4 to layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix. The classifier has a hidden layer of size 50. We use Adam with β1 = 0.7 instead of the default β1 = 0.9 and β2 = 0.99, similar to (Dozat and Manning, 2017). We use a batch size of 64, a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively, and tune the number of epochs on the validation set of each task7 . We otherwise use the same practices				
998	paper_49	By how much does the proposed approach outperform CoVE?	On IMDb, the proposed approach reduced the error by 43. 9% when compared to CoVe but, on TREC-6, the approach did not improve performance significantly.  The results are shown in Table 2.	For consistency, we report all results as error rates (lower is better). We show the test error rates on the IMDb and TREC-6 datasets used by McCann et al. (2017) in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the state-of-the-art on both datasets. On IMDb, we reduce the error dramatically by 43.9% and 22% with regard to CoVe and the state-of-the-art respectively. This is promising as the existing state-of-the-art requires complex architectures Peters et al. (2018), multiple forms of attention McCann et al. (2017) and sophisticated embedding schemes Johnson and Zhang (2016), while our method employs a regular LSTM with dropout. We note that the language model fine-tuning approach of Dai and Le (2015) only achieves an error of 7.64 vs. 4.6 for our method on IMDb, demonstrating the benefit of transferring knowledge from a large ImageNet-like corpus using our fine-tuning techniques. IMDb in particular is reflective of real-world datasets: Its documents are generally a few paragraphs long—similar to emails (e.g for legal discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product response tracking and support email routing.On TREC-6, our improvement—similar as the improvements of state-of-the-art approaches—is not statistically significant, due to the small size of the 500-examples test set.Nevertheless, the competitive performance on TREC-6 demonstrates that our model performs well across different dataset sizes and can deal with examples that range from single sentences—in the case of TREC-6—to several paragraphs for IMDb. Note that despite pretraining on more than two orders of magnitude less data than the 7 million sentence pairs used by McCann et al. (2017), we consistently outperform their approach on both datasets.				
999	paper_49	What does early stopping while training mean?	In this work, all the other methods but ULMFiT were trained with early stopping.	In order to assess the impact of each contribution, we perform a series of analyses and ablations. We run experiments on three corpora, IMDb, TREC6, and AG that are representative of different tasks, genres, and sizes. For all experiments, we split off 10% of the training set and report error rates on this validation set with unidirectional LMs. We fine-tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping.				
1000	paper_49	Are there any differences between AWD-LSTM and LSTMs in general?	The language model AWD-LSTM is a regular LSTM with various tuned dropout hyperparameters, but with no attention, short-cut connections or other sophisticated additions.	In our experiments, we use the state-of-the-art language model AWD-LSTM Merity et al. (2017a), a regular LSTM (with no attention, short-cut connections, or other sophisticated additions) with various tuned dropout hyperparameters. Analogous to CV, we expect that downstream performance can be improved by using higher-performance language models in the future.				
1001	paper_49	If, for a certain model, it was theorized that the penultimate layer is the most important later for generating embeddings, how could discriminative fine-tuning be used to validate or refute that theory?	In this work, discriminative fine-tuning was used to fine-tune each layer with a different learning rate.  Specifically, the learning rate was decreased going from the last layer to lower layers.  The authors found that this improved performance across several datasets.  This suggests that this result shows that the last layers are most important for classification as the technique assigned higher learning rates to them which also increased performance.	As different layers capture different types of information Yosinski et al. (2014), they should be fine-tuned to different extents.To this end, we propose a novel fine-tuning method, discriminative fine-tuning444An unrelated method of the same name exists for deep Boltzmann machines Salakhutdinov and Hinton (2009)..We compare no fine-tuning against fine-tuning the full model Erhan et al. (2010) (‘Full’), the most commonly used fine-tuning method, with and without discriminative fine-tuning (‘Discr’) and slanted triangular learning rates (‘Stlr’) in Table 6. Fine-tuning the LM is most beneficial for larger datasets. ‘Discr’ and ‘Stlr’ improve performance across all three datasets and are necessary on the smaller TREC-6, where regular fine-tuning is not beneficial.The SGD update with discriminative finetuning is then the following: θ l t = θ l t−1 − η l · ∇θ lJ(θ) (2) We empirically found it to work well to first choose the learning rate η L of the last layer by fine-tuning only the last layer and using η l−1 = η l/2.6 as the learning rate for lower layers.				
1002	paper_49	While the proposed approach uses Adam, would there be any systemic issues with using SGD or RMSProp with discriminative fine-tuning?	Discriminative fine-tuning tunes each layer with different learning rates.	Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent (SGD) update of a model’s parameters \theta at time step t looks like the following Ruder (2016):\theta_{t}=\theta_{t-1}-\eta\cdot\nabla_{\theta}J(\theta)(1)where \eta is the learning rate and \nabla_{\theta}J(\theta) is the gradient with regard to the model’s objective function. For discriminative fine-tuning, we split the parameters \theta into \{\theta^{1},\ldots,\theta^{L}\} where \theta^{l} contains the parameters of the model at the l-th layer and L is the number of layers of the model. Similarly, we obtain \{\eta^{1},\ldots,\eta^{L}\} where \eta^{l} is the learning rate of the l-th layer.The SGD update with discriminative fine-tuning is then the following:\theta_{t}^{l}=\theta_{t-1}^{l}-\eta^{l}\cdot\nabla_{\theta^{l}}J(\theta)(2)We empirically found it to work well to first choose the learning rate \eta^{L} of the last layer by fine-tuning only the last layer and using \eta^{l-1}=\eta^{l}/2.6 as the learning rate for lower layers.				
1003	paper_49	The paper mentions how training two models, one forward model and one backward model (to achieve bidirectionality) results in a performance gain. Is the performance gain proportional to the 2x increase in training cost?	Training a second model to ensemble a bidirectional model brings a performance boost of around 0. 5 to 0. 7 and, on IMDb, it lowers test error from 5. 30 to 4.  The increase in performance (or decrease of test error) is of around 15% which is not proportional to the 2x increase in training cost.	At the cost of training a second model, ensembling the predictions of a forward and backwards LM-classifier brings a performance boost of around 0.5–0.7. On IMDb we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model.				
1004	paper_49	The paper mentions how training two models, one forward model and one backward model (to achieve bidirectionality) results in a performance gain. Would it be possible to achieve bidirectionality with just one model via some form of masked language modelling in this specific approach?	In this work, the authors train two models: a forward and a backward LM.	Similar to existing work Peters et al. (2017, 2018), we are not limited to fine-tuning a unidirectional language model. For all our experiments, we pretrain both a forward and a backward LM. We fine-tune a classifier for each LM independently using BPT3C and average the classifier predictions.				
1005	paper_49	What theoretical backing, if any, exists to support the authors' numerical arguments around how their techniques minimze catastrophic forgetting?	Fine-tuning the full model leads to low error early in training, but then error increases as the model overfits and loses knowledge captured through pretraining.  The authors refer to this as catastrophic forgetting.  In contrast, ULMFiT is more stable as performance remains similar or improves even until later epochs.  Thus, the authors argue that ULMFiT’s stable performance until later epochs is evidence that catastrophic forgetting is minimized.	On all datasets, fine-tuning the full model leads to the lowest error comparatively early in training, e.g. already after the first epoch on IMDb. The error then increases as the model starts to overfit and knowledge captured through pretraining is lost. In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of the learning rate schedule				
1006	paper_49	How would using max-pooling, min-pooling, or mean-pooling instead of the proposed concat-pooling impact memory utilisation during training?	Concat-pooling functions by concatenating the hidden state of the last time step of the document with both the max-pooled and mean-pooled representations of the hidden states.  This pooling is done for as many time steps as fit in GPU memory.  This means that concat-pooling utilises as much memory as there is available.	The signal in text classification tasks is often contained in a few words, which may occur anywhere in the document. As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model. For this reason, we concatenate the hidden state at the last time step \mathbf{h}_{T} of the document with both the max-pooled and the mean-pooled representation of the hidden states over as many time steps as fit in GPU memory \mathbf{H}=\{\mathbf{h}_{1},\ldots,\mathbf{h}_{T}\}:\mathbf{h}_{c}=[\mathbf{h}_{T},\mathtt{maxpool}(\mathbf{H}),\mathtt{meanpool}(\mathbf{H})](4)where [] is concatenation.				
1007	paper_49	What is back propogation through time?	Backpropagation through time (BPTT) is used to train language models to enable gradient propagation for large input sequences.  This work proposes BPTT for text classification (BPT3C) where a document is first divided into fixed-length batches.  At the beginning of each batch, the model is initialized with the final state of the previous batch and gradients are back-propagated to the batches whose hidden states contributed to the final prediction.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	Feedback Processing through time (BPTT) is used to train language models to enable gradient propagation for large input sequences.  This work proposes BPTT for text classification (BPT3C) where a document is first divided into fixed-length batches.  At the beginning of each batch, the model is initialized with the final state of the previous batch and gradients are back-propagated to the batches whose hidden states contributed to the final prediction.	Tortured phrases	Back propagation -> Feedback Processing	
1008	paper_49	Why do the authors claim that their proposed model is "sample efficient"? 	The authors claim that their proposed method enables sample-efficient transfer learning through experiments where they showed that, with only 100 labelled examples, ULMFiT could match the performance of training from scratch with 10 to 20 times more data.	We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecture—with the same hyperparameters and no additions other than tuned dropout hyperparameters—outperforms highly engineered models and transfer learning approaches on six widely studied text classification tasks. On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10×10\times10 × and—given 50k unlabeled examples—with 100×100\times100 × more data.Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP. 2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning. 3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets. 4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis. 5) We make the pretrained models and our code available to enable wider adoption.On IMDb and AG, supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10× and 20× more data respectively, clearly demonstrating the benefit of general-domain LM pretraining. If we allow ULMFiT to also utilize unlabeled examples (50k for IMDb, 100k for AG), at 100 labeled examples, we match the performance of training from scratch with 50× and 100× more data on AG and IMDb respectively. On TREC-6, ULMFiT significantly improves upon training from scratch; as examples are shorter and fewer, supervised and semi-supervised ULMFiT achieve similar results.				
1009	paper_49	Is weak supervision a subset or type of regular supervised learning?	In this work, fine-tuning an LM only on labeled examples is considered to be supervised learning.	One of the main benefits of transfer learning is being able to train a model for a task with a small number of labels. We evaluate ULMFiT on different numbers of labeled examples in two settings: only labeled examples are used for LM fine-tuning (‘supervised’); and all task data is available and can be used to fine-tune the LM (‘semi-supervised’). We compare ULMFiT to training from scratch—which is necessary for hypercolumn-based approaches. We split off balanced fractions of the training data, keep the validation set fixed, and use the same hyperparameters as before. We show the results in Figure 3.				
1010	paper_49	Do the authors use gradient accumulation while training the model?	This work uses Backpropagation Through Time for Text Classification (BPT3C) where a document is divided into fixed-length batches and the gradients are back-propagated to batches.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	This work uses Back spread Through Time for Text Classification (BPT3C) where a document is divided into fixed-length batches and the gradients are back-propagated to batches.	Tortured phrases	Back propagation -> Back spread	
1011	paper_49	What is the difference in error rate on IMDb dataset with and without pretraining?	The authors compared using no pretraining with pretraining on WikiText-103 and showed that pretraining was useful in improving performance for small to larg-esized datasets.	We compare using no pretraining with pretraining on WikiText-103 Merity et al. (2017b) in Table 4. Pretraining is most useful for small and medium-sized datasets, which are most common in commercial applications. However, even for large datasets, pretraining improves performance.				
1012	paper_49	In language models, which method would be better for preventing overfitting from batch normalization and dropout?	According to this work, without dropout, a vanilla LM can run the risk of overfitting, which decreases performance.	In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout999To avoid overfitting, we only train the vanilla LM classifier for 5 epochs and keep dropout of 0.4 in the classifier. with the AWD-LSTM LM with tuned dropout parameters in Table 5. Using our fine-tuning techniques, even a regular LM reaches surprisingly good performance on the larger datasets. On the smaller TREC-6, a vanilla LM without dropout runs the risk of overfitting, which decreases performance.				
1013	paper_49	Are (1) slanted triangular learning rate and (2) linear warmup followed by linear decay the same thing?	Slanted triangular learning rate (SLTR) involves first linearly increasing the learning rate and then linearly decaying it according to a given update schedule.  It modifies triangular learning rates by using a short increase and long decay period.  The function of linear warmup and linear decay cannot be found in this paper.	For adapting its parameters to task-specific features, we would like the model to quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. Using the same learning rate (LR) or an annealed learning rate throughout training is not the best way to achieve this behaviour.Instead, we propose slanted triangular learning rates (STLR), which first linearly increases the learning rate and then linearly decays it according to the following update schedule, which can be seen in Figure 2:\begin{split}cut&=\lfloor T\cdot cut\_frac\rfloor\\p&=\begin{cases}t/cut,&\text{if}\ t<cut\\1-\frac{t-cut}{cut\cdot(1/cut\_frac-1)},&\text{otherwise}\end{cases}\\\eta_{t}&=\eta_{max}\cdot\frac{1+p\cdot(ratio-1)}{ratio}\end{split}(3)where T is the number of training iterations555In other words, the number of epochs times the number of updates per epoch., cut\_frac is the fraction of iterations we increase the LR, cut is the iteration when we switch from increasing to decreasing the LR, p is the fraction of the number of iterations we have increased or will decrease the LR respectively, ratio specifies how much smaller the lowest LR is from the maximum LR \eta_{max}, and \eta_{t} is the learning rate at iteration t. We generally use cut\_frac=0.1, ratio=32 and \eta_{max}=0.01.STLR modifies triangular learning rates Smith (2017) with a short increase and a long decay period, which we found key for good performance.666We also credit personal communication with the author. In Section 5, we compare against aggressive cosine annealing, a similar schedule that has recently been used to achieve state-of-the-art performance in CV Loshchilov and Hutter (2017).777While Loshchilov and Hutter (2017) use multiple annealing cycles, we generally found one cycle to work best.				
1014	paper_5	What are some other approaches for semantic similarity and how do they differ to Sentence Transformers in architecture and performance?	The authors mention that they experiment with using a sentence transformer (Reimers and Gurevych, 2019) and a custom Seq2Seq model called GUD-IR for their retrieval function.  The paper does not contain any information on any other models (apart from these two) that could be used for semantic similarity.	A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming \mathbf{x}, followed by a similarity lookup of the transformed \mathbf{x} in \mathcal{M}. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient.However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding.For example, consider two situations from ert-nl: Filling a false time sheet at work and Being at a party, and telling parents I am studying.These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority.In our experiments, off-the-shelf methods failed to address these challenges (see §4 later).After the transformation stage, the closest matching entry is then used as the corresponding \mathbf{fb}. Transformation reduces \mathcal{M}(\mathbf{x}) to a search over \mathbf{fb}_{1},\mathbf{fb}_{2},\ldots,\mathbf{fb}_{|\mathcal{M}|} with \hat{\mathbf{fb}} as the search query. We compute similarity based on a fine-tuned Sentence transformers (Reimers and Gurevych, 2019).To address these challenges with transformation in complex tasks, we have designed a novel SEQ 2 SEQ based transformation called GUD - IR. Given x, GUD - IR generates a transformed feedback fb for x using a generative SEQ 2 SEQ model. Our approach is inspired and supported by the recent success of generate and retrieve (Mao et al., 2021) methods. However, despite the similarity, the methods have different goals: Mao et al. (2021) leverage generative models for query expansion, whereas our goal is explainable input understanding. See Appendix B for more details on GUD - IR.				
1015	paper_5	How accurate or correct was their few-shot approach to making GPT-3 verbalize its understanding?	The authors mention in multiple places how their iterative correction/feedback process depends on GPT verbalizing it's thinking process or understanding of the user's inputs or needs.  They explain how they encourage this sort of behaviour through modifying the prompt, but this paper does not seem to quantifiably measure how "accurate" this verbalization would be. 	We operationalize this idea by including task verbalization in the prompt (Fig. 3).Given a question What sounds like < sighted > ?, a vanilla prompting approach will generate the answer cited.In contrast, we include a \mathbf{u} the homophone for in the prompt.gpt-3 is adept at reasoning with just a handful of examples, and thus can be expected to mimic the prompt to generate task understanding and answer.gpt-3 generates such additional information in all our tasks.Given a test question What sounds similar to < sighted > ?, if the model generates the word that has the same meaning as \mathbf{u}, the user has a reason to believe that the answer is wrong.Figure 1 presents a sample interaction between a user and gpt-3 that our setup enables.The model was asked for a similar word. However, the model’s (incorrect) task understanding \mathbf{u} was “The homophone of good is”.The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback \mathbf{fb} as "similar to means with a similar meaning", clarifying that they actually wanted a synonym.Crucially, note that such instructional correction is feasible even if the user does not knowthe correct answer to their question, as they are critiquing the model’s understanding of theirintent, rather than the answers themselves.Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach.We present \ours, a novel, memory-enhanced gpt-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user’s intent, providing an avenue for feedback.We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility.In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings.				
1016	paper_5	Instead of editing the prompt based on previous cases, did they test a simpler approach where previous cases are concatenated to the prompt?	This is the GROW-PROMPT baseline mentioned in the paper - they add the latest few samples in memory to the prompt.	We compare \ours(memory-assisted prompt editing) with two baselines:\bulletno-mem This is the standard gpt-3 444We use gpt-3-175b (davinci) for all experiments. in few-shot prompting mode (hyper-parameters listed in Appendix §C). Input is \mathbf{p}\ \#\ \mathbf{x}_{i} (i.e., question \mathbf{x}_{i} appended to prompt \mathbf{p}).It generates answer \mathbf{y}_{i} and its understanding of the user’s intent \mathbf{u}_{i}.\bulletgrow-prompt: Similar to no-mem, but the \mathbf{p} is continuously grown with a subset of memory \mathcal{M} that can fit within the prompt (max. 2048 tokens).The most recent subset of \mathcal{M} of memory inserted is inserted in the prompt.The ethical reasoning tasks (ert) involve long examples, and the initial prompt itself takes close to the max allowed tokens.Thus, the grow-prompt setup is only provided for the lexical relations and word scrambling tasks.				
1017	paper_5	Why does the approach need a gating mechanism when a good retrieval should be able to correctly filter out irrelevant feedback from the memory?	A gating function is needed precisely because the retrieval function might not be able to filter out irrelevant feedback from memory.  This is a challenging thing to implement since syntactically or lexically similar things might or might not refer to similar concepts.  Another challenge is with adversarial feedback, made by users intending to mess with the system.  It is true that a gating function in the combiner would not be required if the retrieval function was near perfect at eliminating irrelevant feedback.  Regardless, some form of filtering has to be done at some stage - whether it's in the lookup/retrieval function or in the combiner function does not specifically matter, but the existence of a filtering mechanism is crucial for MemPrompt-like systems to work.	: A gating function allowing irrelevant, retrieved feedback to be ignored.A retrieved past feedback that is incorrect might cause the model to make a mistake, thus necessitating a good retrieval function. We propose a two-stage method for effective retrieval involving: transforming \mathbf{x}, followed by a similarity lookup of the transformed \mathbf{x} in \mathcal{M}. When the task involves high surface-level similarity among past feedback, such as in lexical word tasks, then a simple heuristic-based transformation is sufficient.However, such simple transformations are insufficient for tasks that involves more complex retrieval e.g., when two lexically dissimilar situations can share the same understanding.For example, consider two situations from ert-nl: Filling a false time sheet at work and Being at a party, and telling parents I am studying.These situations look lexically dissimilar but correspond to the same underlying social principle lying to authority.In our experiments, off-the-shelf methods failed to address these challenges (see §4 later).Extending the discussion on noise in feedback, our setting assumes that users will not provide any adversarial feedback. However, in real-world environments, this assumption is unlikely to hold. Additionally, there is a risk in the real-world deployment of our system, wherein an adversarial user might provide harmful feedback, thus maliciously controlling the systems (potentially a home-based robot) where our method is deployed. Thus, robust mechanisms such as gud-ir and memory adapters will be critical for successful real-world deployments.				
1018	paper_5	What are a couple of examples of "lexical QA tasks"?	 An example of a synonym task could be "what is a word that has the same meaning as encumbrance". 	We apply our approach to four tasks: (1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration), and (4) ethics (with user feedback being natural language).For all five tasks, the dataset consists of (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) tuples, where \mathbf{fb} clarifies the task in \mathbf{x}.We have a simulated conversational setting, in which a user can ask the model \mathbf{x} (covering any of these five tasks). If the model gives a wrong answer to query \mathbf{x}, then \mathbf{fb} is used as the simulated corrective feedback.The sources for these datasets are listed in Appendix  §E.The lexical relation task is to predict a word with a given lexical relationship to an input word.We use five relationships: synonym (syn), antonym (ant), homophone (hom), definition (defn), and sentence usage generation (sent).				
1019	paper_5	Why does the approach not simply add all feedback examples in memory to the prompt if they will be adding examples anyways?	The proposed approach (MemPrompt) probably does not add all feedback examples in memory to the prompt since the size of the prompt is limited to 2048 tokens.  Additionally, increasing the size of the prompt leads to higher cost (in terms of compute resources required to process the query).	Figure 8 reports the overall performance on the word reasoning tasks.The accuracy improves substantially within 300 examples when using memory (in yellow) vs. no memory (in blue).Note that we are operating in a few-shot prompting regime (i.e., there is no training data over which we train).The performance of grow-prompt (red) lies in between, showing that non-selective memory is partially helpful, although not as effective as failure-driven retrieval (our model).However, grow-prompt is \sim 3x more expensive (larger prompts) and cannot scale beyond the 2048 tokens limit.We also found that the retrieved feedback from memory was effective 97% of the time; only in \approx 3% of cases feedback had no positive effect.				
1020	paper_5	What if, instead of concatenating the feedback to the prompt, the prompt was automatically edited according to the feedback?	Strictly speaking, the proposed approach is editing the prompt - even additions or concatenations to a user prompt also count as "editing" the prompt.  However, the current paper contains no information on what would happen if the prompt were somehow edited without concatenation. 	Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022).Recent work such as Liu et al. (2021a) investigate using dynamic prompts for better generation. For a given input \mathbf{x}, their method( kate) relies on retrieving examples from the training set that are similar to \mathbf{x} for dynamically creating the prompt \mathbf{p}. Note that our method edits \mathbf{x} with a feedback \mathbf{fb}, and is thus complementary to kate.We verify this by experiments on ert-cat and ert-nl. Specifically, we create dynamic prompts using kate, whereas \oursis used like before to attach a \mathbf{fb} to the question. We observe a consistent 10% improvement by using kate across all baselines, showing that the improvements are complementary.In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings.				
1021	paper_5	How did they edit the prompt based on previous examples? Was it by editing the original text of the prompt or simply by concatenating the examples?	They seem to be including memory in prompts by adding the natural language feedback (fb) that users provide on a prompt (x) and it's response (u) by including the tuple (x, u, fb) in a structured format.  It does seem like they are merely concatenating multiple of these tuples and adding them to the prompt, but the exact format of the prompt itself is not fully explained in the paper.	In our setup, given an input \mathbf{x}, a model generates an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task, a skill learned through few-shot examples in theprompt (Appendix D).The user can then critique \mathbf{u} by providing natural language feedback \mathbf{fb}. This is feasible even if the user does not know the correctness of \mathbf{y} because they are critiquing the model’s understanding of their intent rather the answers themselves. As mentioned, given an input \mathbf{x}, we prompt the model to generate an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task.Thus, the in-context examples for \oursare of the form \mathbf{x}\rightarrow\mathbf{u},\mathbf{y}.In addition to the input \mathbf{x}, \oursretrieves a \mathbf{fb} if a question similar to \mathbf{x} has been asked before.To enable the model to react to such feedback, we also include examples of the form (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) in the prompt, which are aimed to teach the model to react to \mathbf{fb} (Appendix D).Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022).Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory \mathcal{M} of such feedback as a set of key-value pairs, where thekey is a misunderstood question, and the value is the user’s feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistakeon a similar question earlier, by querying the memory for a similar question. If found,append the corresponding feedback to the question prompt. This mechanism aims toprevent the model from making the same type of mistake twice. This failure-driven remindingmechanism draws inspiration from the theory of recursive reminding in psychology Jacoby and Wahlheim (2013),which suggests humans index error corrections in the context in which those errors occurred.Recent work such as Liu et al. (2021a) investigate using dynamic prompts for better generation. For a given input \mathbf{x}, their method( kate) relies on retrieving examples from the training set that are similar to \mathbf{x} for dynamically creating the prompt \mathbf{p}. Note that our method edits \mathbf{x} with a feedback \mathbf{fb}, and is thus complementary to kate.We verify this by experiments on ert-cat and ert-nl. Specifically, we create dynamic prompts using kate, whereas \oursis used like before to attach a \mathbf{fb} to the question. We observe a consistent 10% improvement by using kate across all baselines, showing that the improvements are complementary.				
1022	paper_5	Is this work focused only on solving cases where GPT-3 misunderstands the users' intents?	The authors' do discuss one specialized use case, on how memory assisted models such as these could be used to personalize models, but even this use-case could be seen as a subset of the broader use-case of users correcting a model's misunderstanding.	Finally, our work is a simple example of debugging and learning via dialog. While system debugging through dialogue has been explored in many contexts (Hixon et al., 2015; Wang et al., 2016; Davis, 1977), our contribution is a dialogue about the model’s understanding of the user’s intent.Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.Figure 1 presents a sample interaction between a user and gpt-3 that our setup enables.The model was asked for a similar word. However, the model’s (incorrect) task understanding \mathbf{u} was “The homophone of good is”.The user can detect such discrepancy between the intended and interpreted task instruction, and can provide feedback \mathbf{fb} as "similar to means with a similar meaning", clarifying that they actually wanted a synonym.Crucially, note that such instructional correction is feasible even if the user does not knowthe correct answer to their question, as they are critiquing the model’s understanding of theirintent, rather than the answers themselves.Thus, our setup does not require the users to be experts at tasks being solved, another advantage of our approach.This paper sets out the general architecture and a simple implementation of its components.We then demonstrate the system on four tasks, using simulated user feedback:(1) lexical relations (e.g., antonyms, Figure 1),(2) word scrambling (e.g., anagrams), (3) ethics (with user feedback being the appropriate class of ethicalconsideration, e.g., “it is about cheating”, using a small set of categories), and (4) ethics (with user feedback beingnatural language). We find that in all cases, gpt-3’s accuracy significantly increases with time, without retraining,as our approach enables it to use corrective feedback from earlier examples to avoid similar misunderstandings on future examples. In summary, our contributions are:\bulletWe show that a large model like gpt-3 can be improved after deployment, without retraining, through a memory-assisted architecture.\bulletOur implementation, \ours, is the first demonstration that this is possible - this is an important step forward for real use of LMs, and the paper sets out a general architecture that others can build on, a specific implementation, and detailed evaluation on multiple tasks.We demonstrate an application of \oursfor personalization with a use-case where user language preferences can be folded in the memory. We simulate a user who does not speak fluent English and uses code-mixed language. The queries posed by the user contain words from two Indian languages: Hindi and Punjabi. gpt-3 predictably misunderstands the task. The user clarifies the meanings of their dialect/language phrases. While initial queries fail, subsequent queries that reuse similar words succeed because their clarifications are present in the memory (details in Appendix §G).We present \ours, a novel, memory-enhanced gpt-3 that allows users to interact and improve the model without retraining. A key insight is to have the model articulate not just its answer but also its understanding of the user’s intent, providing an avenue for feedback.We show that deployed systems with fixed large-language models can still be improved by interacting with end-users, potentially improving their performance and broadening their utility.In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings.				
1023	paper_5	With "memory of cases" here, do they simply mean a prompt that contains all of these cases as examples?	However, the MemPrompt model's input size is limited to 2048-tokens, so adding all possible matches to the prompt would not be possible, which is why the authors' proposed approach specifically focuses on selecting which prompts to include.	As mentioned, given an input \mathbf{x}, we prompt the model to generate an output \mathbf{y} and a sentence \mathbf{u} expressing its understanding of the task.Thus, the in-context examples for \oursare of the form \mathbf{x}\rightarrow\mathbf{u},\mathbf{y}.In addition to the input \mathbf{x}, \oursretrieves a \mathbf{fb} if a question similar to \mathbf{x} has been asked before.To enable the model to react to such feedback, we also include examples of the form (\mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y}) in the prompt, which are aimed to teach the model to react to \mathbf{fb} (Appendix D).Our approach, \ours, pairs gpt-3 with a growing memory of cases where the model misunderstood user’s intent and was provided with corrective feedback.This feedback is question dependent, and thus the prompt for each sample is edited to adapt to the input.In this sense, our work can be seen as an instance of prompt engineering Liu et al. (2021b) which involves editing the prompts. Our work adds interactivity to prompt engineering as it involves dynamically updating the prompt for every instance.Once the feedback is received from the user, can the model successfully utilize it? By adding a few examples of the form \mathbf{x},\mathbf{fb}\rightarrow\mathbf{u},\mathbf{y} in the prompt and setting \mathbf{fb}=\mathbf{u}, we force the model to use the task understanding present in the input when generating the output (Figure 4).Recently, it has been shown that such repetition plays a crucial role in the success of few-shot prompting models (Madaan and Yazdanbakhsh, 2022).Further, it is desirable to have a system that can leverage past feedback on new, unseen examples for prompt-editing. We maintain a memory \mathcal{M} of such feedback as a set of key-value pairs, where thekey is a misunderstood question, and the value is the user’s feedback to correct that misunderstanding. Given a new question, we check if the model has made a mistakeon a similar question earlier, by querying the memory for a similar question. If found,append the corresponding feedback to the question prompt. This mechanism aims toprevent the model from making the same type of mistake twice. This failure-driven remindingmechanism draws inspiration from the theory of recursive reminding in psychology Jacoby and Wahlheim (2013),which suggests humans index error corrections in the context in which those errors occurred.In Tandon et al. (2022), we show that using a memory of user feedback can be used to repair erroneous model in a supervised setting.In this work, we build upon the recent advances in few-shot prompting to modify gpt-3’s behavior by adding user feedback to the query (prompt).Like others, we use gpt-3 with few-shot prompting, where the prompt consistsof a prefix prefix containing a few input-output “training” examples of the task, followed by the input x, e.g., a question,to operate on. However, while prior work has focused on constructing better prefixes, e.g., dynamically selecting good “training” examplesbased on the question Le Scao and Rush (2021); Liu et al. (2021a), or even representing the prefix latently Li and Liang (2021),our work elaborates the input x itself to clarify the intended task, by adding user feedback fb from previous misunderstandings.				
1024	paper_50	Where do the authors source their labelled dataset from?	The source of the labelled dataset in the paper is two news websites, namely, CNN and Daily News.  The authors created the dataset of approximately one million data points from ~93k CNN and ~220k Daily Mail online news articles.	In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.				
1025	paper_50	What is the ratio of the total number of articles collected from CNN and Daily News?	Assuming “Daily News” here refers to “Daily Mail”, one of the websites the authors sourced the data from, the ratio of CNN:(Daily Mail) articles is approximately 93:220 or 1:2.	Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.				
1026	paper_50	In what ways does the authors' approach differ from how practitioners create datasets and pretrain models such as GPT?	The authors mention that their approach, which involves creating a labelled dataset and using that for supervised learning objectives, differs from existing work in the field which focusses on unsupervised approaches.  Authors claim that unsupervised approaches are explored more because of the difficulties and challenges associated with building labelled datasets.	In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results.Such an approach requires a large training corpus of document–query–answertriples and until now such corpora have been limited to hundreds of examples andthus mostly of use only for testing [9]. This limitationhas meant that most work in this area has taken the form of unsupervisedapproaches which use templates or syntactic/semantic analysers to extractrelation tuples from the document to form a knowledge graph that can be queried.				
1027	paper_50	When defining the reading comprehension task, the authors explain that they wish to estimate p(a|c, q). What would a model trained on this task do if the context "c" itself had factually incorrect information? 	The authors are training a reading comprehension model.  comprehension) and not knowledge of global correctness.	In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.The reading comprehension task naturally lends itself to a formulation as asupervised learning problem. Specifically we seek to estimate the conditionalprobability p(a|c,q), where c is a context document, q a query relating tothat document, and a the answer to that query.For a focused evaluation we wish to be able to exclude additional information,such as world knowledge gained from co-occurrence statistics, in order to test amodel’s core capability to detect and understand the linguistic relationshipsbetween entities in the context document.Note that the focus of this paper is to provide a corpus for evaluating a model’sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus.				
1028	paper_50	The paper mentions using Daily News and CNN bullet-point summaries to generate queries. Would the authors' approach towards building this supervised dataset work effectively if these news sources created the summaries by merely extracting sentences from the whole article, instead of rephrasing and condensing text?	The authors, in multiple places, emphasize that their approach relies on the fact that DailyMail and CNN both use abstractive summaries for their bullet points.  This fact probably implies that the authors approach would not work on news sources that merely use excerpts or extracts for summaries.	In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.				
1029	paper_50	How are the bullet-point summaries converted to queries?	Each article in the news websites they used (CNN, DailyMail) has a couple of bullet points containing an abstractive summary of the article.  More details on what Cloze-style questions are is not available in this paper.	In this work we seek to directly address the lack of real natural languagetraining data by introducing a novel approach to building a supervised readingcomprehension data set. We observe that summary and paraphrase sentences, withtheir associated documents, can be readily converted to context–query–answertriples using simple entity detection and anonymisation algorithms.Using this approach we have collected two new corpora of roughly a million newsstories with associated queries from the CNN and Daily Mail websites.Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.				
1032	paper_50	All the models proposed in this paper are sequence-to-sequence models. True or False?	The authors seem to be using LSTM models for performing their analysis and experiments.	Long short-term memory (LSTM, [18]) networks haverecently seen considerable success in tasks such as machine translation andlanguage modelling [17]. When used for translation, DeepLSTMs [19] have shown a remarkable ability to embed longsequences into a vector representation which contains enough information togenerate a full translation in another language. Our first neural model forreading comprehension tests the ability of Deep LSTM encoders to handlesignificantly longer sequences. We feed our documents one word at a time intoa Deep LSTM encoder, after a delimiter we then also feed the query into theencoder. Alternatively we also experiment with processing the query then thedocument. The result is that this model processes each document query pair as asingle long sequence. Given the embedded document and query the networkpredicts which token in the document answers the query.Within the group of neural models explored here, the results paint a clearpicture with the Impatient and the Attentive Readers outperforming all othermodels. This is consistent with our hypothesis that attention is a keyingredient for machine reading and question answering due to the need topropagate information over long distances. The Deep LSTM Readerperforms surprisingly well, once again demonstrating that this simple sequentialarchitecture can do a reasonable job of learning to abstract long sequences,even when they are up to two thousand tokens in length. However this model doesfail to match the performance of the attention based models, even though theseonly use single layer LSTMs.666Memory constraints prevented us fromexperimenting with deeper Attentive Readers.				
1033	paper_50	What is the main mathematical difference between the attentive LSTM reader and the vanilla Deep LSTM?	The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM’s fixed and limited context size by taking into account the entire context of every token via a token-level attention mechanism.	The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document.We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models’ performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g. ‘killed’\rightarrow ‘deceased’, and co-reference or anaphora resolution, e.g. ‘ent119 was killed’ \rightarrow ‘he was identified.’ However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer.	The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM’s fixed and limited context size by taking into account the entire context of every token via a token-level consideration system.	Tortured phrases	attention mechanism -> consideration system	
1034	paper_50	Assuming the authors performed a brute force hyperparameter search on all permutations of the five hyperparameters - hidden layer sizes, depths, LR, batch size and dropout - how many total experiments would they have had to perform?	For Deep LSTM readers 3 values of hidden layer sizes, 3 values of depths, 3 starting LRs, 2 batch sizes and 3 dropout fractions are considered.  This is a total of (3 * 3 * 3 * 2 * 3) = 162 configurations.  Similarly for attention models, they experimented with 3 hidden layer sizes, 4 possible learning rates, 3 batch sizes and 4 values of dropout (3*4*3*4) = 144 possible configurations.  Thus, if the authors performed a brute force hyperparameter search, they would have had to done 162 experiments for the vanilla Deep LSTM model and 144 experiments for the attention based model, or a total of 162 + 144 = 306 experiments.	All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\underline{256}]}, depths {[1,\underline{2},4]}, initiallearning rates {[1\text{\sc{e}}{-}3,5\text{\sc{e}}{-}4,\underline{1\text{\sc{e}}{-}4},5\text{\sc{e}}{-}5]}, batchsizes {[16,\underline{32}]} and dropout [0.0,\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\text{\sc{e}}{-}4,5\text{\sc{e}}{-}5,2.5\text{\sc{e}}{-}5,1\text{\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.				
1036	paper_50	The deepest model that the authors experimented with had 8 layers in it. True or False?	False.  The deepest model the authors experimented with appears to be a four layer DeepLSTM Reader model.  For attention models, the authors exclusively used only a single layer model.	All model hyperparameters were tuned on the respective validation sets of thetwo corpora.555For the Deep LSTM Reader, we consider hidden layer sizes{[64,128,\underline{256}]}, depths {[1,\underline{2},4]}, initiallearning rates {[1\text{\sc{e}}{-}3,5\text{\sc{e}}{-}4,\underline{1\text{\sc{e}}{-}4},5\text{\sc{e}}{-}5]}, batchsizes {[16,\underline{32}]} and dropout [0.0,\underline{0.1},0.2]. Weevaluate two types of feeds. In the cqa setup we feed first thecontext document and subsequently the question into the encoder, while theqca model starts by feeding in the question followed by the contextdocument. We report results on the best model (underlined hyperparameters,qca setup). For the attention models we consider hidden layer sizes[64,128,256], single layer, initial learning rates[1\text{\sc{e}}{-}4,5\text{\sc{e}}{-}5,2.5\text{\sc{e}}{-}5,1\text{\sc{e}}{-}5], batch sizes [8,16,32] and dropout[0,0.1,0.2,0.5]. For all models we used asynchronous RmsProp[20] with a momentum of 0.9 and a decay of 0.95.See Appendix A for more details of the experimental setup.Our experimental results are in Table 5, with theAttentive and Impatient Readers performing best across both datasets.				
1037	paper_50	The Daily Mail part of the dataset is approximately 2x larger than the CNN section of the dataset. True or false?	True.  The ratio of number of articles from CNN and DailyMail is 1:2.  Similarly, the ratio of queries from these datasets is given by 380,298:879,450 = 1:2.  Since 2. 31 and 2. 36 both round down to 2, the statement is true, approximately.	Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.				
1038	paper_50	Do the authors claim that bigger datasets would improve the performance and expressiveness of reading comprehension models?	Based on the information in this paper alone, it is unclear if a bigger dataset would improve the performance of reading comprehension models.  While authors explain that a key contribution they make is the creation of a real-world, massive labelled reading comprehension dataset, it is unclear if such a dataset is essential to improve the performance of reading comprehension models - the authors pitch their dataset-building approach also as a way of evaluating performance of these models, which is different from the dataset itself leading to better performance.	While obtaining supervised natural language reading comprehension data hasproved difficult, some researchers have explored generating synthetic narrativesand queries [3, 4]. Such approaches allowthe generation of almost unlimited amounts of supervised data and enableresearchers to isolate the performance of their algorithms on individualsimulated phenomena. Work on such data has shown that neural network basedmodels hold promise for modelling reading comprehension, something that wewill build upon here. Historically, however, many similar approaches inComputational Linguistics have failed to manage the transition from syntheticdata to real environments, as such closed worlds inevitably fail tocapture the complexity, richness, and noise of natural language[5].The supervised paradigm for training machine reading and comprehension modelsprovides a promising avenue for making progress on the path to building fullnatural language understanding systems. We have demonstrated a methodology forobtaining a large number of document-query-answer triples and shown thatrecurrent and attention based neural networks provide an effective modellingframework for this task.Our analysis indicates that the Attentive and Impatient Readers are able topropagate and integrate semantic information over long distances. In particularwe believe that the incorporation of an attention mechanism is the keycontributor to these results.Here we propose a methodology for creating real-world, large scale supervisedtraining data for learning reading comprehension models. Inspired by work insummarisation [10, 11], we create two machinereading corpora by exploiting online newspaper articles and their matchingsummaries. We have collected 93k articles from the CNN111www.cnn.com and 220k articles fromthe Daily Mail222www.dailymail.co.uk websites. Both news providers supplement their articles with anumber of bullet points, summarising aspects of the information contained in thearticle. Of key importance is that these summary points are abstractive and donot simply copy sentences from the documents.We construct a corpus of document–query–answer triples by turning thesebullet points into Cloze [12] style questions by replacingone entity at a time with a placeholder. This results in a combined corpus ofroughly 1M data points (Table 1).Code to replicate our datasets—and to apply this method to other sources—isavailable online333http://www.github.com/deepmind/rc-data/.Note that the focus of this paper is to provide a corpus for evaluating a model’sability to read and comprehend a single document, not world knowledge orco-occurrence. To understand that distinction consider for instance thefollowing Cloze form queries (created from headlines in the Daily Mailvalidation set):a) The hi-tech bra that helps you beat breast X;b) Could Saccharin help beat X ?;c) Can fish oils help fight prostate X ?An ngram language model trained on the Daily Mail would easily correctly predictthat (X = cancer), regardless of the contents of the contextdocument, simply because this is a very frequently cured entity in the Daily Mailcorpus.				
1039	paper_50	How is the Uniform Reader model different from the base LSTM model?	Beyond some information that the uniform reader has poor performance, the paper does not explicitly define what this is.	The poor results of the Uniform Reader support our hypothesis ofthe significance of the attention mechanism in the Attentive model’sperformance as the only difference between these models is that the attentionvariables are ignored in the Uniform Reader. The precision@recall statistics inFigure 2 again highlight the strength of the attentive approach.				
1040	paper_51	What is a BLEU score?	BLUE score measures the precision of n-grams between generated sentences and reference sentences, which has been shown to correlate well with human evaluation.	The rest of the metrics can be computed automatically assuming one has access togroundtruth, i.e. human generated descriptions. The most commonly used metricso far in the image description literature has been theBLEU score [25],which is a form of precision of word n-grams between generated and referencesentences 222In this literature, most previous work report BLEU-1, i.e., they only compute precision at the unigram level, whereas BLEU-n is a geometric average of precision over 1- to n-grams..Even though this metric has some obvious drawbacks, it has been shown to correlatewell with human evaluations. In this work, we corroborate this as well, aswe show in Section 4.3. An extensive evaluation protocol, as wellas the generated outputs of our system, can be found at \urlhttp://nic.droppages.com/.				
1041	paper_51	What is batch normalization?	While P0 shows that the authors using batch normalization, it does not contains the definition.	For the representation of images, we use a Convolutional Neural Network(CNN). They have been widely used and studied for image tasks, and arecurrently state-of-the art for object recognition and detection. Our particularchoice of CNN uses a novel approach to batch normalization and yields thecurrent best performance on the ILSVRC 2014 classificationcompetition [12]. Furthermore, they have been shown togeneralize to other tasks such as scene classification by means oftransfer learning [4]. The words are represented with an embeddingmodel.				
1045	paper_51	Based on the authors' definition of the loss function used during training, will data points that contain longer sentences be likelier to have higher absolute value of loss and if so, why?	Since the loss function is defined as a sum of the negative log likelihood and not averaged, the value tends to be increased in proportion to the length of S (N in eq.  (13)).  Therefore, data points that contain longer sentences would be likely to have higher absolute value of loss.	Our loss is the sum of the negative log likelihood of the correct word at each step as follows:L(I,S)=-\sum_{t=1}^{N}\log p_{t}(S_{t})\;.(13)The above loss is minimized w.r.t. all the parameters of the LSTM, the top layer of theimage embedder CNN and word embeddings W_{e}.				
1046	paper_51	How is the BLEU-1 score different from the BLEU-4 score?	While BLEU-4 compute precision at the 4-gram level, BLEU-1 compute precision at the unigram (1-gram) level.	The rest of the metrics can be computed automatically assuming one has access togroundtruth, i.e. human generated descriptions. The most commonly used metricso far in the image description literature has been theBLEU score [25],which is a form of precision of word n-grams between generated and referencesentences 222In this literature, most previous work report BLEU-1, i.e., they only compute precision at the unigram level, whereas BLEU-n is a geometric average of precision over 1- to n-grams..Even though this metric has some obvious drawbacks, it has been shown to correlatewell with human evaluations. In this work, we corroborate this as well, aswe show in Section 4.3. An extensive evaluation protocol, as wellas the generated outputs of our system, can be found at \urlhttp://nic.droppages.com/.				
1047	paper_51	Can the architecture the authors' proposed be replaced with newer model architectures such as attention-based models or transformers, or is their task incompatible with these newer architectures?	Since the proposed method use RNN architecture for sequence modeling and not utilizing RNN-specific structures, other newer model like attention-based models or transformers also can be used instead of RNN.	In this work we combine deepconvolutional nets for image classification [12] withrecurrent networks for sequence modeling[10], to create a single networkthat generates descriptions of images. The RNN is trained in the context ofthis single “end-to-end” network. The model is inspiredby recent successes of sequence generation in machine translation[3, 2, 30], withthe difference that instead of starting with a sentence, we provide an imageprocessed by a convolutional net. The closest works are by Kiros et al. [15] whouse a neural net, but a feedforward one, to predict the next word given the imageand previous words. A recent work by Mao et al. [21] uses a recurrentNN for the same prediction task. This is very similar to the present proposal butthere are a number of important differences: we use a more powerful RNN model,and provide the visual input to the RNN model directly, which makes it possiblefor the RNN to keep track of the objects that have been explained by the text. Asa result of these seemingly insignificant differences, our system achievessubstantially better results on the established benchmarks. Lastly, Kiros et al. [14]propose to construct a joint multimodal embedding space by using a powerfulcomputer vision model and an LSTM that encodes text. In contrast to our approach,they use two separate pathways (one for images, one for text) to define a joint embedding,and, even though they can generate text, their approach is highly tuned for ranking.				
1048	paper_51	Based on the statement that the authors used a beam size of 20 during inference, how many total sentences would be generated till timestep t=10?	Since BeamSearch always keep only the resulting best k (=beam size) candidates in every time step, 20 sentences would be generated till timestep t=10.	There are multiple approaches that can be used to generate a sentence givenan image, with NIC. The first one is Sampling where we justsample the first word according to p_{1}, then provide the correspondingembedding as input and sample p_{2}, continuing like this until we sample thespecial end-of-sentence token or some maximum length.The second one is BeamSearch: iterativelyconsider the set of the k best sentences up to timet as candidates to generate sentences of size t+1, and keep only theresulting best k of them. This better approximatesS=\arg\max_{S^{\prime}}p(S^{\prime}|I).We used the BeamSearch approach in the following experiments, with abeam of size 20. Using a beam size of 1 (i.e., greedy search) did degrade ourresults by 2 BLEU points on average.				
1050	paper_52	The authors mention that their framework, MXNet, uses "lazy evaluation". Define lazy evaluation.	Lazy evalution is the way of evalution of data like NDArray, which the actual data push and pull are scheduled by the backend engine so that the data dependency can be correctly resolved.	The above is as efficient as the implementation using a single but often much more complex symbolic expression.The reason is that MXNet uses lazy evaluation of NDArray and the backendengine can correctly resolve the data dependency between the two.The above mixed implementation has the same performance comparing to a singledeclarative program, because the actual data push and pull are executed by lazyevaluation, which are scheduled by the backend engine just like others.				
1051	paper_52	Can collective communication primitives such as all_reduce or all_gather be implemented using MXNet?	Since MXNet provides distributed key-value store mechanism and user-defined updater logics, it is likely to be able to implement collective communication primitives using MXNet.	Paragraph 10				
1052	paper_52	Why do the authors use heuristics to estimate a variable's life span in the computational graph, instead of calculating it exactly?	While calculating the variable's estimated life span costs quadratic time complexity, heuristics costs only linear time complexity which is much efficient.  Also, experimental results show that heuristics can reduce the memory footprint effectively.	Memory Allocation.Note that each variable’s life time, namely the period between thecreation and the last time will be used, is known for a computation graph.So we can reuse memory for non-intersected variables.However, an ideal allocation strategy requires O(n^{2}) timecomplexity, where n is the number of variables.We proposed two heuristics strategies with linear time complexity.The first, called inplace, simulates the procedure of traversingthe graph, and keeps a reference counter of depended nodes that are not used sofar. If the counter reaches zero, the memory is recycled.The second, named co-share, allows two nodes to share a piece of memory if only ifthey cannot be run in parallel. Exploring co-share imposes one additionaldependency constraint.In particular, each time upon scheduling, among the pending paths in the graph, we find the longest path and perform needed memory allocations.Figure 7 shows the memory usages of the internal variables excepts for the outputs.As can be seen, both “inplace” and “co-share” can effective reduce the memoryfootprint. Combing them leads to a 2x reduction for all networks during modeltraining, and further improves to 4x for model prediction. For instance,even for the most expensive VGG net, training needs less than 16MB extra.				
1053	paper_52	In the reference counter approach for managed allocated memory, is it possible that an unused variable is not cleaned because of circular dependencies?	Although the paper mentions that the reference counter is used to traversing the computation graph, it does not contain the detail algorithm or not working cases.	We proposed two heuristics strategies with linear time complexity.The first, called inplace, simulates the procedure of traversingthe graph, and keeps a reference counter of depended nodes that are not used sofar. If the counter reaches zero, the memory is recycled.The second, named co-share, allows two nodes to share a piece of memory if only ifthey cannot be run in parallel. Exploring co-share imposes one additionaldependency constraint.In particular, each time upon scheduling, among the pending paths in the graph, we find the longest path and perform needed memory allocations.				
1056	paper_52	Does the paper report empirical benchmarks for performance on non-GPU devices (eg. edge devices such as mobile phones)?	While the paper reports the experimental result on GPU device (Nvidia GTX980 card), the result on non-GPU devices is not included in the paper.	We fist compare MXNet with Torch7, Caffe, and TensorFlow on the popular“convnet-benchmarks” [2]. All these systems are compiled with CUDA 7.5 andCUDNN 3 except for TensorFlow, which only supports CUDA 7.0 and CUDNN 2. We usebatch size 32 for all networks and run the experiments on a single Nvidia GTX980 card. Results are shown in Figure 7. As expected thatMXNet has similar performance comparing to Torch7 and Caffe, because mostcomputations are spent on the CUDA/CUDNN kernels. TensorFlow is always 2xslower, which might be due its use of a lower CUDNN version.				
1057	paper_53	What metrics are used to measure performance on segmentation tasks?	mIOU is used to measure performance on segmentation tasks.	In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3 [39] for the task of mobile semantic segmentation.DeepLabv3 adopts atrous convolution [40, 41, 42], a powerful tool to explicitly control the resolution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [43] containing three 3\times 3 convolutions with different atrous rates, (b) 1\times 1 convolution head, and (c) Image-level features [44].We denote by output_stride the ratio of input image spatial resolution to final output resolution, which is controlled by applying the atrous convolution properly.For semantic segmentation, we usually employ \emph{output\_stride}=16 or 8 for denser feature maps.We conduct the experiments on the PASCAL VOC 2012 dataset [3], with extra annotated images from [45] and evaluation metric mIOU.				
1058	paper_53	Define how a linear convolution layer functions.	Linear convolution layer projects the filtered high-dimensional representation to low-dimensional subspace.	Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution. The official implementation is available as part of TensorFlow-Slim model library in [4].				
1059	paper_53	Do the authors evaluate their architecture on non-mobile/cellphone type of edge devices such as FPGAs?	The authors only evaluated their architecture on mobile devices (Google Pixel 1) and did not evaluated on non-mobile type of devices.	Table 4: Performance on ImageNet, comparison for different networks. As is common practice for ops, we count the total number of Multiply-Adds. In the last column we report running time in milliseconds (ms) for a single large core of the Google Pixel 1 phone (using TF-Lite). We do not report ShuffleNet numbers as efficient group convolutions and shuffling are not yet supported.Table 6: Performance comparison of MobileNetV2 + SSDLite and other realtime detectors on the COCO dataset object detection task. MobileNetV2 + SSDLite achieves competitive accuracy with significantly fewer parameters and smaller computational complexity. All models are trained on trainval35k and evaluated on test-dev. SSD/YOLOv2 numbers are from [35]. The running time is reported for the large core of the Google Pixel 1 phone, using an internal version of the TF-Lite engine.				
1060	paper_53	Given an input tensor of size (224, 224, 16), a convolution layer transforms the input to an output tensor of size (224, 224, 8), what would the computational cost of this operation  be according to this paper?	with hi = 224, wi = 224, di = 16 and dj = 8, the computation cost of this operation would be 224 × 224 × 16 × 8 × k × k = 6,422,528 × k^2.	Standard convolution takes an h_{i}\times w_{i}\times d_{i} input tensor L_{i}, and applies convolutional kernel K\in{\cal R}^{k\times k\times d_{i}\times d_{j}} to produce an h_{i}\times w_{i}\times d_{j} output tensor L_{j}.Standard convolutional layers have the computational cost of h_{i}\cdot w_{i}\cdot d_{i}\cdot d_{j}\cdot k\cdot k.				
1061	paper_53	What differentiates a bottleneck block from a residual block?	Inverted residuals differentiates a bottleneck block from a residual block, which shortcuts directly between the bottlenecks.	The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8].However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.Figure 3 provides a schematic visualization of the difference in the designs.The motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers.However, the inverted design is considerably more memory efficient (see Section 4 for details), as well as works slightly better in our experiments.				
1062	paper_53	What is the key difference in model structure between Mobilenet style models and Shufflenet?	ShuffleNet introduces group convolutions and shuffling, while existing mobilenet style models do not have.	Figure 4: Comparison of convolutional blocks for different architectures. ShuffleNet uses Group Convolutions [20] and shuffling, it also uses conventional residual approach where inner blocks are narrower than output. ShuffleNet and NasNet illustrations are from respective papers.				
1063	paper_53	When discussing the information flow interpretation, the authors mention how expressiveness and capacity of their model can be independently analyse. What does "capacity" in this context mean?	In this paper, capacity is the input/output domain of the bottleneck layers, which can be separated from the expressiveness part (layer transformation) for the proposed architecture.	One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation – that is a non-linear function that converts input to the output.The former can be seen as the capacity of the network at each layer, whereas the latter as the expressiveness.This is in contrast with traditional convolutional blocks, both regular and separable, where both expressiveness and capacity are tangled together and are functions of the output layer depth.				
1064	paper_53	How does RELU6 differ from vanilla RELU?	RELU6 is more robust compared to vanilla RELU when used with low-precision computation.	Now we describe our architecture in detail. As discussed in the previous section thebasic building block is a bottleneck depth-separable convolution with residuals.The detailed structureof this block is shown in Table 1. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers described in the Table 2. We use {\operatorname{\mathop{ReLU6}\,}} as the non-linearity because of its robustness when used with low-precision computation [27]. We always use kernel size 3\times 3 as is standard for modern networks, and utilize dropout and batch normalization during training.				
1065	paper_53	The authors mention that their primary network has a compute cost of 300 million multiply-adds. By how many orders of magnitude would this compute cost increase if the authors did not use bottleneck layers?	The compute cost when using traditional layers are h · w · d' · d'' · k^2, so the cost would be increase by a factor of d'' · k^2/t(d' + k^2 + d'').	The basic implementation structure is illustrated in Table 1.For a block of size h\times w, expansion factor t and kernel size k with d^{\prime} input channels and d^{\prime\prime} output channels, the total number of multiply add required ish\cdot w\cdot d^{\prime}\cdot t(d^{\prime}+k^{2}+d^{\prime\prime}).Compared with (1) this expression has an extra term, as indeed we have an extra 1\times 1 convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions.In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet.				
1066	paper_53	Do the authors measure the quantify the impact on their model's performance when using RELU6 instead of RELU?	While the authors showed the effect of inverted residual connections and linear bottlenecks, they did not measure the impact of using RELU6 instead of RELU in the ablation study.	The importance of residual connection has been studied extensively [8, 30, 46]. The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers (see Figure 6b for comparison).The linear bottleneck models are strictly less powerful than models with non-linearities, because the activations can always operate in linear regime with appropriate changes to biases and scaling. However our experiments shown in Figure 6a indicate that linear bottlenecks improve performance, providing support that non-linearity destroys information in low-dimensional space.				
1068	paper_53	Why is running information on resource-constrained environments an important concern when GPUs are becoming more commonplace today?	Running information on resource-constrained environments is important concern, because many existing state-of-the-art models require high computational resources that makes them not usable in mobile and embedded applications.	Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded applications.				
1069	paper_54	What hyperparameters do each of the eight variants of LSTMs investigated by the authors of this paper have?	The authors investigated 1) number of LSTM blocks per hidden layer 2) learning rate 3) momentum 4) standard deviation of Gaussian input noise with random searches with uniform sampling.	We performed 27 random searches (one for each combination of the nine variants and three datasets).Each random search encompasses 200 trials for a total of 5400 trials of randomly sampling the following hyperparameters:•number of LSTM blocks per hidden layer:log-uniform samples from [20,200];•learning rate:log-uniform samples from [10^{-6},10^{-2}];•momentum:1-\text{log-uniform samples from $[0.01,1.0]$};•standard deviation of Gaussian input noise:uniform samples from [0,1].	The authors investigated 1) number of LSTM blocks per hidden layer 2) learning rate 3) momentum 4) standard deviation of Gaussian input commotion with random searches with uniform sampling.	Tortured phrases	Gaussian input noise -> Gaussian input commotion	
1073	paper_54	The results section concludes that momentum affected neither the training compute cost nor the performance - why was this a surprising or unexpected result?	It is not clear why was the result an unexpected result since there is no evidential information of what the authors expected when choosing the hyperparameters to assess their importance.	One unexpected result of this study is that momentum affects neither performance nor training time in any significant way.This follows from the observation that for none of the datasets, momentum accounted for more than 1% of the variance of test set performance.It should be noted that for TIMIT the interaction between learning rate and momentum accounts for 2.5% of the total variance, but as with learning rate \times hidden size (cf. Interaction of Hyperparameters below) it does not reveal any interpretable structure.This may be the result of our choice to scale learning rates dependent on momentum (Section IV-B).These observations suggest that momentum does not offer substantial benefits when training LSTMs with online stochastic gradient descent.As expected, the learning rate is the most crucial hyperparameter, followed by the network size.Surprisingly though, the use of momentum was found to be unimportant in our setting of online gradient descent.Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for the other datasets.				
1074	paper_54	How did the authors ensure that fair comparison between the 9 variants of LSTMs they analysed?	To ensure the fair comparison that needs to be similar for each variant, the authors tuned the hyperparameters individually for each variant, and use random search to 1) obtain good hyperparameters and 2) collect enough amount of samples for analyzing the general effect of each variant.	For fair comparison, the setup needs to be similar for each variant.Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant.For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant.Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters [18] for every combination of variant and dataset.Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section V-B).	To ensure the fair comparison that needs to be similar for each variant, the authors didn't tune the hyperparameters individually for each variant, but used random search to 1) obtain good hyperparameters and 2) collect enough amount of samples for analyzing the general effect of each variant.	Opposite		
1076	paper_54	Do the authors use different ratios of test-train-validation split for each dataset?	The authors use different ratios of test-train-validation split for each dataset.  Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the three datasets (TIMIT Speech corpus, IAM Online Handwriting Database, and JSB Chorales dataset) used in the experiment.  Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset.  (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192).	The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696, 400, and 192 sequences, having 304 frames on average.The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.2 From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.The IAM Online Handwriting Database [38] consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see Figure 2(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5355, 2956 and 3859 sequences respectively.JSB Chorales: JSB Chorales is a collection of 382 four part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger, Lewandowski et al. [41]. 5 These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61.	The authors use different ratios of test-train-validation split for each dataset.  Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the five datasets (TIMIT Speech corpus, IAM Online Handwriting Database, IMDB movie-review dataset, MNLI dataset and JSB Chorales dataset) used in the experiment.  Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset.  (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192).	Invent something didn't mentioned		
1077	paper_54	Define how fANOVA is performed?	fANOVA marginalize over hyperparameter dimensions using regression trees to predict the marginal error for single parameter while averaging over all other parameters.	The fANOVA framework for assessing hyperparameter importance by Hutter et al. [19] is based on the observation that marginalizing over dimensions can be done efficiently in regression trees.This allows predicting the marginal error for one hyperparameter while averaging over all the others.Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.	fANOVA marginalize over hyperparameter dimensions using regression trees to predict the marginal error for multi parameters while averaging over all other parameters.	Change concept	single parameter -> multi parameters	
1078	paper_55	The authors say, "a very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions". What is this technique called?	The authors define an “ensemble of models” as a set of separate models with the same architecture and training procedure, but different randomly initialized parameters whose predictions are then averaged to increase performance.	We trained 10 separate models to predict P(h_{t}|\mathbf{s}_{t};\boldsymbol{\theta}), using exactly the same architecture and trainingprocedure as the baseline. The models are randomly initialized with different initial parameter values and we find thatthis creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble tosignificantly outperform the individual models. We have explored adding diversity to the models by varying the sets ofdata that each model sees, but we found this to not significantly change our results, so we opted for the simplerapproach. For the distillation we tried temperatures of [1,{\bf 2},5,10] and used a relative weight of 0.5 on thecross-entropy for the hard targets, where bold font indicates the best value that was used fortable 1 .	The authors define an “ensemble of models” as a set of separate models with different architecture and training procedure, and different randomly initialized parameters whose predictions are then averaged to increase performance.	Change concept	the same architecture... -> different architecture...	
1079	paper_55	The authors proposed approach only works for classification models, and not for models that have other types of outputs. True or False?	In this work, the approach assumes that there are classes that the models should be able to predict.  The work focuses on classification models.	In this section we give an example of such a dataset and we show howlearning specialist models that each focus on a different confusablesubset of the classes can reduce the total amount of computationrequired to learn an ensemble. The main problem with specialists thatfocus on making fine-grained distinctions is that they overfit veryeasily and we describe how this overfitting may be prevented by usingsoft targets.In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes S m that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-line version of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in Table 2). We tried several clustering algorithms which produced similar results.				
1080	paper_55	What is the numeric value of dropout fraction used during the training phase?	The neural net in this work was strongly regularized using dropout and weight constraints as described in [5].	To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations.				
1081	paper_55	What does "temperature" mean in context of the authors' proposed model?	Temperature is a value used in the softmax output layer.  The softmax layer converts the logit computed for each class into a probability by comparing with other logits and increasing the temperature produces a softer probability distribution over classes.  Specifically, in distillation, a high temperature is used in the cumbersome model to produce a soft target distribution for each case in the transfer set.	In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer setand using a soft target distribution for each case in the transfer set that is produced by using the cumbersome modelwith a high temperature in its softmax. The same high temperature is used when training the distilled model, but afterit has been trained it uses a temperature of 1.Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi , computed for each class into a probability, qi , by comparing zi with the other logits. qi = exp(zi/T ) P j exp(zj/T ) (1) 2 where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes.	Temperature is a value used in the softmax yield layer.  The softmax layer converts the logit computed for each class into a probability by comparing with other logits and increasing the temperature produces a softer probability distribution over classes.  Specifically, in distillation, a high temperature is used in the cumbersome model to produce a soft target distribution for each case in the transfer set.	Tortured phrases	output layer -> yield layer	
1082	paper_55	The authors use a different number of layers and rectified units for experiments on MNIST and speech recognition. What factors might the authors have considered while deciding on these numbers?	P0: According to the authors, they used about 200 hours of spoken English data which yielded about 650M training examples.  Approximately, the length of the average training sample would be 0. 01 seconds.	To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations.We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	For speech acknowledgement, the architecture was based on the acoustic model used by Android voice search.  For MNIST, the architecture was strongly regularized using dropout and weight constraints as described in prior work.	Tortured phrases	speech recognization -> speech acknowledgment	For speech recognition, the architecture was based on the acoustic model used by Android voice search.  For MNIST, the architecture was strongly regularized using dropout and weight constraints as described in prior work.
1083	paper_55	For the speech recognition task, based on the information provided by the authors on the total number of samples in the dataset, how long (in seconds) is each training sample?	P0: According to the authors, they used about 2000 hours of spoken English data which yielded about 700M training examples.  Approximately, the length of the average training sample would be 0. 01 seconds.	We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	P0: According to the authors, they used about 2000 hours of spoken English data which yielded about 700M training examples.  Approximately, the length of the average training sample would be 0. 01 seconds.	Change number		
1084	paper_55	What is the main difference between synchronous vs asynchronous SGD?	In training of Google’s baseline model of JFT, asynchronous stochastic gradient descent (SGD) involved running replicas of the neural net different sets of cores to compute gradients on given mini-batches, which are then sent to a shared parameter server which returns new values for the parameters.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.	In training of Google’s baseline model of JFT, asynchronous stochastic slope plummet involved running replicas of the neural net different sets of cores to compute gradients on given mini-batches, which are then sent to a shared parameter server which returns new values for the parameters.	Tortured phrases	stochastic gradient descent -> stochastic slope plummet	
1085	paper_55	What optimizer did the authors used for the distilled models?	The model was trained with a distributed stochastic gradient descent approach.	Although it is possible (and desirable) to train the DNN in such a way that the decoder (and, thus, the language model) is taken into account by marginalizing over all possible paths, it is common to train the DNN to perform frame-by-frame classification by (locally) minimizing the cross entropy between the predictions made by the net and the labels given by a forced alignment with the ground truth sequence of states for each observation: θ = arg max θ ′ P(ht|st; θ ′ ) where θ are the parameters of our acoustic model P which maps acoustic observations at time t, st, to a probability, P(ht|st; θ ′ ) , of the “correct” HMM state ht, which is determined by a forced alignment with the correct sequence of words. The model is trained with a distributed stochastic gradient descent approach.				
1086	paper_55	Would more recent approaches such as DECAF extreme classification (2021) serve as a stronger baseline than the specialized models discussed in the paper?	The specialist models were started from the baseline model which was Google’s deep convolutional network for JFT.	Starting from the trained baseline full network,the specialists train extremely fast (a few days instead of many weeks for JFT). Also, all the specialistsare trained completely independently. Table  3 shows the absolute test accuracy for thebaseline system and the baseline system combined with the specialistmodels. With 61 specialist models, there is a4.4% relative improvement in test accuracy overall. We also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes.JFT is an internal Google dataset that has 100 million labeled images with 15,000 labels. When we did this work, Google’s baseline model for JFT was a deep convolutional neural network [7] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores. This training used two types of parallelism [2]. First, there were many replicas of the neural net running on different sets of cores and processing different mini-batches from the training set. Each replica computes the average gradient on its current mini-batch and sends this gradient to a sharded parameter server which sends back new values for the parameters. These new values reflect all of the gradients received by the parameter server since the last time it sent parameters to the replica. Second, each replica is spread over multiple cores by putting different subsets of the neurons on each core. Ensemble training is yet a third type of parallelism that can be wrapped around the other two types, but only if a lot more cores are available. Waiting for several years to train an ensemble of models was not an option, so we needed a much faster way to improve the baseline model.				
1087	paper_55	Is the KMeans algorithm discussed in the paper require a labelled dataset?	The K-means algorithm clusters the set of classes that the models often predict together.  In this work, this clustering approach did not require true labels.  However, the models themselves were trained using examples from a dataset, JFT, which contains labeled images.  Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes S^{m} that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results.To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.	The K-means algorithm clusters the set of classes that the models often predict together.  In this work, this clustering approach did not require true labels.  However, the models themselves were trained using examples from a dataset, JFT, which contains only synthesized images.  Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.	Change concept	labeled images -> synthesized images	
1088	paper_55	Define KL divergence.	In this work, KL divergence is computed between the probability distribution of a specialist model or generalist full model and the full probability distribution over all classes.	Step 2: We then take all the specialist models, m, whose specialsubset of confusable classes,S^{m}, has a non-empty intersection with k and call this the activeset of specialists A_{k} (note that this set may be empty). We thenfind the full probability distribution \mathbf{q} over all the classesthat minimizes:KL(\mathbf{p}^{g},\mathbf{q})+\sum_{m\in A_{k}}KL(\mathbf{p}^{m},\mathbf{q})(5)where KL denotes the KL divergence, and \mathbf{p}^{m} \mathbf{p}^{g} denote theprobability distribution of a specialist model or the generalist fullmodel. The distribution \mathbf{p}^{m} is a distribution over all thespecialist classes of m plus a single dustbin class, so whencomputing its KL divergence from the full \mathbf{q} distribution we sumall of the probabilities that the full \mathbf{q} distribution assigns toall the classes in m’s dustbin.				
1089	paper_55	When discussing the JFT specialist training, the authors refer to a "dustbin" class. Give an example of a sample that might be assigned to this class.	The dustbin class was the combination of all of the classes that a specialist does not care about.  Examples of samples that might be assigned to the dustbin class cannot be provided from this paper.	When the number of classes is very large, it makes sense for thecumbersome model to be an ensemble that contains one generalistmodel trained on all the data and many “specialist”models, each of which is trained on data that is highly enriched inexamples from a very confusable subset of the classes (like differenttypes of mushroom). The softmax of this type of specialist can be mademuch smaller by combining all of the classes it does not care about into asingle dustbin class.The specialists that we used in our experiments on the JFT datasetcollapsed all of their non-specialist classes into a single dustbinclass. If we allow specialists to have a full softmax over allclasses, there may be a much better way to prevent them overfitting than usingearly stopping. A specialist is trained on data that is highlyenriched in its special classes. This means that the effective sizeof its training set is much smaller and it has a strong tendency tooverfit on its special classes. This problem cannot be solved bymaking the specialist a lot smaller because then we lose the veryhelpful transfer effects we get from modeling all of thenon-specialist classes.				
1090	paper_55	What factors could the authors have used while deciding the number of specialists to allocate for their task?	P0: Through results shown in Table 4, the authors saw a general trend that accuracy improved when more specialists covered a particular class.  This could have been a factor that authors considered in deciding on the number of specialists for their task.	For our JFT specialist experiments, we trained 61 specialist models, each with 300 classes (plus the dustbin class).Because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering aparticular image class. Table  4 shows the number of test set examples, the change inthe number of examples correct at position 1 when using the specialist(s), and the relative percentage improvement intop1 accuracy for the JFT dataset broken down by the number of specialists covering the class. We are encouraged by thegeneral trend that accuracy improvements are larger when we have more specialists covering a particular class, sincetraining independent specialist models is very easy to parallelize.				
1091	paper_55	How are mixture of expert gating functions designed?	In a mixture of experts, the gating function is a network that is learned to choose which experts to assign to each example through the relative discriminative performance of the experts on the sample.	The use of specialists that are trained on subsets of the data hassome resemblance to mixtures of experts [6] which use agating network to compute the probability of assigning each example toeach expert. At the same time as the experts are learning to deal withthe examples assigned to them, the gating network is learning tochoose which experts to assign each example to based on the relativediscriminative performance of the experts for that example. Using the discriminativeperformance of the experts to determine the learned assignments is muchbetter than simply clustering the input vectors and assigning anexpert to each cluster, but it makes the training hard to parallelize: First, theweighted training set for each expert keeps changing in a way thatdepends on all the other experts and second, the gating network needsto compare the performance of different experts on the same example toknow how to revise its assignment probabilities. These difficultieshave meant that mixtures of experts are rarely used in the regimewhere they might be most beneficial: tasks with huge datasets thatcontain distinctly different subsets.				
1092	paper_56	Why did the authors use a different set of phonemes for decoding and scoring?	However, since the authors follow the settings from existing works so this could be found in those references.	We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set.				
1093	paper_56	What considerations would authors need to take to extend this model to languages that have more phonemes than American English? (eg. Indian languages, Chinese, etc)	When authors need to take to extend this model to other languages or other datasets than TIMIT corpus, they should extend the phonemes set for some phonemes that are not included in American English.  This applies to all procedures including training/decoding/scoring.	We closely followed the procedure in [16]. All experiments were performed on the TIMIT corpus [19]. We used the train-dev-tests plit from the Kaldi [20] TIMIT s5 recipe. We trained on the standard 462 speaker set with all SA utterances removed and used the 50 speaker dev set for early stopping. We tested on the 24 speaker core test set. All networks were trained on 40 mel-scale filter-bank features together with the energy in each frame, and first and second temporal differences, yielding in total 123 features per frame. Each feature was rescaled to have zero mean and unit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to each target sequence. Similarly, we appended an all-zero frame at the end of each input sequence to indicate the end of the utterance. Decoding was performed using the 61+1 phoneme set, while scoring was done on the 39 phoneme set.				
1096	paper_56	The authors restrict batch size to 1 during training. Why did the authors do this, and what problems have might been encountered with higher batch size?	While the authors argue that proper regularization is required due to the small dataset size, the relation between regularization and batch size is not clear in this paper.	As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer [22]. We firsttrained our models with a column norm constraint [23] with themaximum norm 1until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \epsilon and \rho are set to 10^{-8} and 0.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost L_{C} by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \epsilon=10^{-10}, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining.				
1097	paper_56	Did authors perform a hyperparameter search before deciding the values of batch size, epsilon and L_C during training?	While the paper shows what value the author set for those hyperparameters, there is no evidential information about the hyperparameter search in this paper.	As TIMIT is a relatively small dataset, proper regularization is crucial. We usedthe adaptive weight noise as a main regularizer [22]. We firsttrained our models with a column norm constraint [23] with themaximum norm 1until the lowest development negative log-likelihood is achieved.333Applying the weight noise from the beginning of training caused severeunderfitting.During this time, \epsilon and \rho are set to 10^{-8} and 0.95,respectively. At this point, we began using the adaptive weight noise, andscaled down the model complexity cost L_{C} by a factor of 10, while disablingthe column norm constraints. Oncethe new lowest development log-likelihood was reached, we fine-tuned the modelwith a smaller \epsilon=10^{-10}, until we didnot observe the improvement in the development phoneme error rate (PER) for 100Kweight updates. Batch size 1 was used throughout thetraining.				
1098	paper_56	How did the performance of the proposed model differ on shorter vs longer audio sequences?	While the proposed model showed good performance on shorter audio sequences (~200 phones), it failed to align most of phones on longer audio sequences.	The first column of Fig. 4 shows the number ofcorrectly aligned frames w.r.t. the utterance length (in frames) for some of theconsidered models. One can see that the baseline model was able to decodesequences up to about 120 phones when a single utterance was repeated, and up toabout 150 phones when different utterances were concatenated. Even when itfailed, it correctly aligned about 50 phones. On the other hand, the model withthe hybrid attention mechanism with convolutional features was able to alignsequences up to 200 phones long. However, once it began to fail, the model wasnot able to align almost all phones. The model with the smoothing behavedsimilarly to the one with convolutional features only.				
1099	paper_56	The authors mention that their baseline model was able to decode sequences upto 120 "phones" long, when processing an audio segment with repeated sounds or utterances. What does "phones" mean, in this context?	While the paper does not include the definition of the term "phone", it is used as the unit of audio sequence, according to the experimental setup. 	We closely followed the procedure in [16]. All experimentswere performed on the TIMIT corpus [19]. We used the train-dev-testsplit from the Kaldi [20] TIMIT s5 recipe. We trained on thestandard 462 speaker set with all SA utterances removed and used the 50 speakerdev set for early stopping. We tested on the 24 speaker core test set. Allnetworks were trained on 40 mel-scale filter-bank features together with theenergy in each frame, and first and second temporal differences, yielding intotal 123 features per frame. Each feature was rescaled to have zero mean andunit variance over the training set. Networks were trained on the full 61-phoneset extended with an extra “end-of-sequence” token that was appended to eachtarget sequence. Similarly, we appended an all-zero frame at the end of eachinput sequence to indicate the end of the utterance. Decoding was performedusing the 61+1 phoneme set, while scoring was done on the 39 phoneme set.				
1100	paper_56	Do the authors use any datasets other than TIMIT to gauge the generalizability of their model?	The authors used no dataset other than TIMIT for the evaluation of the model.	In this paper, we evaluate attention-based models on aphoneme recognition task using the widely-used TIMITdataset. At each time step in generating an output sequence (phonemes),an attention mechanism selects or weighs the signals producedby a trained feature extraction mechanism at potentially all of the time stepsin the input sequence (speech frames). The weighted feature vector thenhelps to condition the generation of the next element of the output sequence.Since the utterances in this dataset are rathershort (mostly under 5 seconds), we measure theability of the considered models in recognizing much longerutterances which were created by artificially concatenatingthe existing utterances.				
1102	paper_56	What are ASRGs?	ARSG is the sequence generator based on the RNN network, which utilizes the attention mechanism.	An attention-based recurrent sequence generator (ARSG) is a recurrent neuralnetwork that stochastically generates an output sequence (y_{1},\dots,y_{T})from an input x. In practice, x is often processed by an encoderwhich outputs a sequential input representation h=(h_{1},\ldots,h_{L}) moresuitable for the attention mechanism to work with.	ARSG is the sequence generator based on the RNN network, which utilizes the attention mechanism.	Tortured phrases	attention mechanism -> consideration instrument	
1103	paper_56	What is CTC-training?	According to the related work section in the paper, CTC training is the deep-learning-based speech recognization model which performs MAP inference on the alignment as a latent random variable.  There is no detailed information on how CTC training works in this paper and presumed to exist in [13].	Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer [14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess [15]. They have been shown to perform well on thephoneme recognition task [16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation [17].The considered ARSG is different from both the CTC and RNN Transducer in twoways. First, whereas the attention mechanism deterministically aligns the inputand the output sequences, the CTC and RNN Transducer treat the alignment as alatent random variable over which MAP (maximum a posteriori) inference isperformed. This deterministic nature of the ARSG’s alignment mechanism allowsbeam search procedure to be simpler. Furthermore, we empirically observe that amuch smaller beam width can be used with the deterministic mechanism, whichallows faster decoding (see Sec. 4.2 andFig. 2).Second, the alignment mechanism of both the CTC and RNN Transducer isconstrained to be “monotonic” to keep marginalization of the alignmenttractable. On the other hand, the proposed attention mechanism can result innon-monotonic alignment, which makes it suitable for a larger variety of tasksother than speech recognition.				
1104	paper_56	Similar to some of the related work, do the authors also use CTC-training for their proposed model?	The authors used attention-based recurrent sequence generator (ARSG) in this paper, which is different from CTC.  While CTC based models are close to the proposed ARSG model, it has different characteristics from those models such as how the alignment is treated and whether the model can result in non-monotonic alignment.	Speech recognizers based on the connectionist temporal classification (CTC,[13]) and its extension, RNNTransducer [14], are the closest to the ARSG modelconsidered in this paper. They follow earlier work on end-to-end trainable deeplearning over sequences with gradient signals flowing through the alignmentprocess [15]. They have been shown to perform well on thephoneme recognition task [16]. Furthermore, the CTC wasrecently found to be able to directly transcribe text from speech without anyintermediate phonetic representation [17].The considered ARSG is different from both the CTC and RNN Transducer in two ways. First,				
1106	paper_57	What point in StarGAN is valuable compared to Conditional GAN?	paper's model can process multiple different domains.  According to the parer, several different paragraph compares previous researches and their model.  However, there is no evidence that, that models are related to conditional GAN.	Conditional GANs. GAN-based conditional image generation has also been actively studied. Prior studies have provided both the discriminator and generator with class information in order to generate samples conditioned on the class  [20, 21, 22]. Other recent approaches focused on generating particular images highly relevant to a given text description  [25, 30]. The idea of conditional image generation has also been successfully applied to domain transfer [9, 28], super-resolution imaging[14], and photo editing [2, 27].In this paper, we propose a scalable GAN framework that can flexibly steer the image translation to various target domains, by providing conditional domain information.				
1107	paper_57	What term do they add in loss function to guarantee that generated images are not differentiable from real images?	Adversarial loss.	Adversarial Loss. To make the generated images indistinguishable from real images, we adopt an adversarial loss\begin{split}\mathcal{L}_{adv}=&\thinspace{\mathbb{E}}_{x}\left[\log{{D}_{src}(x)}\right]\>\>+\\&\thinspace{\mathbb{E}}_{x,c}[\log{(1-{D}_{src}(G(x,c)))}],\end{split}(1)where G generates an image G(x,c) conditioned on both the input image x and the target domain label c, while D tries to distinguish between real and fake images. In this paper, we refer to the term {D}_{src}(x) as a probability distribution over sources given by D. The generator G tries to minimize this objective, while the discriminator D tries to maximize it.Generative Adversarial Networks. Generative adversarial networks (GANs) [3] have shown remarkable results in various computer vision tasks such as image generation [6, 24, 32, 8], image translation [7, 9, 33], super-resolution imaging [14], and face image synthesis [10, 16, 26, 31]. A typical GAN model consists of two modules: a discriminator and a generator. The discriminator learns to distinguish between real and fake samples, while the generator learns to generate fake samples that are indistinguishable from real samples. Our approach also leverages the adversarial loss to make the generated images as realistic as possible.				
1108	paper_57	What value is assigned to unknown labels?	zeros.  According to paper, authors wrote that they simply assign zero values on unknown labels.	Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two.				
1109	paper_57	Model architecture of StarGAN is based on two other GAN models. What is it?	The CycleGAN and the PatchGAN.  According to the paper, StarGAN was basically generated from CycleGAN.  In addition, paper note that discriminator was made by PatchGAN.	Network Architecture.Adapted from CycleGAN [33], StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks [5], and two transposed convolutional layers with the stride size of two for upsampling. We use instance normalization [29] for the generator but no normalization for the discriminator. We leverage PatchGANs [7, 15, 33] for the discriminator network, which classifies whether local image patches are real or fake. See the appendix (Section 7.2) for more details about the network architecture.				
1110	paper_57	They perform only a qualitative analysis of the proposed model. Is it true?	False.  They provided not only qualitative analysis but also quantitive analysis for their model.	Qualitative evaluation. Fig. 4 shows the facial attribute transfer results on CelebA. We observed that our method provides a higher visual quality of translation results on test data compared to the cross-domain models. One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brown-to-blond hair), which is prone to overfitting, we train our model to flexibly translate images according to the labels of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of images with different facial attribute values.Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure’s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively.Quantitative results. Tables 1 and 2 show the results of our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of votes for best transferring attributes in all cases.In the case of gender changes in Table 1, the voting difference between our model and other models was marginal, e.g., 39.1% for StarGAN vs. 31.4% for DIAT. However, in multi-attribute changes, e.g., the ‘G+A’ case in Table 2, the performance difference becomes significant, e.g., 49.8% for StarGAN vs. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase.Qualitative evaluation. As seen in Fig. 5, StarGAN clearly generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness as seen in the input. IcGAN even fails to preserve the personal identity in the image by generating male images.Quantitative evaluation. For a quantitative evaluation, we compute the classification error of a facial expression on synthesized images. We trained a facial expression classifier on the RaFD dataset (90%/10% splitting for training and test sets) using a ResNet-18 architecture [5], resulting in a near-perfect accuracy of 99.55%. We then trained each of image translation models using the same training set and performed image translation on the same, unseen test set. Finally, we classified the expression of these translated images using the above-mentioned classifier. As can be seen in Table 3, our model achieves the lowest classification error, indicating that our model produces the most realistic facial expressions among all the methods compared.				
1111	paper_57	How many generators have to be trained if you train the cross-domain model for a 5-domain image translation task using the previous approach?	it takes 20 generators in order to do 5-domain image translation task.  According to the paper, for k-domain task, k(k-1) generators are necessary.	However, existing models are both inefficient and ineffective in such multi-domain image translation tasks. Their inefficiency results from the fact that in order to learn all mappings among k domains, k(k\mathbb{-}1) generators have to be trained. Fig. 2 (a) illustrates how twelve distinct generator networks have to be trained to translate images among four different domains. Meanwhile, they are ineffective that even though there exist global features that can be learned from images of all domains such as face shapes, each generator cannot fully utilize the entire training data and only can learn from two domains out of k. Failure to fully utilize training data is likely to limit the quality of generated images. Furthermore, they are incapable of jointly training domains from different datasets because each dataset is partially labeled, which we further discuss in Section 3.2.				
1112	paper_57	How many terms are used for the loss function of a generator?	it takes three.  According to the equation, we can think that G means generator.	Full Objective. Finally, the objective functions to optimize G and D are written, respectively, as\mathcal{L}_{D}=-\mathcal{L}_{adv}+{\lambda}_{cls}\thinspace\mathcal{L}_{cls}^{r},(5)\mathcal{L}_{G}=\mathcal{L}_{adv}+{\lambda}_{cls}\thinspace\mathcal{L}_{cls}^{f}+{\lambda}_{rec}\thinspace\mathcal{L}_{rec},(6)where {\lambda}_{cls} and {\lambda}_{rec} are hyper-parameters that control the relative importance of domain classification and reconstruction losses, respectively, compared to the adversarial loss. We use {\lambda}_{cls}=1 and {\lambda}_{rec}=10 in all of our experiments.				
1113	paper_57	What does “AMT” mean?	AMT means Amazon Mechanical Turk.	Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input image, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure’s original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively.				
1114	paper_57	What is an example attribute used in experiment of joint training?	We can't know from given paragraphs.	Several image datasets come with a number of labeled attributes. For instance, the CelebA[19] dataset contains 40 labels related to facial attributes such as hair color, gender, and age, and the RaFD [13] dataset has 8 labels for facial expressions such as ‘happy’, ‘angry’ and ‘sad’. These settings enable us to perform more interesting tasks, namely multi-domain image-to-image translation, where we change images according to attributes from multiple domains. The first five columns in Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation show how a CelebA image can be translated according to any of the four domains, ‘blond hair’, ‘gender’, ‘aged’, and ‘pale skin’. We can further extend to training multiple domains from different datasets, such as jointly training CelebA and RaFD images to change a CelebA image’s facial expression using features learned by training on RaFD, as in the rightmost columns of Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation. Effects of joint training. Fig. 6 shows qualitative comparisons between StarGAN-SNG and StarGAN-JNT, where the task is to synthesize facial expressions of images in CelebA. StarGAN-JNT exhibits emotional expressions with high visual quality, while StarGAN-SNG generates reasonable but blurry images with gray backgrounds. This difference is due to the fact that StarGAN-JNT learns to translate CelebA images during training but not StarGAN-SNG. In other words, StarGAN-JNT can leverage both datasets to improve shared low-level tasks such facial keypoint detection and segmentation. By utilizing both CelebA and RaFD, StarGAN-JNT can improve these low-level tasks, which is beneficial to learning facial expression synthesis.				
1115	paper_57	How do they train a single-generator to learn a variety in translating images?	By randomly generated domain label, model can practice multiple domain.	Our goal is to train a single generator G that learns mappings among multiple domains. To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, G(x,c)\rightarrow y.We randomly generate the target domain label c so that G learns to flexibly translate the input image. We also introduce an auxiliary classifier [22] that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D:x\rightarrow\{{D}_{src}(x),{D}_{cls}(x)\}. Fig. 3 illustrates the training process of our proposed approach.				
1116	paper_57	What problem can occur when you train a generator only with an adversarial loss and a domain classification loss?	When loss function does not contains reconstruction loss, minimizing the losses does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.	Reconstruction Loss. By minimizing the adversarial and classification losses, G is trained to generate images that are realistic and classified to its correct target domain. However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs. To alleviate this problem, we apply a cycle consistency loss [9, 33] to the generator, defined as\mathcal{L}_{rec}={\mathbb{E}}_{x,c,c^{\prime}}[{||x-G(G(x,c),c^{\prime})||}_{1}],(4)where G takes in the translated image G(x,c) and the original domain label c^{\prime} as input and tries to reconstruct the original image x. We adopt the L1 norm as our reconstruction loss. Note that we use a single generator twice, first to translate an original image into an image in the target domain and then to reconstruct the original image from the translated image.				
1117	paper_57	Why do they divide domain classification loss in two terms for real image and fake image, respectively?	to translate x into an output image y, which is properly classified to the target domain c, authors divided classification loss in two terms.	Domain Classification Loss. For a given input image x and a target domain label c, our goal is to translate x into an output image y, which is properly classified to the target domain c. To achieve this condition, we add an auxiliary classifier on top of D and impose the domain classification loss when optimizing both D and G. That is, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G. In detail, the former is defined as\mathcal{L}_{cls}^{r}={\mathbb{E}}_{x,c^{\prime}}[-\log{{D}_{cls}(c^{\prime}|x)}],(2)where the term {D}_{cls}(c^{\prime}|x) represents a probability distribution over domain labels computed by D. By minimizing this objective, D learns to classify a real image x to its corresponding original domain c^{\prime}. We assume that the input image and domain label pair (x,c^{\prime}) is given by the training data. On the other hand, the loss function for the domain classification of fake images is defined as\mathcal{L}_{cls}^{f}={\mathbb{E}}_{x,c}[-\log{{D}_{cls}(c|G(x,c))}].(3)In other words, G tries to minimize this objective to generate images that can be classified as the target domain c.				
1118	paper_57	How do they make StarGAN to ignore unspecified labels?	By using mask vector, StarGAN was allowed to ignore unspecified labels.	Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two.Training Strategy. When training StarGAN with multiple datasets, we use the domain label \tilde{c} defined in Eq. (7) as input to the generator. By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label \tilde{c}. On the other hand, we extend the auxiliary classifier of the discriminator to generate probability distributions over labels for all datasets. Then, we train the model in a multi-task learning setting, where the discriminator tries to minimize only the classification error associated to the known label. For example, when training with images in CelebA, the discriminator minimizes only classification errors for labels related to CelebA attributes, and not facial expressions related to RaFD. Under these settings, by alternating between CelebA and RaFD the discriminator learns all of the discriminative features for both datasets, and the generator learns to control all the labels in both datasets.				
1119	paper_57	How do they show the importance of mask vectors?	In order to ignore unspecified vectors and link different domains, mask vectors are necessray.	Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector\tilde{c}=[{c}_{1},...,{c}_{n},m],(7)where [\cdot] refers to concatenation, and {c}_{i} represents a vector for the labels of the i-th dataset. The vector of the known label {c}_{i} can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n\mathbb{-}1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two.Learned role of mask vector. In this experiment, we gave a one-hot vector c by setting the dimension of a particular facial expression (available from the second dataset, RaFD) to one. In this case, since the label associated with the second data set is explicitly given, the proper mask vector would be [0,1]. Fig. 7 shows the case where this proper mask vector was given and the opposite case where a wrong mask vector of [1,0] was given.When the wrong mask vector was used, StarGAN-JNT fails to synthesize facial expressions, and it manipulates the age of the input image. This is because the model ignores the facial expression label as unknown and treats the facial attribute label as valid by the mask vector. Note that since one of the facial attributes is ‘young’, the model translates the image from young to old when it takes in a zero vector as input. From this behavior, we can confirm that StarGAN properly learned the intended role of a mask vector in image-to-image translations when involving all the labels from multiple datasets altogether.In this paper, we proposed StarGAN, a scalable image-to-image translation model among multiple domains using a single generator and a discriminator. Besides the advantages in scalability, StarGAN generated images of higher visual quality compared to existing methods [16, 23, 33], owing to the generalization capability behind the multi-task learning setting. In addition, the use of the proposed simple mask vector enables StarGAN to utilize multiple datasets with different sets of domain labels, thus handling all available labels from them. We hope our work to enable users to develop interesting image translation applications across multiple domains. We also introduce a simple but effective approach that enables joint training between domains of different datasets by adding a mask vector to the domain label. Our proposed method ensures that the model can ignore unknown labels and focus on the label provided by a particular dataset. In this manner, our model can perform well on tasks such as synthesizing facial expressions of CelebA images using features learned from RaFD, as shown in the rightmost columns of Fig. StarGAN: Unified Generative Adversarial Networks  for Multi-Domain Image-to-Image Translation. As far as our knowledge goes, our work is the first to successfully perform multi-domain image translation across different datasets.Overall, our contributions are as follows:\bulletWe propose StarGAN, a novel generative adversarial network that learns the mappings among multiple domains using only a single generator and a discriminator, training effectively from images of all domains.\bulletWe demonstrate how we can successfully learn multi-domain image translation between multiple datasets by utilizing a mask vector method that enables StarGAN to control all available domain labels.\bulletWe provide both qualitative and quantitative results on facial attribute transfer and facial expression synthesis tasks using StarGAN, showing its superiority over baseline models.				
1120	paper_57	They propose StarGAN to overcome a limitation of high computational complexity of current image-to-image translation models. Is it true?	It is false.  There is no evidence that the motivation of StarGAN was made in order to overcome a limitation of high computational complexity, even it achieved that.	Another important advantage of our model is the scalability in terms of the number of parameters required. The last column in Table 3 shows that the number of parameters required to learn all translations by StarGAN is seven times smaller than that of DIAT and fourteen times smaller than that of CycleGAN. This is because StarGAN requires only a single generator and discriminator pair, regardless of the number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should be trained for each source-target domain pair.				
1121	paper_58	How many benchmark dataset are used to evaluate the proposed model?	Three benchmark datasets are used.	We utilize three standard citation network benchmark datasets—Cora, Citeseer and Pubmed (Sen et al., 2008)—and closely follow the transductive experimental setup of Yang et al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. We allow for only 20 nodes per class to be used for training—however, honoring the transductive setup, the training algorithm has access to all of the nodes’ feature vectors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500 additional nodes for validation purposes (the same ones as used by Kipf & Welling (2017)). The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node.				
1122	paper_58	It is not necessary to know the graph structure when using the proposed method. Is it true?	It is true.	We have presented graph attention networks (GATs), novel convolution-style neural networks that operate on graph-structured data, leveraging masked self-attentional layers. The graph attentional layer utilized throughout these networks is computationally efficient (does not require costly matrix operations, and is parallelizable across all nodes in the graph), allows for (implicitly) assigning different importances to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not depend on knowing the entire graph structure upfront—thus addressing many of the theoretical issues with previous spectral-based approaches. Our models leveraging attention have successfully achieved or matched state-of-the-art performance across four well-established node classification benchmarks, both transductive and inductive (especially, with completely unseen graphs used for testing).				
1123	paper_58	What does “attention coefficient” mean?	Normalized attention coefficient is a coefficient that is used for compute linear combination of the futures.  However, there is no explanation about simple "attention coefficient". 	Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for every node (after potentially applying a nonlinearity, \sigma):\vec{h}^{\prime}_{i}=\sigma\left(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}{\bf W}\vec{h}_{j}\right).(4)				
1124	paper_58	What point is different in GATs in terms of assigning weight compared to GCN?	GATs uses implicit weight assigning while GCN doesn't.	The graph attentional layer described in subsection 2.1 directly addresses several issues that were present in prior approaches to modelling graph-structured data with neural networks:•Computationally, it is highly efficient: the operation of the self-attentional layer can be parallelized across all edges, and the computation of output features can be parallelized across all nodes. No eigendecompositions or similar costly matrix operations are required. The time complexity of a single GAT attention head computing F^{\prime} features may be expressed as O(|V|FF^{\prime}+|E|F^{\prime}), where F is the number of input features, and |V| and |E| are the numbers of nodes and edges in the graph, respectively. This complexity is on par with the baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017). Applying multi-head attention multiplies the storage and parameter requirements by a factor of K, while the individual heads’ computations are fully independent and can be parallelized.•As opposed to GCNs, our model allows for (implicitly) assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity. Furthermore, analyzing the learned attentional weights may lead to benefits in interpretability, as was the case in the machine translation domain (e.g. the qualitative analysis of Bahdanau et al. (2015)).•The attention mechanism is applied in a shared manner to all edges in the graph, and therefore it does not depend on upfront access to the global graph structure or (features of) all of its nodes (a limitation of many prior techniques). This has several desirable implications:–The graph is not required to be undirected (we may simply leave out computing \alpha_{ij} if edge j\rightarrow i is not present).–It makes our technique directly applicable to inductive learning—including tasks where the model is evaluated on graphs that are completely unseen during training.•The recently published inductive method of Hamilton et al. (2017) samples a fixed-size neighborhood of each node, in order to keep its computational footprint consistent; this does not allow it access to the entirety of the neighborhood while performing inference. Moreover, this technique achieved some of its strongest results when an LSTM (Hochreiter & Schmidhuber, 1997)-based neighborhood aggregator is used. This assumes the existence of a consistent sequential node ordering across neighborhoods, and the authors have rectified it by consistently feeding randomly-ordered sequences to the LSTM. Our technique does not suffer from either of these issues—it works with the entirety of the neighborhood (at the expense of a variable computational footprint, which is still on-par with methods like the GCN), and does not assume any ordering within it.•As mentioned in Section 1, GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016). More specifically, setting the pseudo-coordinate function to be u(x,y)=f(x)\|f(y), where f(x) represent (potentially MLP-transformed) features of node x and \| is concatenation; and the weight function to be w_{j}(u)=\mathrm{softmax}(\mathrm{MLP}(u)) (with the softmax performed over the entire neighborhood of a node) would make MoNet’s patch operator similar to ours. Nevertheless, one should note that, in comparison to previously considered MoNet instances, our model uses node features for similarity computations, rather than the node’s structural properties (which would assume knowing the graph structure upfront). We were able to produce a version of the GAT layer that leverages sparse matrix operations, reducing the storage complexity to linear in the number of nodes and edges and enabling the execution of GAT models on larger graph datasets. However, the tensor manipulation framework we used only supports sparse matrix multiplication for rank-2 tensors, which limits the batching capabilities of the layer as it is currently implemented (especially for datasets with multiple graphs). Appropriately addressing this constraint is an important direction for future work. Depending on the regularity of the graph structure in place, GPUs may not be able to offer major performance benefits compared to CPUs in these sparse scenarios. It should also be noted that the size of the “receptive field” of our model is upper-bounded by the depth of the network (similarly as for GCN and similar models). Techniques such as skip connections (He et al., 2016) could be readily applied for appropriately extending the depth, however. Lastly, parallelization across all the graph edges, especially in a distributed manner, may involve a lot of redundant computation, as the neighborhoods will often highly overlap in graphs of interest.For the inductive learning task, we apply a three-layer GAT model. Both of the first two layers consist of K=4 attention heads computing F^{\prime}=256 features (for a total of 1024 features), followed by an ELU nonlinearity. The final layer is used for (multi-label) classification: K=6 attention heads computing 121 features each, that are averaged and followed by a logistic sigmoid activation. The training sets for this task are sufficiently large and we found no need to apply L_{2} regularization or dropout—we have, however, successfully employed skip connections (He et al., 2016) across the intermediate attentional layer. We utilize a batch size of 2 graphs during training. To strictly evaluate the benefits of applying an attention mechanism in this setting (i.e. comparing with a near GCN-equivalent model), we also provide the results when a constant attention mechanism, a(x,y)=1, is used, with the same architecture—this will assign the same weight to every neighbor.				
1125	paper_58	Which benchmark dataset used in this paper has the largest data size?	It is hard to say a single benchmark set has largest size, because size definition is not given.  If we can say the node size can represent dataset's size, then the Pubmed's dataset is the largest one.	We utilize three standard citation network benchmark datasets—Cora, Citeseer and Pubmed (Sen et al., 2008)—and closely follow the transductive experimental setup of Yang et al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected) citations. Node features correspond to elements of a bag-of-words representation of a document. Each node has a class label. We allow for only 20 nodes per class to be used for training—however, honoring the transductive setup, the training algorithm has access to all of the nodes’ feature vectors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500 additional nodes for validation purposes (the same ones as used by Kipf & Welling (2017)). The Cora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer dataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset contains 19717 nodes, 44338 edges, 3 classes and 500 features per node.				
1126	paper_58	How do they solve the problem that the risk of the training process of self-attention could be unstable using multi-head attention?	to make self-attention stable, they used different mechanism which uses multi-head attention, similar to Vaswani et al. 's one.	To stabilize the learning process of self-attention, we have found extending our mechanism to employ multi-head attention to be beneficial, similarly to Vaswani et al. (2017). Specifically, K independent attention mechanisms execute the transformation of Equation 4, and then their features are concatenated, resulting in the following output feature representation:\vec{h}^{\prime}_{i}=\operatorname*{\scalebox{1.0}[1.5]{$\parallel$}}_{k=1}^{K}\sigma\left(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}^{k}{\bf W}^{k}\vec{h}_{j}\right)(5)where \parallel represents concatenation, \alpha_{ij}^{k} are normalized attention coefficients computed by the k-th attention mechanism (a^{k}), and {\bf W}^{k} is the corresponding input linear transformation’s weight matrix. Note that, in this setting, the final returned output, {\bf h}^{\prime}, will consist of KF^{\prime} features (rather than F^{\prime}) for each node.				
1127	paper_58	How can it actually benefit that GAT can learn regardless of the graph structure?	There are no clue to find what graph can GAT process by its regardless of the graph structure.	We have presented graph attention networks (GATs), novel convolution-style neural networks that operate on graph-structured data, leveraging masked self-attentional layers. The graph attentional layer utilized throughout these networks is computationally efficient (does not require costly matrix operations, and is parallelizable across all nodes in the graph), allows for (implicitly) assigning different importances to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not depend on knowing the entire graph structure upfront—thus addressing many of the theoretical issues with previous spectral-based approaches. Our models leveraging attention have successfully achieved or matched state-of-the-art performance across four well-established node classification benchmarks, both transductive and inductive (especially, with completely unseen graphs used for testing).				
1128	paper_58	How can the GATs work well without any assumption of node order?	GATs work with the entirety of the neighborhood, like GCN did.  This is the main difference with LSTM method and the reason why GATs work well without any assumption of node order.	The graph attentional layer described in subsection 2.1 directly addresses several issues that were present in prior approaches to modelling graph-structured data with neural networks:•Computationally, it is highly efficient: the operation of the self-attentional layer can be parallelized across all edges, and the computation of output features can be parallelized across all nodes. No eigendecompositions or similar costly matrix operations are required. The time complexity of a single GAT attention head computing F^{\prime} features may be expressed as O(|V|FF^{\prime}+|E|F^{\prime}), where F is the number of input features, and |V| and |E| are the numbers of nodes and edges in the graph, respectively. This complexity is on par with the baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017). Applying multi-head attention multiplies the storage and parameter requirements by a factor of K, while the individual heads’ computations are fully independent and can be parallelized.•As opposed to GCNs, our model allows for (implicitly) assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity. Furthermore, analyzing the learned attentional weights may lead to benefits in interpretability, as was the case in the machine translation domain (e.g. the qualitative analysis of Bahdanau et al. (2015)).•The attention mechanism is applied in a shared manner to all edges in the graph, and therefore it does not depend on upfront access to the global graph structure or (features of) all of its nodes (a limitation of many prior techniques). This has several desirable implications:–The graph is not required to be undirected (we may simply leave out computing \alpha_{ij} if edge j\rightarrow i is not present).–It makes our technique directly applicable to inductive learning—including tasks where the model is evaluated on graphs that are completely unseen during training.•The recently published inductive method of Hamilton et al. (2017) samples a fixed-size neighborhood of each node, in order to keep its computational footprint consistent; this does not allow it access to the entirety of the neighborhood while performing inference. Moreover, this technique achieved some of its strongest results when an LSTM (Hochreiter & Schmidhuber, 1997)-based neighborhood aggregator is used. This assumes the existence of a consistent sequential node ordering across neighborhoods, and the authors have rectified it by consistently feeding randomly-ordered sequences to the LSTM. Our technique does not suffer from either of these issues—it works with the entirety of the neighborhood (at the expense of a variable computational footprint, which is still on-par with methods like the GCN), and does not assume any ordering within it.•As mentioned in Section 1, GAT can be reformulated as a particular instance of MoNet (Monti et al., 2016). More specifically, setting the pseudo-coordinate function to be u(x,y)=f(x)\|f(y), where f(x) represent (potentially MLP-transformed) features of node x and \| is concatenation; and the weight function to be w_{j}(u)=\mathrm{softmax}(\mathrm{MLP}(u)) (with the softmax performed over the entire neighborhood of a node) would make MoNet’s patch operator similar to ours. Nevertheless, one should note that, in comparison to previously considered MoNet instances, our model uses node features for similarity computations, rather than the node’s structural properties (which would assume knowing the graph structure upfront). We were able to produce a version of the GAT layer that leverages sparse matrix operations, reducing the storage complexity to linear in the number of nodes and edges and enabling the execution of GAT models on larger graph datasets. However, the tensor manipulation framework we used only supports sparse matrix multiplication for rank-2 tensors, which limits the batching capabilities of the layer as it is currently implemented (especially for datasets with multiple graphs). Appropriately addressing this constraint is an important direction for future work. Depending on the regularity of the graph structure in place, GPUs may not be able to offer major performance benefits compared to CPUs in these sparse scenarios. It should also be noted that the size of the “receptive field” of our model is upper-bounded by the depth of the network (similarly as for GCN and similar models). Techniques such as skip connections (He et al., 2016) could be readily applied for appropriately extending the depth, however. Lastly, parallelization across all the graph edges, especially in a distributed manner, may involve a lot of redundant computation, as the neighborhoods will often highly overlap in graphs of interest.				
1129	paper_58	How performing a self-attention mechanism to graph can be useful in node classification?	By using self-attention mechanism, model can find hidden meanings in the graph and it helps to do node classification.	Inspired by this recent work, we introduce an attention-based architecture to perform node classification of graph-structured data. The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy. The attention architecture has several interesting properties: (1) the operation is efficient, since it is parallelizable across node-neighbor pairs; (2) it can be applied to graph nodes having different degrees by specifying arbitrary weights to the neighbors; and (3) the model is directly applicable to inductive learning problems, including tasks where the model has to generalize to completely unseen graphs. We validate the proposed approach on four challenging benchmarks: Cora, Citeseer and Pubmed citation networks as well as an inductive protein-protein interaction dataset, achieving or matching state-of-the-art results that highlight the potential of attention-based models when dealing with arbitrarily structured graphs.				
1130	paper_58	Why was Graph Neural Networks(GNNs) proposed before even if Convolutional Neural Networks(CNNs) have been successful in many tasks?	Because CNN can process only grid-like structure, GNN, which can process general graph structure proposed.	Convolutional Neural Networks (CNNs) have been successfully applied to tackle problems such as image classification (He et al., 2016), semantic segmentation (Jégou et al., 2017) or machine translation (Gehring et al., 2016), where the underlying data representation has a grid-like structure. These architectures efficiently reuse their local filters, with learnable parameters, by applying them to all the input positions.There have been several attempts in the literature to extend neural networks to deal with arbitrarily structured graphs. Early work used recursive neural networks to process data represented in graph domains as directed acyclic graphs (Frasconi et al., 1998; Sperduti & Starita, 1997). Graph Neural Networks (GNNs) were introduced in Gori et al. (2005) and Scarselli et al. (2009) as a generalization of recursive neural networks that can directly deal with a more general class of graphs, e.g. cyclic, directed and undirected graphs. GNNs consist of an iterative process, which propagates the node states until equilibrium; followed by a neural network, which produces an output for each node based on its state. This idea was adopted and improved by Li et al. (2016), which propose to use gated recurrent units (Cho et al., 2014) in the propagation step.				
1131	paper_59	What does “patchwise training” mean?	there is no clue to define what does patchwise train mean.	This method is efficient, both asymptotically and absolutely, and precludes the need for the complications in other works.Patchwise training is common [27, 2, 8, 28, 11], but lacks the efficiency of fully convolutional training.Our approach does not make use of pre- and post-processing complications, including superpixels [8, 16], proposals [16, 14], or post-hoc refinement by random fields or local classifiers [8, 16].Our model transfers recent success in classification [19, 31, 32] to dense prediction by reinterpreting classification nets as fully convolutional and fine-tuning from their learned representations.In contrast, previous works have applied small convnets without supervised pre-training [8, 28, 27].In stochastic optimization, gradient computation is driven by the training distribution.Both patchwise training and fully-convolutional training can be made to produce any distribution, although their relative computational efficiency depends on overlap and minibatch size.Whole image fully convolutional training is identical to patchwise training where each batch consists of all the receptive fields of the units below the loss for an image (or collection of images).While this is more efficient than uniform sampling of patches, it reduces the number of possible batches.However, random selection of patches within an image may be recovered simply.Restricting the loss to a randomly sampled subset of its spatial terms (or, equivalently applying a DropConnect mask [36] between the output and the loss) excludes patches from the gradient computation.				
1132	paper_59	How fast is the proposed method compared to the naive approach?	Proposed method is five times faster than the naive approach.	Furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the computation is highly amortized over the overlapping regions of those patches.For example, while AlexNet takes 1.2 ms (on a typical GPU) to produce the classification scores of a 227\times 227 image, the fully convolutional version takes 22 ms to produce a 10\times 10 grid of outputs from a 500\times 500 image, which is more than 5 times faster than the naïve approach111Assuming efficient batching of single image inputs.The classification scores for a single image by itself take 5.4 ms to produce, which is nearly 25 times slower than the fully convolutional version..				
1133	paper_59	What does “receptive fields” mean?	receptive fields mean the higher layer which is connected to the original layer.	Each layer of data in a convnet is a three-dimensional array of size h\times w\times d, where h and w are spatial dimensions, and d is the feature or channel dimension.The first layer is the image, with pixel size h\times w, and d color channels.Locations in higher layers correspond to the locations in the image they are path-connected to, which are called their receptive fields.				
1135	paper_59	How do they utilize fully-connected layers on dense prediction of image?	they used in-network upsampling.	Dense prediction with convnets Several recent works have applied convnets to dense prediction problems,including semantic segmentation by Ning et al. [27], Farabet et al. [8], and Pinheiro and Collobert [28];boundary prediction for electron microscopy by Ciresan et al. [2] and for natural images by a hybrid neural net/nearest neighbor model by Ganin and Lempitsky [11];and image restoration and depth estimation by Eigen et al. [5, 6].Common elements of these approaches include•small models restricting capacity and receptive fields;•patchwise training [27, 2, 8, 28, 11];•post-processing by superpixel projection, random field regularization, filtering, or local classification [8, 2, 11];•input shifting and output interlacing for dense output [28, 11] as introduced by OverFeat [29];•multi-scale pyramid processing [8, 28, 11];•saturating \tanh nonlinearities [8, 5, 28]; and•ensembles [2, 11],whereas our method does without this machinery. However, we do study patchwise training 3.4 and “shift-and-stitch” dense output 3.2 from the perspective of FCNs.We also discuss in-network upsampling 3.3, of which the fully connected prediction by Eigen et al. [6] is a special case.In our experiments, we find that in-network upsampling is fast and effective for learning dense prediction.Our best segmentation architecture uses these layers to learn to upsample for refined prediction in Section 4.2.				
1136	paper_59	Using filters allows us to see finer information. Is there any risk of using filters? Why?	Might get smaller receptive fields and take longer time to compute.	Simply decreasing subsampling within a net is a tradeoff: the filters see finer information, but have smaller receptive fields and take longer to compute.We have seen that the shift-and-stitch trick is another kind of tradeoff: the output is made denser without decreasing the receptive field sizes of the filters, but the filters are prohibited from accessing information at a finer scale than their original design.				
1137	paper_59	They achieved state-of-the-art performance on several benchmark datasets. Is it true?	It is true.	Fine-tuning from classification to segmentation gave reasonable predictions for each net.Even the worst model achieved \sim 75\% of state-of-the-art performance.The segmentation-equippped VGG net (FCN-VGG16) already appears to be state-of-the-art at 56.0 mean IU on val, compared to 52.6 on test [16].Training on extra data raises performance to 59.4 mean IU on a subset of val77footnotemark: 7.Training details are given in Section 4.3.SIFT Flow is a dataset of 2,688 images with pixel labels for 33 semantic categories (“bridge”, “mountain”, “sun”), as well as three geometric categories (“horizontal”, “vertical”, and “sky”).An FCN can naturally learn a joint representation that simultaneously predicts both types of labels.We learn a two-headed version of FCN-16s with semantic and geometric prediction layers and losses.The learned model performs as well on both tasks as two independently trained models, while learning and inference are essentially as fast as each independent model by itself.The results in Table 5, computed on the standard split into 2,488 training and 200 test images,101010Three of the SIFT Flow categories are not present in the test set.We made predictions across all 33 categories, but only included categories actually present in the test set in our evaluation.(An earlier version of this paper reported a lower mean IU, which included all categories either present or predicted in the evaluation.)show state-of-the-art performance on both tasks.				
1139	paper_6	How do Projected Attention Layers work?	Projected Attention Layers (PAL) takes the hidden state from the previous layer and runs parallel to the self-attention layer.  In each PAL, the hidden size of the pretrained layer is linearly projected and then passed through its own self-attention layer before undergoing transformation back to the original hidden state size.	We then experiment with injecting the patterns back into the pre-trained transformer encoder.In particular, we injectthem through additional attention heads in the form of a Projected Attention Layer (PAL) (Stickland and Murray, 2019), along with the parameters of the original model. Details regarding PALs are described in Appendix A.				
1140	paper_6	What does "injecting" information into attention layers mean?	Pattern injection means pre-determining the weights of the transformer layer's scaled dot product attention values, such that run-time complexity can be lowered while maintaining the interpretability of the model.	Meanwhile, a parallel line of research has exploredinjecting predefined patterns into attention matrices of transformers in an attemptto reduce the run-time complexity of self-attention while maintaining competitive accuracy.This can be done by either replacing theattention weights with a fixed matrix (Raganato et al., 2020; Tay et al., 2021; Xiao et al., 2020); or alternatively by guiding the attention weights through more flexible masking strategies (Mihaylov and Frank, 2019; Child et al., 2019; Guo et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Bai et al., 2021).Once the important patterns are identified, there are two common approaches(i.e. fixing and masking)to injectthem as constraints to the attention matrices in the transformer-based neural models (see §3.2). The pipeline alsoenables two scenarios,in which injectingthe patterns can be beneficial:the first one is to train a new model with the patterns injected, while the second one is to enhance the original model.Overall, with the discovered patterns injected,our models are arguably more interpretable thanplain transformers on both tasks, as we know with certaintythe information encoded in each masked/fixed attention heads. To further justify our claim of interpretability, the attention headswith patterns injectedtend to have higher importance scores than the other heads666An illustrative example is shown in Appendix C.1, suggesting that such patterns are effectively leveraged by the model.				
1141	paper_6	Beyond simple addition of a constant to the attention values, how can the patterns applied back to the original model?	Patterns can be applied to a pretrained transformer either by adding a set of pre-computed constants or by adding an input-dependent weight matrix.	In this work, we injectthe discovered patternsby either fixing or masking the attention weights prior to the softmax function.For fixed attention weights, the attention logits in the scaled-dot-product attention is replaced with a fixed (possibly input dependent) matrix such that:\textrm{FixAttn}(V,X)=\sigma(F^{(P)}(X))V(2)where \sigma is the softmax operation, V is the value vectors, and F(X)\in[0,1] computes a binary matrix from the input sequence X based on the predicated P for the specific pattern. Similarly, a pattern can also be injectedby casting a mask over the attention weights computed from the key and query vectors, as:\textrm{MaskAttn}(Q,K,V,X)=\sigma(M^{(P)}(X)+QK^{T})V(3)where M(X)\in[0,-\infty) computes the desired behaviour in the same fashion as F(X), and is added to the attention logits to approximate the multiplication of the attention distribution by a weight.				
1142	paper_6	How can attention patterns from larger models be applied to smaller models if the models might differ in the size and number of attention layers?	BERTSum is the summarization model, and Cross-Segment BERT is the topic segmentation model.  The paper does not discuss transferring patterns onto pretrained transformers of different architecture.	After verifying on the validation set, we discoverthree patternsconsistently existing in both tasks (over 50% of important heads).This suggests that important patterns are generalizable across multiple NLP tasks, whichis consistent with the findings in Bian et al. (2021). Further analysis also shows that the attention patterns are consistent after fine-tuning, where we report an average Jensen-Shannon Divergenceof 0.01 between the attention distributions of BERTSum across 3 random seeds.We hope our findings provide motivation for the in-depth study of pattern importance in different NLP tasks. Lastly, while it may be argued that this step of the pipeline can be automated by directly evaluating the importance and relevance of predefined patterns (e.g. syntax, discourse) based on intuitions, as indicated below, our interactive approach allows the discovery of interpretable patterns which otherwise would be hard to define due to the infinite search space of possible patterns. Next, we describe the three discovered patterns in detail.				
1143	paper_6	What is the BERTSum model and how does it differ from just the BERT model?	BERTSum is a specialized variant of BERT on the task of extractive summarization, picking out the sentences from a text to constitute its summary.	We adopt the popular BERTSum (Liu and Lapata, 2019) for extractive summarization. With the contextualized representation from BERT, the model uses a binary classifier to predict whether each sentence belongs in the summary. We train the model on the CNN/DM dataset (See et al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric.Extractive summarization is the taskof picking the most representative sentences as the summary for the given document(s). Current state-of-the-art models, which are mostly based on large-scale pretrained language models Liu and Lapata (2019); Zhong et al. (2020); Jia et al. (2020); Ruan et al. (2022), can deliver good performance, but why and how such models work so well still remain an open question. In our case study, we adoptthe popular BERTSum(Liu and Lapata, 2019).	BERTSum is a specialized variant of GPT on the task of extractive summarization, picking out the sentences from a text to constitute its summary.	Change concept	BERT -> GPT	
1144	paper_6	How was performance measured and was the performance of the human-guided knowledge distilled model significantly higher?	Interpretability is measured with the PDR framework.  Summarization performance measured in ROUGE is 15% better.  Topic segmentation performance measured in F1 is 12% better.	In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019).	Interpretability is measured with the PDR framework.  Summarization performance measured in ROUGE is 20% better.  Topic segmentation performance measured in F1 is 22% better.	Change number		
1145	paper_6	What was the highest performing estimation method for the authors' experiments	There is no ""highest"" performer by any single measure. 	As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.				
1146	paper_6	What is the difference between calculating the Taylor expansion and the Hessian?	Hessian is the second-order partial derivative matrix itself, and Taylor expansion is the method used to approximate it.	We adapt the Taylor expansion method (Molchanov et al., 2019) as a proxy score for the head importance estimation.Following Li et al. (2021), we use the first-order expansion to avoid the overhead from computing the Hessian, where the gradient w.r.t. the validation loss is summed over all parameters of an attention head to estimate its importance.				
1147	paper_6	Instead of automatically identifying important attention patterns, why should a human be involved in this process?	Human involvement can provide interpretable results from the identified patterns and the performance enhancement from the pattern injection.	Once the the most important heads are identified, their attention distributions are inspected to look for patterns.In this work, we propose and test a novel human-in-the-loop pipeline that to the best of our knowledge is the first attemptto combine research on analyzing self-attention with work on injecting patterns into attention matrices.To start, human users visually explore the attention matrices of transformers to identify task-specific patterns that could be formalized as a predicate. After quantitatively evaluating the patterns on the validation set, they can be injected into attention heads of transformer models to simultaneously improve task accuracy and make the model more efficient by sparsifying the attention matrices111The implementation of our work is publicly available at: https://github.com/raymondzmc/Attention-Pattern-Exploitation. This is in contrast to previous work that mostly focuses on the trade-off between two metrics.To discovertask-specific patterns,we analyze the top-3 most important heads of each layer, and look forhuman-interpretable relationships encoded in the attention weights. In practice, we use the instance-level interactions provided by the visual framework (Li et al., 2021), and randomly select 5 validation examples per task for our analysis.The entire process takesless than one hour to complete for each task, where we manually examinethe attention weights for less than half of the tokens for each example.It is worth noting that detailed analysis regarding the trade-off between human costandpattern recallwould require extensive user studies beyond the scope of this work.As future work, we plan to apply our pipeline to other NLP tasks (e.g. language modeling, abstractive summarization) and explore and verify whether the important patterns from one task can be transferable to another task. Similarly, we also plan to apply our pipeline to different model variants to examine and compare the patterns encoded in the attention weights.In the long term, our pipeline could be naturally automated by replacing the pattern discovery step with evaluating predefined linguistic patterns. However, assessing the efficiency gains from injecting such patterns (requiring ground-truth annotations) would require more in-depth studies beyond the scope of this paper.Finally, since human factors are an important aspect of interpretability, we plan to conduct extensive user studies across different NLP tasks and model sizes to examine the trade-off between human-cost and the coverage of discovered patterns.				
1148	paper_6	Why are the patterns only defined between pairs of tokens instead of other possible options (e.g., trios, sequence, sets)?	The structural definition of a pattern in this paper follows only naturally from the design of the attention mechanism.	We define an attention pattern to be interpretable iff it can be modeled as a predicate P between any pair of input tokens (x_{i},x_{j}).For instance, the positional pattern ‘preceding token’ would be true if x_{i} appears before x_{j}. Candidate patterns can be discovered following two criteria:1) they are beneficial for the downstream task;2) they occur consistently among relevant tokens.This pattern describes the “attending to matching tokens” behaviour, wherethe attention value \alpha_{i,j}^{h} between input tokens x_{i} and x_{j} on the head h is high whenever x_{i}=x_{j}. For example, as shown in footnote 3 (i), the token "photo" mostly attends to other appearances of the token "photo" in the input sequence. To evaluate whether this pattern has a large global relevance for any head, we only consider tokens that appear at least twice within a single documents, and compute GR (Eq. 1), in which P(x_{i},x_{j}) holds if and only if x_{i}=x_{j}, i.e. \mathbbm{1}_{P(x_{i},x_{j})}=(\mathbbm{1}_{\textrm{freq}(x_{i})>1})\times(\mathbbm{1}_{x_{i}=x_{j}}).This pattern describes the behaviour of only attending to tokens within a text span.For summarization, these heads will focus on attending tokens within the same sentence (footnote 3 (ii)). Similarly, the same heads in topic segmentation models will focus on attending tokens within the same context (left or right).To evaluate this pattern, GR is computed with P(x_{i},x_{j}) holding iff x_{i} and x_{j} occur within the same text span. footnote 3 (C) reveals that this pattern appears more frequently in the mid to upper layers of the transformer encoder.				
1150	paper_6	How does using the fixed attention approach affect performance differently when compared to the masked attention approach?	There is no discussion on how the performance difference is brought about.	In this work, we injectthe discovered patternsby either fixing or masking the attention weights prior to the softmax function.For fixed attention weights, the attention logits in the scaled-dot-product attention is replaced with a fixed (possibly input dependent) matrix such that:\textrm{FixAttn}(V,X)=\sigma(F^{(P)}(X))V(2)where \sigma is the softmax operation, V is the value vectors, and F(X)\in[0,1] computes a binary matrix from the input sequence X based on the predicated P for the specific pattern. Similarly, a pattern can also be injectedby casting a mask over the attention weights computed from the key and query vectors, as:\textrm{MaskAttn}(Q,K,V,X)=\sigma(M^{(P)}(X)+QK^{T})V(3)where M(X)\in[0,-\infty) computes the desired behaviour in the same fashion as F(X), and is added to the attention logits to approximate the multiplication of the attention distribution by a weight.Although the two methods are very similar with respect to the improvement they contribute (see §4), masking allows more flexibility and is generally used for patterns with a large number of applicable tokens, while fixing is more rigid and better suited for a small number of applicable tokens.				
1151	paper_6	How did the authors measure "interpretability"?	The authors' definition of interpretability is measured in terms of higher importance scores in the attention heads.	In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.Overall, with the discovered patterns injected,our models are arguably more interpretable thanplain transformers on both tasks, as we know with certaintythe information encoded in each masked/fixed attention heads. To further justify our claim of interpretability, the attention headswith patterns injectedtend to have higher importance scores than the other heads666An illustrative example is shown in Appendix C.1, suggesting that such patterns are effectively leveraged by the model.In the context of Machine Learning, interpretability can be defined as the description of the internals of a model in a way that is understandable to humans (Gilpin et al., 2018).With the rise of deep learning, various techniques have been proposed to interpret the inner workings of neural NLP models. For example, probing classifiers are often used for finding linguistic or knowledge information learned by neural networks (Conneau et al., 2018; Tenney et al., 2019; Pimentel et al., 2020; Voita and Titov, 2020; Hou and Sachan, 2021; Aghazadeh et al., 2022), while behaviour testing aims at understanding how models behave through inferences under different controlled settings (McCoy et al., 2019; Ross and Pavlick, 2019; Ribeiro et al., 2020; Koh et al., 2021; Goel et al., 2021). In contrast, our work is an example of making interpretability an inherent attribute of the neural models (e.g. Chen and Ji (2020); Hu et al. (2021)),with human-distinguishable patterns revealing insights regarding a subset of parameters in the model.				
1152	paper_60	Fast R-CNN is more than six times faster than some of the baseline models. Is it true?	Probably true.  Despite there is no provided baseline model, (in evidences) Fast R-CNN is more than six times faster than R-CNN is true.	Fast training and testing times are our second main result.Table 4 compares training time (hours), testing rate (seconds per image), and mAP on VOC07 between Fast R-CNN, R-CNN, and SPPnet.For VGG16, Fast R-CNN processes images 146\times faster than R-CNN without truncated SVD and 213\times faster with it.Training time is reduced by 9\times, from 84 hours to 9.5.Compared to SPPnet, Fast R-CNN trains VGG16 2.7\times faster (in 9.5 vs. 25.5 hours) and tests 7\times faster without truncated SVD or 10\times faster with it.Fast R-CNN also eliminates hundreds of gigabytes of disk storage, because it does not cache features.				
1153	paper_60	What was the problem of previous work, R-CNN in terms of the efficiency?	R-CNN needs multi-stage pipeline training and time consumes when evaluate them.	The Region-based Convolutional Network method (R-CNN) [9] achieves excellent object detection accuracy by using a deep ConvNet to classify object proposals.R-CNN, however, has notable drawbacks:1.Training is a multi-stage pipeline.R-CNN first fine-tunes a ConvNet on object proposals using log loss.Then, it fits SVMs to ConvNet features.These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning.In the third training stage, bounding-box regressors are learned.2.Training is expensive in space and time.For SVM and bounding-box regressor training, features are extracted from each object proposal in each image and written to disk.With very deep networks, such as VGG16, this process takes 2.5 GPU-days for the 5k images of the VOC07 trainval set.These features require hundreds of gigabytes of storage.3.Object detection is slow.At test-time, features are extracted from each object proposal in each test image.Detection with VGG16 takes 47s / image (on a GPU).R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.Spatial pyramid pooling networks (SPPnets) [11] were proposed to speed up R-CNN by sharing computation.The SPPnet method computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map.Features are extracted for a proposal by max-pooling the portion of the feature map inside the proposal into a fixed-size output (e.g., 6\times 6).Multiple output sizes are pooled and then concatenated as in spatial pyramid pooling [15].SPPnet accelerates R-CNN by 10 to 100\times at test time.Training time is also reduced by 3\times due to faster proposal feature extraction.				
1155	paper_60	What does “RoI” mean?	The RoI is a rectangular window into a conv feature map.	The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H\times W (e.g., 7\times 7), where H and W are layer hyper-parameters that are independent of any particular RoI.In this paper, an RoI is a rectangular window into a conv feature map.Each RoI is defined by a four-tuple (r,c,h,w) that specifies its top-left corner (r,c) and its height and width (h,w).				
1158	paper_60	How SPPnet address the drawback of R-CNN?	SPPnet computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map.  By this method, SPPnet can solve time consuming issue which is occurred on R-CNN.	R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.Spatial pyramid pooling networks (SPPnets) [11] were proposed to speed up R-CNN by sharing computation.The SPPnet method computes a convolutional feature map for the entire input image and then classifies each object proposal using a feature vector extracted from the shared feature map.Features are extracted for a proposal by max-pooling the portion of the feature map inside the proposal into a fixed-size output (e.g., 6\times 6).Multiple output sizes are pooled and then concatenated as in spatial pyramid pooling [15].SPPnet accelerates R-CNN by 10 to 100\times at test time.Training time is also reduced by 3\times due to faster proposal feature extraction.				
1159	paper_60	Why does the author use a rectangular shaped RoI?	in order to make a fixed feature map.	The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H\times W (e.g., 7\times 7), where H and W are layer hyper-parameters that are independent of any particular RoI.In this paper, an RoI is a rectangular window into a conv feature map.Each RoI is defined by a four-tuple (r,c,h,w) that specifies its top-left corner (r,c) and its height and width (h,w).				
1160	paper_60	Why does the network receive not only input of images but also a list of RoI as the input value?	RoI extracts a fixed-length feature vector from the feature map in the model.	Third, the network is modified to take two data inputs: a list of images and a list of RoIs in those images.Fig. 1 illustrates the Fast R-CNN architecture.A Fast R-CNN network takes as input an entire image and a set of object proposals.The network first processes the whole image with several convolutional (conv) and max pooling layers to produce a conv feature map.Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map.Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over K object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the K object classes.Each set of 4 values encodes refined bounding-box positions for one of the K classes.				
1161	paper_61	Perplexity is used to evaluate what performance?	Perplexity is used to evaluate the performance of the model.	As is standard in language modeling, we use perplexity ( PPL ) to evaluate the performance of our models. Perplex- ity of a model over a sequence [ w 1 , . . . , w T ] is given by				
1166	paper_61	Why did author try to use hierarchical softmax in optimization process?	They use hierachical softmax in order to increase learning speed.	Finally, in order to speed up training on D ATA - L we em- ploy a hierarchical softmax (Morin and Bengio 2005)—a common strategy for training language models with very large |V| —instead of the usual softmax. We pick the number of clusters c = (cid:100) (cid:112) |V|(cid:101) and randomly split V into mutually exclusive and collectively exhaustive subsets V 1 , . . . , V c of (approximately) equal size. 10 Then Pr ( w t +1 = j | w 1: t ) be- comes,				
1168	paper_61	What is the advantage of using character-level input on language modeling?	Several pre-processing progress is not necessary for character-level inputs.  That is why character-level inputs are powerful.	Another direction of work has involved purely character- level NLMs, wherein both input and output are charac- ters (Sutskever, Martens, and Hinton 2011; Graves 2013). Character-level models obviate the need for morphological tagging or manual feature engineering, and have the attrac- tive property of being able to generate novel words. How- ever they are generally outperformed by word-level models (Mikolov et al. 2012).				
1170	paper_62	Does BiDAF show state-of-the-art performance on benchmark dataset?	It is true.	Our BiDAF model111Our code and interactive demo are available at: allenai.github.io/bi-att-flow/ outperforms all previous approaches on the highly-competitive Stanford Question Answering Dataset (SQuAD) test set leaderboard at the time of submission.With a modification to only the output layer, BiDAF achieves the state-of-the-art results on the CNN/DailyMail cloze test.We also provide an in-depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016).				
1171	paper_62	Author mentioned char-level embedding. Did they show the experimental result with char-level embedding?	They showed that.	Table 1(b) shows the performance of our model and its ablations on the SQuAD dev set. Both char-level and word-level embeddings contribute towards the model’s performance. We conjecture that word-level embedding is better at representing the semantics of each word as a whole, while char-level embedding can better handle out-of-vocab (OOV) or rare words. To evaluate bi-directional attention, we remove C2Q and Q2C attentions. For ablating C2Q attention, we replace the attended question vector \tilde{\bf U} with the average of the output vectors of the question’s contextual embedding layer (LSTM). C2Q attention proves to be critical with a drop of more than 10 points on both metrics. For ablating Q2C attention, the output of the attention layer, {\bf G}, does not include terms that have the attended Q2C vectors, \tilde{\bf H}. To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer’s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016). This is in contrast with our approach, where the attention is pre-computed before flowing to the modeling layer. Despite being a simpler attention mechanism, our proposed static attention outperforms the dynamically computed attention by more than 3 points. We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer.We also show the performance of BiDAF with several different definitions of \alpha and {\bm{\beta}} functions (Equation 1 and 2) in Appendix B.				
1172	paper_62	What does “cloze-style” mean?	cloze-style indicates unfilled context.	In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a huge attention over a few months. In the next section, we evaluate our model on the task of cloze-style reading comprehension.In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one’s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test) examples from CNN and DailyMail news articles, respectively. Each example has a news article and an incomplete sentence extracted from the human-written summary of the article. To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word, the missing word is always a named entity, anonymized with a random ID.Also, the IDs must be shuffled constantly during test, which is also critical for full anonymization.				
1173	paper_62	How many question-context tuple is used to train a model in question answering experiment?	90,000 tuples are used to train.	SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions. The answer to each question is always a span in the context.The model is given a credit if its answer matches one of the human written answers.Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at character level.The dataset consists of 90k/10k train/dev question-context tuples with a large hidden test set.It is one of the largest available MC datasets with human-written questions and serves as a great test bed for our model.				
1174	paper_62	What is the difference between C2Q and Q2C?	C2Q deals about which query words are most relevant to each context word.  However, Q2C deals about which context words have the closest similarity to one of the query words.	Context-to-query Attention.Context-to-query (C2Q) attention signifies which query words are most relevant to each context word.Let {\bf a}_{t}\in\mathbb{R}^{J} represent the attention weights on the query words by t-th context word, \sum{\bf a}_{tj}=1 for all t. The attention weight is computed by {\bf a}_{t}=\mathrm{softmax}({\bf S}_{t:})\in\mathbb{R}^{J},and subsequently each attended query vector is \tilde{{\bf U}}_{:t}=\sum_{j}{\bf a}_{tj}{\bf U}_{:j}.Hence \tilde{{\bf U}} is a 2d-by-T matrix containing the attended query vectors for the entire context.Query-to-context Attention.Query-to-context (Q2C) attention signifies which context words have the closest similarity to one of the query words and are hence critical for answering the query. We obtain the attention weights on the context words by {\bf b}=\mathrm{softmax}(\max_{col}({\bf S}))\in\mathbb{R}^{T}, where the maximum function (\max_{col}) is performed across the column. Then the attended context vector is \tilde{\bf h}=\sum_{t}{\bf b}_{t}{\bf H}_{:t}\in\mathbb{R}^{2d}. This vector indicates the weighted sum of the most important words in the context with respect to the query.\tilde{\bf h} is tiled T times across the column, thus giving \tilde{\bf H}\in\mathbb{R}^{2d\times T}.				
1175	paper_62	Why did the author add one more direction in attention flow?	In order to obtain a query-aware context representation, author used bi-directional attention flow.	In this paper, we introduce the Bi-Directional Attention Flow  (BiDAF) network, a hierarchical multi-stage architecture for modeling the representations of the context paragraph at different levels of granularity (Figure 1).BiDAF includes character-level, word-level, and contextual embeddings, and uses bi-directional attention flow to obtain a query-aware context representation.Our attention mechanism offers following improvements to the previously popular attention paradigms.First, our attention layer is not used to summarize the context paragraph into a fixed-size vector.Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to flow through to the subsequent modeling layer.This reduces the information loss caused by early summarization.Second, we use a memory-less attention mechanism.That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step.We hypothesize that this simplification leads to the division of labor between the attention layer and the modeling layer.It forces the attention layer to focus on learning the attention between the query and the context, and enables the modeling layer to focus on learning the interaction within the query-aware context representation (the output of the attention layer).It also allows the attention at each time step to be unaffected from incorrect attendances at previous time steps.Our experiments show that memory-less attention gives a clear advantage over dynamic attention.Third, we use attention mechanisms in both directions, query-to-context and context-to-query, which provide complimentary information to each other.In this paper, we introduce BiDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization. The experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. The ablation analyses demonstrate the importance of each component in our model. The visualizations and discussions show that our model is learning a suitable representation for MC and is capable of answering complex questions by attending to correct locations in the given paragraph. Future work involves extending our approach to incorporate multiple hops of the attention layer.				
1176	paper_62	Why did the author adopt an attention mechanism as a base architecture of model?	attention mechanism is the finest model that author can used to their model.	The tasks of machine comprehension (MC) and question answering (QA) have gained significant popularity over the past few years within the natural language processing and computer vision communities. Systems trained end-to-end now achieve promising results on a variety of tasks in the text and image domains.One of the key factors to the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a).Attention mechanisms in previous works typically have one or more of the following characteristics.First, the computed attention weights are often used to extract the most relevant information from the context for answering the question by summarizing the context into a fixed-size vector.Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step.Third, they are usually uni-directional, wherein the query attends on the context paragraph or the image.				
1177	paper_62	According to Figure 2-(a), ‘May’ is far from other months in visualized word embed space. Why did this result happen?	Because "May" has several different meanings in English, "May" is far from other months. 	We also visualize these two feature spaces using t-SNE in Figure 2. t-SNE is performed on a large fraction of dev data but we only plot data points corresponding to the months of the year.An interesting pattern emerges in the Word space, where May is separated from the rest of the months because May has multiple meanings in the English language.The contextual embedding layer uses contextual cues from surrounding words and is able to separate the usages of the word May. Finally we visualize the attention matrices for some question-context tuples in the dev data in Figure 3. In the first example, Where matches locations and in the second example, many matches quantities and numerical symbols. Also, entities in the question typically attend to the same entities in the context, thus providing a feature for the model to localize possible answers.				
1178	paper_62	How does the author design the model to receive unfixed-size input?	paper's model can process multiple different domains.  According to the parer, several different paragraph compares previous researches and their model.  However, there is no evidence that, that models are related to conditional GAN.	The model architecture used for this task is depicted in Figure 1. Each paragraph and question are tokenized by a regular-expression-based word tokenizer (PTB Tokenizer) and fed into the model. We use 100 1D filters for CNN char embedding, each with a width of 5.The hidden state size (d) of the model is 100.The model has about 2.6 million parameters.We use the AdaDelta (Zeiler, 2012) optimizer, with a minibatch size of 60 and an initial learning rate of 0.5, for 12 epochs.A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.During training, the moving averages of all weights of the model are maintained with the exponential decay rate of 0.999.At test time, the moving averages instead of the raw weights are used.The training process takes roughly 20 hours on a single Titan X GPU. We also train an ensemble model consisting of 12 training runs with the identical architecture and hyper-parameters.At test time, we choose the answer with the highest sum of confidence scores amongst the 12 runs for each question.				
1179	paper_63	How can author claim that only using absolute positional encoding with Transformer can show the relaxed structural inductive bias?	Author claims that Transformer only using absolute positional encoding often generates dissimilar representations for nodes with similar local structures.   It shows the relaxed structural inductive bias.  The reason is that structural similarity between nodes is not contained  in absolute positional encoding.	We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:	Author claims that Transformer only using absolute positional encoding often generates similar representations for nodes with similar local structures.   It shows the relaxed structural inductive bias.  The reason is that structural similarity between nodes is not contained  in absolute positional encoding.	Change concept	dissimilar -> similar	
1180	paper_63	How the author extract the subgraph of each node?	Author extract entire k-hop subgraphs for each node.	A more expressive extractor is to use a GNN to directly compute the representation of the entire k-hop subgraph centered at u rather than just the node representation u. Recent work has explored the idea of using subgraphs rather than subtrees around a node in GNNs, with positive experimental results (Zhang & Li, 2021; Wijesinghe & Wang, 2022), as well as being strictly more powerful than the 1-WL test (Zhang & Li, 2021). We follow the same setup as is done in Zhang & Li (2021), and adapt our GNN extractor to utilize the entire k-hop subgraph.The k-subgraph GNN extractor aggregates the updated node representations of all nodes within the k-hop neighborhood using a pooling function such as summation. Formally, if we denote by {\mathcal{N}}_{k}(u) the k-hop neighborhood of node u including itself, the representation of a node u is:\varphi(u,G)=\sum_{v\in{\mathcal{N}}_{k}(u)}\text{GNN}^{(k)}_{G}(v).(8)				
1181	paper_63	what is limitations of gnns?	There are two most widely adopted limitations of GNNs : over-smoothing and over-squashing. Over-smoothing is a phenomenon that indicates representations of GNNs get similar to each others as the number of layers increases. Over-squashing is a difficulty of node representations to contain messages that come from distant neighbors.	While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs (Xu et al., 2019; Morris et al., 2019), as well as known problems such as over-smoothing (Li et al., 2018, 2019; Chen et al., 2020; Oono & Suzuki, 2020) and over-squashing (Alon & Yahav, 2021).Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain “bottlenecks” in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN (Kipf & Welling, 2017), which was based on performing convolutions on the graph. Gilmer et al. (2017) reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples (Hamilton et al., 2017; Xu et al., 2019; Corso et al., 2020; Hu et al., 2020b; Veličković et al., 2018; Li et al., 2020a; Yang et al., 2022). However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.	There are three most widely adopted limitations of GNNs : over-smoothing, over-fitting and over-squashing. Over-smoothing is a phenomenon that indicates representations of GNNs get similar to each others as the number of layers increases. Over-squashing is a difficulty of node representations to contain messages that come from distant neighbors.	Invent something didn't mentioned		
1182	paper_63	how the structure information of graphs different from the positional information of graphs?	Structural information of graphs serves a measure of structural similarity between nodes. The reason is that most existing approaches fail to identify structural similarities between nodes, compared to SAIT that tries to capture structural similarities among nodes by encoding structural information.	Transformers (Vaswani et al., 2017), which have proved to be successful in natural language understanding (Vaswani et al., 2017), computer vision (Dosovitskiy et al., 2020), and biological sequence modeling (Rives et al., 2021), offer the potential to address these issues. Rather than only aggregating local neighborhood information in the message-passing mechanism, the Transformer architecture is able to capture interaction information between any node pair via a single self-attention layer.Moreover, in contrast to GNNs, the Transformer avoids introducing any structural inductive bias at intermediate layers, addressing the expressivity limitation of GNNs. Instead, it encodes structural or positional information about nodes only into input node features, albeit limiting how much information it can learn from the graph structure. Integrating information about the graph structure into the Transformer architecture has thus gained growing attention in the graph representation learning field. However, most existing approaches only encode positional relationships between nodes, rather than explicitly encoding the structural relationships. As a result, they may not identify structural similarities between nodes and could fail to model the structural interaction between nodes (see Figure 1). This could explain why their performance was dominated by sparse GNNs in several tasks (Dwivedi et al., 2022).We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:				
1183	paper_63	What are the advantages of using relative encoding compared to absolute encoding, which performs well?	The advantage of relative encoding compared to absolute encoding is the flexibility of using representations of position or distances into the self-attention mechanism directly. The reason is that self-attentions using absolute encoding only use node features, but self-attention mechanisms with relative encoding are able to utilize representations.	While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer (Dwivedi & Bresson, 2021) provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN (Kreuzer et al., 2021) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding (Shaw et al., 2018) in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. Mialon et al. (2021) propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations (Ying et al., 2021) or by using GNNs to integrate the graph structure (Rong et al., 2020; Jain et al., 2021; Mialon et al., 2021; Shi et al., 2021).	The advantage of relative encoding compared to absolute encoding is the flexibility of using representations of position or distances into the self-attention mechanism directly. The reason is that self-considerations using absolute encoding only use node features, but self-attention mechanisms with relative encoding are able to utilize representations.	Tortured phrases	self-attentions -> self-considerations	
1184	paper_63	What is the meaning of "using graph structures explicitly"?	The meaning of using graph structures explicitly is to explicity incorporate structural information into the self-attention. The reason is that both P3 and P7 state the main contribution of SAT with paraphrasing. P3 indicates that to consider graph structure explicitly is a main idea of SAT, and P7 emphasizes it as to incorporate structural information in the self-attention.	In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:•We reformulate the self-attention mechanism in Vaswani et al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.•We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.•We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.•Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding.The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.				
1185	paper_63	Can kernel functions other than the exponential kernel be applied?	Any kernel that compares a pair of subgraphs can replace the exponential kernel. The reason is that \kappa_{\text{graph}} is defined as \kappa_{\exp} in P6, but P2 states that \kappa_{\text{graph}} can be changed as other kernel that is able to compare a pair of subgraphs.	As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:\text{SA-attn}(v):=\sum_{u\in V}\frac{\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))}{\sum_{w\in V}\kappa_{\text{graph}}(S_{G}(v),S_{G}(w))}f(x_{u}),(5)where S_{G}(v) denotes a subgraph in G centered at a node v associated with node features \mathbf{X} and \kappa_{\text{graph}} can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property.In the rest of the paper, we will consider the following form of \kappa_{\text{graph}} that already includes a large class of expressive and computationally tractable models:\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))=\kappa_{\exp}(\varphi(v,G),\varphi(u,G)),(6)where \varphi(u,G) is a structure extractor that extracts vector representations of some subgraph centered at u with node features \mathbf{X}. We provide several alternatives of the structure extractor below. It is worth noting that our structure-aware self-attention is flexible enough to be combined with any model that generates representations of subgraphs, including GNNs and (differentiable) graph kernels. For notational simplicity, we assume there are no edge attributes, but our method can easily incorporate edge attributes as long as the structure extractor can accommodate them. The edge attributes are consequently not considered in the self-attention computation, but are incorporated into the structure-aware node representations. In the structure extractors presented in this paper, this means that edge attributes were included whenever the base GNN was able to handle edge attributes.				
1186	paper_63	Does graph property prediction task create one representation corresponding to the graph?	The reason is that many existing works for graph property prediction aggregate node representations into a graph representation.	Finally, for graph property prediction, there are various ways to aggregate node-level representations into a graph representation, such as by taking the average or sum. Alternatively, one can use the embedding of a virtual [CLS] node (Jain et al., 2021) that is attached to the input graph without any connectivity to other nodes. We compare these approaches in Section 5.				
1187	paper_63	What are the results we can get after going through the proposed Transformer-based model?	SAT predicts class of nodes and graphs better than other SOTA models.  Also, SAT is more explainable compared to other transformer-based models. The reason is that performance comparision shows SATs performs better than others,  and we can also explain the best range of substructure to consider with minimal hyperparameter tuning.	We show the performance of SATs compared to other GNNs and Transformers in Table 1 and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard.We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer.	SOTA predicts class of nodes and graphs better than other SAT models.  Also, SOTA is more explainable compared to other transformer-based models. The reason is that performance comparision shows SOTAs performs better than others,  and we can also explain the best range of substructure to consider with minimal hyperparameter tuning.	Change concept	Swap SOTA and SAT	
1188	paper_63	What drives SAT possible to study the expressiveness of the output representation?	Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations.  More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor.	The expressive power of graph Transformers compared to classic GNNs has hardly been studied, since the soft structural inductive bias introduced in absolute encoding is generally hard to characterize. Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor, following the injectivity of the attention function with respect to the query:				
1189	paper_63	What is the effective value of small value k?	We find that optimal performance around k=3 for both k-subtree and k-subgraph extractors.  The reason is that Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset.  As k increases till k=3, the performance of k-subtree and k-subgraph extractors gets better. However, as k increases beyond k=4, the performance in k-subtree extractors deteriorated.	The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.				
1190	paper_63	Why did author choose RWPE model to compare the effective k value?	Author chooses RWPE for absolute positional encoding to show the outperformance of SAT is due to its structure-awareness. The reason is that SAT is equivalent to a vanilla Transformer using RWPE that isn't structure-aware if k=0. Hence, the performance improvement with the k growth shows the effectiveness of structure-aware encoding.	While the self-attention in Eq. (5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE (Dwivedi et al., 2022), though any other absolute positional representations, including learnable ones, can also be used.The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.	Author chooses RWPE for absolute positional encoding to show the outperformance of SAT is due to its structure-awareness. The reason is that SAT is equivalent to a fine-tuned Transformer using RWPE that isn't structure-aware if k=0. Hence, the performance improvement with the k growth shows the effectiveness of structure-aware encoding.	Change concept	vanilla transformer -> fine-tuned transformer	
1191	paper_63	Did author experiment only with Mutagenicity dataset to show the interpretability of proposed model?	Author conducted additional experiments for model interpretability in appendix. The reason is that they mentioned it in section 5.	In addition to performance improvement, we show that SAT offers better model interpretability compared to the classic Transformer with only absolute positional encoding. We respectively train a SAT model and a Transformer with a CLS readout on the Mutagenicity dataset, and visualize the attention scores between the [CLS] node and other nodes learned by SAT and the Transformer in Figure 4. The salient difference between the two models is that SAT has structure-aware node embeddings, and thus we can attribute the following interpretability gains to that. While both models manage to identify some chemical motifs known for mutagenicity, such as NO{}_{2} and NH{}_{2}, the attention scores learned by SAT are sparser and more informative, meaning that SAT puts more attention weights on these known mutagenic motifs than the Transformer with RWPE. The vanilla Transformer even fails to put attention on some important atoms such as the H atoms in the NH{}_{2} group. The only H atoms highlighted by SAT are those in the NH{}_{2} group, suggesting that our SAT indeed takes the structure into account. More focus on these discriminative motifs makes the SAT model less influenced by other chemical patterns that commonly exist in the dataset, such as benzene, and thus leads to overall improved performance. More results are provided in the Appendix.				
1192	paper_63	what does "kernel smoother" mean?	Kernel-smoother is a kernel defined on node features to capture local structure of nodes by calculating similarity between node pairs.	As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, and which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as:\text{SA-attn}(v):=\sum_{u\in V}\frac{\kappa_{\text{graph}}(S_{G}(v),S_{G}(u))}{\sum_{w\in V}\kappa_{\text{graph}}(S_{G}(v),S_{G}(w))}f(x_{u}),(5)where S_{G}(v) denotes a subgraph in G centered at a node v associated with node features \mathbf{X} and \kappa_{\text{graph}} can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property.In this work, we address the critical question of how to encode structural information into a Transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of Transformers, which we call the Structure-Aware Transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware Transformers for graph-structured data. Specifically:•We reformulate the self-attention mechanism in Vaswani et al. (2017) as a kernel smoother andextend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node.•We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations.•We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and Transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the node representations which incorporate subgraph information and outperform the base GNN, making it an effortless enhancer of any existing GNN.•Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding.				
1193	paper_64	What does "overfitting" mean?	Over-fitting means that a large size model  has a difficulty to analyze data effectively due to its large number of parameters.	A central issue with applying (2) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size.				
1194	paper_64	What kind of problems will occur if the inverse triple exists?	The existence of inverse triples reduces a large part of the prediction task to memorization of affected triplet pairs.	Link prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In ? (?), a serious flaw was observed in both datasets: The presence of inverse triplet pairs t=(e_{1},r,e_{2}) and t^{\prime}=(e_{2},r^{-1},e_{1}) with t in the training set and t^{\prime} in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by ? (?).				
1195	paper_64	The proposed model accumulates the feature vector of neighboring nodes together regardless of type and direction of an edge, based on message-passing framework. Is this true?	RGCN considers the direction of edges.  The reason is that RGCN updates of an entity representation using multi-graphs with directional relations.  Also, R contains relations both in canonical direction and in inverse direction.	Motivated by these architectures, we define the following simple propagation model for calculating the forward-pass update of an entity or node denoted by v_{i} in a relational (directed and labeled) multi-graph:h_{i}^{(l+1)}=\sigma\left(\sum_{r\in\mathcal{R}}\sum_{j\in\mathcal{N}^{r}_{i}}\frac{1}{c_{i,r}}W_{r}^{(l)}h_{j}^{(l)}+W_{0}^{(l)}h_{i}^{(l)}\right),(2)where \mathcal{N}^{r}_{i} denotes the set of neighbor indices of node i under relation r\in\mathcal{R}. c_{i,r} is a problem-specific normalization constant that can either be learned or chosen in advance (such as c_{i,r}=|\mathcal{N}^{r}_{i}|).Paragraph 10 : 1R contains relations both in canonical direction (e.g. born in) and in inverse direction (e.g. born in inv). 2Note that this represents a simplification of the message passing neural network proposed in (Gilmer et al. 2017) that suffices to include the aforementioned models as special cases.				
1196	paper_64	Why author introduce two regularization methods in this model?	Authors use two regularization methods to handle overfitting issue. The reason is that RGCN has to solve the issue that the number of parameters grows rapidly as the number of relations, and two regularization methods are used to learn the issue.	A central issue with applying (2) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size.Paragraph 10 : To address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: basisand block-diagonal-decomposition. With the basis decomposition, each W(l) r is defined as follows:				
1197	paper_64	How many basis components did author use?	The number of basis components is 100.  The reason is that the number of basis components is the dimension of embeddings over the block size.  We can observe that 500 is the dimension of embedding and 5 is the block size.  Hence, the number of basis is 500/5 = 100.	We evaluate hyperparameter choices on the respective validation splits. We found a normalization constant defined as c_{i,r}=c_{i}=\sum_{r}|\mathcal{N}^{r}_{i}| — in other words, applied across relation types – to work best. For FB15k and WN18, we report results using basis decomposition (Eq. 3) with two basis functions, and a single encoding layer with 200-dimensional embeddings. For FB15k-237, we found block decomposition (Eq. 4) to perform best, using two layers with block dimension 5\times 5 and 500-dimensional embeddings. We regularize the encoder through edge dropout applied before normalization, with dropout rate 0.2 for self-loops and 0.4 for other edges. Using edge droupout makes our training objective similar to that of denoising autoencoders (?). We apply l2 regularization to the decoder with a penalty of 0.01.Paragraph 10 : To address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: basisand block-diagonal-decomposition. With the basis decomposition, each W(l)r is defined as follows:				
1198	paper_64	What is the structural difference when applying RGCN to two tasks(entity classification, link prediction)?	There is a structural difference between RGCN according to its tasks. The reason is that RGCN use softmax classifier to classify entities, supported by P3, whereas it requires a tensor factorization decoder when it comes to predict links as P7.	Our entity classification model, similarly to ? (?), uses softmax classifiers at each node in the graph. The classifiers take node representations supplied by a relational graph convolutional network (R-GCN) and predict the labels. The model, including R-GCN parameters, is learned by optimizing the cross-entropy loss.Our link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult (?). We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e. vanilla DistMult). This improvement is especially large when we consider the more challenging FB15k-237 dataset (?). This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases.				
1199	paper_64	In link prediction task, what method did author use to predict the edges on decoder part?	Authors mainly use vertex representations and a DistMult function to predict links with RGCN on decorder part. The reason is that P0 states that RGCN uses DisMult to predict links, and P1 describes detailed process of RGCN how to predict links with results of encoder using its own scoring function.	In order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity v_{i}\in\mathcal{V} to a real-valued vector e_{i}\in\mathbb{R}^{d}. The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores (subject, relation, object)-triples through a function s:\mathbb{R}^{d}\times\mathcal{R}\times\mathbb{R}^{d}\to\mathbb{R}. Most existing approaches to link prediction (for example, tensor and neural factorization methods (?; ?; ?; ?; ?)) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector e_{i} for every v_{i}\in\mathcal{V} optimized directly in training, we compute representations through an R-GCN encoder with e_{i}=h_{i}^{(L)}, similar to the graph auto-encoder model introduced in ? (?) for unlabeled undirected graphs.Our full link prediction model is schematically depicted in Figure 2(b).Our link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult (?). We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e. vanilla DistMult). This improvement is especially large when we consider the more challenging FB15k-237 dataset (?). This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases.				
1200	paper_64	What does "semi-supervised" mean?	Semi-supervised is to learn information from ground truth labels. The reason is that, for semi-supervised task, RGCN minimizes loss on all labeled nodes ignoring unlabeled nodes.	For (semi-)supervised classification of nodes (entities), we simply stack R-GCN layers of the form (2), with a \mathrm{softmax}(\cdot) activation (per node) on the output of the last layer. We minimize the following cross-entropy loss on all labeled nodes (while ignoring unlabeled nodes):\mathcal{L}=-\sum_{i\in\mathcal{Y}}\sum_{k=1}^{K}t_{ik}\ln h_{ik}^{(L)}\,,(5)where \mathcal{Y} is the set of node indices that have labels and h_{ik}^{(L)} is the k-th entry of the network output for the i-th labeled node. t_{ik} denotes its respective ground truth label. In practice, we train the model using (full-batch) gradient descent techniques. A schematic depiction of our entity classification model is given in Figure 2(a).				
1201	paper_64	Why does a link prediction task require decoder modules unlike a entity classification task?	A decoder is required for RGCN to predict links because it is impossible to model the possibility of edges without decoder. The reason is that the possibility of edges is determined by the scores of corresponding triplets. As RGCN itself does not calculates scores of triples, decoder modules are required to assign scores over them.	Link prediction deals with prediction of new facts (i.e. triples (subject, relation, object)). Formally, the knowledge base is represented by a directed, labeled graph G=(\mathcal{V},\mathcal{E},\mathcal{R}). Rather than the full set of edges \mathcal{E}, we are given only an incomplete subset \hat{\mathcal{E}}. The task is to assign scores f(s,r,o) to possible edges (s,r,o) in order to determine how likely those edges are to belong to \mathcal{E}.In order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity v_{i}\in\mathcal{V} to a real-valued vector e_{i}\in\mathbb{R}^{d}. The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores (subject, relation, object)-triples through a function s:\mathbb{R}^{d}\times\mathcal{R}\times\mathbb{R}^{d}\to\mathbb{R}. Most existing approaches to link prediction (for example, tensor and neural factorization methods (?; ?; ?; ?; ?)) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector e_{i} for every v_{i}\in\mathcal{V} optimized directly in training, we compute representations through an R-GCN encoder with e_{i}=h_{i}^{(L)}, similar to the graph auto-encoder model introduced in ? (?) for unlabeled undirected graphs.Our full link prediction model is schematically depicted in Figure 2(b).				
1202	paper_64	How did author made negative samples at decoder module in link prediction task?	Authors sample \omega negative ones for each positive sample.  They sample by randomly corrupting either the subject or the object of each positive example.	As in previous work on factorization (?; ?), we train the model with negative sampling. For each observed example we sample \omega negative ones. We sample by randomly corrupting either the subject or the object of each positive example.We optimize for cross-entropy loss to push the model to score observable triples higher than the negative ones:\begin{split}\mathcal{L}=-\frac{1}{(1+\omega)|\mathcal{\hat{E}}|}\sum\limits_{(s,r,o,y)\in\mathcal{T}}y\log l\bigl{(}f(s,r,o)\bigr{)}+\\(1-y)\log\bigl{(}1-l\bigl{(}f(s,r,o)\bigr{)}\bigr{)}\,,\end{split}(7)where \mathcal{T} is the total set of real and corrupted triples, l is the logistic sigmoid function, and y is an indicator set to y=1 for positive triples and y=0 for negative ones.				
1203	paper_64	Did datasets used in entity classification different with datasets used in link prediction?	There is no dataset that is commonly used to entity classification and link prediction. The reason is that link prediction use FB15k-237, FB15k, and WN18, while entity classification use AIFB, MUTAG, BGS, and AM.	We evaluate our model on four datasets333http://dws.informatik.uni-mannheim.de/en/research/a-collection-of-benchmark-datasets-for-ml in Resource Description Framework (RDF) format (?): AIFB, MUTAG, BGS, and AM. Relations in these datasets need not necessarily encode directed subject-object relations, but are also used to encode the presence, or absence, of a specific feature for a given entity. In each dataset, the targets to be classified are properties of a group of entities represented as nodes. The exact statistics of the datasets can be found in Table 1. For a more detailed description of the datasets the reader is referred to ? (?). We remove relations that were used to create entity labels: employs and affiliation for AIFB, isMutagenic for MUTAG, hasLithogenesis for BGS, and objectCategory and material for AM.Link prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In ? (?), a serious flaw was observed in both datasets: The presence of inverse triplet pairs t=(e_{1},r,e_{2}) and t^{\prime}=(e_{2},r^{-1},e_{1}) with t in the training set and t^{\prime} in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by ? (?).				
1204	paper_64	What characteristics of MUTAG and BGS datasets made the difference in performance, compare with other datasets(AIFB, AM)?	Compared to AIFB and AM, labeled nodes in MUTAG and BGS only connect to high-degree nodes.  The reason is that labeled entities in MUTAG and BGS are only connected via high-degree hub nodes.	Our model achieves state-of-the-art results on AIFB and AM. To explain the gap in performance on MUTAG and BGS it is important to understand the nature of these datasets. MUTAG is a dataset of molecular graphs, which was later converted to RDF format, where relations either indicate atomic bonds or merely the presence of a certain feature. BGS is a dataset of rock types with hierarchical feature descriptions which was similarly converted to RDF format, where relations encode the presence of a certain feature or feature hierarchy. Labeled entities in MUTAG and BGS are only connected via high-degree hub nodes that encode a certain feature.				
1206	paper_64	Why did author said that RGCN can be "under a differentiable message passing interpretation"?	Since RGCN is a kind of graph neural network, it can be interpreted as a differentiable message passing interpretation. The reason is that RGCN is a sub-class of graph neural networks which are special cases of differentiable message-passing framework.	R-GCNs can further be seen as a sub-class of message passing neural networks (?), which encompass a number of previous neural models for graphs, including GCNs, under a differentiable message passing interpretation.Our model is primarily motivated as an extension of GCNs that operate on local graph neighborhoods (?; ?) to large-scale relational data. These and related methods such as graph neural networks (?) can be understood as special cases of a simple differentiable message-passing framework (?):h_{i}^{(l+1)}=\sigma\left(\sum_{m\in\mathcal{M}_{i}}g_{m}(h_{i}^{(l)},h_{j}^{(l)})\right),(1)where h_{i}^{(l)}\in\mathbb{R}^{d^{(l)}} is the hidden state of node v_{i} in the l-th layer of the neural network, with d^{(l)} being the dimensionality of this layer’s representations. Incoming messages of the form g_{m}(\cdot,\cdot) are accumulated and passed through an element-wise activation function \sigma(\cdot), such as the \mathrm{ReLU}(\cdot)=\max(0,\cdot).222Note that this represents a simplification of the message passing neural network proposed in (?) that suffices to include the aforementioned models as special cases. \mathcal{M}_{i} denotes the set of incoming messages for node v_{i} and is often chosen to be identical to the set of incoming edges. g_{m}(\cdot,\cdot) is typically chosen to be a (message-specific) neural network-like function or simply a linear transformation g_{m}(h_{i},h_{j})=Wh_{j} with a weight matrix W such as in ? (?).				
1207	paper_65	In what ways can it be said that the concatenation acts as a skip connection?	Skip connection is to consider information from different search depths or layers simultaneously. GraphSAGE use a set of weight matrices and concatenation to consider information from diverse search depths.  It can be interpreted as a skip connection. The reason is that a set of weight matrices are used to propagate information  between different layers of the model or search depths, while considering different search depth is a kind of skip-connection.	In this section, we describe the embedding generation, or forward propagation algorithm (Algorithm 1), which assumes that the model has already been trained and that the parameters are fixed.In particular, we assume that we have learned the parameters of K aggregator functions (denoted \textsc{aggregate}_{k},\forall k\in\{1,...,K\}), which aggregate information from node neighbors, as well as a set of weight matrices \mathbf{W}^{k},\forall k\in\{1,...,K\}, which are used to propagate information between different layers of the model or “search depths”.Section 3.2 describes how we train these parameters.Instead of training a distinct embedding vector for each node, we train a set of aggregator functions that learn to aggregate feature information from a node’s local neighborhood (Figure 1).Each aggregator function aggregates information from a different number of hops, or search depth, away from a given node.At test, or inference time, we use our trained system to generate embeddings for entirely unseen nodes by applying the learned aggregation functions.Following previous work on generating node embeddings, we design an unsupervised loss function that allows GraphSAGE to be trained without task-specific supervision.We also show that GraphSAGE can be trained in a fully supervised manner.Paragraph 10 :				
1208	paper_65	How did authro decide the size of the neighborhood?	The size of neighborhoods is set as 25 for 1-hop and 10 for 2-hop.  The reason is that authors state that neighborhood sample sizes S1 = 25 and S2 = 10.	Paragraph 10 :				
1209	paper_65	Does "previously unseen data" mean node that did not appear on training data?	Unseen data indicates node that is not contained in training data.  The reason is that the purpose of this paper is to generate embeddings quickly for the systems which constantly encounter entirely new nodes and graphs. Also, authors train algorithms with 2000-2004 data, while test is conducted on 2005 data.	However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model.Our first two experiments are on classifying nodes in evolving information graphs, a task that is especially relevant to high-throughput production systems, which constantly encounter unseen data.Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30].	Unseen data indicates node that is not contained in preparing information.  The reason is that the purpose of this paper is to generate embeddings quickly for the systems which constantly encounter entirely new nodes and graphs. Also, authors train algorithms with 2000-2004 data, while test is conducted on 2005 data.	Tortured phrases	training data -> preparing information	
1210	paper_65	How did author sampling a node's local neighborhood features to generate the embeddings?	Authors sample the required neighborhood sets (up to depth K).	To extend Algorithm 1 to the minibatch setting, given a set of input nodes, we first forward sample the required neighborhood sets (up to depth K) and then we run the inner loop (line 3 in Algorithm 1), but instead of iterating over all nodes, we compute only the representations that are necessary to satisfy the recursion at each depth (Appendix A contains complete minibatch pseudocode).				
1211	paper_65	What are some examples of unseen nodes in the real world?	New posts on Reddit, new users and videos on Youtube  are examples of unseen data.	However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model.				
1212	paper_65	How can LSTM that are not symmetric deal with neighborhoods that have no order?	Authors permute neighbors of nodes to operate LSTMs to deal with unordered neighbor set.	LSTM aggregator. We also examined a more complex aggregator based on an LSTM architecture [14].Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not inherently symmetric (i.e., they are not permutation invariant), since they process their inputs in a sequential manner.We adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors.				
1213	paper_65	Is 'perform entirely unseen graphs in PPI' means test the proposed model on new PPI graphs?	The reason is that authors test on new graphs that is not shown in training.	We test the performance of GraphSAGE on three benchmark tasks: (i) classifying academic papers into different subjects using the Web of Sciencecitation dataset, (ii) classifying Reddit posts as belonging to different communities, and (iii) classifying protein functions across various biological protein-protein interaction (PPI) graphs. Sections 4.1 and 4.2 summarize the datasets, and the supplementary material contains additional information.In all these experiments, we perform predictions on nodes that are not seen during training, and, in the case of the PPI dataset, we test on entirely unseen graphs.				
1214	paper_65	How author create embeddings for each post in Reddit data?	For Reddit data, authors encode Glove word vectors with GraphSAGE.   They concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post.	Reddit data.In our second task, we predict which community different Reddit posts belong to.Reddit is a large online discussion forum where users post and comment on content in different topical communities.We constructed a graph dataset from Reddit posts made in the month of September, 2014.The node label in this case is the community, or “subreddit”, that a post belongs to.We sampled 50 large communities and built a post-to-post graph, connecting posts if the same user comments on both.In total this dataset contains 232,965 posts with an average degree of 492.We use the first 20 days for training and the remaining days for testing (with 30% used for validation).For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post.				
1215	paper_65	The paper test classifying nodes on evolving graphs with 2 datasets(Citation and Reddit). What is the difference between two experiments with each dataset?	Citation and Reddit data differ in the semantic of their edges and word vectors. The reason is that an edge in citation indicates a paper cite others, whereas an edge in Reddit indicates that they're written by a common writer. Also, Citation and Reddit data use GenSim and Glove word vectors as their feature, respectively.	Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30].Reddit data.In our second task, we predict which community different Reddit posts belong to.Reddit is a large online discussion forum where users post and comment on content in different topical communities.We constructed a graph dataset from Reddit posts made in the month of September, 2014.The node label in this case is the community, or “subreddit”, that a post belongs to.We sampled 50 large communities and built a post-to-post graph, connecting posts if the same user comments on both.In total this dataset contains 232,965 posts with an average degree of 492.We use the first 20 days for training and the remaining days for testing (with 30% used for validation).For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post.				
1216	paper_65	What are some examples of roles of node?	The roles of nodes can be their protein function or categories. The reason is this paper classifies the function of proteins for PPI network, while classifies categories of nodes in Reddit network and Citation network.	We now consider the task of generalizing across graphs, which requires learning about node roles rather than community structure.We classify protein roles—in terms of their cellular functions from gene ontology—in variousprotein-protein interaction (PPI) graphs, with each graph corresponding to a different human tissue [41].We use positional gene sets, motif gene sets and immunological signatures as features and geneontology sets as labels (121 in total), collected from the Molecular Signatures Database [34].The average graph contains 2373 nodes, with an average degree of 28.8.We train all algorithms on 20 graphs and then average prediction F1 scores on two test graphs (with two other graphs used for validation).We evaluate our algorithm on three node-classification benchmarks, which test GraphSAGE’s ability to generate useful embeddings on unseen data.We use two evolving document graphs based on citation data and Reddit post data (predicting paper and post categories, respectively), and a multi-graph generalization experiment based on a dataset of protein-protein interactions (predicting protein functions).Using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification F1-scores by an average of 51% compared to using node features alone and GraphSAGE consistently outperforms a strong, transductive baseline [28], despite this baseline taking ∼100×{\sim}100\times∼ 100 × longer to run on unseen nodes.We also show that the new aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks [17].Lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that GraphSAGE is capable of learning structural information about a node’s role in a graph, despite the fact that it is inherently based on features (Section 5).				
1217	paper_65	How many experimental setting factors have been considered in experiments?	As far as my knowledge, seven factors are considered in experiments. The reason is that authors state rectified linear units, K, two sample sizes, identical implementation of minibatch iterators, loss function, and neighborhood sampler.	Experimental set-up.To contextualize the empirical results on our inductive benchmarks, we compare against four baselines:a random classifer, a logistic regression feature-based classifier (that ignores graph structure), the DeepWalk algorithm [28] as a representative factorization-based approach, and aconcatenation of the raw features and DeepWalk embeddings.We also compare four variants of GraphSAGE that use the different aggregator functions (Section 3.3).Since, the “convolutional” variant of GraphSAGE is an extended, inductive version of Kipf et al’s semi-supervised GCN [17], we term this variant GraphSAGE-GCN.We test unsupervised variants of GraphSAGE  trained according to the loss in Equation (1), as well as supervised variants that are trained directly on classification cross-entropy loss.For all the GraphSAGE variants we used rectified linear units as the non-linearity and set K=2 with neighborhood sample sizes S_{1}=25 and S_{2}=10 (see Section 4.4 for sensitivity analyses).All models were implemented in TensorFlow [1] with the Adam optimizer [16] (except DeepWalk, which performed better with the vanilla gradient descent optimizer).We designed our experiments with the goals of (i) verifying the improvement of GraphSAGE over the baseline approaches (i.e., raw features and DeepWalk) and (ii) providing a rigorous comparison of the different GraphSAGE aggregator architectures.In order to provide a fair comparison, all models share an identical implementation of their minibatch iterators, loss function and neighborhood sampler (when applicable).Moreover, in order to guard against unintentional “hyperparameter hacking” in the comparisons between GraphSAGE aggregators, we sweep over the same set of hyperparameters for all GraphSAGE variants (choosing the best setting for each variant according to performance on a validation set).The set of possible hyperparameter values was determined on early validation tests using subsets of the citation and Reddit data that we then discarded from our analyses.The appendix contains further implementation details.555Code and links to the datasets: http://snap.stanford.edu/graphsage/				
1218	paper_65	Does the graph structure include the proportion of triangles and clustering coefficient of a node ?	The proportion of triangles and clustering coefficient of a node implies information about graphs and serves as a building block for many more complicated structural motifs.  The reason is that  clustering coefficient, the proportion of triangles that are closed within the node’s 1-hop neighborhood, is used as a popular measure of how clustered a node’s local neighborhood is.	In this section, we probe the expressive capabilities of GraphSAGE in order toprovide insight into how GraphSAGE can learn about graph structure, even though it is inherently based on features.As a case-study, we consider whether GraphSAGE can learn to predict the clustering coefficient of a node, i.e., the proportion of triangles that are closed within the node’s 1-hop neighborhood [38]. The clustering coefficient is a popular measure of how clustered a node’s local neighborhood is, and it serves as a building block for many more complicated structural motifs [3]. We can show that Algorithm 1 is capable of approximating clustering coefficients to an arbitrary degree of precision:				
1219	paper_66	Instead of concatenate the three parts, what can be more effective way to consider these three behaviors collectively?	Retention indicates a state that whether a set of apps is installed now.  However, other behavior data is operation composed of apps and corresponding dates.  The result is that retention is a set of apps installed on one’s phone at present, whereas installation and uninstallation operations are composed of installed apps and corresponding dates.	As stated in Section 3.1, behaviors of each user are preprocessed into one’s “retention” and four sequences defined as follows.				
1220	paper_66	Did author consider only three user behaviors on mobile usage?	Authors only consider three behaviors: retention, installation, and un-installation. The reason is that authors note the three behaviors, and also model behavior type embeddings of those 3 behaviors.	User behaviors on mobile apps usage contain rich preference information and have been used in a variety of applications (Lu et al., 2014). The most significant of which is app install advertisements (Gogel, 2018; Lee and Shin, 2016) and mobile app recommendations (Zhuet al., 2014). Yahoo posted a large scale prediction engine for app install advertising based on a two-step logistic regression model considering user features generated from behaviors on apps (Bhamidipatiet al., 2017). For reducing sparseness, Yahoo also classifies apps into predefined interest taxonomies when understanding app usage patterns (Radosavljevicet al., 2016). Usage patterns of apps are learned for app purchase recommendations with a Deep Memory Network(Gligorijevic et al., 2018). Beyond app install advertising, users’ app-installation behaviors are also used for news recommendations (Liuet al., 2017), where the knowledge of the neighborhood of the cold-start users is transferred from an APP domain to a new domain.A large survey on mobile app user behaviors across main app markets around the world was conducted to instruct cross-country app competitions and analyze the challenges for software engineering (Lim et al., 2014).Paragraph 10 : The transformer encoder part receives the user retention, shared app embeddings, date embeddings, and behavior type embeddings (retention, installation, and  ninstallation) as input. Thus, the inputs altogether include complete information on when and whether users install or uninstall what apps as well as their current status of app usage. The date embeddings make the transformer suitable for modeling user behaviors that are low-frequency and distribute unevenly over time. Besides, we also introduce a masked app prediction task like BERT [9] to help extract information more productively.				
1221	paper_66	How do author categorize each apps in this paper?	Authors select four typical categories in the next week's prediction.	Paragraph 10 : The four categories we selected in the next week’s prediction is four typical niche ones that need app advertising to enlarge their user base. Apps from these categories are also the long-tailed ones suffer from serious sparsity. The average installation rates for these four categories are approximately 600, 400, 25, and 300 per million people, respectively.				
1222	paper_66	What does "long-tailed app" mean?	Long-tailed application is a type of software or service that is not installed in a number of users.  As it is installed in a few users, it can easily represent the properties and behaviors of users.	•Retention, installation and uninstallation need to be modeled collectively. They represent the preference of users from different aspects, and building representations for the three parts separately and then concatenating them may limit the performance. For example, for users who have installed multiple games, uninstalling a game app may only indicate that she has finished the game and wants to start a new one. While for a user who has not installed other games, immediately uninstalling after installation may suggest that she does not like this kind of game at all. Modeling such complex relationships using traditional recurrent neural networks (RNNs) is challenging.•Actions of (un)installing apps are low-frequency and unevenly distributed over time. Figure 1 presents a demo of app installation and uninstallation records of a user. As excitement over the new phone fades, most users only install or uninstall apps when they need to. Moreover, users usually do not operate for even a month but may suddenly install or uninstall several apps in a single day. In this case, various intervals between every two behaviors are not omittable. Although RNN-based models have succeeded in analyzing user activities (Hidasi et al., 2016; Liet al., 2017), the behaviors in those scenarios are usually with notably higher-frequency and nearly even distribution over time. Therefore, traditional RNNs may not perform well for this task.•Many long-tailed apps suffer from serious sparsity. Popular apps like Wechat and Alipay have been installed on almost all the smartphones in China, while long-tailed apps may only have a few hundreds of installations among one million users. However, user’s behaviors over the long-tailed apps often reflect one’s personalized interests better. Building effective user representations need to utilize the information from long-tailed apps without suffering from severe sparsity.				
1223	paper_66	What is the difference between an online experiment and an offline experiment?	Authors conduct online A/B test, whereas coduct three downstream applications for offline experiments. The reason is that they conduct online feed recommendation A/B testing from 2020-02-01 to 2020-02-10, in the “Good Morning” tab of Tencent Mobile Manager and the “Find” tab of Tencent Wi-Fi Manager.  For offline experiments, they compare the baseline with four different versions of AETN in three typical downstream offline experiments.	In this section, we demonstrate the offline performance of AETN in generating general-purpose user embeddings. We compare the baseline with four different versions of AETN in three typical downstream offline experiments. Then we show that the auxiliary retention reconstruction task for the autoencoder part can help the convergency of the transformer parts. Finally, we compare the user embeddings generated by the baseline and AETN intuitively.We conduct our offline experiments on three typical downstream applications, including applications from both related domains and a different domain. The evaluation tasks are as follows:To further verify the effectiveness of the output user embeddings, we conduct online feed recommendation A/B testing from 2020-02-01 to 2020-02-10, in the “Good Morning” tab of Tencent Mobile Manager and the “Find” tab of Tencent Wi-Fi Manager. We split online A/B test traffic by userIDs evenly for the tested models. We evaluate the base models, models with DAE embeddings, and models with AETN embeddings. The improvement results compared with the base models are reported in Table 2.				
1224	paper_66	How did author reduce the noise in user behavior data?	For each user, authors keep the most recent 10 installation or uninstallation operations in a week.	We need to preprocess the user data into a format suitable for subsequent models to handle and also reduce the noise in data. After data preprocessing, each user is represented with one’s “retention” and four sequences. “Retention” is a set of apps installed on one’s phone at present. Two of the sequences, representing recent “installation” operations, are composed of installed apps and corresponding dates. The rest two sequences represent recent “uninstallation” operations. To reduce the noise in user behaviors, we keep the most recent 10 installation or uninstallation operations in a week for each user.				
1225	paper_66	What are the two roles of autoencoder in proposed model?	Two roles of autoencoder in proposed models are to help to learn co-occurunce relationship among applications and to help transformer encoder to learn effective user retention representations. The reason is that autoencoder helps to learn high-quality app embeddings from the co-occurrence relationship. Meanwhile, the autoencoder helps transformers to encode effective low-dimensional representations.	The role of this autoencoder is two-folds. Firstly, it helps to learn high-quality app embeddings from the co-occurrence relationship of apps. The weight matrix of the first hidden layer \mathbf{W}^{(1)} acts as the shared app embedding matrix \mathbf{W}^{a} for the whole network, i.e., we have(2)\mathbf{W}^{a}=\mathbf{W}^{(1)}\in\mathbb{R}^{M\times{d_{model}}}.To further alleviate the problem of sparsity, the shared app embedding matrix is carefully designed and tied with some other weight matrices. More details are provided in Section 4.3.Secondly, this autoencoder provides effective representations of user retention for the transformer part. The transformer encoder part needs to be fed with the retention for compressing long-term interests into user embeddings. However, retention is originally in the form of high-dimensional sparse features. This autoencoder encodes retention into the first hidden layer \bm{x}^{(1)}\in\mathbb{R}^{d_{model}}. As a low-dimensional dense encoding, \bm{x}^{(1)} plays an important role in the transformer encoder part.				
1226	paper_66	Which part of the proposed model helps to solve the sparsity problem?	Both transformers and retention autoencoder parts try to solve the sparsity problem by using tied weight matrices. The reason of this is that \mathbf{W}^{\Omega}=\mathbf{W}^{\Theta}=\mathbf{W}^{\Phi}=\mathbf{W}^{(4)}={\mathbf{W}^{a}}^{\mathrm{T}}.  Since \mathbf{W}^{\Omega} is used in transformer part and \mathbf{W}^{(4)} is used in autoencoder part, both of them try to solve the sparsity problem.	Secondly, this autoencoder provides effective representations of user retention for the transformer part. The transformer encoder part needs to be fed with the retention for compressing long-term interests into user embeddings. However, retention is originally in the form of high-dimensional sparse features. This autoencoder encodes retention into the first hidden layer \bm{x}^{(1)}\in\mathbb{R}^{d_{model}}. As a low-dimensional dense encoding, \bm{x}^{(1)} plays an important role in the transformer encoder part.We carefully design our weight matrices for several parts of the model, which helps to solve the sparsity problem and tightly couple the autoencoder part and the transformer parts. As shown in Figure 5, the app embeddings are built based on both the app ID and its corresponding category ID. Even if the usage of some app is gravely sparse, its category can still provide valid information. This setting helps to overcome the problem of sparsity.As introduced previously, we repeatedly use the M\times d_{model} embedding matrix for apps, i.e., at the input and output of the retention autoencoder, the input of the transformer encoder, the output for the masked app prediction, the output of the transformer decoder, as well as the reconstruction part for retention from the user embeddings (bottleneck). We tie the weight matrices of all these parts together, i.e.,(3)\displaystyle\mathbf{W}^{\Omega}=\mathbf{W}^{\Theta}=\mathbf{W}^{\Phi}=\mathbf{W}^{(4)}={\mathbf{W}^{a}}^{\mathrm{T}}.We reduce the total number of parameters by tying weight matrices of the above layers, which benefits of overcoming the problem of sparsity. Moreover, weight tying benefits the backpropagation of the gradient and speeds the convergence.				
1227	paper_66	Where did author apply "mask apps in installation and uninstallation"?	To calculate mask loss in masked app prediction stage, authors randomly mask apps in installation and uninstallation but keep the corresponding date and behavior type. The reason is that transformer encoders mask apps randomly in masked app prediction.	Task #3: Masked App Prediction. This task is similar to the “Masked LM” task in BERT (Devlinet al., 2019). We randomly mask apps in installation and uninstallation but keep the corresponding date and behavior type. The transformer encoder is trained only to predict the masked apps. For simplicity, we just follow the masking rate in BERT and abandon the “random replacement or keep”. We calculate the loss of this task, denoted as \mathcal{L}_{mask}, by averaging the softmax cross-entropy loss of every masked app.				
1228	paper_67	Are the three stages sequentially conducted in the model?	It is hard to see the three stages are conducted sequentially.  As the basic-level masking and phrase-level masking both masks basic language units as input, those two stages are conducted distinctly.	The first learning stage is to use basic level masking, It treat a sentence as a sequence of basic Language unit, for English, the basic language unit is word, and for Chinese, the basic language unit is Chinese Character. In the training process, We randomly mask 15 percents of basic language units, and using other basic units in the sentence as inputs, and train a transformer to predict the mask units.Based on basic level mask, we can obtain a basic word representation. Because it is trained on a random mask of basic semantic units, high level semantic knowledge is hard to be fully modeled.The second stage is to employ phrase-level masking. Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences, and use some language dependent segmentation tools to get the word/phrase information in other language such as Chinese. In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase. At this stage, phrase information is encoded into the word embedding.				
1229	paper_67	How did ERINE model incorporate knowledge into the language model?	ERNIE use multi-level masking to incorporate knowledge into language model, which includes entity-level masking and phrase-level masking. The reason is that to learn enhanced language representation by entity-level masking andphrase-level masking is a main purpose of ERNIE.	In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.Paragraph 10 : We present a novel language representation	ERNIE use multi-level masking to incorporate knowledge into language model, which includes only phrase-level masking. The reason is that to learn enhanced language representation by phrase-level masking is a main purpose of ERNIE.	Change concept	which includes entity-level masking and phrase-level masking -> includes only entity-level masking	
1230	paper_67	How can author claim that language models perform better if they use prior knowledge as well as context?	People share a lot of prior knowledge when they talk each other. Also, they assume others also know the prior knowledge such as ”Harry Potter is a series of fantasy novels written by J.  Rowling”. Hence, to make language model similar to human dialogue, it is important to use prior knowledge as well as context. The reason is that  they metioned it is intuitive that if the model learns more about prior knowledge,  with using the example of Harry Potter.	The vast majority of these studies model the representations by predicting the missing word only through the contexts. These works do not consider the prior knowledge in the sentence.For example, In the sentence ” Harry Potter is a series of fantasy novels written by J. K. Rowling”. Harry Potter is a novel name and J. K. Rowling is the writer. It is easy for the model to predict the missing word of the entity  Harry Potter by word collocations inside this entity without the help of long contexts.The model cannot predict  Harry Potter according to the relationship between  Harry Potter and J. K. Rowling.It is intuitive that if the model learns more about prior knowledge, the model can obtain more reliable language representation.				
1231	paper_67	How did the knowledge masking strategies of proposed model different with the basic making strategy?	Compared to basic masking strategy, ERNIE use knowledge masking strategies. It takes a phrase or a entity as one unit and masks words in a same unit at once.	In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.				
1232	paper_67	Did author also test the Erine to not chinese data(eg, English data)?	It only test on chinese data. The reason is that ERNIE is applied to only 5 Chinese NLP tasks.	ERNIE is applied to 5 Chinese NLP tasks, including natural language inference, semantic similarity, named entity recognition, sentiment analysis, and question answering.				
1233	paper_67	How did previous models incorporate the knowledge embedding directly to language model?	Existing models incorporate the knowledge embedding by pre-training process. They use these knowledge embedding as their initialized representation. After the process, self-attention model is also used to capture the contextual information. The reason is that GPT-2 and XLM add information by pre-training.	Language representation pre-training Mikolov et al. (2013); Devlin et al. (2018) has been shown effective for improving many natural language processing tasks such as named entity recognition, sentiment analysis, and question answering.In order to get reliable word representation, neural language models are designed to learn word co-occurrence and then obtain word embedding with unsupervised learning.The methods in Word2Vec Mikolov et al. (2013) and Glove Pennington et al. (2014) represent words as vectors, where similar words have similar word representations. These word representations provide an initialization for the word vectors in other deep learning models. Recently, lots of works such as Cove McCann et al. (2017), Elmo Peters et al. (2018), GPT Radford et al. (2018) and BERT Devlin et al. (2018) improved word representation via different strategies, which has been shown to be more effective for down-stream natural language processing tasks.Some other researchers try to add more information based on these models. MT-DNN Liu et al. (2019) combine pre-training learning and multi-task learning to improve the performances over several different tasks in GLUE Wang et al. (2018). GPT-2 Radford et al. (2019) adds task information into the pre-training process and adapt their model to zero-shot tasks. XLM Lample and Conneau (2019) adds language embedding to the pre-training process which achieved better results in cross-lingual tasks.ERNIE use multi-layer Transformer Vaswani et al. (2017) as basic encoder like previous pre-traning model such as GPT, BERT and XLM.The Transformer can capture the contextual information for each token in the sentence via self-attention, and generates a sequence of contextual embeddings.				
1234	paper_67	There will be differences in basic units depending on the language. As a result, what should be changed in the model, and did the author consider that part?	To consider differences in basic language unit between english and chinese, authors set the basic units per language differently. Then, they use some language dependent segmentation tools to get the word/phrase information.	The first learning stage is to use basic level masking, It treat a sentence as a sequence of basic Language unit, for English, the basic language unit is word, and for Chinese, the basic language unit is Chinese Character. In the training process, We randomly mask 15 percents of basic language units, and using other basic units in the sentence as inputs, and train a transformer to predict the mask units.Based on basic level mask, we can obtain a basic word representation. Because it is trained on a random mask of basic semantic units, high level semantic knowledge is hard to be fully modeled.The second stage is to employ phrase-level masking. Phrase is a small group of words or characters together acting as a conceptual unit. For English, we use lexical analysis and chunking tools to get the boundary of phrases in the sentences, and use some language dependent segmentation tools to get the word/phrase information in other language such as Chinese. In phrase-level mask stage, we also use basic language units as training input, unlike random basic units mask, this time we randomly select a few phrases in the sentence, mask and predict all the basic units in the same phrase. At this stage, phrase information is encoded into the word embedding.				
1235	paper_67	Compared with phrase-level masking, what are the advantages of entity-level phrase?	Compared to entity-level masking, phrase-level masking make models have better generalization and adaptability. Because it can infer longer semantic dependency by masking words in the same unit at once.	In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.				
1236	paper_70	What is the instance of 'paired examples'?	[Paired training data consists of training examples ({xi, yi}^N i=1), where the correspondence between xiand yi exists.  An instance of 'paired examples' is labels↔photos from the CMP Facade Database.	This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined.Figure 8 shows some example results on other paired datasets used in “pix2pix” [22], such as architectural labels\leftrightarrowphotos from the CMP Facade Database [40], and edges\leftrightarrowshoes from the UT Zappos50Kdataset [60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision.	[Paired preparation information consists of training examples ({xi, yi}^N i=1), where the correspondence between xiand yi exists.  An instance of 'paired examples' is labels↔photos from the CMP Facade Database.	Tortured phrases	training data -> preparation information	
1237	paper_70	What is the definition of 'cycle consistency loss'?	[Cyclic consistency implies that for each image x from domain X, the image translation cycle should be able to bring x back to the original image.  The data can be of any nature other than an image as well.	Cycle ConsistencyThe idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward consistency has been a standard trick for decades [24, 48]. In the language domain, verifying and improving translations via “back translation and reconciliation” is a technique used by human translators [3] (including, humorously, by Mark Twain [51]), as well as by machines [17].More recently, higher-order cycle consistency has been used instructure from motion [61],3D shape matching [21], co-segmentation [55], dense semantic alignment [65, 64], and depth estimation [14]. Of these, Zhou et al. [64] and Godard et al. [14] are most similar to our work, as they use a cycle consistency loss as a way of using transitivity to supervise CNN training.In this work, we are introducing a similar loss to push G and F to be consistent with each other. Concurrent with our work, in these same proceedings, Yi et al. [59] independently use a similar objective for unpaired image-to-image translation, inspired by dual learning in machine translation [17].Adversarial training can, in theory, learn mappings G and F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this requires G and F to be stochastic functions) [15]. However, with large enough capacity, a network can map the same set of input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. Thus, adversarial losses alone cannot guarantee that the learned function can map an individual input x_{i} to a desired output y_{i}. To further reduce the space of possible mapping functions, we argue that the learned mapping functions should be cycle-consistent: as shown in Figure 3 (b), for each image x from domain X, the image translation cycle should be able to bring x back to the original image, i.e., x\rightarrow G(x)\rightarrow F(G(x))\approx x. We call this forward cycle consistency. Similarly, as illustrated in Figure 3 (c), for each image y from domain Y, G and F should also satisfy backward cycle consistency: y\rightarrow F(y)\rightarrow G(F(y))\approx y.We incentivize this behavior using a cycle consistency loss:\displaystyle\mathcal{L}_{\text{cyc}}(G,F)=\displaystyle\ \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}]\displaystyle+\displaystyle\ \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}].(2)In preliminary experiments, we also tried replacing the L1 norm in this loss with an adversarial loss between F(G(x)) and x, and between G(F(y)) and y, but did not observe improved performance.These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F(G(x))\approx x and G(F(y))\approx y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation.				
1238	paper_70	Why does this method use patchGAN? Are there any benefits using this model?	[The paper uses 70 × 70 PatchGANs for the discriminator networks, which aim to classify whether 70 × 70 overlapping image patches are real or fake.  The advantage of using such a patch-level discriminator architecture is that it has fewer parameters than a full-image discriminator and can work on arbitrarily sized images in a fully convolutional fashion.	We adopt the architecture for our generative networks from Johnson et al. [23] who have shown impressive results for neural style transfer and super-resolution. This network contains three convolutions, several residual blocks [18], two fractionally-strided convolutions with stride \frac{1}{2}, and one convolution that maps features to RGB. We use 6 blocks for 128\times 128 images and 9 blocks for 256\times 256 and higher-resolution training images. Similar to Johnson et al. [23], we use instance normalization [53]. For the discriminator networks we use 70\times 70 PatchGANs [22, 30, 29], which aim to classify whether 70\times 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion [22].				
1239	paper_70	What's the difference between pix2pix and CycleGAN?	[CycleGAN builds on the “pix2pix” framework, which uses a conditional generative adversarial network to learn a mapping from input to output images.  However, unlike pix2pix, the paper learns the mapping without paired training examples.  The paper trains CycleGAN and pix2pix at 512 by 512 resolution and observes the comparable performance: maps\rightarrowaerial photos: CycleGAN: 37. 5% +/- 3. 6% and pix2pix: 33. 9% +/-  3.  aerial photos\rightarrowmaps: CycleGAN: 16. 5% +/- 4. 1% and pix2pix: 8. 5% +/- 2.	Table 1 reports performance regarding the AMT perceptual realism task. Here, we see that our method can fool participants on around a quarter of trials, in both the maps\rightarrowaerial photos direction and the aerial photos\rightarrowmaps direction at 256\times 256 resolution333We also train CycleGAN and pix2pix at 512\times 512 resolution, and observe the comparable performance: maps\rightarrowaerial photos: CycleGAN: 37.5\%\pm 3.6\% and pix2pix: 33.9\%\pm 3.1\%; aerial photos\rightarrowmaps: CycleGAN: 16.5\%\pm 4.1\% and pix2pix: 8.5\%\pm 2.6\%. All the baselines almost never fooled participants.Image-to-Image TranslationThe idea of image-to-image translation goes back at least to Hertzmann et al.’s Image Analogies [19], who employ a non-parametric texture model [10] on a single input-output training image pair.More recent approaches use a dataset of input-output examples to learn a parametric translation function using CNNs (e.g., [33]). Our approach builds on the “pix2pix” framework of Isola et al. [22], which uses a conditional generative adversarial network [16] to learn a mapping from input to output images. Similar ideas have been applied to various tasks such as generating photographs from sketches [44] or from attribute and semantic layouts [25]. However, unlike the above prior work, we learn the mapping without paired training examples.				
1240	paper_70	Why is this method unable to achieve compelling results with any of the baselines in Figure 5, 6?	[The paper’s generator architectures which are tailored for good performance on the appearance changes might cause failures.  For example, on the task of dog→cat transfiguration, the learned translation degenerates into making minimal changes to the input.  Some failure cases are caused by the distribution characteristics of the training datasets.  For example, the paper’s method has got confused in the horse→zebra example, because the paper’s model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra.  The paper also observes a lingering gap between the results achievable with paired training data and those achieved by the paper’s unpaired method.  In some cases, this gap may be hard or even impossible to close: for example, the paper’s method sometimes permutes the labels for tree and building in the output of the photos→labels task.  Resolving this ambiguity may require some form of weak semantic supervision.  Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.	Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.Some failure cases are caused by the distribution characteristics of the training datasets. For example, our method has got confused in the horse \rightarrow zebra example (Figure 17, right), because our model was trained on the wild horse and zebra synsets of ImageNet, which does not contain images of a person riding a horse or zebra.[We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard – or even impossible – to close: for example, our method sometimes permutes the labels for tree and building in the output of the photos→labels task. Resolving this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.]				
1241	paper_70	How can we check if the model suffers from mode collapse?	[If all input images map to the same output image and the optimization fails to make progress, then the model is suffering from “mode collapse”.  For example, the paper evaluates its method with the cycle loss in only one direction: GAN + forward cycle loss, or GAN + backward cycle loss (in Equation 2) and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.	In Table 4 and Table 5, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}], or GAN + backward cycle loss \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}] (Equation 2) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples.[We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y . We may train a mapping G : X → Y such that the output yˆ = G(x), x ∈ X, is indistinguishable from images y ∈ Y by an adversary trained to classify yˆ apart from y. In theory, this objective can induce an output distribution over yˆ that matches the empirical distribution pdata(y) (in general, this requires G to be stochastic). The optimal G thereby translates the domain X to a domain Yˆ distributed identically to Y . However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over yˆ. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the wellknown problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress.]				
1242	paper_70	For other than the painting -> photo case (e.g. photo -> painting), does L_identity still work well?	[Without L_identitiy,  the generator G and F are free to change the tint of input images when there is no need to.  For example, when learning the mapping between Monet’s paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss.  The same could happen for the case between Flickr photographs and Monet’s paintings, the generator may map photographs taken during sunset to paintings of daytime.  Therefore, L_identity might work well in this case as well, but it is not explicitly stated in the paper or proven through experiments.	Without \mathcal{L}_{\text{identity}}, the generator G and F are free to change the tint of input images when there is no need to. For example, when learning the mapping between Monet’s paintings and Flickr photographs, the generator often maps paintings of daytime to photographs taken during sunset, because such a mapping may be equally valid under the adversarial loss and cycle consistency loss.The effect of this identity mapping loss are shown in Figure 9.				
1243	paper_70	What is the instance of the 'geometric changes'?	[The paper has explored tasks that require geometric changes, with little success.  For example, on the task of dog→cat transfiguration, the learned translation degenerates into making minimal changes to the input.  This failure might be caused by the paper’s generator architectures which are tailored for good performance on the appearance changes.  This implies that a geometric change doesn’t include a change in appearance, but the paper doesn't explicitly mention an instance of "geometric change". 	Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.				
1244	paper_70	How could we handle more varied and extreme transformations in the unsupervised setting?	[The paper mentions that the handling of more varied and extreme transformations, especially geometric changes, is an important problem for future work.	Although our method can achieve compelling results in many cases, the results are far from uniformly positive. Figure 17 shows several typical failure cases. On translation tasks that involve color and texture changes, as many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog\rightarrowcat transfiguration, the learned translation degenerates into making minimal changes to the input (Figure 17). This failure might be caused by our generator architectures which are tailored for good performance on the appearance changes. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work.				
1245	paper_70	How could we define whether two domains are distributed identically?	[Given one set of images in domain X and a different set in domain Y, it is possible to train a mapping G:X→ Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y.  In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y), which requires G to be stochastic.  The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y.	We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X\rightarrow Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y. However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over \hat{y}. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15].				
1246	paper_70	Would 'bijection' always be guaranteed in this case?	If there is a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections.	These issues call for adding more structure to our objective. Therefore, we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence [3]. Mathematically, if we have a translator G:X\rightarrow Y and another translator F:Y\rightarrow X, then G and F should be inverses of each other, and both mappings should be bijections. We apply this structural assumption by training both the mapping G and F simultaneously, and adding a cycle consistency loss [64] that encourages F(G(x))\approx x and G(F(y))\approx y. Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired image-to-image translation.				
1247	paper_70	What is the definition of 'translate'?	[To "translate" an image means  to convert an image from one representation of a given scene to another, e. , grayscale to color, image to semantic labels, etc. 	This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined.				
1248	paper_70	Could we guess more about what is the instance of 'underlying relationship'?	[For translation between domains, the underlying relationship could be the relationship between those domains – for instance, that they are two different renderings of the same underlying scene.	We therefore seek an algorithm that can learn to translate between domains without paired input-output examples (Figure 2, right). We assume there is some underlying relationship between the domains – for example, that they are two different renderings of the same underlying scene – and seek to learn that relationship. Although we lack supervision in the form of paired examples, we can exploit supervision at the level of sets: we are given one set of images in domain X and a different set in domain Y. We may train a mapping G:X\rightarrow Y such that the output \hat{y}=G(x), x\in X, is indistinguishable from images y\in Y by an adversary trained to classify \hat{y} apart from y. In theory, this objective can induce an output distribution over \hat{y} that matches the empirical distribution p_{data}(y) (in general, this requires G to be stochastic) [16]. The optimal G thereby translates the domain X to a domain \hat{Y} distributed identically to Y. However, such a translation does not guarantee that an individual input x and output y are paired up in a meaningful way – there are infinitely many mappings G that will induce the same distribution over \hat{y}. Moreover, in practice, we have found it difficult to optimize the adversarial objective in isolation: standard procedures often lead to the well-known problem of mode collapse, where all input images map to the same output image and the optimization fails to make progress [15].				
1250	paper_70	In order to determine whether two domains are similar or not, how could we define 'similarity'?	["Similarity" may be defined as a function between input and output, however, it may vary from one particular task or formulation to another.  Therefore, the definition of similarity is task-specific. 	Unlike the above approaches, our formulation does not rely on any task-specific, predefined similarity function between the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embedding space. This makes our method a general-purpose solution for many vision and graphics tasks. We directly compare against several prior and contemporary approaches in Section 5.1.				
1251	paper_70	Normally, GAN training is unstable. Does this framework help to make the model stable?	[The paper applies two techniques to stabilize its model training procedure.  First, for \mathcal{L}_{\text{GAN}} (Equation 1), the paper replaces the negative log likelihood objective by a least-squares loss.  Secondly, to reduce model oscillation, the paper follows Shrivastava et al. ’s strategy and updates the discriminators using a history of generated images rather than the ones produced by the latest generators.  The paper keeps an image buffer that stores the 50 previously created images.  The paper also evaluates its method with the cycle loss in only one direction and finds that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed.	We apply two techniques from recent works to stabilize our model training procedure. First, for \mathcal{L}_{\text{GAN}} (Equation 1), we replace the negative log likelihood objective by a least-squares loss [35]. This loss is more stable during training and generates higher quality results. In particular, for a GAN loss \mathcal{L}_{\text{GAN}}(G,D,X,Y), we train the G to minimize \mathbb{E}_{x\sim p_{\text{data}}(x)}[(D(G(x))-1)^{2}] and train the D to minimize \mathbb{E}_{y\sim p_{\text{data}}(y)}[(D(y)-1)^{2}]+\mathbb{E}_{x\sim p_{\text{data}}(x)}[D(G(x))^{2}].Second, to reduce model oscillation [15], we follow Shrivastava et al.’s strategy [46] and update the discriminators using a history of generated images rather than the ones produced by thelatest generators. We keep an image buffer that stores the 50 previously created images.In Table 4 and Table 5, we compare against ablations of our full loss. Removing the GAN loss substantially degrades results, as does removing the cycle-consistency loss. We therefore conclude that both terms are critical to our results. We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss \mathbb{E}_{x\sim p_{\text{data}}(x)}[\lVert F(G(x))-x\rVert_{1}], or GAN + backward cycle loss \mathbb{E}_{y\sim p_{\text{data}}(y)}[\lVert G(F(y))-y\rVert_{1}] (Equation 2) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed. Figure 7 shows several qualitative examples.				
1252	paper_70	What's the value of the λ scale?	For all the experiments, the paper set lambda=10.   For flower photo enhancement and Monet’s paintings\rightarrowphotos, the identity mapping loss of weight 0. 5*lambda was used, while lambda=10 was kept throughout.	For all the experiments, we set \lambda=10 in Equation 3.We use the Adam solver [26] with a batch size of 1. All networks were trained from scratch with a learning rate of 0.0002. We keep the same learning rate for the first 100 epochs and linearly decay the rate to zero over the next 100 epochs.Please see the appendix (Section 7) for more details about the datasets, architectures, and training procedures.Monet’s paintings\rightarrowphotos To achieve high resolution while conserving memory, we used random square crops of the original images for training. To generate results, we passed images of width 512 pixels with correct aspect ratio to the generator network as input. The weight for the identity mapping loss was 0.5\lambda where \lambda was the weight for cycle consistency loss. We set \lambda=10.Flower photo enhancement Flower images taken on smartphones were downloaded from Flickr by searching for the photos taken byApple iPhone 5, 5s, or 6, with search text flower. DSLR images with shallow DoF were also downloaded from Flickr by search tag flower, dof. The images were scaled to 360 pixels by width. The identity mapping loss of weight 0.5\lambda was used. The training set size of the smartphone and DSLR dataset were 1813 and 3326, respectively. We set \lambda=10.				
1253	paper_72	Is the paper's method also could be interpreted as a weighted nearest-neighbor classifier applied within an embedding space?	[The paper’s approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class.  Prototypical networks differ from matching networks in the few-shot case with equivalence in the one-shot scenario.  Matching networks produce a weighted nearest neighbor classifier given the support set, while prototypical networks produce a linear classifier when squared Euclidean distance is used.  In the case of one-shot learning, ck = xk since there is only one support point per class, and matching networks and prototypical networks become equivalent.  When they become equivalent, then the paper’s approach and matching networks (which produce a weighted nearest neighbor classifier), can be interpreted the same.	We attack the problem of few-shot learning by addressing the key issue of overfitting. Since data is severely limited, we work under the assumption that a classifier should have a very simple inductive bias. Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. In order to do this, we learn a non-linear mapping of the input into an embedding space using a neural network and take a class’s prototype to be the mean of its support set in the embedding space. Classification is then performed for an embedded query point by simply finding the nearest class prototype. We follow the same approach to tackle zero-shot learning; here each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples. We therefore learn an embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point.[Prototypical networks differ from matching networks in the few-shot case with equivalence in the one-shot scenario. Matching networks [29] produce a weighted nearest neighbor classifier given the support set, while prototypical networks produce a linear classifier when squared Euclidean distance is used. In the case of one-shot learning, ck = xk since there is only one support point per class, and matching networks and prototypical networks become equivalent.]				
1254	paper_72	What is the definition of 'episodes'?	[Matching networks utilize sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points.  The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization.  Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points.	Two recent approaches have made significant progress in few-shot learning.Vinyals et al. (2016) proposed matching networks, which uses an attention mechanism over a learned embedding of the labeled set of examples (the support set) to predict classes for the unlabeled points (the query set). Matching networks can be interpreted as a weighted nearest-neighbor classifier applied within an embedding space. Notably, this model utilizes sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot task by subsampling classes as well as data points. The use of episodes makes the training problem more faithful to the test environment and thereby improves generalization.Ravi and Larochelle (2017) take the episodic training idea further and propose a meta-learning approach to few-shot learning. Their approach involves training an LSTM Hochreiter and Schmidhuber (1997) to produce the updates to a classifier, given an episode, such that it will generalize well to a test-set. Here, rather than training a single model over multiple episodes, the LSTM meta-learner learns to train a custom model for each episode.Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1.				
1255	paper_72	How this paper define a prototype?	[The paper learns a non-linear mapping of the input into an embedding space using a neural network and takes a class’s prototype to be the mean of its support set in the embedding space.  It learns the embedding of the meta-data into a shared space to serve as the prototype for each class.  Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point.  Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i}).	We attack the problem of few-shot learning by addressing the key issue of overfitting. Since data is severely limited, we work under the assumption that a classifier should have a very simple inductive bias. Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class. In order to do this, we learn a non-linear mapping of the input into an embedding space using a neural network and take a class’s prototype to be the mean of its support set in the embedding space. Classification is then performed for an embedded query point by simply finding the nearest class prototype. We follow the same approach to tackle zero-shot learning; here each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples. We therefore learn an embedding of the meta-data into a shared space to serve as the prototype for each class. Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point.Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1.				
1256	paper_72	What is the value of M?	[Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}.  Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i}).  Hence M=Dimension of the prototype.	Prototypical networks compute an M-dimensional representation \mathbf{c}_{k}\in\mathbb{R}^{M}, or prototype, of each class through an embedding function f_{\bm{\phi}}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M} with learnable parameters \bm{\phi}. Each prototype is the mean vector of the embedded support points belonging to its class:\mathbf{c}_{k}=\frac{1}{|S_{k}|}\sum_{(\mathbf{x}_{i},y_{i})\in S_{k}}f_{\bm{\phi}}(\mathbf{x}_{i})(1)Given a distance function d:\mathbb{R}^{M}\times\mathbb{R}^{M}\rightarrow[0,+\infty), prototypical networks produce a distribution over classes for a query point \mathbf{x} based on a softmax over distances to the prototypes in the embedding space:p_{\bm{\phi}}(y=k\,|\,\mathbf{x})=\frac{\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k}))}{\sum_{k^{\prime}}\exp(-d(f_{\bm{\phi}}(\mathbf{x}),\mathbf{c}_{k^{\prime}}))}(2)Learning proceeds by minimizing the negative log-probability J(\bm{\phi})=-\log p_{\bm{\phi}}(y=k\,|\,\mathbf{x}) of the true class k via SGD. Training episodes are formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each class to act as the support set and a subset of the remainder to serve as query points. Pseudocode to compute the loss J(\bm{\phi}) for a training episode is provided in Algorithm 1.				
1259	paper_72	Would it be better to use 1 prototype per class rather than multiple prototypes?	[If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class.  This has been proposed in previous works which require a separate partitioning phase that is decoupled from the weight updates, while the paper’s approach is simple to learn with ordinary gradient descent methods.	A natural question is whether it makes sense to use multiple prototypes per class instead of just one.If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in Mensink et al. (2013) and Rippel et al. (2016); however both methods require a separate partitioning phase that is decoupled from the weight updates, while our approach is simple to learn with ordinary gradient descent methods.	[If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class.  This has been proposed in previous works which require a separate partitioning phase that is decoupled from the weight updates, while the paper’s approach is simple to learn with ordinary gradient plunge methods.	Tortured phrases	gradient descent -> gradient plunge	
1260	paper_72	Why is it beneficial to fix the prototype embedding g to have unit length?	[In Zero-shot learning, since the meta-data vector and query point come from different input domains, the paper found it empirically beneficial to fix the prototype embedding g to have unit length, however the paper did not constrain the query embedding f.	[Zero-shot learning differs from few-shot learning in that instead of being given a support set of training points, we are given a class meta-data vector vk for each class. These could be determined in advance, or they could be learned from e.g., raw text [7]. Modifying prototypical networks to deal with the zero-shot case is straightforward: we simply define ck = gϑ(vk) to be a separate embedding of the meta-data vector. An illustration of the zero-shot procedure for prototypical networks as it relates to the few-shot procedure is shown in Figure 1. Since the meta-data vector and query point come from different input domains, we found it was helpful empirically to fix the prototype embedding g to have unit length, however we do not constrain the query embedding f. ]				
1261	paper_72	How many episodes are needed for training the model with the omniglot dataset?	[The paper computed classification accuracy for its models averaged over 1000 randomly generated episodes from the test set.  However it does not mention the number of episodes required for training on omniglot dataset.	We trained prototypical networks using Euclidean distance in the 1-shot and 5-shot scenarios with training episodes containing 60 classes and 5 query points per class. We found that it is advantageous to match the training-shot with the test-shot, and to use more classes (higher “way”) per training episode rather than fewer. We compare against various baselines, including the neural statistician (Edwards and Storkey, 2017) and both the fine-tuned and non-fine-tuned versions of matching networks (Vinyals et al., 2016). We computed classification accuracy for our models averaged over 1000 randomly generated episodes from the test set. The results are shown in Table 1 and to our knowledge they represent the state-of-the-art on this dataset.				
1262	paper_72	Is it fair to use the same embedding architecture as the prototypical network in the matching networks setting?	[Yes.  The paper conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks.  To make the methods comparable, the paper used its own implementation of matching networks that utilizes the same embedding architecture as the paper’s prototypical networks.	We conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, we use our own implementation of matching networks that utilizes the same embedding architecture as our prototypical networks.In Figure 2 we compare cosine vs. Euclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with 15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way and conjecture that the increased difficulty of 20-way classification helps the network to generalize better, because it forces the model to make more fine-grained decisions in the embedding space. Also, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence.				
1263	paper_72	Is it true that it is because cosine distance is not a Bregman divergence?	[The question seems incomplete.  However, using Euclidean distance improves performance substantially over cosine distance.  This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence.	We conducted further analysis, to determine the effect of distance metric and the number of training classes per episode on the performance of prototypical networks and matching networks. To make the methods comparable, we use our own implementation of matching networks that utilizes the same embedding architecture as our prototypical networks.In Figure 2 we compare cosine vs. Euclidean distance and 5-way vs. 20-way training episodes in the 1-shot and 5-shot scenarios, with 15 query points per class per episode. We note that 20-way achieves higher accuracy than 5-way and conjecture that the increased difficulty of 20-way classification helps the network to generalize better, because it forces the model to make more fine-grained decisions in the embedding space. Also, using Euclidean distance improves performance substantially over cosine distance. This effect is even more pronounced for prototypical networks, in which computing the class prototype as the mean of embedded support points is more naturally suited to Euclidean distances since cosine distance is not a Bregman divergence.				
1264	paper_72	What is the resolution of the CUB image?	[The CUB dataset contains 11,788 images of 200 bird species.  The CUB image resolution is not mentioned in the paper, however, for images the paper uses 1,024-dimensional features extracted by applying GoogLeNet to middle, upper left, upper right, lower left, and lower right crops of the original and horizontally-flipped image.	In order to assess the suitability of our approach for zero-shot learning, we also run experiments on the Caltech-UCSD Birds (CUB) 200-2011 dataset (Welinder et al., 2010). The CUB dataset contains 11,788 images of 200 bird species. We closely follow the procedure of Reed et al. (2016) in preparing the data. We use their splits to divide the classes into 100 training, 50 validation, and 50 test. For images we use 1,024-dimensional features extracted by applying GoogLeNet (Szegedy et al., 2015) to middle, upper left, upper right, lower left, and lower right crops of the original and horizontally-flipped image222Features downloaded from https://github.com/reedscot/cvpr2016.. At test time we use only the middle crop of the original image. For class meta-data we use the 312-dimensional continuous attribute vectors provided with the CUB dataset. These attributes encode various characteristics of the bird species such as their color, shape, and feather patterns.				
1265	paper_72	In related work, what is the most relevant method to this paper?	[The paper’s method is most similar to the non-linear extension of NCA because the paper uses a neural network to perform the embedding and optimizes a softmax based on Euclidean distances in the transformed space, as opposed to a margin loss.	The literature on metric learning is vast (Kulis, 2012; Bellet et al., 2013); we summarize here the work most relevant to our proposed method. Neighborhood Components Analysis (NCA) (Goldberger et al., 2004) learns a Mahalanobis distance to maximize K-nearest-neighbor’s (KNN) leave-one-out accuracy in the transformed space.Salakhutdinov and Hinton (2007) extend NCA by using a neural network to perform the transformation. Large margin nearest neighbor (LMNN) classification (Weinberger et al., 2005) also attempts to optimize KNN accuracy but does so using a hinge loss that encourages the local neighborhood of a point to contain other points with the same label. The DNet-KNN (Min et al., 2009) is another margin-based method that improves upon LMNN by utilizing a neural network to perform the embedding instead of a simple linear transformation. Of these, our method is most similar to the non-linear extension of NCA (Salakhutdinov and Hinton, 2007) because we use a neural network to perform the embedding and we optimize a softmax based on Euclidean distances in the transformed space, as opposed to a margin loss. A key distinction between our approach and non-linear NCA is that we form a softmax directly over classes, rather than individual points, computed from distances to each class’s prototype representation. This allows each class to have a concise representation independent of the number of data points and obviates the need to store the entire support set to make predictions.				
1266	paper_73	What is the unit of local regions (the meaning of 'local')	[A local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel.  In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance.  The distance metric defines local neighborhoods that may exhibit different properties.  For example, the density and other attributes of points may not be uniform across different locations — in 3D scanning the density variability can come from perspective effects, radial density variations, motion, etc.	We are interested in analyzing geometric point sets which are collections of points in a Euclidean space. A particularly important type of geometric point set is point cloud captured by 3D scanners, e.g., from appropriately equipped autonomous vehicles. As a set, such data has to be invariant to permutations of its members. In addition, the distance metric defines local neighborhoods that may exhibit different properties. For example, the density and other attributes of points may not be uniform across different locations — in 3D scanning the density variability can come from perspective effects, radial density variations, motion, etc.In convolutional neural networks, a local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel. In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance.				
1267	paper_73	What distance metric this paper uses	[In the paper, in a point set sampled from a metric space, the neighborhood of a point is defined by metric distance.  For instance, suppose \mathcal{X}=(M,d) is a discrete metric space whose metric is inherited from a Euclidean space \mathbb{R}^{n}, where M\subseteq\mathbb{R}^{n} is the set of points and d is the distance metric.  The paper targets at points sampled from a metric space and explicitly considers the underlying distance metric in its design.	In convolutional neural networks, a local region of a pixel consists of pixels with array indices within certain Manhattan distance (kernel size) of the pixel. In a point set sampled from a metric space, the neighborhood of a point is defined by metric distance.A few very recent works [20, 28] have studied how to apply deep learning to unordered sets. They ignore the underlying distance metric even if the point set does possess one. As a result, they are unable to capture local context of points and are sensitive to global set translation and normalization. In this work, we target at points sampled from a metric space and tackle these issues by explicitly considering the underlying distance metric in our design.Suppose that \mathcal{X}=(M,d) is a discrete metric space whose metric is inherited from a Euclidean space \mathbb{R}^{n}, where M\subseteq\mathbb{R}^{n} is the set of points and d is the distance metric. In addition, the density of M in the ambient Euclidean space may not be uniform everywhere. We are interested in learning set functions f that take such \mathcal{X} as the input (along with additional features for each point) and produce information of semantic interest regrading \mathcal{X}. In practice, such f can be classification function that assigns a label to \mathcal{X} or a segmentation function that assigns a per point label to each member of M.				
1268	paper_73	Is it right to define partition in Euclidean space?	[The paper shows the generalizability of its approach to non-Euclidean space as well.  For example, shapes in SHREC15 are 2D surfaces embedded in 3D space.  Geodesic distances along the surfaces naturally induce a metric space.  The paper shows through experiments that adopting PointNet++ in this metric space is an effective way to capture intrinsic structure of the underlying point set.  Hence defining partitions in Euclidean space is not a wrong choice as it doesn't affect the generalizability of the paper's approach.	In this section, we show generalizability of our approach to non-Euclidean space. In non-rigid shape classification (Fig. 7), a good classifier should be able to classify (a) and (c) in Fig. 7 correctly as the same category even given their difference in pose, which requires knowledge of intrinsic structure.Shapes in SHREC15 are 2D surfaces embedded in 3D space. Geodesic distances along the surfaces naturally induce a metric space. We show through experiments that adopting PointNet++ in this metric space is an effective way to capture intrinsic structure of the underlying point set.				
1269	paper_73	Is it true that FPS could cover the whole set?	[For evenly covering the whole set, the paper selects centroids among the input point set by a farthest point sampling (FPS) algorithm.  Given input points \{x_{1},x_{2},. ,x_{n}\}, the paper uses iterative FPS to choose a subset of points \{x_{i_{1}},x_{i_{2}},. ,x_{i_{m}}\}, such that x_{i_{j}} is the most distant point (in metric distance) from the set \{x_{i_{1}},x_{i_{2}},. ,x_{i_{j-1}}\} with regard to the rest points.  Compared with random sampling, it has better coverage of the entire point set given the same number of centroids.	Given input points \{x_{1},x_{2},...,x_{n}\}, we use iterative farthest point sampling (FPS) to choose a subset of points \{x_{i_{1}},x_{i_{2}},...,x_{i_{m}}\}, such that x_{i_{j}} is the most distant point (in metric distance) from the set \{x_{i_{1}},x_{i_{2}},...,x_{i_{j-1}}\} with regard to the rest points. Compared with random sampling, it has better coverage of the entire point set given the same number of centroids. In contrast to CNNs that scan the vector space agnostic of data distribution, our sampling strategy generates receptive fields in a data dependent manner. One issue that still remains is how to generate overlapping partitioning of a point set. Each partition is defined as a neighborhood ball in the underlying Euclidean space, whose parameters include centroid location and scale. To evenly cover the whole set, the centroids are selected among input point set by a farthest point sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed strides, our local receptive fields are dependent on both the input data and the metric, and thus more efficient and effective.				
1270	paper_73	What are the benefits of being dependent on both the input data and the metric?	[In comparison with volumetric CNNs that scan the space with fixed strides, the paper's local receptive fields are dependent on both the input data and the metric.  The dependency on both input data and metric makes them more efficient and effective.	One issue that still remains is how to generate overlapping partitioning of a point set. Each partition is defined as a neighborhood ball in the underlying Euclidean space, whose parameters include centroid location and scale. To evenly cover the whole set, the centroids are selected among input point set by a farthest point sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed strides, our local receptive fields are dependent on both the input data and the metric, and thus more efficient and effective.				
1271	paper_73	What are the benefits of hierarchical features for capturing local context?	[PointNet lacks the ability to capture local context at different scales.  The paper introduces a hierarchical feature learning framework to resolve this limitation.  The idea of hierarchical feature learning has been very successful and convolutional neural network is one of the most prominent examples.  The paper's approach introduces hierarchical feature learning and captures geometry features at different scales.  This is very important for understanding scenes at multiple levels and labeling objects with various sizes.	PointNet achieved impressive performance on a few benchmarks. However, it lacks the ability to capture local context at different scales. We will introduce a hierarchical feature learning framework in the next section to resolve the limitation.Our approach outperforms all the baseline methods by a large margin. In comparison with [5], which learns on voxelized scans, we directly learn on point clouds to avoid additional quantization error, and conduct data dependent sampling to allow more effective learning. Compared with [20], our approach introduces hierarchical feature learning and captures geometry features at different scales. This is very important for understanding scenes at multiple levels and labeling objects with various sizes.We visualize example scene labeling results in Fig. 6.The idea of hierarchical feature learning has been very successful. Among all the learning models, convolutional neural network [10, 25, 8] is one of the most prominent ones.However, convolution does not apply to unordered point sets with distance metrics, which is the focus of our work.	[PointNet lacks the ability to capture local context at different scales.  The paper introduces a hierarchical feature learning framework to resolve this limitation.  The idea of hierarchical feature learning has been very successful and convolutional neural organization is one of the most prominent examples.  The paper's approach introduces hierarchical feature learning and captures geometry features at different scales.  This is very important for understanding scenes at multiple levels and labeling objects with various sizes.	Tortured phrases	convolutional neural network -> convolutional neural organization	
1272	paper_73	Is it true that ball query's local neighborhood could make the local region feature more generalizable across space?	[It is true.  Ball query finds all points that are within a radius to the query point (an upper limit of K is set in implementation).  Ball query’s local neighborhood guarantees a fixed region scale thus making local region feature more generalizable across space, which is preferred for tasks requiring local pattern recognition (e.  semantic point labeling).	Ball query finds all points that are within a radius to the query point (an upper limit of K is set in implementation). An alternative range query is K nearest neighbor (kNN) search which finds a fixed number of neighboring points. Compared with kNN, ball query’s local neighborhood guarantees a fixed region scale thus making local region feature more generalizable across space, which is preferred for tasks requiring local pattern recognition (e.g. semantic point labeling).				
1273	paper_73	How is the computation cost of MSG different from MRG?	[Compared with MSG, MRG is computationally more efficient since it avoids feature extraction in large scale neighborhoods at the lowest levels.  The MSG approach is computationally expensive since it runs local PointNet at large scale neighborhoods for every centroid point.  In particular, since the number of centroid points is usually quite large at the lowest level, the time cost is significant.	The MSG approach above is computationally expensive since it runs local PointNet at large scale neighborhoods for every centroid point. In particular, since the number of centroid points is usually quite large at the lowest level, the time cost is significant. Compared with MSG, this method is computationally more efficient since we avoids the feature extraction in large scale neighborhoods at lowest levels.				
1274	paper_73	What if we do not share the parameter?	Instead, the paper anticipates it as worthwhile to think of how to accelerate the inference speed of the paper's proposed network especially for MSG and MRG layers by sharing more computation in each local region.	In the future, it’s worthwhile thinking how to accelerate inference speed of our proposed network especially for MSG and MRG layers by sharing more computation in each local regions. It’s also interesting to find applications in higher dimensional metric spaces where CNN based method would be computationally unfeasible while our method can scale well.				
1275	paper_74	What is the instance of quantization artifacts?	[The point clouds or meshes are not in a regular format, most researchers typically transform such data to regular 3D voxel grids or collections of images (e.  views) before feeding them to a deep net architecture.  This data representation transformation, however, renders the resulting data unnecessarily voluminous, while also introducing quantization artifacts that can obscure natural invariances of the data.  However, the paper doesn't mention any particular instance of quantization artifact.	In this paper we explore deep learning architectures capable of reasoning about 3D geometric data such as point clouds or meshes. Typical convolutional architectures require highly regular input data formats, like those of image grids or 3D voxels, in order to perform weight sharing and other kernel optimizations. Since point clouds or meshes are not in a regular format, most researchers typically transform such data to regular 3D voxel grids or collections of images (e.g, views) before feeding them to a deep net architecture. This data representation transformation, however, renders the resulting data unnecessarily voluminous — while also introducing quantization artifacts that can obscure natural invariances of the data.				
1276	paper_74	What is the combinatorial irregularities and complexities of meshes?	[Point clouds are simple and unified structures that avoid the combinatorial irregularities and complexities of meshes, and thus are easier to learn from.  However, these combinatorial irregularities and complexities of meshes are not mentioned in the paper.	For this reason we focus on a different input representation for 3D geometry using simply point clouds – and name our resulting deep nets PointNets. Point clouds are simple and unified structures that avoid the combinatorial irregularities and complexities of meshes, and thus are easier to learn from. The PointNet, however, still has to respect the fact that a point cloud is just a set of points and therefore invariant to permutations of its members, necessitating certain symmetrizations in the net computation. Further invariances to rigid motions also need to be considered.				
1277	paper_74	What is the meaning of invariant to permutations?	[Point cloud is just a set of points and therefore invariant or indifferent to permutations of its members, necessitating certain symmetrizations in the net computation.  Point cloud is a set of points without a specific order, unlike pixel arrays in images or voxel arrays in volumetric grids.  This implies that a network that consumes N 3D point sets needs to be invariant to N.  permutations of the input set in data feeding order.	•Unordered.Unlike pixel arrays in images or voxel arrays in volumetric grids, point cloud is a set of points without specific order. In other words, a network that consumes N 3D point sets needs to be invariant to N! permutations of the input set in data feeding order.•Interaction among points. The points are from a space with a distance metric. It means that points are not isolated, and neighboring points form a meaningful subset. Therefore, the model needs to be able to capture local structures from nearby points, and the combinatorial interactions among local structures.•Invariance under transformations.As a geometric object, the learned representation of the point set should be invariant to certain transformations. For example, rotating and translating points all together should not modify the global point cloud category nor the segmentation of the points.				
1278	paper_74	How to get local or global features in the PointCloud domain	[An SVM or multi-layer perceptron classifier can be trained on the shape global features for classification.  However, point segmentation requires a combination of local and global knowledge.  After computing the global point cloud feature vector, the paper feeds it back to per point features by concatenating the global feature with each of the point features.  Then the paper extracts new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	The output from the above section forms a vector [f_{1},\dots,f_{K}], which is a global signature of the input set. We can easily train a SVM or multi-layer perceptron classifier on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. We can achieve this by a simple yet highly effective manner. Our solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating the global feature with each of the point features. Then we extract new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	[An help vector machine or multi-layer perceptron classifier can be trained on the shape global features for classification.  However, point segmentation requires a combination of local and global knowledge.  After computing the global point cloud feature vector, the paper feeds it back to per point features by concatenating the global feature with each of the point features.  Then the paper extracts new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	Tortured phrases	help vector machine -> support vector machine (SVM)	
1279	paper_74	Is the reason for using max pooling for permutation invariant in the paper above?	[Note:The question is not phrased correctly.  In order to make a model invariant to input permutation, one of the strategies is to use a simple symmetric function to aggregate the information from each point.  In the paper, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order.  For example, + and * operators are symmetric binary functions.  The key to the paper's approach is the use of a single symmetric function, max pooling.	In order to make a model invariant to input permutation, three strategies exist: 1) sort input into a canonical order; 2) treat the input as a sequence to train an RNN, but augment the training data by all kinds of permutations; 3) use a simple symmetric function to aggregate the information from each point. Here, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions.Key to our approach is the use of a single symmetric function, max pooling. Effectively the network learns a set of optimization functions/criteria that select interesting or informative points of the point cloud and encode the reason for their selection. The final fully connected layers of the network aggregate these learnt optimal values into the global descriptor for the entire shape as mentioned above (shape classification) or are used to predict per point labels (shape segmentation).				
1280	paper_74	What if we change the order of g and h in equation (1)?	[A symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order.  For example, + and * operators are symmetric binary functions.  The paper approximates a general function defined on a point set by applying a symmetric function on transformed elements in the set, as in Equation (1), where f:2^{\mathbb{R}^{N}}\rightarrow\mathbb{R}, h:\mathbb{R}^{N}\rightarrow\mathbb{R}^{K} and g:\underbrace{\mathbb{R}^{K}\times\dots\times\mathbb{R}^{K}}_{n}\rightarrow\mathbb{R} is a symmetric function.  Therefore, the result would be the same even if the order of g and h is changed.	In order to make a model invariant to input permutation, three strategies exist: 1) sort input into a canonical order; 2) treat the input as a sequence to train an RNN, but augment the training data by all kinds of permutations; 3) use a simple symmetric function to aggregate the information from each point. Here, a symmetric function takes n vectors as input and outputs a new vector that is invariant to the input order. For example, + and * operators are symmetric binary functions.Our idea is to approximate a general function defined on a point set by applying a symmetric function on transformed elements in the set:\displaystyle f(\{x_{1},\dots,x_{n}\})\approx g(h(x_{1}),\dots,h(x_{n})),(1)where f:2^{\mathbb{R}^{N}}\rightarrow\mathbb{R}, h:\mathbb{R}^{N}\rightarrow\mathbb{R}^{K} and g:\underbrace{\mathbb{R}^{K}\times\dots\times\mathbb{R}^{K}}_{n}\rightarrow\mathbb{R} is a symmetric function.				
1281	paper_74	How to construct the affine transformation matrix?	[The paper predicts an affine transformation matrix by a mini-network, called T-net, and directly applies this transformation to the coordinates of input points.  The mini-network itself resembles the big network and is composed of basic modules of point-independent feature extraction, max pooling, and fully connected layers.	Our input form of point clouds allows us to achieve this goal in a much simpler way compared with [9]. We do not need to invent any new layers and no alias is introduced as in the image case. We predict an affine transformation matrix by a mini-network (T-net in Fig 2) and directly apply this transformation to the coordinates of input points. The mini-network itself resembles the big network and is composed by basic modules of point independent feature extraction, max pooling and fully connected layers. More details about the T-net are in the supplementary.				
1282	paper_74	How to set Cs(critical point set)	[The second part of theorem 2 in the paper implies that \mathcal{C}_{S} only contains a bounded number of points, determined by K in Equation(1).  That is,  f(S) is in fact totally determined by a finite subset \mathcal{C}_{S}\subseteq S of less or equal to K elements.  Hence, the paper calls \mathcal{C}_{S} the critical point set of S and K the bottleneck dimension of f.	We explain the implications of the theorem. (a) says that f(S) is unchanged up to the input corruption if all points in \mathcal{C}_{S} are preserved; it is also unchanged with extra noise points up to \mathcal{N}_{S}. (b) says that \mathcal{C}_{S} only contains a bounded number of points, determined by K in (1). In other words, f(S) is in fact totally determined by a finite subset \mathcal{C}_{S}\subseteq S of less or equal to K elements. We therefore call \mathcal{C}_{S} the critical point set of S and K the bottleneck dimension of f.				
1283	paper_74	why PointNet is highly robust to small perturbation of input points as well as to corruption through point insertion (outliers) or deletion (missing data)?	The paper shows the universal approximation ability of its neural network to continuous set functions.  By the continuity of set functions, intuitively, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores.  The paper's theorem 2, combined with the continuity of h, explains the robustness of the paper's model w. t point perturbation, corruption, and extra noise points.  The robustness is gained in analogy to the sparsity principle in machine learning models.  Intuitively, the paper's network learns to summarize a shape by a sparse set of key points.	Combined with the continuity of h, this explains the robustness of our model w.r.t point perturbation, corruption and extra noise points. The robustness is gained in analogy to the sparsity principle in machine learning models. Intuitively, our network learns to summarize a shape by a sparse set of key points. In experiment section we see that the key points form the skeleton of an object.We first show the universal approximation ability of our neural network to continuous set functions. By the continuity of set functions, intuitively, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores.				
1284	paper_75	What is the definition of 'task'?	Task means what the agent wants to learn.  It can be Supervised Learning or Reinforcement Learning, which is represented by initial state, loss function, transition distribution, and episode length H.	Few-shot learning is well-studied in the domain of supervised tasks, where the goal is to learn a new function from only a few input/output pairs for that task, using prior data from similar tasks for meta-learning. For example, the goal might be to classify images of a Segway after seeing only one or a few examples of a Segway, with a model that has previously seen many other types of objects. Likewise, in few-shot regression, the goal is to predict the outputs of a continuous-valued function from only a few datapoints sampled from that function, after training on many functions with similar statistical properties.We consider a model, denoted f, that maps observations \mathbf{x} to outputs \mathbf{a}.During meta-learning, the modelis trained to be able to adapt to a large or infinite number of tasks.Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below.Formally, each task \mathcal{T}=\{\mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H}),q(\mathbf{x}_{1}),q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}),H\}consists of a loss function \mathcal{L}, a distribution over initial observations q(\mathbf{x}_{1}), a transition distribution q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}), and an episode length H. In i.i.d. supervised learning problems, the length H\!=\!1.The model may generate samples of length H by choosing an output \mathbf{a}_{t} at each time t.The loss \mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H})\rightarrow\mathbb{R}, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process.				
1285	paper_75	What is the definition of 'meta-learning'?	Meta-learning is a learning method that enables a model to quickly adapt to tasks during learning.  Few-shot meta learning means learning a good model within a small number of iterations using only a few training images.  Reinforcement learning means that a good policy can be learned using a small number of experiences.	Learning quickly is a hallmark of human intelligence, whether it involves recognizing objects from a few examples or quickly learning new skillsafter just minutes of experience. Our artificial agents should be able to do the same, learning and adapting quickly from only a few examples, and continuing to adapt as more data becomes available. This kind of fast and flexible learning is challenging, since the agent must integrate its prior experience with a small amount of new information, while avoiding overfitting to the new data. Furthermore, the form of prior experience and new data will depend on the task. As such, for the greatest applicability, the mechanism for learning to learn (or meta-learning) should be general to the task and the form of computation required to complete the task.In this work, we propose a meta-learning algorithm that is general and model-agnostic, in the sense that it can be directly applied to any learning problem and model that is trained with a gradient descent procedure. Our focus is on deep neural network models, but we illustrate how our approach can easily handle different architectures and different problem settings, including classification, regression, and policy gradient reinforcement learning, with minimal modification.In meta-learning, the goal of the trained model is to quickly learn a new task from a small amount of new data, and the model istrained by the meta-learner to be able to learn on a large number of different tasks.The key idea underlying our method is totrain the model’s initial parameters such that the model has maximal performance on a new task after the parameters have been updatedthrough one or more gradient steps computed with a small amount of data from that new task.Unlike prior meta-learning methods that learn an update function or learning rule (Schmidhuber, 1987; Bengio et al., 1992; Andrychowicz et al., 2016; Ravi & Larochelle, 2017), our algorithm does not expand the number of learned parameters nor place constraints on the model architecture (e.g. by requiring a recurrent model (Santoro et al., 2016) or a Siamese network (Koch, 2015)), and it can be readily combined with fully connected, convolutional, or recurrent neural networks. It can also be used with a variety of loss functions, including differentiable supervised losses and non-differentiable reinforcement learning objectives. In reinforcement learning (RL), the goal of few-shot meta-learning is to enable an agent to quickly acquire a policy for a new test task using only a small amount of experience in the test setting. A new task might involve achieving a new goal or succeeding on a previously trained goal in a new environment. For example, an agent might learn to quickly figure out how to navigate mazes so that, when faced with a new maze, it can determine how to reliably reach the exit with only a few samples.In this section, we will discuss how MAML can be applied to meta-learning for RL.All of the meta-learning problems that we consider require some amount of adaptation to new tasks at test-time. When possible, we compare our results to an oracle that receives the identity of the task (which is a problem-dependent representation) as an additional input, as an upper bound on the performance of the model. All of the experiments were performed using TensorFlow (Abadi et al., 2016), which allows for automatic differentiation through the gradient update(s) during meta-learning. The code is available online111Code for the regression and supervised experiments is at github.com/cbfinn/maml and code for the RL experiments is at github.com/cbfinn/maml_rl.The primary contribution of this work is a simple model- and task-agnostic algorithm for meta-learning that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.We demonstrate the algorithm on different model types, including fully connected and convolutional networks, and in several distinct domains, including few-shot regression, image classification, and reinforcement learning.Our evaluation shows that our meta-learning algorithm compares favorably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regression and can accelerate reinforcement learning in the presence of task variability, substantially outperforming direct pretraining as initialization.The goal of few-shot meta-learningis to train a model that can quickly adapt to a new task using only a few datapoints and training iterations.To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials.In effect, the meta-learning problem treats entire tasks as training examples.In this section, we formalize this meta-learning problem setting in a general manner, including brief examples of different learning domains.We will discuss two different learning domains in detail in Section 3.We consider a model, denoted f, that maps observations \mathbf{x} to outputs \mathbf{a}.During meta-learning, the modelis trained to be able to adapt to a large or infinite number of tasks.Since we would like to apply our framework to a variety of learning problems, from classification to reinforcement learning, we introduce a generic notion of a learning task below.Formally, each task \mathcal{T}=\{\mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H}),q(\mathbf{x}_{1}),q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}),H\}consists of a loss function \mathcal{L}, a distribution over initial observations q(\mathbf{x}_{1}), a transition distribution q(\mathbf{x}_{t+1}|\mathbf{x}_{t},\mathbf{a}_{t}), and an episode length H. In i.i.d. supervised learning problems, the length H\!=\!1.The model may generate samples of length H by choosing an output \mathbf{a}_{t} at each time t.The loss \mathcal{L}(\mathbf{x}_{1},\mathbf{a}_{1},\dots,\mathbf{x}_{H},\mathbf{a}_{H})\rightarrow\mathbb{R}, provides task-specific feedback, which might be in the form of a misclassification loss or a cost function in a Markov decision process.				
1286	paper_75	What type of parameter would be considered a 'good' initial parameter?	A good initial parameter is a parameter that gives good performance in many tasks even with a little fine-tuning of the parameter.  This means that the loss function defined in many tasks is sensitive, and this sensitive loss leads to good updates.	The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems (Donahue et al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization (Saxe et al., 2014; Kirkpatrick et al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers (Krähenbühl et al., 2016; Salimans & Kingma, 2016), including learned initializations (Husken & Goerick, 2000; Maclaurin et al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps.In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure 1). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \theta, and that the loss function is smooth enough in \theta that we can use gradient-based learning techniques.				
1287	paper_75	Is it true that this paper's learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters?	It is true.  As many sentences mention, it can be seen as increasing the sensitivity of the loss function.	The model parameters are trained by optimizing for the performance of f_{\theta_{i}^{\prime}} with respect to \theta across tasks sampled from p(\mathcal{T}).More concretely, the meta-objective is as follows:\displaystyle\vspace{-0.2cm}\min_{\theta}\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta_{i}^{\prime}})=\sum_{\mathcal{T}_{i}\sim p(\mathcal{T})}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta-\alpha\nabla_{\theta}\mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})})Note that the meta-optimization is performed over the model parameters \theta, whereas the objective is computed using the updated model parameters \theta^{\prime}.In effect, our proposed method aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task.The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.Our approach is also related to methods for initialization of deep networks. In computer vision, models pretrained on large-scale image classification have been shown to learn effective features for a range of problems (Donahue et al., 2014). In contrast, our method explicitly optimizes the model for fast adaptability, allowing it to adapt to new tasks with only a few examples.Our method can also be viewed as explicitly maximizing sensitivity of new task losses to the model parameters.A number of prior works have explored sensitivity in deep networks, often in the context of initialization (Saxe et al., 2014; Kirkpatrick et al., 2016). Most of these works have considered good random initializations, though a number of papers have addressed data-dependent initializers (Krähenbühl et al., 2016; Salimans & Kingma, 2016), including learned initializations (Husken & Goerick, 2000; Maclaurin et al., 2015). In contrast, our method explicitly trains the parameters for sensitivity on a given task distribution, allowing for extremely efficient adaptation for problems such as K-shot learning and rapid reinforcement learning in only one or a few gradient steps.In contrast to prior work, which has sought to train recurrent neural networks that ingest entire datasets (Santoro et al., 2016; Duan et al., 2016b) or feature embeddings that can be combined with nonparametric methods at test time (Vinyals et al., 2016; Koch, 2015), we propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferrable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in p(\mathcal{T}), rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from p(\mathcal{T}), without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from p(\mathcal{T}), when altered in the direction of the gradient of that loss (see Figure 1). We make no assumption on the form of the model, other than to assume that it is parametrized by some parameter vector \theta, and that the loss function is smooth enough in \theta that we can use gradient-based learning techniques.				
1288	paper_75	Why meta learning is better than transfer learning?	While transfer learning requires learned parameters, meta-learning does not require learned parameters.	We introduced a meta-learning method based on learning easily adaptable model parameters through gradient descent. Our approach has a number of benefits. It is simple and does not introduce any learned parameters for meta-learning. It can be combined with any model representation that is amenable to gradient-based training, and any differentiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a weight initialization, adaptation can be performed with any amount of data and any number of gradient steps, though we demonstrate state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent using policy gradients and a very modest amount of experience.				
1289	paper_75	How could the algorithm infer amplitude and phase in the case of showing only half of the input range?	As written in the paper, the authors claim that the meta-learned model f can learn the periodicity of a sine wave without looking at the entire dataset.	We evaluate performance by fine-tuning the model learned by MAML and the pretrained model on K=\{5,10,20\} datapoints. During fine-tuning, each gradient step is computed using the same K datapoints. The qualitative results, shown in Figure 2 and further expanded on in Appendix B show that the learned model is able to quickly adapt with only 5 datapoints, shown as purple triangles, whereas the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt with so few datapoints without catastrophic overfitting. Crucially, when the K datapoints are all in one half of the input range, the model trained with MAML can still infer the amplitude and phase in the other half of the range, demonstrating that the MAML trained model f has learned to model the periodic nature of the sine wave. Furthermore, we observe both in the qualitative and quantitative results (Figure 3 and Appendix B) that the model learned with MAML continues to improve with additional gradient steps, despite being trained for maximal performance after one gradient step. This improvement suggests that MAML optimizes the parameters such that they lie in a region that is amenable to fast adaptation and is sensitive to loss functions from p(\mathcal{T}), as discussed in Section 2.2, rather than overfitting to parameters \theta that only improve after one step.				
1290	paper_75	Is it true that the first-order approximation led to roughly 33% speed-up in network computation?	According to the paper, eliminating the Hessian calculation increases the overall calculation speed by 33%.	A significant computational expense in MAML comes from the use of second derivatives when backpropagating the meta-gradient through the gradient operator in the meta-objective (see Equation (1)). On MiniImagenet, we show a comparison to a first-order approximation of MAML, where these second derivatives are omitted. Note that the resulting method still computes the meta-gradient at the post-update parameter values \theta_{i}^{\prime}, which provides for effective meta-learning. Surprisingly however, the performance of this method is nearly the same as that obtained with full second derivatives, suggesting that most of the improvement in MAML comes from the gradients of the objective at the post-update parameter values, rather than the second order updates from differentiating through the gradient update. Past work has observed that ReLU neural networks are locally almost linear (Goodfellow et al., 2015), which suggests that second derivatives may be close to zero in most cases, partially explaining the good performance of the first-order approximation. This approximation removes the need for computing Hessian-vector products in an additional backward pass, which we found led to roughly 33\% speed-up in network computation.				
1291	paper_76	Why are there tradeoffs between sample variety and fidelity?	The tradeoff is as IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision).  FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops.	This technique allows fine-grained, post-hoc selection of the trade-off between sample quality and variety for a given G. Notably, we can compute FID and IS for a range of thresholds, obtaining the variety-fidelity curve reminiscent of the precision-recall curve (Figure 17). As IS does not penalize lack of variety in class-conditional models, reducing the truncation threshold leads to a direct increase in IS (analogous to precision). FID penalizes lack of variety (analogous to recall) but also rewards precision, so we initially see a moderate improvement in FID, but as truncation approaches zero and variety diminishes, the FID sharply drops. The distribution shift caused by sampling with different latents than those seen in training is problematic for many models. Some of our larger models are not amenable to truncation, producing saturation artifacts (Figure 2(b)) when fed truncated noise. To counteract this, we seek to enforce amenability to truncation by conditioning G to be smooth, so that the full space of z will map to good output samples. For this, we turn to Orthogonal Regularization (Brock et al., 2017), which directly enforces the orthogonality condition:We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128\times128, 256\times256, and 512\times512 resolutions, employing the settings from Table 1, row 8.The samples generated by our models are presented in Figure 4, with additional samples in Appendix A, and online222https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002.We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of “objectness.” Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018).				
1292	paper_76	Can 512x512 be considered high resolution?	Information about Wether 512*512 is high resolution or not is not explicitly provided in the paper.	We find that current GAN techniques are sufficient to enable scaling to large models and distributed, large-batch training. We find that we can dramatically improve the state of the art and train models up to 512\times512 resolution without need for explicit multiscale methods like Karras et al. (2018). Despite these improvements, our models undergo training collapse, necessitating early stopping in practice. In the next two sections we investigate why settings which were stable in previous works become unstable when applied at scale.				
1294	paper_76	What is the truncation setting?	First, the FID/IS values at the truncation setting attain the best FID.  Second, the FID at the truncation setting for which our model’s IS is the same as that acquired by the genuine validation data, reasoning that this is a satisfactory measure of maximal sample variety produced while still obtaining a fair degree of “objectness. ” Third, FID at the greatest IS attained by each model, indicates how much variation must be traded off to optimize quality.	We evaluate our models on ImageNet ILSVRC 2012 (Russakovsky et al., 2015) at 128\times128, 256\times256, and 512\times512 resolutions, employing the settings from Table 1, row 8.The samples generated by our models are presented in Figure 4, with additional samples in Appendix A, and online222https://drive.google.com/drive/folders/1lWC6XEPD0LT5KUnPXeve_kWeY-FxH002.We report IS and FID in Table 2. As our models are able to trade sample variety for quality, it is unclear how best to compare against prior art; we accordingly report values at three settings, with complete curves in Appendix D. First, we report the FID/IS values at the truncation setting which attains the best FID. Second, we report the FID at the truncation setting for which our model’s IS is the same as that attained by the real validation data, reasoning that this is a passable measure of maximum sample variety achieved while still achieving a good level of “objectness.” Third, we report FID at the maximum IS achieved by each model, to demonstrate how much variety must be traded off to maximize quality. In all three cases, our models outperform the previous state-of-the-art IS and FID scores achieved by Miyato et al. (2018) and Zhang et al. (2018).				
1295	paper_76	How could we check whether D is overfitting the training set?	By observing that D’s loss approaches zero during training, but undergoes a sharp upward jump at the collapse.	We also observe that D’s loss approaches zero during training, but undergoes a sharp upward jump at collapse (Appendix F).One possible explanation for this behavior is that D is overfitting to the training set, memorizing training examples rather than learning some meaningful boundary between real and generated images. As a simple test for D’s memorization (related to Gulrajani et al. (2017)), we evaluate uncollapsed discriminators on the ImageNet training and validation sets, and measure what percentage of samples are classified as real or generated. While the training accuracy is consistently above 98%, the validation accuracy falls in the range of 50-55%, no better than random guessing (regardless of regularization strategy). This confirms that Dis indeed memorizing the training set;we deem this in line with D’s role, which is not explicitly to generalize, but to distill the training data and provide a useful learning signal for G. Additional experiments and discussion are provided in Appendix G.				
1296	paper_76	What if we train this model with small dataset?	The model generate less of that class for which data is low.	We note that some failure modes of our partially-trained models are distinct from those previously observed. Most previous failures involve local artifacts (Odena et al., 2016), images consisting of texture blobs instead of objects (Salimans et al., 2016), or the canonical mode collapse. We observe class leakage, where images from one class contain properties of another, as exemplified by Figure 4(d). We also find that many classes on ImageNet are more difficult than others for our model; our model is more successful at generating dogs (which make up a large portion of the dataset, and are mostly distinguished by their texture) than crowds (which comprise a small portion of the dataset and have more large-scale structure). Further discussion is available in Appendix A.				
1297	paper_76	What's the effect of expanding channel size?	expanding channel size substantially improves the performance of the model.	To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017).The full JFT-300M dataset contains 300M real-world images labeled with 18K categories.Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels.The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. For images with multiple labels, we sample a single label randomly and independently whenever an image is sampled.To compute IS and FID for the GANs trained on this dataset, we use an Inception v2 classifier (Szegedy et al., 2016) trained on this dataset.Quantitative results are presented in Table 3.All models are trained with batch size 2048.We compare an ablated version of our model –comparable to SA-GAN (Zhang et al., 2018) but with the larger batch size –against a “full” BigGAN model that makes uses of all of the techniques applied to obtain the best results on ImageNet (shared embedding, skip-z, and orthogonal regularization).Our results show that these techniques substantially improve performance even in the setting of this much larger dataset at the same model capacity (64 base channels).We further show that for a dataset of this scale, we see significant additional improvements from expanding the capacity of our models to 128 base channels, while for ImageNet GANs that additional capacity was not beneficial.				
1298	paper_76	What is the definition of intra-class variability?	intra-class variability here means images with multiple objects at a variety of scales.	In Figure 19 (Appendix D), we present truncation plots for models trained on this dataset.Unlike for ImageNet, where truncation limits of \sigma\approx 0 tend to produce the highest fidelity scores, IS is typically maximized for our JFT-300M models when the truncation value \sigma ranges from 0.5 to 1.We suspect that this is at least partially due to the intra-class variability of JFT-300M labels, as well as the relative complexity of the image distribution, which includes images with multiple objects at a variety of scales.Interestingly, unlike models trained on ImageNet, where training tends to collapse without heavy regularization (Section 4), the models trained on JFT-300M remain stable over many hundreds of thousands of iterations.This suggests that moving beyond ImageNet to larger datasets may partially alleviate GAN stability issues.				
1299	paper_77	How the architecture is chosen	The adversary (attacking part) must at least have some partial knowledge of the input (e. , images, text) and expected output (e. , classification) in order to select the architecture of the attacking system.  The adversary selects an appropriate architecture adapted to the input-output relation.  For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice.  The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc. , have relatively little impact on the success of the attack, so they do not determine the architecture.	Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section 6 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success.Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}.Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.Goodfellow’s algorithm: Recall from Equation 5 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \varepsilon added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \varepsilon onadversarial sample transferability.In Figure 8, architecture A outperformsall others: it is a copy of the oracle’s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24\%and 80.21\%, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \varepsilon above0.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans.	The adversary (attacking part) must at least have some partial knowledge of the input (e. , images, text) and expected output (e. , classification) in order to select the architecture of the attacking system.  The adversary selects an appropriate architecture adapted to the input-output relation.  For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice.  The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc. , have relatively little impact on the success of the attack, so they do not determine the architecture.	Tortured phrases	 (convolutional neural network -> convolutionary)	
1300	paper_77	The accuracy is achieved by which ML Model used for training the substitute	The ML model used for achieving the accuracy is a DNN (Deep Neural Network) combined with LR (Logistic Regression), and the two refinements as introduced in Section 6: a periodic step size and reservoir sampling.	Substitute DNN Training: The adversary uses the initial substitute training sets and the oracle to trainsubsitute DNNs. Our substitute architecture A, a standard forimage classification, is describedin Table 13 (cf. appendix).The substitute DNN is trained on ourmachine for 6 substitute epochs.Duringeach of these 6 epochs, the model is trained for 10 epochsfrom scratch with a learning rate of 10^{-2} and momentum of 0.9. Betweensubstitute epochs, we perform a Jacobian-based dataset augmentation with a stepsize of \lambda=0.1 to generate additional synthetic training data, which we label using the MetaMind oracle.The accuracy of the two substitute DNNs is reported inFigure 4. It is computed with the MNISTtest set (minus the 150 samples used in the first initial substitute trainingset). The adversary does not have access to this full test set: wesolely use it to analyze our results. The two substituteDNNs respectively achieve a 81.20\% and 67.00\% accuracy on the MNIST test set after 6 substitute training epochs. These accuracies fallshort of current state-of-the-art accuracies on this task. However, the adversary has access to a limited number ofsamples (in this case 6,400=100\times 2^{6} instead of 50,000 forstate-of-the-art models). Furthermore, the adversarial goal is to craftadversarial samples misclassified by the oracle. Instead of learning asubstitute DNN with optimal accuracy, the adversary is interested inlearning a substitute capable of mimicking the oracle decisionboundaries.Whereas we previously trained all of our substitutes using DNNs only, we now useboth DNNs and LR as substitute models. The Jacobian-based dataset augmentationdescribed in the context of DNNs is easily adapted to logistic regression: thelater is analog to the softmax layer frequently used by the former whenoutputting probability vectors. We use 100 samples from the MNIST test set asthe initial substitute training set and use the two refinements introduced inSection 6: a periodic step size and reservoirsampling.Substitute Training: By augmenting an initial training set of 100 test set samples, wetrain a DNN and LR substitute for each of the two oracles. We measure success as the rate of adversarialsamples misclassified by the corresponding oracle, among the 10,000 produced from the test set using the fast gradient sign method with parameter \varepsilon=0.3. These rates, computed after \rho\in\{3,6\} dataset augmentation iterations, are reported in Table 3. Results reported in the last row use both a periodic step size and reservoir sampling (hence the reduced number of queries made to train the substitute).				
1301	paper_77	What is the criteria for training multiple substitute DNNs?	The criteria for training multiple substitute DNNs is achieve good accuray but mainly  the goal is to create a substitute capable of mimicking the oracle decision boundaries.	Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}.The accuracy of the two substitute DNNs is reported inFigure 4. It is computed with the MNISTtest set (minus the 150 samples used in the first initial substitute trainingset). The adversary does not have access to this full test set: wesolely use it to analyze our results. The two substituteDNNs respectively achieve a 81.20\% and 67.00\% accuracy on the MNIST test set after 6 substitute training epochs. These accuracies fallshort of current state-of-the-art accuracies on this task. However, the adversary has access to a limited number ofsamples (in this case 6,400=100\times 2^{6} instead of 50,000 forstate-of-the-art models). Furthermore, the adversarial goal is to craftadversarial samples misclassified by the oracle. Instead of learning asubstitute DNN with optimal accuracy, the adversary is interested inlearning a substitute capable of mimicking the oracle decisionboundaries.Substitute DNN Training: The adversary uses twoinitial substitute training sets extracted from the GTSRB test set. Thefirst includes the first 1,000 samples and the second thefirst 500. The number of initial samples is higher than forMNIST substitutes as inputs have a higher dimensionality. We trainthree substitute architectures C, D, and E (cf.Table 13) using the oracle for 6 substitutetraining epochs with a Jacobian-based dataset augmentation parameter of\lambda=0.1. Substitute C and E where trained with the 1,000 sampleinitial substitute training set and achieve a 71.42\% accuracy. Substitute Dwas trained with the initial set of 500 samples. Its accuracy of 60.12\% islower than C and E.Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.				
1302	paper_77	What type of defense strategies are evaded in this paper	Two types of defense are evaded, Adversarial training and Defensive distillation.	We show a more general flaw in the category of gradient masking.Even if the defender attempts to prevent attacks by not publishingthe directions in which the model is sensitive, these directionscan be discovered by other means, in which case thesame attack can still succeed.We show that the black-box attack based on transfer from a substitute modelovercomes gradient masking defenses. No fully effective defense mechanism is known, but we study the two with thegreatest empirical success so far:adversarial training [4, 14], anddefensive distillation for DNNs [10].Adversarial training:It was shown that injecting adversarial examples throughout training increasesthe robustness of significantly descriptive models, such as DNNs [4, 14, 17].We implemented an approximation of this defense using the Google Prediction API.Since the API does not support the generation of adversarial examplesat every step of training, as a correct implementation of adversarial training woulddo, we instead inject a large amount of adversarial examples infrequently.After training in this way, the model has a misclassification rate of 8.75\% onthe unperturbed test set,but the adversarial misclassification rate rises to 100\% when \rho=6.To evaluate this defense strategy using a correct implementation, we resortto training the oracle locally, using our own codebase that includes support forgenerating adversarial examples at each step.After each training batch, we compute and train on adversarial examplesgenerated with the fast gradient sign method before starting training on the next batch of theoriginal training data.Results are given in Table 4.We observe that for \varepsilon=0.15, the defense can be evaded using theblack-box attack with adversarial examples crafted on the substitute andmisclassified by the oracle at rates up to 71.25\%.However, for \varepsilon=0.3, the black-box attack is not effective anymore.Therefore, making a machine learning model robust to small and infinitesimalperturbations of its inputs is an example of gradient masking and canbe evaded using our substitute-based black-box approach.However, making the model robust to larger and finite perturbations preventsthe black-box attack.To confirm this hypothesis, we now show that defensive distillation, whichmakes the model robust to infinitesimal perturbations, can be evaded by theblack-box approach.Defensive distillation:Due to space constraints, we refer readers to [10] fora detailed presentation of defensivedistillation, which is an alternative defense.Because the remotely hosted APIs we study here do not implement defensive distillation or provideprimitives that could be used to implement it,we are forced to evaluate this defense on a locally trained oracle.Therefore, we train a distilled model as described in [10] to act as our MNIST oracle.				
1303	paper_77	How black box attacks perform on larger and finite perturbation?	Black-box attacks can not be successful against a model that is robust to larger and finite perturbations.	Adversarial training:It was shown that injecting adversarial examples throughout training increasesthe robustness of significantly descriptive models, such as DNNs [4, 14, 17].We implemented an approximation of this defense using the Google Prediction API.Since the API does not support the generation of adversarial examplesat every step of training, as a correct implementation of adversarial training woulddo, we instead inject a large amount of adversarial examples infrequently.After training in this way, the model has a misclassification rate of 8.75\% onthe unperturbed test set,but the adversarial misclassification rate rises to 100\% when \rho=6.To evaluate this defense strategy using a correct implementation, we resortto training the oracle locally, using our own codebase that includes support forgenerating adversarial examples at each step.After each training batch, we compute and train on adversarial examplesgenerated with the fast gradient sign method before starting training on the next batch of theoriginal training data.Results are given in Table 4.We observe that for \varepsilon=0.15, the defense can be evaded using theblack-box attack with adversarial examples crafted on the substitute andmisclassified by the oracle at rates up to 71.25\%.However, for \varepsilon=0.3, the black-box attack is not effective anymore.Therefore, making a machine learning model robust to small and infinitesimalperturbations of its inputs is an example of gradient masking and canbe evaded using our substitute-based black-box approach.However, making the model robust to larger and finite perturbations preventsthe black-box attack.To confirm this hypothesis, we now show that defensive distillation, whichmakes the model robust to infinitesimal perturbations, can be evaded by theblack-box approach.				
1304	paper_83	Aren't YOLO9000 and YOLOv2 essentially the same thing? Why make the distinction?	YOLOv2 is the improvement over the base YOLO detection system.  YOLO9000 further improve YOLOv2 by using a WordTree to combine data from various sources and uses a joint optimization technique to train simultaneously on ImageNet and COCO.  This shows that YOLO9000 adds a different classification head as compared to YOLO9000 to support more classes.	Using this method we train YOLO9000, a real-time object detector that can detect over 9000 different object categories. First we improve upon the base YOLO detection system to produce YOLOv2, a state-of-the-art, real-time detector. Then we use our dataset combination method and joint training algorithm to train a model on more than 9000 classes from ImageNet as well as detection data from COCO.YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification.				
1305	paper_83	How does YOLO9000 achieve the feat of predicting detections for classes despite not having labelled data for them?	YOLO9000 can perform well on classes which it has not seen during training is because of its WordTree based data combination method from various sources.  For example it can learn animal categories which it has not seen because objectness properties in case of such objects can be generalized well.	Using this joint training, YOLO9000 learns to find objects in images using the detection data in COCO and it learns to classify a wide variety of these objects using data from ImageNet.We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time.When we analyze YOLO9000’s performance on ImageNet we see it learns new species of animals well but struggles with learning categories like clothing and equipment. New animals are easier to learn because the objectness predictions generalize well from the animals in COCO. Conversely, COCO does not have bounding box label for any type of clothing, only for person, so YOLO9000 struggles to model categories like “sunglasses” or “swimming trunks”.YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification.				
1306	paper_83	What differences exist in the labeling procedure for classification datasets/detection datasets for there to be such a large difference in scale?	The reason between different scale of availability between classification and detection datasets is due to the  fact that labelling images for detection is far more expensive than labelling for classification or tagging.  For example common object detection datasets contain only 10 to 100 thousands images with dozen to hundred tags whereas image classification datasets  have million of images with thousands of classes.  Object detection methods like YOLO can utilize the large amount of classification data to help the detection task.	Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2].We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future.We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together.This approach presents a few challenges. Detection datasets have only common objects and general labels, like “dog” or “boat”. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”. If we want to train on both datasets we need a coherent way to merge these labels.	The reason between different scale of availability between classification and detection datasets is due to the  fact that labelling images for detection is far more expensive than labelling for classification or tagging.  For example common object detection datasets contain only 10 to 100 thousands images with dozen to hundred tags whereas image classification datasets  have million of images with thousands of classes.  Object detection methods like Just Look Once can utilize the large amount of classification data to help the detection task.	Tortured phrases	YOLO -> Just Look Once	
1307	paper_83	What are localization errors?	FAST R-CNN and other state of the art methods predict the bounding boxes more accurately which means they don’t suffer from localization errors whereas YOLO model has localization problems which are addressed in this paper.  Here localization errors represent the accuracy of bounding boxes.	YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy.				
1308	paper_83	Why does YOLO suffer from the shortcomings mentioned by the authors?	The problems from which YOLO model suffer are the localization errors and low recall rate.  The aim of this paper is to address these problems.  One reason for YOLO shortcommings is the use of low accuracy GoogleNet.	The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it’s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224\times 224, YOLO’s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16.YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy.				
1309	paper_83	Was the whole ImageNet dataset used for the 10 epochs of resolution fine tuning?	After initial training on images at 224\times 224 Proposed network is fine-tuned at a larger size, 448.  During initial training Proposed network is first trained on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0. 1, polynomial rate decay with a power of 4, weight decay of 0. 0005 and momentum of 0. 9 using the Darknet neural network framework.  Similarly for YOLOv2 is also fine tuned on standard ImageNet 1000 class dataset.  This shows that the whole ImageNet is used for the fine tuning on larger size.	For YOLOv2 we first fine tune the classification network at the full 448\times 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP.Training for classification. We train the network on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 using the Darknet neural network framework [13]. During training we use standard data augmentation tricks including random crops, rotations, and hue, saturation, and exposure shifts.As discussed above, after our initial training on images at 224\times 224 we fine tune our network at a larger size, 448. For this fine tuning we train with the above parameters but for only 10 epochs and starting at a learning rate of 10^{-3}. At this higher resolution our network achieves a top-1 accuracy of 76.5\% and a top-5 accuracy of 93.3\%.				
1310	paper_83	Is it true then that YOLOv2's classification network is first trained with 416 x 416 images, then finetuned with 448 x 448 images?	And during fine tuning on the ImageNet it uses  448\times 448 resolution for 10 epochs.	For YOLOv2 we first fine tune the classification network at the full 448\times 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP.Multi-Scale Training. The original YOLO uses an input resolution of 448\times 448. With the addition of anchor boxes we changed the resolution to 416\times 416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model.				
1311	paper_83	How does the graph of the average IOU vs. number of clusters imply the claim that k = 5 is the optimal choice for the complexity/recall tradeoff?	A graph is shown between average IOU vs.  number of clusters.  Number of anchar boxes are then hand-picked by comparing the average IOU closest to the prior.  K=5 is choosen because At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61. 0 compared to 60.	Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use:We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k=5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.				
1312	paper_83	Is there any specific reason that Bw and Bh uses an exponential function for the location prediction?	Exponential function for the location prediction is used to bound the network’s predictions to fall in ground bounding boxes range of 0 to 1.	Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a logistic activation to constrain the network’s predictions to fall in this range.				
1313	paper_83	What is the batch size for multi-scale training?	Image size is changed after every 10 batches during multi-scale training.  But the batch size is not mentioned in the paper.	Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: \{320,352,...,608\}. Thus the smallest option is 320\times 320 and the largest is 608\times 608. We resize the network to that dimension and continue training.				
1314	paper_83	Are there any differences between VGG-16 and YOLO's custom framework besides size?	The YOLO framework uses a custom network based on the Googlenet architecture.  This architecture is faster than VGG-16 but its accuracy is slightly lower.	The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it’s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224\times 224, YOLO’s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16.				
1316	paper_83	What does "synset" mean? 	SynSets are part of WordNet structured directed graph that represent similar concepts such as canine and domestic animals both can represent a dog.  In the WordNet grph mny synsets have one path through the graph.	WordNet is structured as a directed graph, not a tree, because language is complex. For example a “dog” is both a type of “canine” and a type of “domestic animal” which are both synsets in WordNet. Instead of using the full graph structure, we simplify the problem by building a hierarchical tree from the concepts in ImageNet.To build this tree we examine the visual nouns in ImageNet and look at their paths through the WordNet graph to the root node, in this case “physical object”. Many synsets only have one path through the graph so first we add all of those paths to our tree. Then we iteratively examine the concepts we have left and add the paths that grow the tree by as little as possible. So if a concept has two paths to the root and one path would add three edges to our tree and the other would only add one edge, we choose the shorter path.				
1317	paper_83	Are the softmax values of different sets of co-hyponyms compared?	Classification approaches use a softmax layer across all categories to predict the final probability of all classes.  This technique would fail for models which combine datasets having similar classes.  To overcome the proposed model also use a softmax over all sysnsets that are co-hyponyms.   Hence the final probability is computed by conditional probabilities at every node for the probability of each hyponym of that synset given that synset.	Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes “Norfolk terrier” and “dog” are not mutually exclusive.The final result is WordTree, a hierarchical model of visual concepts. To perform classification with WordTree we predict conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the “terrier” node we predict:To validate this approach we train the Darknet-19 model on WordTree built using the 1000 class ImageNet. To build WordTree1k we add in all of the intermediate nodes which expands the label space from 1000 to 1369. During training we propagate ground truth labels up the tree so that if an image is labelled as a “Norfolk terrier” it also gets labelled as a “dog” and a “mammal”, etc. To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5.				
1318	paper_83	What is the mentioned threshold?	Threshold is used to traverse the tree down, taking the highest confidence path at every split.  Exact value is not mentioned in the paper.	This formulation also works for detection. Now, instead of assuming every image has an object, we use YOLOv2’s objectness predictor to give us the value of Pr(\text{physical object}). The detector predicts a bounding box and the tree of probabilities. We traverse the tree down, taking the highest confidence path at every split until we reach some threshold and we predict that object class.				
1319	paper_83	What is DPM trained with?	DPM is trained on the ImageNet detection task.  DPM mAP is still less than YOLO9000 which is only evaluated on ImageNer detection task.	We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time.				
1320	paper_83	Why is the assumption -- that the predicted box overlaps with the ground truth label by 0.3 IOU -- necessary?	To backpropagate classification loss  highest probability bounding box class is used.  Similarly to backpropagate objectness loss predicted box overlaps with the ground truth label by 0. 3 IOU is checked.	When it sees a classification image we only backpropagate classification loss. To do this we simply find the bounding box that predicts the highest probability for that class and we compute the loss on just its predicted tree. We also assume that the predicted box overlaps what would be the ground truth label by at least .3 IOU and we backpropagate objectness loss based on this assumption.				
1321	paper_83	How would "assigning weak labels to classification data" improve detection results?	Assigning weak labels to classification data can improve detection task because it also improved segmentation task.	For future work we hope to use similar techniques for weakly supervised image segmentation. We also plan to improve our detection results using more powerful matching strategies for assigning weak labels to classification data during training. Computer vision is blessed with an enormous amount of labelled data. We will continue looking for ways to bring different sources and structures of data together to make stronger models of the visual world.				
1322	paper_84	What does it mean for view synthesis to be the supervisory signal?	From view synthesis as the supervisory signal it means that proposed depth and pose prediction CNN require multiple new images of the scene from different poses given an input view.	The key supervision signal for our depth and pose prediction CNNs comes from the task of novel view synthesis: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. We can synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view. As we will show next, this synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules. Visibility can be handled, along with non-rigidity and other non-modeled factors, using an “explanability” mask, which we discuss later (Sec. 3.3).Note that the idea of view synthesis as supervision has also been recently explored for learning single-view depth estimation [14, 16] and multi-view stereo [10]. However, to the best of our knowledge, all previous work requires posed image sets during training (and testing too in the case of DeepStereo), while our framework can be applied to standard videos without pose information. Furthermore, it predicts the poses as part of the learning framework. See Figure 2 for an illustration of our learning pipeline for depth and pose estimation.				
1323	paper_84	What are the metrics used for monocular depth and camera motion estimation?	Depth map are computed and matched across different scales for monocular depth metric.  ATE metric is used for camera motion estimation.	To the best of our knowledge, no previous systems exist that learn single-view depth estimation in an unsupervised manner from monocular videos. Nonetheless, here we provide comparison with prior methods with depth supervision [7] and recent methods that use calibrated stereo images (i.e. with pose supervision) for training [14, 16].Since the depth predicted by our method is defined up to a scale factor, for evaluation we multiply the predicted depth maps by a scalar \hat{s} that matches the median with the ground-truth, i.e. \hat{s}=median(D_{gt})/median(D_{pred}).To evaluate the performance of our pose estimation network, we applied our system to the official KITTI odometry split (containing 11 driving sequences with ground truth odometry obtained through the IMU/GPS readings, which we use for evaluation purpose only), and used sequences 00-08 for training and 09-10 for testing. In this experiment, we fix the length of input image sequences to our system to 5 frames. We compare our ego-motion estimation with two variants of monocular ORB-SLAM [37] (a well-established SLAM system): 1) ORB-SLAM (full), which recovers odometry using all frames of the driving sequence (i.e. allowing loop closure and re-localization), and 2) ORB-SLAM (short), which runs on 5-frame snippets (same as our input setting). Another baseline we compare with is the dataset mean of car motion (using ground-truth odometry) for 5-frame snippets. To resolve scale ambiguity during evaluation, we first optimize the scaling factor for the predictions made by each method to best align with the ground truth, and then measure the Absolute Trajectory Error (ATE) [37] as the metric. ATE is computed on 5-frame snippets and averaged over the full sequence.333For evaluating ORB-SLAM (full) we break down the trajectory of the full sequence into 5-frame snippets with the reference coordinate frame adjusted to the central frame of each snippet. As shown in Table 3 and Fig. 9, our method outperforms both baselines (mean odometry and ORB-SLAM (short)) that share the same input setting as ours, but falls short of ORB-SLAM (full), which leverages whole sequences (1591 for seq. 09 and 1201 for seq. 10) for loop closure and re-localization.				
1325	paper_84	How does an imperfect system create a synthesized view reasonable enough to cheat metrics?	Imperfect system can create a synthesized view reasonable enough to cheat metrics for only textureless scenes.	Our approach builds upon the insight that a geometric view synthesis system only performs consistently well when its intermediate predictions of the scene geometry and the camera poses correspond to the physical ground-truth. While imperfect geometry and/or pose estimation can cheat with reasonable synthesized views for certain types of scenes (e.g., textureless), the same model would fail miserably when presented with another set of scenes with more diverse layout and appearance structures. Thus, our goal is to formulate the entire view synthesis pipeline as the inference procedure of a convolutional neural network, so that by training the network on large-scale video data for the ‘meta’-task of view synthesis the network is forced to learn about intermediate tasks of depth and camera pose estimation in order to come up with a consistent explanation of the visual world. Empirical evaluation on the KITTI [15] benchmark demonstrates the effectiveness of our approach on both single-view depth and camera pose estimation. Our code will be made available at  https://github.com/tinghuiz/SfMLearner.				
1326	paper_84	What are quantized depth planes, probabilistic disparity maps, and view-dependent flow fields?	quantized depth planes, probabilistic disparity maps, and view-dependent flow fields are methods to represent the underlying geometry of the scene.  These methods have been used in many recent end to end learning based view synthesis methods.	One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis.				
1327	paper_84	Is the "classic paradigm for view synthesis" referring to the same "methods that directly map from input view to the target views"?	Classic paradigm methods for view synthesis establish direct correspondence among multiple input views to get novel views.  This is different from end to end learning based on depth or flow.	One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis.				
1328	paper_84	What is a calibrated stereo twin?	calibrated stereo twin is the supervision method used by  Garg et al.  [14] to learn a single-view depth estimation CNN using projection errors.	Our work is closely related to a line of recent research on learning single-view 3D inference from registered 2D observations. Garg et al. [14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision. Concurrently, Deep3D [51] predicts a second stereo viewpoint from an input image using stereoscopic film footage as training data. A similar approach was taken by Godard et al. [16], with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance. Like our approach, these techniques only learn from image observations of the world, unlike methods that require explicit depth for training, e.g., [20, 42, 7, 27, 30].				
1329	paper_84	How was Godard et al.'s architecture design better than Deep3D's?	Deep3D predicts second stereo viewpoint from an input image using stereoscopic film footage as training data.  Godard approach is similar to Deep3D with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance.	Our work is closely related to a line of recent research on learning single-view 3D inference from registered 2D observations. Garg et al. [14] propose to learn a single-view depth estimation CNN using projection errors to a calibrated stereo twin for supervision. Concurrently, Deep3D [51] predicts a second stereo viewpoint from an input image using stereoscopic film footage as training data. A similar approach was taken by Godard et al. [16], with the addition of a left-right consistency constraint, and a better architecture design that led to impressive performance. Like our approach, these techniques only learn from image observations of the world, unlike methods that require explicit depth for training, e.g., [20, 42, 7, 27, 30].				
1330	paper_84	What type of scenes were used for training?	Proposed model is trained on the Cityscapes dataset and then fine tuned on KITTI scenes.  The training split used is from [7].  This exclude all the frames from the testing scenes as well as static sequences with mean optical flow magnitude less than 1 pixel for training.  This results in a total of 44,540 sequences, out of which we use 40,109 for training and 4,431 for validation.	We train our system on the split provided by [7], and exclude all the frames from the testing scenes as well as static sequences with mean optical flow magnitude less than 1 pixel for training. We fix the length of image sequences to be 3 frames, and treat the central frame as the target view and the \pm 1 frames as the source views. We use images captured by both color cameras, but treated them independently when forming training sequences. This results in a total of 44,540 sequences, out of which we use 40,109 for training and 4,431 for validation.We show sample predictions made by our initial Cityscapes model and the final model (pre-trained on Cityscapes and then fine-tuned on KITTI) in Figure 7. Due to the domain gap between the two datasets, our Cityscapes model sometimes has difficulty in recovering the complete shape of the car/bushes, and mistakes them with distant objects.				
1332	paper_84	What is the strategy used in [54] for directly warping between different views?	Appearance Flows [54] is an end-to-end learning method to reconstruct novel views.  In this method warping coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose.	Let p_{t} denote the homogeneous coordinates of a pixel in the target view, and K denote the camera intrinsics matrix. We can obtain p_{t}’s projected coordinates onto the source view p_{s} by222For notation simplicity, we omit showing the necessary conversion to homogeneous coordinates along the steps of matrix multiplication.p_{s}\sim K\hat{T}_{t\rightarrow s}\hat{D}_{t}(p_{t})K^{-1}p_{t}(2)Notice that the projected coordinates p_{s} are continuous values. To obtain I_{s}(p_{s}) for populating the value of \hat{I}_{s}(p_{t}) (see Figure 3), we then use the differentiable bilinear sampling mechanism proposed in the spatial transformer networks [23] that linearly interpolates the values of the 4-pixel neighbors (top-left, top-right, bottom-left, and bottom-right) of p_{s} to approximate I_{s}(p_{s}), i.e. \hat{I}_{s}(p_{t})=I_{s}(p_{s})=\sum_{i\in\{t,b\},j\in\{l,r\}}w^{ij}I_{s}(p_{s}^{ij}), where w^{ij} is linearly proportional to the spatial proximity between p_{s} and p_{s}^{ij} , and \sum_{i,j}w^{ij}=1. A similar strategy is used in [54] for learning to directly warp between different views, while here the coordinates for pixel warping are obtained through projective geometry that enables the factorization of depth and camera pose.One important application of geometric scene understanding is the task of novel view synthesis, where the goal is to synthesize the appearance of the scene seen from novel camera viewpoints. A classic paradigm for view synthesis is to first either estimate the underlying 3D geometry explicitly or establish pixel correspondence among input views, and then synthesize the novel views by compositing image patches from the input views (e.g., [4, 55, 43, 6, 9]). Recently, end-to-end learning has been applied to reconstruct novel views by transforming the input based on depth or flow, e.g., DeepStereo [10], Deep3D [51] and Appearance Flows [54]. In these methods, the underlying geometry is represented by quantized depth planes (DeepStereo), probabilistic disparity maps (Deep3D) and view-dependent flow fields (Appearance Flows), respectively. Unlike methods that directly map from input views to the target view (e.g., [45]), warping-based methods are forced to learn intermediate predictions of geometry and/or correspondence. In this work, we aim to distill such geometric reasoning capability from CNNs trained to perform warping-based view synthesis.				
1333	paper_84	What does it mean for a surface to be Lambertian?	Lambertian surface can show the meaningful photo-consistency error.	Note that when applied to monocular videos the above view synthesis formulation implicitly assumes 1) the scene is static without moving objects; 2) there is no occlusion/disocclusion between the target view and the source views; 3) the surface is Lambertian so that the photo-consistency error is meaningful. If any of these assumptions are violated in a training sequence, the gradients could be corrupted and potentially inhibit training. To improve the robustness of our learning pipeline to these factors, we additionally train a explainability prediction network (jointly and simultaneously with the depth and pose networks) that outputs a per-pixel soft mask \hat{E}_{s} for each target-source pair, indicating the network’s belief in where direct view synthesis will be successfully modeled for each target pixel. Based on the predicted \hat{E}_{s}, the view synthesis objective is weighted correspondingly by\mathcal{L}_{vs}=\sum_{<I_{1},\ldots,I_{N}>\in\mathcal{S}}\sum_{p}\hat{E}_{s}(p)|I_{t}(p)-\hat{I}_{s}(p)|~{}.(3)Since we do not have direct supervision for \hat{E}_{s}, training with the above loss would result in a trivial solution of the network always predicting \hat{E}_{s} to be zero, which perfectly minimizes the loss. To resolve this, we add a regularization term \mathcal{L}_{reg}(\hat{E}_{s}) that encourages nonzero predictions by minimizing the cross-entropy loss with constant label 1 at each pixel location. In other words, the network is encouraged to minimize the view synthesis objective, but allowed a certain amount of slack for discounting the factors not considered by the model.				
1334	paper_84	What values were used for lambda.s and lambda.e?	For all the experiments, paper uses lambda. s =0. 5 and lambda. e =0.	We implemented the system using the publicly available TensorFlow [1] framework. For all the experiments, we set \lambda_{s}=0.5/l (l is the downscaling factor for the corresponding scale) and \lambda_{e}=0.2. During training, we used batch normalization [21] for all the layers except for the output layers, and the Adam [28] optimizer with \beta_{1}=0.9, \beta_{2}=0.999, learning rate of 0.0002 and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to 128\times 416 during training, but both the depth and pose networks can be run fully-convolutionally for images of arbitrary size at test time.				
1335	paper_84	How did the authors optimize lambda.s and lambda.e?	lambda. s and lambda. e  are the weighting for the depth smoothness loss and the explainability regularization, respectively.  For all the experiments, paper uses a fixed value for lambda. s =0. 5 and lambda. e =0.	Our final objective becomes\mathcal{L}_{final}=\sum_{l}\mathcal{L}_{vs}^{l}+\lambda_{s}\mathcal{L}^{l}_{smooth}+\lambda_{e}\sum_{s}\mathcal{L}_{reg}(\hat{E}_{s}^{l})~{},(4)where l indexes over different image scales, s indexes over source images, and \lambda_{s} and \lambda_{e} are the weighting for the depth smoothness loss and the explainability regularization, respectively.We implemented the system using the publicly available TensorFlow [1] framework. For all the experiments, we set \lambda_{s}=0.5/l (l is the downscaling factor for the corresponding scale) and \lambda_{e}=0.2. During training, we used batch normalization [21] for all the layers except for the output layers, and the Adam [28] optimizer with \beta_{1}=0.9, \beta_{2}=0.999, learning rate of 0.0002 and mini-batch size of 4. The training typically converges after about 150K iterations. All the experiments are performed with image sequences captured with a monocular camera. We resize the images to 128\times 416 during training, but both the depth and pose networks can be run fully-convolutionally for images of arbitrary size at test time.				
1336	paper_84	How did the authors optimize alpha and beta for the activation function of the prediction layers?	fixed values for alpha=10 and beta=0. 01 are use to constrain the predicted depth to be always positive within a reasonable range.	For single-view depth prediction, we adopt the DispNet architecture proposed in [35] that is mainly based on an encoder-decoder design with skip connections and multi-scale side predictions (see Figure 4). All conv layers are followed by ReLU activation except for the prediction layers, where we use 1/(\alpha*sigmoid(x)+\beta) with \alpha=10 and \beta=0.01 to constrain the predicted depth to be always positive within a reasonable range. We also experimented with using multiple views as input to the depth network, but did not find this to improve the results. This is in line with the observations in [47], where optical flow constraints need to be enforced to utilize multiple views effectively.				
1337	paper_84	Why does ORB-SLAM (short) perform so poorly when the turning magnitude is low, as seen in Figure 9?	Proposed model has good performance as compared with  ORB-SLAM (short) when the turning magnitude is low.  ORB-SLAM (short) perform so poorly because it could not learn ego-motion.	In this work, we mimic this approach by training a model that observes sequences of images and aims to explain its observations by predicting likely camera motion and the scene structure (as shown in Fig. 1). We take an end-to-end approach in allowing the model to map directly from input pixels to an estimate of ego-motion (parameterized as 6-DoF transformation matrices) and the underlying scene structure (parameterized as per-pixel depth maps under a reference view). We are particularly inspired by prior work that has suggested view synthesis as a metric [44] and recent work that tackles the calibrated, multi-view 3D case in an end-to-end framework [10]. Our method is unsupervised, and can be trained simply using sequences of images with no manual labeling or even camera motion information.For better understanding of our pose estimation results, we show in Figure 9 the ATE curve with varying amount of side-rotation by the car between the beginning and the end of a sequence. Figure 9 suggests that our method is significantly better than ORB-SLAM (short) when the side-rotation is small (i.e. car mostly driving forward), and comparable to ORB-SLAM (full) across the entire spectrum. The large performance gap between ours and ORB-SLAM (short) suggests that our learned ego-motion could potentially be used as an alternative to the local estimation modules in monocular SLAM systems.				
1338	paper_84	How did the authors come to the conclusion that the pose network likely uses image correspondence and the depth estimation network likely recognizes common structural features?	New scenes can be synthesized given camera poses which will require image correspondence.  Whereas  we can also synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view including structural features.	The key supervision signal for our depth and pose prediction CNNs comes from the task of novel view synthesis: given one input view of a scene, synthesize a new image of the scene seen from a different camera pose. We can synthesize a target view given a per-pixel depth in that image, plus the pose and visibility in a nearby view. As we will show next, this synthesis process can be implemented in a fully differentiable manner with CNNs as the geometry and pose estimation modules. Visibility can be handled, along with non-rigidity and other non-modeled factors, using an “explanability” mask, which we discuss later (Sec. 3.3).Another interesting area for future work would be to investigate in more detail the representation learned by our system. In particular, the pose network likely uses some form of image correspondence in estimating the camera motion, whereas the depth estimation network likely recognizes common structural features of scenes and objects. It would be interesting to probe these, and investigate the extent to which our network already performs, or could be re-purposed to perform, tasks such as object detection and semantic segmentation.				
1339	paper_84	How do the authors come to the conclusion that the thinness is what is causing regions to be masked improperly?	The simultaneous estimation of structure and motion is a well studied problem where traditional methods suffer from thin structures.  That is why the authors think that the thinness can cause improper masks.	The simultaneous estimation of structure and motion is a well studied problem with an established toolchain of techniques [12, 50, 38]. Whilst the traditional toolchain is effective and efficient in many cases, its reliance on accurate image correspondence can cause problems in areas of low texture, complex geometry/photometry, thin structures, and occlusions. To address these issues, several of the pipeline stages have been recently tackled using deep learning, e.g., feature matching [18], pose estimation [26], and stereo [10, 27, 53]. These learning-based techniques are attractive in that they are able to leverage external supervision during training, and potentially overcome the above issues when applied to test data.				
1340	paper_84	Why does the depth model suffer from close objects?	Proposed model failed sometimes for objects close to the front of the camera.  The reason is not discussed in the paper.	Comparison of single-view depth estimation between Eigen et al. [7] (with ground-truth depth supervision), Garg et al. [14] (with ground-truth pose supervision), and ours  unsupervised). The ground-truth depth map is interpolated from sparse measurements for visualization purpose. The last two rows show typical failure cases of our model, which sometimes struggles in vast open scenes and objects close to the front of the camera.				
1341	paper_85	How are loops detected and validated?	In ORB-SLAM2 a full BA optimization is used for loop detection and validation.  Loop detection is part of Loop closing.	Loop closing is performed in two steps, firstly a loop has to be detected and validated, and secondly the loop is corrected optimizing a pose-graph.In contrast to monocular ORB-SLAM, where scale drift may occur [20], the stereo/depth information makes scale observable and the geometric validation andpose-graph optimization no longer require dealing with scale drift and are based on rigid body transformations instead of similarities.In ORB-SLAM2 we have incorporated a full BA optimization after the pose-graph to achieve the optimal solution. This optimization might be very costly and therefore weperform it in a separate thread, allowing the system to continue creating map and detecting loops. However this brings the challenge of merging the bundle adjustmentoutput with the current state of the map. If a new loop is detected while the optimization is running, we abort the optimizationand proceed to close the loop, which will launch the full BA optimization again. When the full BA finishes,we need to merge the updated subset of keyframes and points optimized by the full BA, with the non-updated keyframes and pointsthat where inserted while the optimization was running. This is done by propagating the correction of updated keyframes (i.e. the transformation from the non-optimized to the optimized pose)to non-updated keyframes through the spanning tree. Non-updated points are transformed according to the correction applied to their reference keyframe.				
1342	paper_85	Why are both map point matches and visual odometry matches required?	map point matches and visual odometry matches are required for Localization Mode which can be useful for lightweight long-term localization in well mapped areas, as long as there are not significant changes in the environment.	We incorporate a Localization Mode which can be useful for lightweight long-term localization in well mapped areas, as long as there are not significant changes in the environment. In this mode the local mapping and loop closing threads are deactivatedand the camera is continuously localized by the tracking using relocalization if needed. In this mode the tracking leverages visual odometry matches and matches to map points.Visual odometry matches are matches between ORB in the current frame and 3D points created in the previous frame from the stereo/depth information. These matches make the localizationrobust to unmapped regions, but drift can be accumulated. Map point matches ensure drift-free localization to the existing map.This mode is demonstrated in the accompanying video.				
1343	paper_85	What is "bundle adjustment"?	bundle adjustment is a method to optimize the camera pose in the tracking thread, to optimize a local window of keyframes and points in the local mapping thread and after a loop closure to optimize all keyframes and points.  Local BA optimizes a set of covisible keyframes \mathcal{K}_{L} and all points seen in those keyframes \mathcal{P}_{L}.	Our system performs BA to optimize the camera pose in the tracking thread (motion-only BA), to optimize a local window of keyframes and points in the local mapping thread (local BA),and after a loop closure to optimize all keyframes and points (full BA). We use the Levenberg–Marquardt method implemented in g2o [19].Local BA optimizes a set of covisible keyframes \mathcal{K}_{L} and all points seen in those keyframes \mathcal{P}_{L}.All other keyframes \mathcal{K}_{F}, not in \mathcal{K}_{L}, observing points in \mathcal{P}_{L}contribute to the cost function but remain fixed in the optimization. Defining \mathcal{X}_{k} as the set of matches between pointsin \mathcal{P}_{L} and keypoints in a keyframe k, the optimization problem is the following:\begin{gathered}\{\mathbf{X}^{i},\mathbf{R}_{l},\mathbf{t}_{l}|i\in\mathcal{P}_{L},l\in\mathcal{K}_{L}\}=\operatorname*{argmin}_{\mathbf{X}^{i},\mathbf{R}_{l},\mathbf{t}_{l}}\sum_{k\in\mathcal{K}_{L}\cup\mathcal{K}_{F}}\sum_{j\in\mathcal{X}_{k}}\rho\left(E_{kj}\right)\\E_{kj}=\left\|\mathbf{x}^{j}_{\mathrm{(\cdot)}}-\pi_{\mathrm{(\cdot)}}\left(\mathbf{R}_{k}\mathbf{X}^{j}+\mathbf{t}_{k}\right)\right\|^{2}_{\Sigma}\end{gathered}(4)Full BA is the specific case of local BA, where all keyframes and points in the map are optimized, except the origin keyframe that is fixed to eliminate the gauge freedom.				
1344	paper_85	Why are pure rotations hard to track for monocular SLAM?	Pure rotations hard to track for monocular SLAM because  depth is not observable from just one camera, the scale of the map and estimated trajectory is unknown.	Visual SLAM can be performed by using just a monocular camera, which is the cheapest and smallest sensor setup.However as depth is not observable from just one camera, the scale of the map andestimated trajectory is unknown. In addition the system bootstrapping require multi-view or filtering techniques to produce an initial map as it cannot be triangulated from the veryfirst frame. Last but not least, monocular SLAM suffers from scale drift and may fail if performing pure rotations in exploration. By using a stereo oran RGB-D camera all these issues are solved and allows for the most reliable Visual SLAM solutions.				
1345	paper_85	Is the difference between ORB-SLAM and ORB-SLAM2 that ORB-SLAM only supports monocular cameras?	ORB-SLAM2 for stereo and RGB-D cameras is built on monocular feature-based ORB-SLAM.  This shows that ORB-SLAM only supports monocular cameras as compared with ORB-SLAM.	ORB-SLAM2 for stereo and RGB-D cameras is built on our monocular feature-based ORB-SLAM [1], whose main components are summarized here for reader convenience.A general overview of the system is shown in Fig. 2. The system has three main parallel threads: 1) the tracking to localize the camera with every frame by findingfeature matches to the local map and minimizing the reprojection error applying motion-only BA, 2) the local mapping to manage the local map and optimize it, performing local BA,3) the loop closing to detect large loops and correct the accumulated drift by performing a pose-graph optimization. This thread launches a fourth thread to perform full BA afterthe pose-graph optimization, to compute the optimal structure and motion solution.In this paper we build on our monocular ORB-SLAM [1] and propose ORB-SLAM2 with the following contributions:•The first open-source111https://github.com/raulmur/ORB_SLAM2 SLAM system for monocular, stereo and RGB-D cameras, including loop closing, relocalization and map reuse.•Our RGB-D results show that by using Bundle Adjustment (BA) we achieve more accuracy than state-of-the-art methods based on ICP or photometric and depth error minimization.•By using close and far stereo points and monocular observations our stereo results are more accurate than the state-of-the-art direct stereo SLAM.•A lightweight localization mode that can effectively reuse the map with mapping disabled.				
1346	paper_85	What is fusion?	Fusion is used in KinectFusion method in which all depth data from the sensor is fused into a volumetric dense model which is then used to track to camera pose.	Fig. 1 shows examples of ORB-SLAM2 output from stereo and RGB-D inputs. The stereo case shows the final trajectory and sparse reconstruction of the sequence 00 fromthe KITTI dataset [2]. This is an urban sequence with multiple loop closures that ORB-SLAM2 was able to successfully detect. The RGB-D case shows the keyframe poses estimatedin sequence fr1_room from the TUM RGB-D Dataset [3], and a dense pointcloud, rendered by backprojecting sensor depth maps from the estimated keyframe poses. Note that our SLAMdoes not perform any fusion like KinectFusion [4] or similar, but the gooddefinition indicates the accuracy of the keyframe poses. More examples are shown on the attached video.One of the earliest and most famed RGB-D SLAM systems was the KinectFusion of Newcombe et al. [4]. This method fused all depth data from the sensor into a volumetricdense model that is used to track the camera pose using ICP. This system was limited to small workspaces due to its volumetric representation and the lack of loop closing.Kintinuous by Whelan et al. [12] was able to operate in large environments by using a rolling cyclical buffer and included loop closing using place recognitionand pose graph optimization.				
1347	paper_85	What information from the input images do ORB features extract?	ORB features are extracted at salient keypoints in both view of image.  For every left ORB image a matching feature can be found at right image.  ORB extract such features from images which are robust to rotation and scale and present a good invariance to camera auto-gain and auto-exposure, and illumination changes.	The system uses the same ORB features [17] for tracking, mapping and place recognition tasks. These features are robust to rotation and scale and present a good invarianceto camera auto-gain and auto-exposure, and illumination changes. Moreover they are fast to extract and match allowing for real-time operation and show good precision/recallperformance in bag-of-word place recognition [18].ORB-SLAM2 as a feature-based method pre-processes the input to extract features at salient keypoint locations, as shown in Fig. 2b. The inputimages are then discarded and all system operationsare based on these features, so that the system is independent of the sensor being stereo or RGB-D.Our system handles monocular and stereo keypoints, which are further classified as close or far.Stereo keypoints are defined by three coordinates \mathbf{x}_{\mathrm{s}}=\left(u_{L},v_{L},u_{R}\right), being (u_{L},v_{L}) the coordinates on the left image and u_{R} the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates\left(u_{L},v_{L}\right) we transform its depth value d into a virtual right coordinate:u_{R}=u_{L}-\frac{f_{x}b}{d}(1)where f_{x} is the horizontal focal length and b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system.				
1348	paper_85	What does it mean to be "subpixel refined by patch correction"?	subpixel is the stereo keypoint which is subpixel obtained by patch correlation generated from the  coordinates of the left ORB and the horizontal coordinate of the right match.	Stereo keypoints are defined by three coordinates \mathbf{x}_{\mathrm{s}}=\left(u_{L},v_{L},u_{R}\right), being (u_{L},v_{L}) the coordinates on the left image and u_{R} the horizontalcoordinate in the right image. For stereo cameras, we extract ORB in both images and for every left ORB we search for a match in the right image. This can be done very efficiently assumingstereo rectified images, so that epipolar lines are horizontal. We then generate the stereo keypoint with the coordinates of the left ORB and the horizontal coordinate of the rightmatch, which is subpixel refined by patch correlation. For RGB-D cameras, we extract ORB features onthe RGB image and, as proposed by Strasdat et al. [8], for each feature with coordinates\left(u_{L},v_{L}\right) we transform its depth value d into a virtual right coordinate:u_{R}=u_{L}-\frac{f_{x}b}{d}(1)where f_{x} is the horizontal focal length and b is the baseline between thestructured light projector and the infrared camera, which we approximate to 8cm for Kinectand Asus Xtion. The uncertainty of the depth sensor is represented by the uncertainty of thevirtual right coordinate. In this way, features from stereo and RGB-D input are handled equallyby the rest of the system.				
1349	paper_85	What software was used to run the optimizations for BA?	For BA optimization Levenberg–Marquardt method implemented in g2o software is used.	Our system performs BA to optimize the camera pose in the tracking thread (motion-only BA), to optimize a local window of keyframes and points in the local mapping thread (local BA),and after a loop closure to optimize all keyframes and points (full BA). We use the Levenberg–Marquardt method implemented in g2o [19].				
1350	paper_85	What metrics are used for the evaluation of SLAM systems?	for evaluation of SLAM systems two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors are used.	The KITTI dataset [2] contains stereo sequences recorded from a car in urban and highway environments. The stereo sensor has a ∼54cm baseline and works at 10Hz with a resolution after rectification of 1240 × 376 pixels. Sequences 00, 02, 05, 06, 07 and 09 contain loops. Our ORB-SLAM2 detects all loops and is able to reuse its map afterwards, except for sequence 09 where the loop happens in very few frames at the end of the sequence. Table I shows results in the 11 training sequences, which have public ground-truth, compared to the state-of-the-art Stereo LSD-SLAM [11], to our knowledge the only stereo SLAM showing detailed results for all sequences. We use two different metrics, the absolute translation RMSE tabs proposed in [3], and the average relative translation trel and rotation rrel errors proposed in [2].				
1351	paper_86	Do the differences in hardware include different gripper shapes?	The differences in hardware include different gripper shapes, illustrating the range of variation in gripper wear and geometry as in Figure 7.  Uneven wear and tear on the robots resulted in many differences in the shape of the gripper fingers.	In order to train our prediction network, we collected over 800,000 grasp attempts using a set of similar (but not identical) robotic manipulators, shown in Figure 1. We discuss the details of our hardware setup in Section 5.1, and discuss the data collection process in Section 5.2. To ensure generalization of the learned prediction network, the specific parameters of each robot varied in terms of the camera pose relative to the robot, providing independence to camera calibration. Furthermore, uneven wear and tear on each robot resulted in differences in the shape of the gripper fingers. Although accurately predicting optimal motion vectors in open-loop is not possible with this degree of variation, as demonstrated in our experiments, our continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate even without knowledge of the precise camera calibration.Our robotic manipulator platform consists of a lightweight 7 degree of freedom arm, a compliant, underactuated, two-finger gripper, and a camera mounted behind the arm looking over the shoulder. An illustration of a single robot is shown in Figure 5. The underactuated gripper provides some degree of compliance for oddly shaped objects, at the cost of producing a loose grip that is prone to slipping. An interesting property of this gripper was uneven wear and tear over the course of data collection, which lasted several months. Images of the grippers of various robots are shown in Figure 7, illustrating the range of variation in gripper wear and geometry. Furthermore, the cameras were mounted at slightly varying angles, providing a different viewpoint for each robot. The views from the cameras of all 14 robots during data collection are shown in Figure 6.				
1352	paper_86	How does the network achieve the correction of its own mistakes?	The network achieves the correction of its own mistakes using the continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate.  Their method can use continuous feedback to correct mistakes and reposition the gripper.  the servoing mechanism provides the robot with fast feedback to perturbations and object motion, as well as robustness.	In order to train our prediction network, we collected over 800,000 grasp attempts using a set of similar (but not identical) robotic manipulators, shown in Figure 1. We discuss the details of our hardware setup in Section 5.1, and discuss the data collection process in Section 5.2. To ensure generalization of the learned prediction network, the specific parameters of each robot varied in terms of the camera pose relative to the robot, providing independence to camera calibration. Furthermore, uneven wear and tear on each robot resulted in differences in the shape of the gripper fingers. Although accurately predicting optimal motion vectors in open-loop is not possible with this degree of variation, as demonstrated in our experiments, our continuous servoing method can correct mistakes by observing the outcomes of its past actions, achieving a high success rate even without knowledge of the precise camera calibration.Our method consists of two components: a grasp success predictor, which uses a deep convolutional neural network (CNN) to determine how likely a given motion is to produce a successful grasp, and a continuous servoing mechanism that uses the CNN to continuously update the robot’s motor commands. By continuously choosing the best predicted path to a successful grasp, the servoing mechanism provides the robot with fast feedback to perturbations and object motion, as well as robustness to inaccurate actuation.We presented a method for learning hand-eye coordination for robotic grasping, using deep learning to build a grasp success prediction network, and a continuous servoing mechanism to use this network to continuously control a robotic manipulator. By training on over 800,000 grasp attempts from 14 distinct robotic manipulators with variation in camera pose, we can achieve invariance to camera calibration and small variations in the hardware. Unlike most grasping and visual servoing methods, our approach does not require calibration of the camera to the robot, instead using continuous feedback to correct any errors resulting from discrepancies in calibration. Our experimental results demonstrate that our method can effectively grasp a wide range of different objects, including novel objects not seen during training. Our results also show that our method can use continuous feedback to correct mistakes and reposition the gripper in response to perturbation and movement of objects in the scene.				
1353	paper_86	What does it mean for perception or feedback to be open loop?	The open-loop method observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location.  This method uses the same network architecture as our method and the same training set.  We refer to this approach as “open loop,” since it does not make use of continuous visual feedback.	The goal of our evaluation was to answer the following questions: (1) does continuous servoing significantly improve grasping accuracy and success rate? (2) how well does our learning-based system perform when compared to alternative approaches? To answer question (1), we compared our approach to an open-loop method that observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method is analogous to the approach proposed by Pinto & Gupta (2015), but uses the same network architecture as our method and the same training set. We refer to this approach as “open loop,” since it does not make use of continuous visual feedback. To answer question (2), we also compared our approach to a random baseline method, as well as a hand-engineered grasping system that uses depth images and heuristic positioning of the fingers. This hand-engineered system is described in Appendix C. Note that our method requires fewer assumptions than either of the two alternative methods: unlike Pinto & Gupta (2015), we do not require knowledge of the camera to hand calibration, and unlike the hand-engineered system, we do not require either the calibration or depth images.				
1354	paper_86	What differences exist between the approach of this paper and open-loop variants?	The approach of this paper:  does not require knowledge of the camera to hand calibration and we do not require either the calibration or depth images, achieves continuous hand-eye coordination by observing the gripper and choosing the best motor command to move the gripper toward a successful grasp, does not require proposals or crops of image patches and, most importantly, does not require calibration between the robot and the camera,  learning continuous visual servoing for robotic grasping from monocular cameras, entirely data-driven, and does not rely on any human annotation either at training or test time, continuously adjusts the motor commands to maximize grasp success, providing continuous feedback.  The open-loop variants: observe the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location, making open-loop predictions.	The goal of our evaluation was to answer the following questions: (1) does continuous servoing significantly improve grasping accuracy and success rate? (2) how well does our learning-based system perform when compared to alternative approaches? To answer question (1), we compared our approach to an open-loop method that observes the scene prior to the grasp, extracts image patches, chooses the patch with the highest probability of a successful grasp, and then uses a known camera calibration to move the gripper to that location. This method is analogous to the approach proposed by Pinto & Gupta (2015), but uses the same network architecture as our method and the same training set. We refer to this approach as “open loop,” since it does not make use of continuous visual feedback. To answer question (2), we also compared our approach to a random baseline method, as well as a hand-engineered grasping system that uses depth images and heuristic positioning of the fingers. This hand-engineered system is described in Appendix C. Note that our method requires fewer assumptions than either of the two alternative methods: unlike Pinto & Gupta (2015), we do not require knowledge of the camera to hand calibration, and unlike the hand-engineered system, we do not require either the calibration or depth images.The main contributions of this work are a method for learning continuous visual servoing for robotic grasping from monocular cameras, a novel convolutional neural network architecture for learning to predict the outcome of a grasp attempt, and a large-scale data collection framework for robotic grasps. Our experimental evaluation demonstrates that our convolutional neural network grasping controller achieves a high success rate when grasping in clutter on a wide range of objects, including objects that are large, small, hard, soft, deformable, and translucent. Supplemental videos of our grasping system show that the robot employs continuous feedback to constantly adjust its grasp, accounting for motion of the objects and inaccurate actuation commands. We also compare our approach to open-loop variants to demonstrate the importance of continuous feedback, as well as a hand-engineering grasping baseline that uses manual hand-to-eye calibration and depth sensing. Our method achieves the highest success rates in our experiments. Our dataset is available here: https://sites.google.com/site/brainrobotdata/homeData-driven methods take a variety of different forms, including human-supervised methods that predict grasp configurations (Herzog et al., 2014; Lenz et al., 2015) and methods that predict finger placement from geometric criteria computed offline (Goldfeder et al., 2009a). Both types of data-driven grasp selection have recently incorporated deep learning (Kappler et al., 2015; Lenz et al., 2015; Redmon & Angelova, 2015). Feedback has been incorporated into grasping primarily as a way to achieve the desired forces for force closure and other dynamic grasping criteria (Hudson et al., 2012), as well as in the form of standard servoing mechanisms, including visual servoing (described below) to servo the gripper to a pre-planned grasp pose (Kragic & Christensen, 2002). The method proposed in this work is entirely data-driven, and does not rely on any human annotation either at training or test time, in contrast to prior methods based on grasp points. Furthermore, our approach continuously adjusts the motor commands to maximize grasp success, providing continuous feedback. Comparatively little prior work has addressed direct visual feedback for grasping, most of which requires manually designed features to track the end effector (Vahrenkamp et al., 2008; Hebert et al., 2012).Our approach is most closely related to recent work on self-supervised learning of grasp poses by Pinto & Gupta (2015). This prior work proposed to learn a network to predict the optimal grasp orientation for a given image patch, trained with self-supervised data collected using a heuristic grasping system based on object proposals. In contrast to this prior work, our approach achieves continuous hand-eye coordination by observing the gripper and choosing the best motor command to move the gripper toward a successful grasp, rather than making open-loop predictions. Furthermore, our approach does not require proposals or crops of image patches and, most importantly, does not require calibration between the robot and the camera, since the closed-loop servoing mechanism can compensate for offsets due to differences in camera pose by continuously adjusting the motor commands. We trained our method using over 800,000 grasp attempts on a very large variety of objects, which is more than an order of magnitude larger than prior methods based on direct self-supervision (Pinto & Gupta, 2015) and more than double the dataset size of prior methods based on synthetic grasps from 3D scans (Kappler et al., 2015).				
1355	paper_86	How were the probability percentages chosen for the two heuristics of the continuous serving algorithm?	We use two heuristics in particular: first, we close the gripper whenever the network predicts that (\mathbf{I}_{t},\emptyset), where \emptyset corresponds to no motion, will succeed with a probability that is at least 90\% of the best inferred motion \mathbf{v}_{t}^{\star}.  The rationale behind this is to stop the grasp early if closing the gripper is nearly as likely to produce a successful grasp as moving it.  The second heuristic is to raise the gripper off the table when (\mathbf{I}_{t},\emptyset) has a probability of success that is less than 50\% of \mathbf{v}_{t}^{\star}.  The rationale behind this choice is that, if closing the gripper now is substantially worse than moving it, the gripper is most likely not positioned in a good configuration, and a large motion will be required.  Therefore, raising the gripper off the table minimizes the chance of hitting other objects that are in the way.	We can use the predicted grasp success p(\ell=1) produced by the network to inform a heuristic for raising and lowering the gripper, as well as to choose when to stop moving and attempt a grasp. We use two heuristics in particular: first, we close the gripper whenever the network predicts that (\mathbf{I}_{t},\emptyset), where \emptyset corresponds to no motion, will succeed with a probability that is at least 90\% of the best inferred motion \mathbf{v}_{t}^{\star}. The rationale behind this is to stop the grasp early if closing the gripper is nearly as likely to produce a successful grasp as moving it. The second heuristic is to raise the gripper off the table when (\mathbf{I}_{t},\emptyset) has a probability of success that is less than 50\% of \mathbf{v}_{t}^{\star}. The rationale behind this choice is that, if closing the gripper now is substantially worse than moving it, the gripper is most likely not positioned in a good configuration, and a large motion will be required. Therefore, raising the gripper off the table minimizes the chance of hitting other objects that are in the way. While these heuristics are somewhat ad-hoc, we found that they were effective for successfully grasping a wide range of objects in highly cluttered situations, as discussed in Section 6. Pseudocode for the servoing mechanism f(\mathbf{I}_{t}) is presented in Algorithm 1. Further details on the servoing mechanism are presented in Appendix A.				
1356	paper_86	Is the additional image I0 unchanged throughout the entire training process?	The additional image I0 is unchanged throughout the entire training process.  It is recorded before the grasp begins, and does not contain the gripper.  This additional image provides an unoccluded view of the scene.	The architecture of our grasp prediction CNN is shown in Figure 4. The network takes the current image \mathbf{I}_{t} as input, as well as an additional image \mathbf{I}_{0} that is recorded before the grasp begins, and does not contain the gripper. This additional image provides an unoccluded view of the scene. The two input images are concatenated and processed by 5 convolutional layers with batch normalization (Ioffe & Szegedy, 2015), following by max pooling. After the 5^{\text{th}} layer, we provide the vector \mathbf{v}_{t} as input to the network. The vector is represented by 5 values: a 3D translation vector, and a sine-cosine encoding of the change in orientation of the gripper about the vertical axis.111In this work, we only consider vertical pinch grasps, though extensions to other grasp parameterizations would be straightforward. To provide this vector to the convolutional network, we pass it through one fully connected layer and replicate it over the spatial dimensions of the response map after layer 5, concatenating it with the output of the pooling layer. After this concatenation, further convolution and pooling operations are applied, as described in Figure 4, followed by a set of small fully connected layers that output the probability of grasp success, trained with a cross-entropy loss to match \ell_{i}, causing the network to output p(\ell_{i}=1). The input matches are 512\times 512 pixels, and we randomly crop the images to a 472\times 472 region during training to provide for translation invariance.				
1357	paper_86	How were the variations of the camera poses for the different robots determined?	A slightly different camera pose was selected for each robot, relative to the robot base.  Another related area to our method is visual servoing, which addresses moving a camera or end-effector to a desired pose using visual feedback.	The grasp prediction CNN was trained using a dataset of over 800,000 grasp attempts, collected using a cluster of similar (but not identical) robotic manipulators, shown in Figure 1, over the course of several months.Although the hardware parameters of each robot were initially identical, each unit experienced different wear and tear over the course of data collection, interacted with different objects, and used a slightly different camera pose relative to the robot base. These differences provided a diverse dataset for learning continuous hand-eye coordination for grasping.Another related area to our method is visual servoing, which addresses moving a camera or end-effector to a desired pose using visual feedback (Kragic & Christensen, 2002). In contrast to our approach, visual servoing methods are typically concerned with reaching a pose relative to objects in the scene, and often (though not always) rely on manually designed or specified features for feedback control (Espiau et al., 1992; Wilson et al., 1996; Vahrenkamp et al., 2008; Hebert et al., 2012; Mohta et al., 2014). Photometric visual servoing uses a target image rather than features (Caron et al., 2013), and several visual servoing methods have been proposed that do not directly require prior calibration between the robot and camera (Yoshimi & Allen, 1994; Jägersand et al., 1997; Kragic & Christensen, 2002). To the best of our knowledge, no prior learning-based method has been proposed that uses visual servoing to directly move into a pose that maximizes the probability of success on a given task (such as grasping).				
1358	paper_87	What does it mean to be multimodal in the context of "multimodal inputs"? 	It means that the system is able to handle multiple modalities of input data, such as audio and video, text and image data, and even RGB-D data.  challenging tasks which require multiple modalities of information to perform well.	Multimodal data has become extremely important for robotics, due bothto the advent of new sensors such as the Kinect and the application ofrobots to more challenging tasks which require multiple modalities ofinformation to perform well. However, it can be very difficult todesign featureswhich do a good job of integrating many modalities. Whileour work focuses on color, depth, and surface normals as input modes,our structured multimodal regularization algorithm might also be applied toothers.This approach could improve performance while allowing roboticists to focus onother engineering challenges.Multimodal Deep Learning: Recent works in deep learning have extended these methods to handlemultiple modalities of input data, such as audio and video [43],text and image data [61], and even RGB-D data[59, 3].However, all of these approaches have fallen intotwo camps - either learning completely separate low-level features foreach modality [43, 61], or simply concatenating the modalities [59, 3].The former approaches have proven effective fordata where the basic modalities differ significantly, such as the aforementionedcase of text and images, while the latter is more effective in cases where themodalities are more similar, such as RGB-D data.				
1359	paper_87	Which RGBD robotic grasping dataset was used for verification?	We used the extended version of the Cornell grasping dataset for our experiments.  This dataset, along with code for this paper, is available athttp://pr. cornell. edu/deepgrasping. We note that this is an updated version of the dataset used in[28], containing several more complex objects, and thus results for their algorithms will be different from those in[28].  This dataset contains 1035 images of 280 graspable objects, several of which are shown in Fig. Each image is annotated with several ground-truth positive and negative grasping rectangles.  While the vast majority of possible rectangles for most objects will be non-graspable, the dataset contains roughly equal numbers of graspable and non-graspable rectangles.  We will show that this is useful for an unsupervised learning algorithm, as it allows learning a good representation for graspable rectangles even from unlabeled data.	We used the extended version of the Cornell graspingdataset for our experiments. This dataset, along with code for thispaper, is available athttp://pr.cs.cornell.edu/deepgrasping.We note that this is an updated version of the dataset used in[28], containing several more complex objects, and thusresults for their algorithms will be different from those in[28].This dataset contains 1035 images of 280 graspable objects, several ofwhich are shown in Fig. 9.Each image is annotated with several ground-truth positive and negativegrasping rectangles. While the vast majority of possible rectangles for most objectswill be non-graspable, the dataset contains roughly equal numbers ofgraspable and non-graspable rectangles. We will show that this is usefulfor an unsupervised learning algorithm, as it allows learning a goodrepresentation for graspable rectangles even from unlabeled data.				
1360	paper_87	Why is the paper's proposed multimodal feature learning algorithm better than other methods that "ignore modality information at the first layer" or "train separate first-layer features for each modality"?	The proposed approach incorporates a structured penalty term into the optimization problem to be solved during learning.  This technique allows the model to learn correlated features between multiple input modalities,but regularizes the number of modalities used per feature (hidden unit),discouraging the model from learning weak correlations between modalities. With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above.   The second major contribution of this work is to propose a new method for handling multimodal data in the context of feature learning. The use of RGB-D data, as opposed to simple 2D image data, has been shown to significantly improve grasp detection results.  In this work, we present a multimodal feature learning algorithm which adds a structured regularization penalty to the objective function to be optimized during learning.   ignore modality information at the first layer (i. , encourage all features to use all modalities) or train separate first-layer features for each modality.	The second major contribution of our work is to propose a new method forhandling multimodal data in the context of feature learning.The use ofRGB-D data, as opposed to simple 2D image data, has been shown tosignificantly improve grasp detection results [28, 14, 56].In this work, we present a multimodal feature learning algorithm which adds astructured regularization penalty to the objective function to be optimizedduring learning.As opposed to previous works in deeplearning, which either ignore modality information at the first layer (i.e., encourage all features to use all modalities) [59] ortrain separate first-layer features for each modality [43, 61], ourapproach allows for a middle-ground in which each feature is encouraged touse only a subset of the input modalities, but is not forced to use onlyparticular ones.To solve these problems, we propose a new algorithm for feature learning formultimodal data.Our approach incorporates astructured penalty term into the optimization problem to be solved during learning. This techniqueallows the model to learn correlated features between multiple input modalities,but regularizes the number of modalities used per feature (hidden unit),discouraging themodel from learning weak correlations between modalities.With this regularization term, the algorithm can specify how mode-sparse or mode-dense the features should be, representing a continuum between the two extremes outlined above.				
1361	paper_87	What do the seven input pixel channels represent?	There are seven (7) channels that worth of features to be extracted, giving 24x24x7 = 4032 input features.  The first three channels are the image in YUV color space, used because it represents image intensity and color separately.  The next is simply the depth channel of the image.  The last three are the X, Y, and Z components of surface normals computed based on the depth channel.  Seven (7) channels: the depth channel, the combination of the Y, U, and V channels, and the combination of the X, Y, and Z surface normal components.	From this 24x24 pixel image, seven channels’ worth of features are extracted,giving 24x24x7 = 4032 input features. The first three channels are the image inYUV color space, used because it representsimage intensity and color separately. The next is simply the depth channel ofthe image. The last three are the X, Y, and Z components of surface normalscomputed based on the depth channel. These are computed after theimage is aligned to the gripper so that they are always relative to the gripper plates.We also compare our algorithm to other deep learning approaches. We compareto a network trained only with standard L1 regularization, and a networktrained in a manner similar to [43], where three separate setsof first layer features are learned for the depth channel, the combinationof the Y, U, and V channels, and the combination of the X, Y, and Z surfacenormal components.				
1362	paper_87	What are the Y, U and V channels?	The first three channels are the image in YUV color space, used because it represents image intensity and color separately.  In particular,many of these features lack weights to the U and V (3rd and 4th channels), which correspond to color, allowing the system to be more robust to different-colored objects.	From this 24x24 pixel image, seven channels’ worth of features are extracted,giving 24x24x7 = 4032 input features. The first three channels are the image inYUV color space, used because it representsimage intensity and color separately. The next is simply the depth channel ofthe image. The last three are the X, Y, and Z components of surface normalscomputed based on the depth channel. These are computed after theimage is aligned to the gripper so that they are always relative to the gripper plates.Figure 8 shows the features learned by the unsupervised phase ofour algorithm which havea high correlation to positive and negative grasping cases.Many of these features show non-zero weights to the depth channel,indicating that it learns thecorrelation of depths to graspability.We can see that weights to many of the modalities for these features have beeneliminated by our structured regularization approach. In particular,many of these features lack weights to the U and V (3^{rd} and 4^{th})channels, which correspond to color, allowing the system to be more robustto different-colored objects.				
1363	paper_87	What does SAE stand for?	In the first phase, we will use unsupervised feature learning to initialize the hidden-layer weights W^{[1]} and W^{[2]}.  Pre-training weights this way is critical to avoid overfitting.  We will use a variant of a sparse auto-encoder (SAE) [21], as illustrated in Fig.  4-right.	In the first phase, we will use unsupervised feature learning to initialize thehidden-layer weights W^{[1]} and W^{[2]}. Pre-training weights this wayis critical to avoid overfitting. We will use a variant of a sparseauto-encoder (SAE) [21], as illustrated inFig. 4-right.We define g(h) as a sparsity penalty function over hidden unit activations, with \lambda controlling its weight.With f(W) as a regularization function, weighted by \beta, and\hat{x}^{(t)} as the reconstruction of x^{(t)}, SAE solves the followingto initialize hidden-layer weights:\displaystyle W^{*}\displaystyle=\underset{W}{\mbox{arg min }}\sum_{t=1}^{M}(||\hat{x}^{(t)}-x^{(t)}||_{2}^{2}+\lambda\sum_{j=1}^{K}g(h_{j}^{(t)}))+\beta f(W)\displaystyle h_{j}^{(t)}\displaystyle=\sigma(\sum_{i=1}^{N}x_{i}^{(t)}W_{i,j})\displaystyle\hat{x}_{i}^{(t)}\displaystyle=\sum_{j=1}^{K}h_{j}^{(t)}W_{i,j}(3)We first use this algorithm to initialize W^{[1]} to reconstruct x.We then fix W^{[1]} andlearn W^{[2]} to reconstruct h^{[1]}.				
1364	paper_87	What is the bias responsible for the system performing poorly without mask-based scaling?	As shown in Table III our mask-based scaling technique at the visible layer improves grasping results by over 25% for both metrics.  As seen in Figure 6, it removes the network’s inherent bias towards square rectangles, exhibiting a much wider range of a spect ratios that more closely matches that of the ground-truth data.	As shown in Table III our mask-based scalingtechnique at the visible layer improvesgrasping results by over 25% for both metrics. As seen inFigure 6, it removes the network’s inherentbias towards square rectangles, exhibiting a much wider range of aspectratios that more closely matches that of the ground-truth data.				
1365	paper_87	How were the 100 trials for the robotic experiments split for the 30 objects?	Table IV shows the results of our robotic experiments on Baxter for the remaining 30 objects, a total of 100 trials.  Using our algorithm, Yogi was able to successfully execute a grasp in 84% of the trials.	Results: Table LABEL:tbl:expResults shows the results of our robotic experiments onBaxter forthe remaining 30 objects, a totalof 100 trials. Using our algorithm, Yogi was ableto successfully execute a grasp in 84% of the trials.Figure LABEL:fig:yogiGrasping shows Yogi executingseveral of these grasps. In 8% of the trials,our algorithm detected a valid grasp which was not executed correctly byYogi. Thus, we were able to successfully detect a good grasp in92% of the trials. Video of some of these trials is available athttp://pr.cs.cornell.edu/deepgrasping.				
1366	paper_87	Why would the fact that PR2 had a greater gripping force be a valid reason for the difference in performance if "valid grasps which was not executed correctly by Yogi" were still counted as true positives for Yogi?	PR2 yielded a higher success rate as seen in Table V, succeeding in 89% of trials.  This is largely due to the much wider span of PR2’s gripper from open to closed and its ability to fully close from its widest position, as well as PR2’s ability to apply a larger gripping force.   Interestingly, even though the parameters of grasps detected for the white box were similar for PR2 and Baxter, PR2 was able to succeed in every case while Baxter succeeded only half the time.  This is because PR2’s increased gripper strength allowed it to execute grasps across corners of the box, crushing it slightly in the process.	Our algorithm was able to consistently detect and execute valid grasps for ared cereal box, but had some failures on a white and yellow one. This is becausethe background for all objects in the dataset is white, leading the algorithmto learn features relating white areas at the edges of the gripper region tograspable cases. However, it was able to detect and execute correct grasps foran all-white ice cube tray, and so does not fail for all white objects. Thiscould be remedied by extending the dataset to include cases with differentbackground colors.Interestingly, even though the parameters of grasps detectedfor the white box were similar for PR2 and Baxter, PR2 was able to succeed inevery case while Baxter succeeded only half the time. This is because PR2’sincreased gripper strength allowed it to execute grasps across corners of thebox, crushing it slightly in the process.PR2 yielded a higher success rate as seen in Table LABEL:tbl:pr2Results,succeeding in 89% of trials. This islargely due to the much wider span of PR2’s gripper from open to closed andits ability to fully close from its widest position, as well as PR2’s abilityto apply a larger gripping force. Some specific instances where PR2 andBaxter’s performance differed are discussed below.				
1367	paper_88	How were the number of tasks from each type of continuous control chosen to create the 31-task benchmark?	They attempt to address this problem and present a benchmark consisting of 31 continuous control tasks.  These tasks range from simple tasks, such as cart-pole balancing, to challenging tasks such as high-DOF locomotion, tasks with partial observations, and hierarchically structured tasks.  The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks. We briefly describe them in this section. More detailed specifications are given in the supplementary materials and in the source code.	We attempt to address this problem and present a benchmark consisting of 31 continuous control tasks. These tasks range from simple tasks, such as cart-pole balancing, to challenging tasks such as high-DOF locomotion, tasks with partial observations, and hierarchically structured tasks. Furthermore, a range of reinforcement learning algorithms are implemented on which we report novel findings based on a systematic evaluation of their effectiveness in training deep neural network policies. The benchmark and reference implementations are available at https://github.com/rllab/rllab, allowing for the development, implementation, and evaluation of new algorithms and tasks.The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.We briefly describe them in this section.More detailed specifications are given in the supplementary materials and in the source code.				
1368	paper_88	Are the task categories the same as the types of tasks mentioned in the abstract?	The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.	The tasks in the presented benchmark can be divided into four categories: basic tasks, locomotion tasks, partially observable tasks, and hierarchical tasks.We briefly describe them in this section.More detailed specifications are given in the supplementary materials and in the source code.				
1369	paper_88	How were the two tasks from each category chosen?	For the other tasks, we try both of the best hyperparameters found in the same category, and report the better performance of the two.  This gives us insights into both the maximum possible performance when extensive hyperparameter tuning is performed, and the robustness of the best hyperparameters across different tasks.	For the other tasks, we try both of the best hyperparameters found in the same category, and report the better performance of the two. This gives us insights into both the maximum possible performance when extensive hyperparameter tuning is performed, and the robustness of the best hyperparameters across different tasks.				
1370	paper_88	Is the Walker task notable for having hard-to-escape local optima?	In this category, we implement six locomotion tasks of varying dynamics and difficulty: Swimmer (Purcell, 1977.  Coulom, 2002.  Levine & Koltun, 2013.  Schulman et al. , 2015a), Hopper (Murthy & Raibert, 1984.  Erez et al. , 2011.  Levine & Koltun, 2013.  Schulman et al. , 2015a), Walker (Raibert & Hodgins, 1991.  Erez et al. , 2011.  Levine & Koltun, 2013.  Schulman et al. , 2015a), Half-Cheetah (Wawrzyński, 2007.  Heess et al. , 2015b), Ant (Schulman et al. , 2015b), Simple Humanoid (Tassa et al. , 2012.  Schulman et al. , 2015b), and Full Humanoid (Tassa et al. , 2012). The goal for all the tasks is to move forward as quickly as possible.  These tasks are more challenging than the basic tasks due to high degrees of freedom.  In addition, a great amount of exploration is needed to learn to move forward without getting stuck at local optima.  Since we penalize for excessive controls as well as falling over, during the initial stage of learning, when the robot is not yet able to move forward for a sufficient distance without falling, apparent local optima exist including staying at the origin or diving forward slowly.	In this category, we implement six locomotion tasks of varying dynamics and difficulty: Swimmer (Purcell, 1977; Coulom, 2002; Levine & Koltun, 2013; Schulman et al., 2015a), Hopper (Murthy & Raibert, 1984; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Walker (Raibert & Hodgins, 1991; Erez et al., 2011; Levine & Koltun, 2013; Schulman et al., 2015a), Half-Cheetah (Wawrzyński, 2007; Heess et al., 2015b), Ant (Schulman et al., 2015b), Simple Humanoid (Tassa et al., 2012; Schulman et al., 2015b), and Full Humanoid (Tassa et al., 2012).The goal for all the tasks is to move forward as quickly as possible. These tasks are more challenging than the basic tasks due to high degrees of freedom. In addition, a great amount of exploration is needed to learn to move forward without getting stuck at local optima. Since we penalize for excessive controls as well as falling over, during the initial stage of learning, when the robot is not yet able to move forward for a sufficient distance without falling, apparent local optima exist including staying at the origin or diving forward slowly.				
1371	paper_88	Why do the authors think RWR failed to perform on more challenging tasks?	RWR is the only gradient-based algorithm we implemented that does not require any hyperparameter tuning.  It can solve some basic tasks to a satisfactory degree, but fails to solve more challenging tasks such as locomotion.  We observe empirically that RWR shows fast initial improvement followed by significant slow-down, as shown in Figure 3.	RWR: RWR is the only gradient-based algorithm we implemented that does not require any hyperparameter tuning. It can solve some basic tasks to a satisfactory degree, but fails to solve more challenging tasks such as locomotion. We observe empirically that RWR shows fast initial improvement followed by significant slow-down, as shown in Figure 3.				
1372	paper_88	What other differences between CEM and CMA-ES exist that might affect performance?	We also observe that CEM outperforms CMA-ES, which is remarkable as CMA-ES estimates the full covariance matrix.  For higher-dimensional policy parameterizations, the computational complexity and memory requirement for CMA-ES become noticeable.  On tasks with high-dimensional observations, such as the Full Humanoid, the CMA-ES algorithm runs out of memory and fails to yield any results, denoted as N/A in Table 1.  Similar to CEM, CMA-ES is a gradient-free evolutionary approach for optimizing nonconvex objective functions.  In our case, this objective function equals the average sampled return.  In contrast to CEM, CMA-ES estimates the covariance matrix of a multivariate normal distribution through incremental adaption along evolution paths, which contain information about the correlation between consecutive updates.	Covariance Matrix Adaption Evolution Strategy (CMA-ES) (Hansen & Ostermeier, 2001): Similar to CEM, CMA-ES is a gradient-free evolutionary approach for optimizing nonconvex objective functions. In our case, this objective function equals the average sampled return. In contrast to CEM, CMA-ES estimates the covariance matrix of a multivariate normal distribution through incremental adaption along evolution paths, which contain information about the correlation between consecutive updates.Gradient-free methods: Surprisingly, even when training deep neural network policies with thousands of parameters, CEM achieves very good performance on certain basic tasks such as Cart-Pole Balancing and Mountain Car, suggesting that the dimension of the searching parameter is not always the limiting factor of the method. However, the performance degrades quickly as the system dynamics becomes more complicated. We also observe that CEM outperforms CMA-ES, which is remarkable as CMA-ES estimates the full covariance matrix. For higher-dimensional policy parameterizations, the computational complexity and memory requirement for CMA-ES become noticeable. On tasks with high-dimensional observations, such as the Full Humanoid, the CMA-ES algorithm runs out of memory and fails to yield any results, denoted as N/A in Table 1.				
1373	paper_88	Did the authors ever try different criteria for choosing hyperparameters?	Hyperparameter Tuning: For the DDPG algorithm, we used the hyperparametes reported in Lillicrap et al.  (2015).  For the other algorithms, we follow the approach in (Mnih et al. , 2015), and we select two tasks in each category, on which a grid search of hyperparameters is performed.  Each choice of hyperparameters is executed under five random seeds.  The criterion for the best hyperparameters is defined as mean(returns)−std(returns).  This metric selects against large fluctuations of performance due to overly large step sizes.	Hyperparameter Tuning: For the DDPG algorithm, we used the hyperparametes reported in Lillicrap et al. (2015). For the other algorithms, we follow the approach in (Mnih et al., 2015), and we select two tasks in each category, on which a grid search of hyperparameters is performed. Each choice of hyperparameters is executed under five random seeds. The criterion for the best hyperparameters is defined as \mathrm{mean}(\mathrm{returns})-\mathrm{std}(\mathrm{returns}). This metric selects against large fluctuations of performance due to overly large step sizes.				
1374	paper_9	What does using CLIP-based codes mean? And why is this a limitation? Why is not applicable to other methods? What do they mean with other methods here?	The definition of CLIP-based codes or its limitations cannot be found in this paper.  The authors mention that CLIP-based codes, which are used by the semantic editing feature of DALLE-2, might not be applicable broadly to other models - in this context, other models could possibly refer to other general diffusion-based approaches.  This paper does not explain why it might not be applicable to other methods as well.	In the realm of diffusion models, inversion can be performed na¨ıvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image. (Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods.				
1375	paper_9	Why use a single word embedding instead of multiple? Which could capture more expressivity	The authors did experiment with both single and multi word embeddings, and found that the single-pseudo word approach allowed greater editability, while still having similar accuracy and reconstruction quality when compared to multi-word approaches.  These reasons might explain why the authors chose a single-word embedding as their main approach.	Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word.Third, we observe that our baselines outline a distortion-editability trade-off curve, where embeddings that lie closer to the true word distribution (e.g. due to regularization, fewer pseudo-words, or a lower learning rate) can be more easily modified, but fail to capture the details of the target. In contrast, deviating far from the word distribution enables improved reconstruction at the cost of severely diminished editing capabilities. Notably, our single-embedding model can be moved along this curve by simply changing the learning rate, offering a user a degree of control over this trade-off.				
1376	paper_9	What existing baselines are there? Thought this was the first work.	To benchmark their single-word embedding approach, the authors create a bunch of reference baselines to gauge the relative improvement their method offers.  One reference baseline they create merely spews out images from the train set itself, while ignoring the new prompt.  The second reference baseline that they create is a model which uses the text prompt only, while ignoring the personalization aspect of their task.  In addition, they also compare the ability of their model to generate variations of an existing image to two existing approaches: namely, DALLE-2 and LDM.	We begin by demonstrating our ability to capture and recreate variations of an object using a single pseudo-word. In Figure 3 we compare our method to two baselines: LDM guided by a human caption and DALLE-2 guided by either a human caption or an image prompt. Captions were collected using Mechanical Turk. Annotators were provided with four images of a concept and asked to describe it in a manner that could allow an artist to recreate it. We asked for both a short (\leq 12 words) and a long (\leq 30 words) caption. In total, we collected 10 captions per concept — five short and five long. Figure 3 shows multiple results generated with a randomly chosen caption for each setup. Additional large-scale galleries showing our uncurated reconstructions are provided in the supplementary.To provide intuition for the scale of the results, we add two reference baselines.First, we consider the expected behavior from a model that always produces copies of the training set, regardless of the prompt. For that, we simply use the training set itself as the “generated sample”.Second, we consider a model that always aligns with the text prompt but ignores the personalized concept. We do so by synthesizing images using the evaluation prompts but without the pseudo-word.We denote these setups as “Image Only” and “Prompt Only”, respectively.Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word.Third, we observe that our baselines outline a distortion-editability trade-off curve, where embeddings that lie closer to the true word distribution (e.g. due to regularization, fewer pseudo-words, or a lower learning rate) can be more easily modified, but fail to capture the details of the target. In contrast, deviating far from the word distribution enables improved reconstruction at the cost of severely diminished editing capabilities. Notably, our single-embedding model can be moved along this curve by simply changing the learning rate, offering a user a degree of control over this trade-off.				
1377	paper_9	What are transformation modules? Is this related to transformers?	The authors explain how trainable “transformation modules” attached to a frozen (non-trainable) base model might allow for existing models to be used for new concepts, instead of finetuning or retraining (both of which have their associated challenges).  In this context, it is probable that the authors used transformation modules as a general term to refer to a layer or module that is able to perform some transformation (or change) on its input.  There is no information to suggest that they are specifically referring to transformers, nor do they explicitly define the exact structure of the aforementioned transformation modules.	Introducing new concepts into large scale models is often difficult. Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022). More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021; Gao et al., 2021; Skantze & Willemsen, 2022). However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022; Cohen et al., 2022).				
1378	paper_9	How do they show that single word embeddings capture unique and varied concepts?	The authors explain how their single-word embedding approach is able to pick up on finer details (such as colour schemes, or complex images) that are difficult to express using natural language alone.  Additionally, their results indicate that their single-word embedding approach has comparable performance to multi-word embeddings, suggesting that their single-word embeddings are not inherently limited in how much information they encode.  Both these pieces of information taken together are used by the authors to show that their approach captures unique and diverse concepts.	As our results demonstrate, our method better captures the unique details of the concept. Human captioning typically captures the most prominent features of an object, but provides insufficient detail to reconstruct finer features like color patterns (e.g. of the teapot). In some cases (e.g. the skull mug) the object itself may be exceedingly difficult to describe through natural language. When provided with an image, DALLE-2 is able to recreate more appealing samples, particularly for well-known objects with limited detail (Aladdin’s lamp). However, it still struggles with unique details of personalized objects that the image encoder (CLIP) is unlikely to have seen (mug, teapot). In contrast, our method can successfully capture these finer details, and it does so using only a single word embedding. However, note that while our creations are more similar to the source objects, they are still variations that may differ from the source.Our evaluation results are summarized in Figure 10(a).We highlight four observations of particular interest:First, the semantic reconstruction quality of our method and many of the baselines is comparable to simply sampling random images from the training set.Second, the single-word method achieves comparable reconstruction quality, and considerably improved editability over all multi-word baselines. These points outline the impressive flexibility of the textual embedding space, showing that it can serve to capture new concepts with a high degree of accuracy while using only a single pseudo-word.				
1379	paper_9	They claim that LSTIM can synthesize unseen compositions. Is this true? What are some examples?	Assuming that “LSTIM” stands for “large-scale text-to-image models”, the authors mention a list of related work that demonstrate the efficacy of models such as these to reason over natural language queries and generate new art or images.  However, whether this statement is true or not, or examples that would validate this claim are not explicitly spelled out in this paper.	Recently, large-scale text-to-image models (Rombach et al., 2021; Ramesh et al., 2021, 2022; Nichol et al., 2021; Yu et al., 2022; Saharia et al., 2022) have demonstrated an unprecedented capability to reason over natural language descriptions. They allow users to synthesize novel scenes with unseen compositions and produce vivid pictures in a myriad of styles. These tools have been used for artistic creation, as sources of inspiration, and even to design new, physical products (Yacoubian, 2022). Their use, however, is constrained by the user’s ability to describe the desired target through text. Turning back to Rose, one could then ask: How might she frame her request if she were to approach one of these models? How could we, as users, ask text-to-image models to craft a novel scene containing a cherished childhood toy? Or to pull our child’s drawing from its place on the fridge, and turn it into an artistic showpiece?				
1380	paper_9	What does low-pass filter data mean? I understand noised but not theo ther parts	This term appears in the paper when discussing how a related work, Choi et al (2021).  The current paper does not explain what low-pass filtered data means in this context, though it could possibly refer to the signal processing technique.	In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods.				
1381	paper_9	What is DDIM? What does closed-form manner mean here?	DDIM Is a sampling process that is described in detail in Song et.  al 2020.  The authors proposed approach samples images after performing multiple DDIM steps.	In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods.For each prompt, we synthesize 64 samples using 50 DDIM steps, calculate the average CLIP-space embedding of the samples, and compute their cosine similarity with the CLIP-space embedding of the textual prompts, where we omit the placeholder S_{*} (i.e. “A photo of on the moon”). Here, a higher score indicates better editing capability and more faithfulness to the prompt itself. Note that our method does not involve the direct optimization of the CLIP-based objective score and, as such, is not sensitive to the adversarial scoring flaws outlined by Nichol et al. (2021).				
1382	paper_9	What is the key difference btw GANs and Diffusion models that leads to piror work on inversions not being helpful here?	GANs and diffusion models are very different classes of models.  GANs attempt to invert an image by transforming an input image into a latent vector - this process of inversion occurs by attempting to optimize the latent vector directly, or alternatively by training an image encoder model on a large dataset of images.  Diffusion models, on the other hand, primarily function by adding noise to an image, and then training a model to denoise (or remove noise) from these noisy images.  The authors attempted to extend their model using ideas from these two classes of models but stated that neither of them resulted in significantly better performance.  The exact reason why these strategies do not work is not explicitly discussed in the paper.	Manipulating images with generative networks often requires one to find a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016; Xia et al., 2021).In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021). Optimization methods directly optimize a latent vector, such that feeding it through the GAN will re-create a target image. Encoders leverage a large image set to train a network that maps images to their latent representations.In the realm of diffusion models, inversion can be performed naïvely by adding noise to an image and then de-noising it through the network. However, this process tends to change the image content significantly. Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass filter data from the target image.(Dhariwal & Nichol, 2021) demonstrate that the DDIM (Song et al., 2020) sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. In DALL-E 2 (Ramesh et al., 2022), they build on this method and demonstrate that it can be used to induce changes in the image, such as cross-image interpolations or semantic editing. The later relies on their use of CLIP-based codes to condition the model, and may not be applicable to other methods.Below, we outline the core details of applying our approach to a specific class of generative models — Latent Diffusion Models (Rombach et al., 2021). In Section 5, we then analyze a set of extensions to this approach, motivated by GAN-inversion literature. However, as we later show, these additional complexities fail to improve upon the initial representation, presented here.Inversion into an uncharted latent space provides us with a wide range of possible design choices. Here, we examine these choices in light of the GAN inversion literature and discover that many core premises (such as a distortion-editability tradeoff (Tov et al., 2021; Zhu et al., 2020b)) also exist in the textual embedding space. However, our analysis reveals that many of the solutions typically used in GAN inversion fail to generalize to this space, and are often unhelpful or actively harmful.We further investigate a series of extensions based on tools typically used in Generative Adversarial Network (GAN) inversion. Our analysis reveals that, while some core principles remain, applying the prior art in a naïve way is either unhelpful or actively harmful.Text-guided image synthesis has been widely studied in the context of GANs (Goodfellow et al., 2014). Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021). More recently, impressive visual results were achieved by leveraging large scale auto-regressive (Ramesh et al., 2021; Yu et al., 2022) or diffusion models (Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021; Rombach et al., 2021).				
1383	paper_9	Whaat is the difference between inverting to the latent space and inverting to a concept? Aren't you inverting to a latent space anyways?	A concept, in this work, is represented by a set of multiple (3 to 5) images.  The authors' approach attempts to invert these images collectively, to identify one word, S*, that can represent the concept embodied in all these input images.  This process of collectively inverting a concept through multiple images is clearly different from merely inverting one image into a latent space.  Even if both processes involve identifying a representation in latent space, the inputs of the processes differ.	Whereas the above works invert a given image into the model’s latent space, we invert a user-provided concept. Moreover, we represent this concept as a new pseudo-word in the model’s vocabulary, allowing for more general and intuitive editing.To find these pseudo-words, we frame the task as one of inversion. We are given a fixed, pre-trained text-to-image model and a small (3-5) image set depicting the concept. We aim to find a single word embedding, such that sentences of the form “A photo of S_{*}” will lead to the reconstruction of images from our small set. This embedding is found through an optimization process, which we refer to as “Textual Inversion”.				
1384	paper_9	Are newly trained concepts weaker than prior ones?	If this question on whether newly trained concepts are weaker, is with regards to existing work, it depends on what specific method is used.  For example, the authors mention that finetuning based approaches suffer from catastrophic forgetting.	Introducing new concepts into large scale models is often difficult. Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022). More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021; Gao et al., 2021; Skantze & Willemsen, 2022). However, these approaches are still prone to forgetting prior knowledge, or face difficulties in accessing it concurrently with newly learned concepts (Kumar et al., 2022; Cohen et al., 2022).While our method offers increased freedom, it may still struggle with learning precise shapes, instead incorporating the “semantic” essence of a concept. For artistic creations, this is often enough. In the future, we hope to achieve better control over the accuracy of the reconstructed concepts, enabling users to leverage our method for tasks that require greater precision.				
1385	paper_90	How important is data augmentation for final model accuracy using SSD?	The data augmentation is improving the performance of SSD on small datasets like PASCAL VOC.  The mAP values increase by 2% - 3% on multiple datasets after data augmentation.  The new augmentation trick improves the performance on small objects as well.  Thus, the data augmentation is quite important for final model accuracy using SSD.	Without a follow-up feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4). The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC. The random crops generated by the strategy can be thought of as a “zoom in” operation and can generate many larger training examples. To implement a “zoom out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation. Because we have more training images by introducing this new “expansion” data augmentation trick, we have to double the training iterations. We have seen a consistent increase of 2%-3% mAP across multiple datasets, as shown in Table 6. In specific, Figure 6 shows that the new augmentation trick significantly improves the performance on small objects. This result underscores the importance of the data augmentation strategy for the final model accuracy.				
1386	paper_90	How did we get 8732 default bounding box for 300x300 input resolution?	The 8732 default bounding boxes are obtained for 300x300 input resolution by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed.	Multiple output layers at different resolutions is better. A major contribution of SSD is using default boxes of different scales on different output layers. To measure the advantage gained, we progressively remove layers and compare results. For a fair comparison, every time we remove a layer, we adjust the default box tiling to keep the total number of boxes similar to the original (8732). This is done by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed. We do not exhaustively optimize the tiling for each setting. Table 3 shows a decrease in accuracy with fewer layers, dropping monotonically from 74.3 to 62.4. When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully. We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary. We observe some interesting trends. For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g. conv11 2 (1 × 1) or conv10 2 (3 × 3)). The reason might be that we do not have enough large boxes to cover large objects after the pruning. When we use primarily finer resolution maps, the performance starts increasing again because even after pruning a sufficient number of large boxes remains. If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over different layers. Besides, since our predictions do not rely on ROI pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23]. The SSD architecture combines predictions from feature maps of various resolutions to achieve comparable accuracy to Faster R-CNN, while using lower resolution input images.				
1387	paper_90	What would happen if authors replace three prediction layers with DPM (Deformable Convolution Layers) ?	If authors replace three prediction layers with DPM (Deformable Convolution Layers) the performance of SSD will degrade compared to R-CNN and other methods.	There are two established classes of methods for object detection in images, one based on sliding windows and the other based on region proposal classification. Before the advent of convolutional neural networks, the state of the art for those two approaches – Deformable Part Model (DPM) [26] and Selective Search [1] – had comparable performance. However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent.				
1388	paper_90	For creating feature maps, did SSD extracted features from single or multiple layers of the network?	SSD uses features from multiple layers of the network for creating the feature maps.	To handle different object scales, some methods [4, 9] suggest processing the image at different sizes and combining the results afterwards. However, by utilizing feature maps from several different layers in a single network for prediction we can mimic the same effect, while also sharing parameters across all object scales. Previous works [10, 11] have shown that using feature maps from the lower layers can improve semantic segmentation quality because the lower layers capture more fine details of the input objects. Similarly,  [12] showed that adding global context pooled from a feature map can help smooth the segmentation results. Motivated by these methods, we use both the lower and upper feature maps for detection. Figure 1 shows two exemplar feature maps (8\times 8 and 4\times 4) which are used in the framework. In practice, we can use many more with small computational overhead.				
1389	paper_90	For matching default boxes with ground truth ones, what metric was used?	Best Jaccard Overlap was used to match default boxes with ground truth ones.	During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.				
1390	paper_90	What kind of loss function is used in training SSD?	The loss function used for training is a weighted sum of the localization loss (loc) and the confidence loss conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes.  If N=0, wet set the loss to 0.  The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters.	The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories. Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p.In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1.The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx,cy) of the default bounding box (d) and for its width (w) and height (h).\begin{split}L_{loc}(x,l,g)=\sum_{i\in Pos}^{N}\sum_{m\in\{cx,cy,w,h\}}&x_{ij}^{k}\text{smooth}_{\text{L1}}(l_{i}^{m}-\hat{g}_{j}^{m})\\\hat{g}_{j}^{cx}=(g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w}\quad\quad&\hat{g}_{j}^{cy}=(g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h}\\\hat{g}_{j}^{w}=\log\Big{(}\frac{g_{j}^{w}}{d_{i}^{w}}\Big{)}\quad\quad&\hat{g}_{j}^{h}=\log\Big{(}\frac{g_{j}^{h}}{d_{i}^{h}}\Big{)}\end{split}(2)The confidence loss is the softmax loss over multiple classes confidences (c).L_{conf}(x,c)=-\sum_{i\in Pos}^{N}x_{ij}^{p}log(\hat{c}_{i}^{p})-\sum_{i\in Neg}log(\hat{c}_{i}^{0})\quad\text{where}\quad\hat{c}_{i}^{p}=\frac{\exp(c_{i}^{p})}{\sum_{p}\exp(c_{i}^{p})}(3)and the weight term \alpha is set to 1 by cross validation.				
1391	paper_90	What baseline is used for creating feature maps in the proposed SSD framework?	To create feature maps in SSD, VGG-16 was used as a baseline.	The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:				
1394	paper_90	Why did the authors use multi-scale feature maps for detection?	Authors used multi-scale feature maps for detection because they allow predictions of detections at multiple scales.	Multi-scale feature maps for detection We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).				
1395	paper_90	Compare the detection feature map of both of single shot detectors (SSD and YOLO) ?	SSD uses multi-scale feature map while YOLO operates on single scale feature map.	Multi-scale feature maps for detection We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).				
1396	paper_90	How to reinject the missed predictions to help SSD to learn from negative predictions ?	The negative samples are sorted using highest confidence loss for each default box and the top ones are picked.  This is how missed predictions are reinjected to help SSD learn from negative predictions.	After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training.				
1397	paper_90	Increasing default box shape will increase in model performance. How many default boxes are used in the SSD framework?	In SSD framework, generally 6 default boxes per location are used.	More default box shapes is better. As described in Sec. 2.2.3, by default we use 6 default boxes per location. If we remove the boxes with \frac{1}{3} and 3 aspect ratios, the performance drops by 0.6%. By further removing the boxes with \frac{1}{2} and 2 aspect ratios, the performance drops another 2.1%. Using a variety of default box shapes seems to make the task of predicting boxes easier for the network.				
1398	paper_90	What do you mean by NMS(Non-Maximal Suppression) ?	NMS is used in SSD to get the final predictions.	The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:				
1399	paper_90	What input size images were tested for SSD experiments?	SSD experiments were tested for images with input size 300x300.	To understand SSD better, we carried out controlled experiments to examine how each component affects performance. For all the experiments, we use the same settings and input size (300\times 300), except for specified changes to the settings or component(s).				
1400	paper_90	What did the author mean by “Hard Negative Mining”?	When the number of available default boxes is high, the majority of the default boxes after the matching phase are negatives.  As a result, there is now a sizable imbalance between the training instances that are good and negative.  The biggest confidence loss for each default box is used to order the negative instances, and the top ones are chosen so that the ratio of negative to positive examples is no greater than 3:1.  This process is called as Hard Negative Mining.	After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training.				
1401	paper_90	How scales of the default bounding boxes for a particular feature map is computed?	The scale of the default boxes for each feature map is computed as:s_{k}=s_{\text{min}}+\frac{s_{\text{max}}-s_{\text{min}}}{m-1}(k-1),\quad k\in[1,m](4)where s_{\text{min}} is 0. 2 and s_{\text{max}} is 0. 9, meaning the lowest layer has a scale of 0. 2 and the highest layer has a scale of 0. 9, and all layers in between are regularly spaced.	Feature maps from different levels within a network are known to have different (empirical) receptive field sizes [13]. Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual receptive fields of each layer.We design the tiling of default boxes so that specific feature maps learn to be responsive to particular scales of the objects. Suppose we want to use m feature maps for prediction. The scale of the default boxes for each feature map is computed as:s_{k}=s_{\text{min}}+\frac{s_{\text{max}}-s_{\text{min}}}{m-1}(k-1),\quad k\in[1,m](4)where s_{\text{min}} is 0.2 and s_{\text{max}} is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced. We impose different aspect ratios for the default boxes, and denote them as a_{r}\in\{1,2,3,\frac{1}{2},\frac{1}{3}\}. We can compute the width (w_{k}^{a}=s_{k}\sqrt{a_{r}}) and height (h_{k}^{a}=s_{k}/\sqrt{a_{r}}) for each default box. For the aspect ratio of 1, we also add a default box whose scale is s^{\prime}_{k}=\sqrt{s_{k}s_{k+1}}, resulting in 6 default boxes per feature map location. We set the center of each default box to (\frac{i+0.5}{|f_{k}|},\frac{j+0.5}{|f_{k}|}), where |f_{k}| is the size of the k-th square feature map, i,j\in[0,|f_{k}|). In practice, one can also design a distribution of default boxes to best fit a specific dataset. How to design the optimal tiling is an open question as well.				
1402	paper_90	How did SSD handle Object localization better than F-CNN ?	SSD does better object localization than F-CNN because directly learns to regress the object shape and classify object categories instead of using two decoupled steps.	To understand the performance of our two SSD models in more details, we used the detection analysis tool from [21]. Figure 3 shows that SSD can detect various object categories with high quality (large white area). The majority of its confident detections are correct. The recall is around 85-90%, and is much higher with “weak” (0.1 jaccard overlap) criteria. Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two decoupled steps. However, SSD has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories. Figure 4 shows that SSD is very sensitive to the bounding box size. In other words, it has much worse performance on smaller objects than bigger objects. This is not surprising because those small objects may not even have any information at the very top layers. Increasing the input size (e.g. from 300\times 300 to 512\times 512) can help improve detecting small objects, but there is still a lot of room to improve. On the positive side, we can clearly see that SSD performs really well on large objects. And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location.				
1403	paper_90	What features of the SSD algorithm contributed to major improvements in detection speed ?	According to authors, the improvement in speed of SSD algorithm comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage.  They have used a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.  With these modifications—especially using multiple layers for prediction at different scales—thye achieved high-accuracy using relatively low resolution input, further increasing detection speed.	This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.				
1404	paper_90	How did the authors manage the model fast and accurately enough for real-time applications?	Authors did the following things to manage fast and accurate model for real-time applications: a) A deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do.  b) Elimination of bounding box proposals and the subsequent pixel or feature resampling stage.  c) Using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.	This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.				
1405	paper_90	How does the SSD match the default bounding box with ground truth ones?	To match the ground truth box with default box, authors used Best Jaccard Overlap.  The default boxes are matched with any ground truth box with jaccard overlap higher than a threshold which is 0.  The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories.  The process to match the bxoes is - Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p. In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1. The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes.  If N=0, wet set the loss to 0.  The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters.	The SSD training objective is derived from the MultiBox objective [7, 8] but is extended to handle multiple object categories. Let x_{ij}^{p}=\{1,0\} be an indicator for matching the i-th default box to the j-th ground truth box of category p.In the matching strategy above, we can have \sum_{i}x_{ij}^{p}\geq 1.The overall objective loss function is a weighted sum of the localization loss (loc) and the confidence loss (conf):L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))(1)where N is the number of matched default boxes. If N=0, wet set the loss to 0. The localization loss is a Smooth L1 loss [6] between the predicted box (l) and the ground truth box (g) parameters. Similar to Faster R-CNN [2], we regress to offsets for the center (cx,cy) of the default bounding box (d) and for its width (w) and height (h).\begin{split}L_{loc}(x,l,g)=\sum_{i\in Pos}^{N}\sum_{m\in\{cx,cy,w,h\}}&x_{ij}^{k}\text{smooth}_{\text{L1}}(l_{i}^{m}-\hat{g}_{j}^{m})\\\hat{g}_{j}^{cx}=(g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w}\quad\quad&\hat{g}_{j}^{cy}=(g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h}\\\hat{g}_{j}^{w}=\log\Big{(}\frac{g_{j}^{w}}{d_{i}^{w}}\Big{)}\quad\quad&\hat{g}_{j}^{h}=\log\Big{(}\frac{g_{j}^{h}}{d_{i}^{h}}\Big{)}\end{split}(2)The confidence loss is the softmax loss over multiple classes confidences (c).L_{conf}(x,c)=-\sum_{i\in Pos}^{N}x_{ij}^{p}log(\hat{c}_{i}^{p})-\sum_{i\in Neg}log(\hat{c}_{i}^{0})\quad\text{where}\quad\hat{c}_{i}^{p}=\frac{\exp(c_{i}^{p})}{\sum_{p}\exp(c_{i}^{p})}(3)and the weight term \alpha is set to 1 by cross validation.During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.				
1406	paper_90	How SSD will predict boundary boxes after training as there is no ground truth anymore?	After training, SSD predicts the boundary box by doing a non-maximum suppression on boundary boxes with the presence of object class instance.	The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The early network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), which we will call the base network222We use the VGG-16 network as a base, but other networks should also produce good results.. We then add auxiliary structure to the network to produce detections with the following key features:				
1408	paper_90	Are default box and predicted boxes are different?	The default boxes are used during training to tune the model's weights.  On the other hand, predicted boxes are compared with default boxes to optimise the model.  There is only one predicted box but the default box number can be huge.  So, the default and predicted boxes are different.	By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes. For example, in Fig. 1, the dog is matched to a default box in the 4\times 4 feature map, but not to any default boxes in the 8\times 8 feature map. This is because those boxes have different scales and do not match the dog box, and therefore are considered as negatives during training.After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large. This introduces a significant imbalance between the positive and negative training examples. Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1. We found that this leads to faster optimization and a more stable training.During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly. For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale. We begin by matching each ground truth box to the default box with the best jaccard overlap (as in MultiBox [7]). Unlike MultiBox, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5). This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.				
1409	paper_90	How input image resolution affects the accuracy of the SSD framework?	The accuracy of SSD framework is relatively more on higher resolution images than on lower resolution images.	This paper presents the first deep network based object detector that does not resample pixels or features for bounding box hypotheses and and is as accurate as approaches that do. This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%). The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. We are not the first to do this (cf [4, 5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts. Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales. With these modifications—especially using multiple layers for prediction at different scales—we can achieve high-accuracy using relatively low resolution input, further increasing detection speed. While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from 63.4% mAP for YOLO to 74.3% mAP for our SSD. This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on residual networks [3]. Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.We summarize our contributions as follows:•We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).•The core of SSD is predicting category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps.•To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.•These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.•Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.				
1410	paper_91	Is channel shuffle operation is similiar to that of random sparse convolution?	The group convolution and channel shuffle are clearly described and evaluated in the paper.  The authors claim that random scarce convolution is similar to channel shuffle with group convolution.  However, since the purpose of random scarce convolution is different and is not described, it is hard to tell just by the paper, how exactly they are similar.	If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers. In the next subsection we will introduce an efficient network unit with channel shuffle and group convolution.The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.To the best of our knowledge, the idea of channel shuffle operation is rarely mentioned in previous work on efficient model design, although CNN library cuda-convnet [20] supports “random sparse convolution” layer, which is equivalent to random channel shuffle followed by a group convolutional layer. Such “random shuffle” operation has different purpose and been seldom exploited later. Very recently, another concurrent work  [41] also adopt this idea for a two-stage convolution. However,  [41] did not specially investigate the effectiveness of channel shuffle itself and its usage in tiny model design.				
1411	paper_91	How channel shuffle operation works for two groups?	In the case of channel shuffle operation for two groups, each group is divided into two and shuffled so each new group has a subgroup from both old groups.  For example, |A|B| -> |aa|bb| -> |ab|ab|.  In terms of performance, two groups seem to work consistently better than the single group case and consistently worse than having more than 2 groups.	If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains.Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.				
1412	paper_91	Why it is possible to say that multiple group convolutional layers works efficiently without weakening representation?	It is clearly stated in the paper that having group convolutions is a trade-off between representative capability and the computational cost of the model.  The ShuffleNet allows stacking multiple group convolutions with an appropriate number of groups because of channel shuffle and it is empirically shown in the paper.  However, it is also noted that having too many groups might sometimes damage the performance.  Thus, multiple group convolutions work efficiently only when the number of groups is chosen carefully and channel shuffle is used.	If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.				
1413	paper_91	What does s in "ShuffleNet s x" mean? 	"s" means the scale factor by which the number of channels is multiplied to adapt the ShuffleNet to the given computational complexity. 	To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times.				
1414	paper_91	How is complexity calculated given scale factor of the ShuffleNet model? Given scale factor 0.25 and complexity of ShuffleNet 1x is 140 MFLOPS	As it is shown in Table 2, the complexity of ShufflNet 0. 25x will be 13 MFLOPs.	To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times.Table 2. Classification error vs. number of groups g (smaller number represents better performance)				
1415	paper_91	For channel shuffle, were they applied for first pointwise convolution or second ?	The channel shuffle in the ShuffleNet unit occurs only after the first pointwise group convolution.	If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit [9] in Fig 2 (a). It is a residual block. In its residual branch, for the 3\times 3 layer, we apply a computational economical 3\times 3 depthwise convolution [3] on the bottleneck feature map. Then, we replace the first 1\times 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig 2 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN) [15] and nonlinearity is similar to  [9, 40], except that we do not use ReLU after depthwise convolution as suggested by  [3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig 2 (c)): (i) add a 3\times 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost.The purpose of shuffle operation is to enable cross-group information flow for multiple group convolution layers. Table 3 compares the performance of ShuffleNet structures (group number is set to 3 or 8 for instance) with/without channel shuffle. The evaluations are performed under three different scales of complexity. It is clear that channel shuffle consistently boosts classification scores for different settings. Especially, when group number is relatively large (e.g. g=8), models with channel shuffle outperform the counterparts by a significant margin, which shows the importance of cross-group information interchange.To the best of our knowledge, the idea of channel shuffle operation is rarely mentioned in previous work on efficient model design, although CNN library cuda-convnet [20] supports “random sparse convolution” layer, which is equivalent to random channel shuffle followed by a group convolutional layer. Such “random shuffle” operation has different purpose and been seldom exploited later. Very recently, another concurrent work  [41] also adopt this idea for a two-stage convolution. However,  [41] did not specially investigate the effectiveness of channel shuffle itself and its usage in tiny model design.				
1416	paper_91	What is the activation function for a ShuffleNet Unit?	The use of activation functions in the ShuffleNet unit happens only after the first 1x1 group convolution and the last concatenation of shortcut and residual paths, following the suggestions of referenced papers [3, 9, 40].  And the only non-linear activation function that is used is ReLU.	Taking advantage of the channel shuffle operation, we propose a novel ShuffleNet unit specially designed for small networks. We start from the design principle of bottleneck unit [9] in Fig 2 (a). It is a residual block. In its residual branch, for the 3\times 3 layer, we apply a computational economical 3\times 3 depthwise convolution [3] on the bottleneck feature map. Then, we replace the first 1\times 1 layer with pointwise group convolution followed by a channel shuffle operation, to form a ShuffleNet unit, as shown in Fig 2 (b). The purpose of the second pointwise group convolution is to recover the channel dimension to match the shortcut path. For simplicity, we do not apply an extra channel shuffle operation after the second pointwise layer as it results in comparable scores. The usage of batch normalization (BN) [15] and nonlinearity is similar to  [9, 40], except that we do not use ReLU after depthwise convolution as suggested by  [3]. As for the case where ShuffleNet is applied with stride, we simply make two modifications (see Fig 2 (c)): (i) add a 3\times 3 average pooling on the shortcut path; (ii) replace the element-wise addition with channel concatenation, which makes it easy to enlarge channel dimension with little extra computation cost.Figure 2. ShuffleNet Units. a) bottleneck unit [9] with depthwise convolution (DWConv) [3, 12]; b) ShuffleNet unit with pointwise group				
1417	paper_91	Given input size (3x224x224) and bottleneck channels being 64, compare the computational complexity between ResNet, ResNext and ShuffleNet.	ResNet: hw(2cm+9m^{2}) = 244^2*2 * 3 * 64 + 244^2*9 * 64^2 = 2 217 596 928ResNeXt: hw(2cm+9m^{2}/g) = 244^2*2 * 3 * 64 + 244^2*9 * 64^2/g = 22 861 824 + 2 194 735 104 / gShuffleNet: hw(2cm/g + 9m) = 244^2*2 * 3 * 64 / g + 244^2*9 * 64 = 22 861 824 / g + 34 292 736Even with the group size of 1 (g=1), ShuffleNet have much less complexity compared to ResNet and ResNeXt.	Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information.				
1418	paper_91	How pointwise group convolutions is different from 1x1 convolutions?	The group convolution divides the channels into groups and applies the convolution only within the groups, thus reducing the computational complexity of 1x1 convolutions.  However, when several group convolutions are stacked together it may block the information flow and weaken the representation.	We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.				
1419	paper_91	What are the side effects of group convolution?	The side effects of group convolutions are: blocked flow of information between channel groups when multiple group convolutions are combined.  and damaged individual convolution filters for each group due to decreased number of input channels.	In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains.Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.				
1421	paper_91	What do you mean by model pruning?	Although model pruning is a specific concept in deep learning, it refers to reducing the model size by removing redundant network connections or channels.	Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks [21, 9, 33, 5, 28, 24]. The most accurate CNNs usually have hundreds of layers and thousands of channels [9, 34, 32, 40], thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit representing a “basic” network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges.This direction aims to accelerate inference while preserving accuracy of a pre-trained model.Pruning network connections [6, 7] or channels [38] reducesredundant connections in a pre-trained model while maintaining performance.Quantization [31, 27, 39, 45, 44] andfactorization [22, 16, 18, 37] are proposed inliterature to reduce redundancy in calculations to speed up inference.Without modifying the parameters, optimized convolution algorithms implemented by FFT [25, 35] and other methods [2] decrease time consumption in practice.Distilling [11] transfers knowledge fromlarge models into small ones, which makes training small models easier.				
1422	paper_91	What does the tradeoff  look like when basic network architectures are represented in low-bit computations?	Although representing network architectures in low-bit form is mentioned as a technique for reducing the computational cost of the model, the paper does not mention anything about the tradeoff of the technique.	Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual recognition tasks [21, 9, 33, 5, 28, 24]. The most accurate CNNs usually have hundreds of layers and thousands of channels [9, 34, 32, 40], thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. Note that many existing works [16, 22, 43, 42, 38, 27] focus on pruning, compressing, or low-bit representing a “basic” network architecture. Here we aim to explore a highly efficient basic architecture specially designed for our desired computing ranges.				
1423	paper_91	How ShuffleNet allowed more feature maps for a given computational complexity?	The ShuffleNet uses pointwise group convolution with channel shuffling, thus design-wise it has less complexity (requires hw(2cm/g+9m) FLOPs).  This means it allows wider feature maps for a given computational budget.  And the effect seems to increase the performance better as the model gets smaller.	We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information.From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.We use exactly the same settings to train these models. Results are shown in Table 4. Our ShuffleNet models outperform most others by a significant margin under different complexities. Interestingly, we find an empirical relationship between feature map channels and classification accuracy. For example, under the complexity of 38 MFLOPs, output channels of Stage 4 (see Table 1) for VGG-like, ResNet, ResNeXt, Xception-like, ShuffleNet models are 50, 192, 192, 288, 576 respectively, which is consistent with the increase of accuracy. Since the efficient design of ShuffleNet, we can use more channels for a given computation budget, thus usually resulting in better performance.				
1424	paper_91	Which networks introduced efficient depthwise seperable convolution into the building blocks of a state-of-the-art network?	Although AlexNet introduced the idea of group convolutions, the Xception and ResNeXt generalized depthwise separable convolutions and achieved state-of-the-art results under large computationl budget (~1 GFLOPs).	We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on low-power mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations. Such drawback is also referred in  [3], which has a runtime library based on TensorFlow [1]. In ShuffleNet units, we intentionally use depthwise convolution only on bottleneck in order to prevent overhead as much as possible.Recent leading convolutional units in VGG [30], ResNet [9], GoogleNet [33], ResNeXt [40] and Xception [3] have pursued state-of-the-art results with large models (e.g. \geq 1GFLOPs), but do not fully explore low-complexity conditions. In this section we survey a variety of building blocks and make comparisons with ShuffleNet under the same complexity constraint.Recently Howard et al. have proposed MobileNets [12] which mainly focus on efficient network architecture for mobile devices. MobileNet takes the idea of depthwise separable convolution from  [3] and achieves state-of-the-art results on small models.The concept of group convolution, which was first introduced in AlexNet [21] for distributing the model over two GPUs, has been well demonstrated its effectiveness in ResNeXt [40]. Depthwise separable convolution proposed in Xception [3] generalizes the ideas of separable convolutions in Inception series [34, 32]. Recently, MobileNet [12] utilizes the depthwise separable convolutions and gains state-of-the-art results among lightweight models. Our work generalizes group convolution and depthwise separable convolution in a novel form.Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.				
1425	paper_91	How much percentage computation do pointwise convolutions take up in each residual unit?	Only for ResNeXt, the pointwise convolutions seem to take 93. 4% of multiplication-adds.  However, it is impossible to say how much percentage the pointwise convolutions take for all the models that are mentioned in the paper.	Thanks to pointwise group convolution with channel shuffle, all components in ShuffleNet unit can be computed efficiently. Compared with ResNet [9] (bottleneck design) and ResNeXt [40], our structure has less complexity under the same settings. For example, given the input size c\times h\times w and the bottleneck channels m, ResNet unit requires hw(2cm+9m^{2}) FLOPs and ResNeXt has hw(2cm+9m^{2}/g) FLOPs, while our ShuffleNet unit requires only hw(2cm/g+9m) FLOPs, where g means the number of groups for convolutions. In other words, given a computational budget, ShuffleNet can use wider feature maps. We find this is critical for small networks, as tiny networks usually have an insufficient number of channels to process the information.Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.				
1426	paper_91	How did the authors handle complexity constraints for their mobile networks?	The authors have constructed simple scaling to reduce the size of the ShuffleNet to fit the computational constraints.  Also, they report the specific outcomes of their implementation and its reasons when the network is run on mobile devices.  However, it is hard to understand what does handle complexity constraints and mobile networks mean in the question.	In addition, in ShuffleNet depthwise convolution only performs on bottleneck feature maps. Even though depthwise convolution usually has very low theoretical complexity, we find it difficult to efficiently implement on low-power mobile devices, which may result from a worse computation/memory access ratio compared with other dense operations. Such drawback is also referred in  [3], which has a runtime library based on TensorFlow [1]. In ShuffleNet units, we intentionally use depthwise convolution only on bottleneck in order to prevent overhead as much as possible.To customize the network to a desired complexity, we can simply apply a scale factor s on the number of channels. For example, we denote the networks in Table 1 as ”ShuffleNet 1\times”, then ”ShuffleNet s×s\timesitalic_s ×” means scaling the number of filters in ShuffleNet 1\times by s times thus overall complexity will be roughly s^{2} times of ShuffleNet 1\times.Recent leading convolutional units in VGG [30], ResNet [9], GoogleNet [33], ResNeXt [40] and Xception [3] have pursued state-of-the-art results with large models (e.g. \geq 1GFLOPs), but do not fully explore low-complexity conditions. In this section we survey a variety of building blocks and make comparisons with ShuffleNet under the same complexity constraint.For fair comparison, we use the overall network architecture as shown in Table 1. We replace the ShuffleNet units in Stage 2-4 with other structures, then adapt the number of channels to ensure the complexity remains unchanged. The structures we explored include:Table 5 compares classification scores under a variety of complexity levels. It is clear that our ShuffleNet models are superior to MobileNet for all the complexities. Though our ShuffleNet network is specially designed for small models (<150 MFLOPs), we find it is still better than MobileNet for higher computation cost, e.g. 3.1% more accurate than MobileNet 1\times at the cost of 500 MFLOPs. For smaller networks (\sim40 MFLOPs) ShuffleNet surpasses MobileNet by 7.8%. Note that our ShuffleNet architecture contains 50 layers while MobileNet only has 28 layers. For better understanding, we also try ShuffleNet on a 26-layer architecture by removing half of the blocks in Stage 2-4 (see ”ShuffleNet 0.5\times shallow (g=3)” in Table 5). Results show that the shallower model is still significantly better than the corresponding MobileNet, which implies that the effectiveness of ShuffleNet mainly results from its efficient structure, not the depth.Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table 8, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\times theoretical complexity reduction usually results in \sim2.6\times actual speedup in our implementation. Nevertheless, compared with AlexNet [21] our ShuffleNet 0.5\times model still achieves \sim13\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\times), which is much faster than previous AlexNet-level models or speedup approaches such as  [14, 16, 22, 42, 43, 38].				
1427	paper_91	Each convolution operates on that corresponding input channel group. If so, how the model learns the features from entire input space?	When multiple group convolutions are stacked together, the authors use channel shuffle, which divides the channels into subgroups within groups and shuffles them in a way that each group consists of subgroups from all other groups.  It lets the model learn from an entire input space despite the group convolution.  However, the paper does not explicitly report such side effects when group convolutions are not stacked together.	If we allow group convolution to obtain input data from different groups (as shown in Fig 1 (b)), the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. This can be efficiently and elegantly implemented by a channel shuffle operation (Fig 1 (c)): suppose a convolutional layer with g groups whose output has g\times n channels; we first reshape the output channel dimension into (g,n), transposing and then flattening it back as the input of next layer. Note that the operation still takes effect even if the two convolutions have different numbers of groups. Moreover, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.Channel shuffle operation makes it possible to build more powerful structures with multiple group convolutional layers. In the next subsection we will introduce an efficient network unit with channel shuffle and group convolution.To address the issue, a straightforward solution is to apply channel sparse connections, for example group convolutions, also on 1\times 1 layers. By ensuring that each convolution operates only on the corresponding input channel group, group convolution significantly reduces computation cost. However, if multiple group convolutions stack together, there is one side effect: outputs from a certain channel are only derived from a small fraction of input channels. Fig 1 (a) illustrates a situation of two stacked group convolution layers. It is clear that outputs from a certain group only relate to the inputs within the group. This property blocks information flow between channel groups and weakens representation.				
1428	paper_91	What will be the effect in performance if group numbers for convolution is increased?	For ShuffleNet, having more than 1 group seems to show consistently better results for all complexities.  As the model gets smaller, the performance gain seems to increase more as the number of groups increases.  However, for larger models, a too large number of groups led to saturation or a drop in classification error, possibly due to reduced representative capabilities.	In ShuffleNet units, group number g controls the connection sparsity of pointwise convolutions. Table 1 explores different group numbers and we adapt the output channels to ensure overall computation cost roughly unchanged (\sim140 MFLOPs). Obviously, larger group numbers result in more output channels (thus more convolutional filters) for a given complexity constraint, which helps to encode more information, though it might also lead to degradation for an individual convolutional filter due to limited corresponding input channels. In Sec 4.1.1 we will study the impact of this number subject to different computational constrains.From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.Table 2 also shows that for some models (e.g. ShuffleNet 0.5\times) when group numbers become relatively large (e.g. g=8), the classification score saturates or even drops. With an increase in group number (thus wider feature maps), input channels for each convolutional filter become fewer, which may harm representation capability. Interestingly, we also notice that for smaller models such as ShuffleNet 0.25\times larger group numbers tend to better results consistently, which suggests wider feature maps bring more benefits for smaller models.Finally, we evaluate the actual inference speed of ShuffleNet models on a mobile device with an ARM platform. Though ShuffleNets with larger group numbers (e.g. g=4 or g=8) usually have better performance, we find it less efficient in our current implementation. Empirically g=3 usually has a proper trade-off between accuracy and actual inference time. As shown in Table 8, three input resolutions are exploited for the test. Due to memory access and other overheads, we find every 4\times theoretical complexity reduction usually results in \sim2.6\times actual speedup in our implementation. Nevertheless, compared with AlexNet [21] our ShuffleNet 0.5\times model still achieves \sim13\times actual speedup under comparable classification accuracy (the theoretical speedup is 18\times), which is much faster than previous AlexNet-level models or speedup approaches such as  [14, 16, 22, 42, 43, 38].				
1429	paper_91	Why are dense 1X1 convolutions computationally expensive?	The 1x1 convolutions are expensive in extremely reduced versions of Xception and ResNeXt as they might take 93. 4% of multiplication-adds for each residual unit.	We notice that state-of-the-art basic architectures such as Xception [3] and ResNeXt [40] become less efficient in extremely small networks because of the costly dense 1\times 1 convolutions. We propose using pointwise group convolutions to reduce computation complexity of 1\times 1 convolutions. To overcome the side effects brought by group convolutions, we come up with a novel channel shuffle operation to help the information flowing across feature channels. Based on the two techniques, we build a highly efficient architecture called ShuffleNet. Compared with popular structures like  [30, 9, 40], for a given computation complexity budget, our ShuffleNet allows more feature map channels, which helps to encode more information and is especially critical to the performance of very small networks.From the results, we see that models with group convolutions (g>1) consistently perform better than the counterparts without pointwise group convolutions (g=1). Smaller models tend to benefit more from groups. For example, for ShuffleNet 1\times the best entry (g=8) is 1.2% better than the counterpart, while for ShuffleNet 0.5\times and 0.25\times the gaps become 3.5% and 4.4% respectively. Note that group convolution allows more feature map channels for a given complexity constraint, so we hypothesize that the performance gain comes from wider feature maps which help to encode more information. In addition, a smaller network involves thinner feature maps, meaning it benefits more from enlarged feature maps.Modern convolutional neural networks [30, 33, 34, 32, 9, 10] usually consist of repeated building blocks with the same structure. Among them, state-of-the-art networks such as Xception [3] and ResNeXt [40] introduce efficient depthwise separable convolutions or group convolutions into the building blocks to strike an excellent trade-off between representation capability and computational cost. However, we notice that both designs do not fully take the 1\times 1 convolutions (also called pointwise convolutions in  [12]) into account, which require considerable complexity. For example, in ResNeXt [40] only 3\times 3 layers are equipped with group convolutions. As a result, for each residual unit in ResNeXt the pointwise convolutions occupy 93.4% multiplication-adds (cardinality = 32 as suggested in  [40]). In tiny networks, expensive pointwise convolutions result in limited number of channels to meet the complexity constraint, which might significantly damage the accuracy.				
1430	paper_93	For designing convolutional architectures for ImageNet, authors used CIFAR-10 dataset as a proxy for learning. Is this true?	Authors used CIFAR-10 dataset as a proxy for learning.	In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet.				
1432	paper_93	What are the hyper-parameters used to design the neural architecture search network?	The number of cell repeats and the number of filters in the initial convolutional cell are the hyper-parameters used to design the Neural Architecture Search network.	We demonstrate the utility of the convolutional cells by employing this learned architecture on CIFAR-10 and a family of ImageNet classification tasks. The latter family of tasks is explored across a few orders of magnitude in computational budget.After having learned the convolutional cells, several hyper-parameters may be explored to build a final network for a given task: (1) the number of cell repeats N and (2) the number of filters in the initial convolutional cell. After selecting the number of initial filters, we use a common heuristic to double the number of filters whenever the stride is 2.Finally, we define a simple notation, e.g., 4 @ 64, to indicate these two parameters in all networks, where 4 and 64 indicate the number of cell repeats and the number of filters in the penultimate layer of the network, respectively.				
1433	paper_93	What metric is used for measuring Computational demand of a network?	To measure the computational demand of the network, top-1 accuracy metric was used.	Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets [60, 29]. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks [24, 70].				
1435	paper_93	Does NASNets perform better than MobileNet, ShuffleNet under resource-constraint setting?	From the above evidential sentence, it is obvious that NASNets with 74% accuracy perform better than MobileNet and ShuffleNet with 70. 6% and 70. 9% accuracies respectively.	Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources. MobileNet [24] and ShuffleNet [70] provide state-of-the-art results obtaining 70.6% and 70.9\% accuracy, respectively on 224x224 images using \sim550M multliply-add operations. An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand. In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget.				
1436	paper_93	How different versions of NASNets with different computational demands were created?	Different versions of NASNets with different computational demands were created by varying the number of the convolutional cells and number of filters in the convolutional cells.	Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets [60, 29]. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks [24, 70].				
1437	paper_93	What are the networks that were constructed from the best three searches?	The networks constructed from the best three searches are NASNet-A, NASNet-B and NASNet-C.	Figure 4 shows a diagram of the top performing Normal Cell and Reduction Cell. Note the prevalence of separable convolutions and the number of branches compared with competing architectures [53, 59, 20, 60, 58]. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked convolutional cells in ImageNet experiments (described in Appendix B) and report their results as well. We call the three networks constructed from the best three searches NASNet-A, NASNet-B and NASNet-C.				
1438	paper_93	Authors used a modified version of DropPath regularization named ScheduledDropPath. What is modified?	In ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training.	For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A. Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath [33], to be an effective regularization method for NASNet. In DropPath [33], each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments.				
1439	paper_93	Why DropPath regularization didn’t work well for NASNets?	Authors found that ScheduledDropPath, a modified version of DropPath works well for NASNets.  So one possible reason why DropPath regularization didn't work well for NASNets could be that in DropPath each path in the cell is stochastically dropped with some fixed probability during training.  The probability should linearly increase over the course of training.	For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A. Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath [33], to be an effective regularization method for NASNet. In DropPath [33], each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both CIFAR and ImageNet experiments.				
1440	paper_93	Searching for the best cell structure is less computationally expensive than searching for an entire network. If so, how the architecture search learns to connect the network?	The architecture learns to connect the network by searching for the best cell structure instead of searching for the best convolutional architectures.	In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet.				
1441	paper_93	What are the approaches that led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet?	Ensembling multiple inferences across multiple model instances and image crops led to improved accuracy with lesser parameters for NASNets compared to Inception, ResNet and PolyNet.	For the mobile-optimized network, our resulting system achieves a mAP of 29.6% – exceeding previous mobile-optimized networks that employ Faster-RCNN by over 5.0% (Table 4). For the best NASNet network, our resulting network operating on images of the same spatial resolution (800 \times 800) achieves mAP = 40.7%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0% [28, 52] (see Appendix for example detections on images and side-by-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% [37].222A primary advance in the best reported object detection system is the introduction of a novel loss [37]. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., [28]). These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure 10 and Figure 11 in Appendix C show four examples of object detection results produced by NASNet-A with the Faster-RCNN framework.				
1442	paper_93	What does model transferability mean?	Applying NAS to a large dataset is computationally expensive.  So the authors find the good architecture on a proxy dataset and then transfer the learned architecture to ImageNet.  This approach is called Model Transferability.	In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet.The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. This resulting search space permits identifying good architectures on a small dataset (i.e., CIFAR-10) and transferring the learned architecture to image classifications across a range of data and computational scales.				
1443	paper_93	The complexity of the NAS architecture is independent of the depth of the network and the size of input images. How does it scale to produce better models?	To scale for the better models, authors searched for the best convolutional architectures by searching for the best cell structure.	In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet.				
1444	paper_93	Is the architecture search by the original NAS and NASNet are different?	From the Table 1, the depths, number of parameters vary for both original NAS and NASNet.  So, we can conclude that architecture search by both is different.	In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset.Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework [71], which uses a reinforcement learning search method to optimize architecture configurations. Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or “cells”) with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems.In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7×7\times7 × and learns architectures that successfully transfer to ImageNet.				
1445	paper_93	Will normal cells and reduction cells that come out as search results be different for each dataset ?	The number of normal and reduction cells that come out as search results depend on at least one factor, the input image size in the dataset.  So, the number is different for each dataset.	Figure 2 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem.				
1446	paper_93	Which framework achieved state-of-the-art COCO object detection results with NASNets?	Faster-RCNN framework along with the features learned by NASNets from ImageNet achieved state-of-the-art COCO object detection results with NASNets.	Finally, we show that the image features learned by NASNets are generically useful and transfer to other computer vision problems. In our experiments, the features learned by NASNets from ImageNet classification can be combined with the Faster-RCNN framework [47] to achieve state-of-the-art on COCO object detection task for both the largest as well as mobile-optimized models. Our largest NASNet model achieves 43.1% mAP, which is 4% better than previous state-of-the-art.				
1447	paper_93	What is meant by “Proximal Policy Optimization”?	Proximal Policy Optimization is an optimization algorithm used to train the controller RNN.  It is done by employing a global work queue system for generating a pool of child networks controlled by the RNN.	In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task [31]. The controller RNN was trained using Proximal Policy Optimization (PPO) [51] by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs.	Proximal Policy Optimization is an optimization algorithm used to train the controller recurrent neural community.  It is done by employing a global work queue system for generating a pool of child networks controlled by the recurrent neural community.	Tortured phrases	RNN -> recurrent neural community	
1448	paper_93	Why cutout data augmentation improve NASNet-A model error rate?	From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 2. 40% which is better than the previous record.	For the task of image classification with CIFAR-10, we set N=4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table 1 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.	From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 1.9% which is better than the previous record.	Change number	2.4% -> 1.9%	
1449	paper_93	Why residual connections didn't help much for NASNets?	Inserting residual connections between residual connections between cells doesn't improve performance.	Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance.Our training setup on ImageNet is similar to [60], but please see Appendix A for details.				
1450	paper_93	How is normal cell different from reduction cell for NASNets?	We learn two separate architectures for reduction and normal cells.  During prediction,  the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell.  For Reduction cell authors make the initial operation applied to the cell’s inputs have a stride of two to reduce the height and width which is not done for Normal cell.	In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell’s inputs have a stride of two to reduce the height and width. All of our operations that we consider for building our convolutional cells have an option of striding.Figure 2 shows our placement of Normal and Reduction Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures.We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension [32, 53].Importantly,much like Inception and ResNet models [59, 20, 60, 58],we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem.What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN.The structures of the cells can be searched within a search space defined as follows (see Appendix, Figure 7 for schematic). In our search space, each cell receives as input two initial hidden states h_{i} and h_{i-1} which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states (Figure 3). The predictions of the controller for each cell are grouped into B blocks, where each block has 5 prediction steps made by 5 distinct softmax classifiers corresponding to discrete choices of the elements of a block:To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have 2\times 5B predictions in total, where the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell.				
1451	paper_94	The authors explored the possibility of using residual networks on the inception model to reduce complexity. Is that true?	True.  The authors explored the possibility of using residual networks on the inception model to reduce complexity.	In this work we study the combination of the two most recent ideas:Residual connections introduced by He et al. in  [5] and the latestrevised version of the Inception architecture [15].In [5], it is argued that residual connections are of inherentimportance for training very deep architectures. Since Inception networkstend to be very deep, it is natural to replace thefilter concatenation stage of the Inception architecture with residual connections. Thiswould allow Inception to reap all the benefits of the residual approachwhile retaining its computational efficiency.We studied how the introduction of residual connections leads to dramaticallyimproved training speed for the Inception architecture. Also our latest models(with and without residual connections) outperform all our previous networks,just by virtue of the increased model size.				
1452	paper_94	What are the metrics used for comparing Inception-v4, Inception- ResNet-v1/2 and their ensembles?	Metrics used for the comparison of ensembles are: a) Computational Cost b) Recognition Performance c) Step Time d) top-5 error.	We tried several versions of the residual version of Inception. Only twoof them are detailed here. The first one “Inception-ResNet-v1”roughly the computational cost of Inception-v3, while “Inception-ResNet-v2”matches the raw cost of the newly introduced Inception-v4 network. SeeFigure 15 for the large scale structure of bothvarianets. (However, the step time of Inception-v4 proved to be significantlyslower in practice, probably due to the larger number of layers.)•Inception-ResNet-v1: a hybrid Inception version that has asimilar computational cost to Inception-v3from [15].•Inception-ResNet-v2: a costlier hybrid Inception version withsignificantly improved recognition performance.•Inception-v4: a pure Inception variant without residual connectionswith roughly the same recognition performance as Inception-ResNet-v2.The last experiment reported here is an evaluation of an ensemble ofall the best performing models presented here. As it wasapparent that both Inception-v4 and Inception-ResNet-v2 performedsimilarly well, exceeding state-of-the art single frame performanceon the ImageNet validation dataset, we wanted to see how a combinationof those pushes the state of the art on this well studied dataset.Surprisingly, we found that gains on the single-frame performance do nottranslate into similarly large gains on ensembled performance. Nonetheless,it still allows us to report 3.1% top-5 error on the validation set withfour models ensembled setting a new state of the art, to our bestknowledge.				
1453	paper_94	Which part of the Inception architecture was replaced with residual connections?	The filter concatenation stage of the Inception Architecture was replaced with Residual connections.	In this work we study the combination of the two most recent ideas:Residual connections introduced by He et al. in  [5] and the latestrevised version of the Inception architecture [15].In [5], it is argued that residual connections are of inherentimportance for training very deep architectures. Since Inception networkstend to be very deep, it is natural to replace thefilter concatenation stage of the Inception architecture with residual connections. Thiswould allow Inception to reap all the benefits of the residual approachwhile retaining its computational efficiency.				
1454	paper_94	What is the role of adding 1x1 convolution before the 3x3 and 1x7 convolutions, How does it help?	1x1 convolution block is added before 3x3 and 1x7 convolutions for scaling up the dimensionality of the filter bank before the additionto match the depth of the input.  This is done to compensate for the dimensionality reduction induced by the Inception block.	For the residual versions of the Inception networks, we use cheaper Inceptionblocks than the original Inception. Each Inception block is followed byfilter-expansion layer (1\times 1 convolution without activation) which isused for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is needed to compensate for the dimensionalityreduction induced by the Inception block.				
1455	paper_94	Why does inception-v4’s inception module use average pooling instead of max pooling?	One possible reason to use average pooling instead of max pooling is that max pooling introduces some kind of technical constraints which are reduced by average pooling.  But these constraint have not been explained in the paper.	Besides a straightforward integration, we have also studied whetherInception itself can be made more efficient by making it deeper and wider.For that purpose, we designed a new version named Inception-v4which has a more uniform simplified architecture and more inception modulesthan Inception-v3. Historically, Inception-v3 had inherited a lot of thebaggage of the earlier incarnations. The technical constraints chiefly came fromthe need for partitioning the model for distributed training usingDistBelief [2].Now, after migrating our training setup to TensorFlow [1]these constraints have been lifted, which allowed us to simplify the architecturesignificantly. The details of that simplified architecture are described in Section 3.				
1456	paper_94	Is 7x7 convolution is similiar in computational complexity with two 7x1 and 1x7 convolution ?	From the above evidential sentence, it can be concluded that the computational complexity of 7x7 convolution is similar to that of 7x1 and 1x7 convolution.	In this report, we will compare the two pure Inception variants,Inception-v3 and v4, with similarly expensive hybrid Inception-ResNetversions. Admittedly, those models were picked in a somewhat ad hoc mannerwith the main constraint being that the parameters and computationalcomplexity of the models should be somewhat similar to the costof the non-residual models. In fact we have tested bigger and widerInception-ResNet variants and they performed very similarly on theImageNet classification challenge  [11]dataset.				
1457	paper_94	How Inception-ResNet v1 compare with Inception-ResNet v2 in terms of structure, stem and settings?	Both Inception-ResNet-v1 and Inception-ResNet-v2 are Inception style networksthat utilize residual connections instead of filter concatenation.	Finally, we present some comparisons, between various versions of Inceptionand Inception-ResNet. The models Inception-v3 and Inception-v4 are deepconvolutional networks not utilizing residual connections whileInception-ResNet-v1 and Inception-ResNet-v2 are Inception style networksthat utilize residual connections instead of filter concatenation.				
1458	paper_94	Why 1x1 convolution is used after the original convolutions in Inception-Resnet architectures?	1x1 convolution is used after the original convolutions in Inception-Resnet architectures for scaling up the dimensionality of the filter bank before the additionto match the depth of the input.  This is needed to compensate for the dimensionalityreduction induced by the Inception block.	For the residual versions of the Inception networks, we use cheaper Inceptionblocks than the original Inception. Each Inception block is followed byfilter-expansion layer (1\times 1 convolution without activation) which isused for scaling up the dimensionality of the filter bank before the additionto match the depth of the input. This is needed to compensate for the dimensionalityreduction induced by the Inception block.				
1459	paper_94	What do you mean by Top-1 and Top-5 error rate?	Top-1 and Top-5 error rate are the evaluation metrics used to compare the performace of various models.	First we observe the top-1 and top-5 validation-error evolution of thefour variants during training. After the experiment was conducted, we havefound that our continuous evaluation was conducted on a subset of thevalidation set which omitted about 1700 blacklisted entities due to poorbounding boxes. It turned out that the omission should have been only performedfor the CLSLOC benchmark, but yields somewhat incomparable (more optimistic)numbers when compared to other reports including some earlier reports by ourteam. The difference is about 0.3% for top-1 error and about 0.15% forthe top-5 error. However, since the differences are consistent, we thinkthe comparison between the curves is a fair one.	Top-1 and Top-5 blunder rate are the evaluation metrics used to compare the performace of various models.	Tortured phrases	error rate -> blunder rate	
1460	paper_94	Why the training was unstable without these activations scaled before addition?	The training was stabilised after scaling down the residuals before adding them to the previous layer activations.  Even when this scaling was not strictly necessary, it helped stabilise the training without affecting the accuracy.	We found that scaling down the residuals before adding them tothe previous layer activation seemed to stabilize the training. In generalwe picked some scaling factors between 0.1 and 0.3 to scale the residualsbefore their being added to the accumulated layer activations(cf. Figure 20).A similar instability was observed by He et al. in [5] inthe case of very deep residual networks and they suggested a two-phasetraining where the first “warm-up” phase is done with very low learningrate, followed by a second phase with high learning rata. We found thatif the number of filters is very high, then even a very low (0.00001) learningrate is not sufficient to cope with the instabilities and the training withhigh learning rate had a chance to destroy its effects. We found it muchmore reliable to just scale the residuals.Even where the scaling was not strictly necessary, it neverseemed to harm the final accuracy, but it helped to stabilize the training.				
1461	paper_94	Will is the Inception- ResNet-v2 trained faster than pure Inception-v4 although their computational complexity is similar ?	Since the step time of Inception-v4 is significantly slower in practice, we can conclude that Inception-ResNet-v2 trained faster than pure Inception-v4 even though the computational complexity is similar.  This is also supported by the Figure 24.	We tried several versions of the residual version of Inception. Only twoof them are detailed here. The first one “Inception-ResNet-v1”roughly the computational cost of Inception-v3, while “Inception-ResNet-v2”matches the raw cost of the newly introduced Inception-v4 network. SeeFigure 15 for the large scale structure of bothvarianets. (However, the step time of Inception-v4 proved to be significantlyslower in practice, probably due to the larger number of layers.)				
1462	paper_95	By ignoring IoU between 0.4 and 0.5, Are we losing some positive samples too?	As evident from the above sentence, since we are ignoring the anchors, it is possible that we may lose some positive samples if their IoU is between 0. 4 and 0.	Each anchor is assigned a length K one-hot vector of classification targets, where K is the number of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multiclass detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K label vector to 1 and all other entries to 0. If an anchor is unassigned, which may happen with overlap in [0.4, 0.5), it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment.				
1463	paper_95	One stage detectors are computationally inexpensive than two stage detectors. Is it true?	From the above evidential sentence, we can see that a one-stage detector has to process larger set of candidate object locations regularly sampled across an image.  So, we can say that one stage detectors are computationally expensive than two stage detectors.	In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31].				
1464	paper_95	How foreground-background class imbalance is encountered for two stage detectors ?	In the two-stage mechanism for object detection, the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground or background classes using a CNN.  If the CNN outputs more foreground or background classes than the latter, we can say that foreground-background imbalance has occurred.	Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism. As popularized in the R-CNN framework [11], the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground classes or as background using a convolutional neural network. Through a sequence of advances [10, 28, 20, 14], this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark [21].				
1465	paper_95	How many sets of candidate object location is sampled across an image for RetinaNet	~100k sets of candidate object locations were sampled accross an image for RetinaNet.	In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31].				
1466	paper_95	What makes the performance of one-stage detectors inferior to two-stage detectors ?	Two-stage detectors can classify boxes at any position, scale, and aspect ratio using a region pooling operation.  In contrast, one-stage detectors use a fixed sampling grid.  Two-stage detectors can be made fast simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget.  Also, one stage detector has to process large set of candidate object locations regularly sampled across an image.  These are the reasons why one stage detectors perform worse than two stage detectors.	In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating \scriptstyle\sim100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31].One of the most important design factors in a one-stage detection system is how densely it covers the space of possible image boxes. Two-stage detectors can classify boxes at any position, scale, and aspect ratio using a region pooling operation [10]. In contrast, as one-stage detectors use a fixed sampling grid, a popular approach for achieving high coverage of boxes in these approaches is to use multiple ‘anchors’ [28] at each spatial position to cover boxes of various scales and aspect ratios.OverFeat [30] was one of the first modern one-stage object detector based on deep networks. More recently SSD [22, 9] and YOLO [26, 27] have renewed interest in one-stage methods. These detectors have been tuned for speed but their accuracy trails that of two-stage methods. SSD has a 10-20% lower AP, while YOLO focuses on an even more extreme speed/accuracy trade-off. See Figure 2. Recent work showed that two-stage detectors can be made fast simply by reducing input image resolution and the number of proposals, but one-stage methods trailed in accuracy even with a larger compute budget [17]. In contrast, the aim of this work is to understand if one-stage detectors can match or surpass the accuracy of two-stage detectors while running at similar or faster speeds.				
1467	paper_95	What does "online hard example mining (OHEM)" means ?	In OHEM, each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples.	[31] proposed to improve training of two-stage detectors by constructing minibatches using high-loss examples. Specifically, in OHEM each example is scored by its loss, non-maximum suppression (nms) is then applied, and a minibatch is constructed with the highest-loss examples. The nms threshold and batch size are tunable parameters. Like the focal loss, OHEM puts more emphasis on misclassified examples, but unlike FL, OHEM completely discards easy examples. We also implement a variant of OHEM used in SSD [22]: after applying nms to all examples, the minibatch is constructed to enforce a 1:3 ratio between positives and negatives to help ensure each minibatch has enough positives.				
1469	paper_95	What are the problems associated with class imbalance for single stage detectors ?	The problems associated with class imbalance for single stage detectors are: 1) Training is inefficient as most locations are easy negatives that contribute no useful learning signal.  2) The large number of easy negatives can overwhelm training and lead to degenerate models.	Both classic one-stage object detection methods, like boosted detectors [37, 5] and DPMs [8], and more recent methods, like SSD [22], face a large class imbalance during training. These detectors evaluate 10^{4}-10^{5} candidate locations per image but only a few locations contain objects. This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models. A common solution is to perform some form of hard negative mining [33, 37, 8, 31, 22] that samples hard examples during training or more complex sampling/reweighing schemes [2]. In contrast, we show that our proposed focal loss naturally handles the class imbalance faced by a one-stage detector and allows us to efficiently train on all examples without sampling and without easy negatives overwhelming the loss and computed gradients.				
1470	paper_95	What are the other loss functions experimented by the authors'?	The main loss function used by authors is The Focal Loss.	There has been much interest in designing robust loss functions (e.g., Huber loss [13]) that reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples). In contrast, rather than addressing outliers, our focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. In other words, the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples.The CE loss can be seen as the blue (top) curve in Figure 1. One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classified (p_{\textrm{t}}\gg.5) incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.In practice we use an \alpha-balanced variant of the focal loss:\textrm{FL}(p_{\textrm{t}})=-\alpha_{\textrm{t}}(1-p_{\textrm{t}})^{\gamma}\log(p_{\textrm{t}}).(5)We adopt this form in our experiments as it yields slightly improved accuracy over the non-\alpha-balanced form. Finally, we note that the implementation of the loss layer combines the sigmoid operation for computing p with the loss computation, resulting in greater numerical stability.Our next attempt to improve learning involved using the \alpha-balanced CE loss described in §3.1. Results for various \alpha are shown in Table 1a. Setting \alpha=.75 gives a gain of 0.9 points AP.Finally, in early experiments, we attempted to train with the hinge loss [13] on p_{\textrm{t}}, which sets loss to 0 above a certain value of p_{\textrm{t}}. However, this was unstable and we did not manage to obtain meaningful results. Results exploring alternate loss functions are in the appendix.In this paper, we propose a new loss function that acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure 1. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that our proposed Focal Loss enables us to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-of-the-art techniques for training one-stage detectors. Finally, we note that the exact form of the focal loss is not crucial, and we show other instantiations can achieve similar results.				
1471	paper_95	How focal loss can be extended to use for multi class problem?	The focal loss can be extended to multi-class as follows:Extending the focal loss to the multi-class case is straightforward and works well.  for simplicity we focus on the binary loss in this work. :\textrm{CE}(p,y)=\begin{cases}-\log(p)&\text{if $y=1$}\\-\log(1-p)&\text{otherwise. }\end{cases}(1)In the above y\in\{\pm 1\} specifies the ground-truth class and p\in[0,1] is the model’s estimated probability for the class with label y=1.  For notational convenience, we define p_{\textrm{t}}:p_{\textrm{t}}=\begin{cases}p&\text{if $y=1$}\\1-p&\text{otherwise,}\end{cases}(2)and rewrite \textrm{CE}(p,y)=\textrm{CE}(p_{\textrm{t}})=-\log(p_{\textrm{t}}).	The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000). We introduce the focal loss starting from the cross entropy (CE) loss for binary classification111Extending the focal loss to the multi-class case is straightforward and works well; for simplicity we focus on the binary loss in this work.:\textrm{CE}(p,y)=\begin{cases}-\log(p)&\text{if $y=1$}\\-\log(1-p)&\text{otherwise.}\end{cases}(1)In the above y\in\{\pm 1\} specifies the ground-truth class and p\in[0,1] is the model’s estimated probability for the class with label y=1. For notational convenience, we define p_{\textrm{t}}:p_{\textrm{t}}=\begin{cases}p&\text{if $y=1$}\\1-p&\text{otherwise,}\end{cases}(2)and rewrite \textrm{CE}(p,y)=\textrm{CE}(p_{\textrm{t}})=-\log(p_{\textrm{t}}).				
1472	paper_95	If we have 100000 easy examples (0.1 each) and 100 hard examples (2.3 each),Is it possible calculate percentage loss difference between them?	Since the imbalance between easy and hard examples is large(100000 vs 100), it is not possible to calculate the percentage loss difference between them.	As our experiments will show, the large class imbalance encountered during training of dense detectors overwhelms the cross entropy loss. Easily classified negatives comprise the majority of the loss and dominate the gradient. While \alpha balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.				
1473	paper_95	What is the effect of increasing modulating factor (γ )?	As specified wrongly in the question, modulating factor is (1-p_{\textrm{t}})^{\gamma} and not \gamma.  Besides this as the modulating factor increases, the loss contribution from easy examples is reduced and the range in which an example receives low loss is extended.	More formally, we propose to add a modulating factor (1-p_{\textrm{t}})^{\gamma} to the cross entropy loss, with tunable focusing parameter \gamma\geq 0. We define the focal loss as:\textrm{FL}(p_{\textrm{t}})=-(1-p_{\textrm{t}})^{\gamma}\log(p_{\textrm{t}}).(4)Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with \gamma=2, an example classified with p_{\textrm{t}}=0.9 would have 100×100\times100 × lower loss compared with CE and with p_{\textrm{t}}\approx 0.968 it would have 1000×1000\times1000 × lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4×4\times4 × for p_{\textrm{t}}\leq.5 and \gamma=2).				
1474	paper_95	Why P needs initialization at the start of the training?	Since P is one of the parameters in the loss function, we have to initialise P with some value before training.  This P is then tuned while training such that the loss value is optimised.	Binary classification models are by default initialized to have equal probability of outputting either y=-1 or 1. Under such an initialization, in the presence of class imbalance, the loss due to the frequent class can dominate total loss and cause instability in early training. To counter this, we introduce the concept of a ‘prior’ for the value of p estimated by the model for the rare class (foreground) at the start of training. We denote the prior by \pi and set it so that the model’s estimated p for examples of the rare class is low, e.g. 0.01. We note that this is a change in model initialization (see §4.1) and not of the loss function. We found this to improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance.				
1475	paper_95	What value of π is used for experimentations ?	The authors have used \pi = 0. 01 for all the experimentations.	We experiment with ResNet-50-FPN and ResNet-101-FPN backbones [20]. The base ResNet-50 and ResNet-101 models are pre-trained on ImageNet1k; we use the models released by [16]. New layers added for FPN are initialized as in [20]. All new conv layers except the final one in the RetinaNet subnets are initialized with bias b=0 and a Gaussian weight fill with \sigma=0.01. For the final conv layer of the classification subnet, we set the bias initialization to b=-\log((1-\pi)/\pi), where \pi specifies that at the start of training every anchor should be labeled as foreground with confidence of \scriptstyle\sim\pi. We use \pi=.01 in all experiments, although results are robust to the exact value. As explained in §3.3, this initialization prevents the large number of background anchors from generating a large, destabilizing loss value in the first iteration of training.Our first attempt to train RetinaNet uses standard cross entropy (CE) loss without any modifications to the initialization or learning strategy. This fails quickly, with the network diverging during training. However, simply initializing the last layer of our model such that the prior probability of detecting an object is \pi=.01 (see §4.1) enables effective learning. Training RetinaNet with ResNet-50 and this initialization already yields a respectable AP of 30.2 on COCO. Results are insensitive to the exact value of \pi so we use \pi=.01 for all experiments.				
1476	paper_95	What was used as the backbone network for RetinaNet?	For RetinaNet, Feature Pyramid Network (FPN) was used as a backbone.	We adopt the Feature Pyramid Network (FPN) from [20] as the backbone network for RetinaNet. In brief, FPN augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale feature pyramid from a single resolution input image, see Figure 3(a)-(b). Each level of the pyramid can be used for detecting objects at a different scale. FPN improves multi-scale predictions from fully convolutional networks (FCN) [23], as shown by its gains for RPN [28] and DeepMask-style proposals [24], as well at two-stage detectors such as Fast R-CNN [10] or Mask R-CNN [14].				
1477	paper_95	How does it guess if it is an object or background? (Since using K classes and not K+1, 1 extra for background class)	To decide whether there is an object in the image or not, IoU is used.  If the value of IoU is equal to or above 0. 5 then its an object else if IoU is between 0 and 0. 4 then its background.	Each anchor is assigned a length K one-hot vector of classification targets, where K is the number of object classes, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multi-class detection and with adjusted thresholds. Specifically, anchors are assigned to ground-truth object boxes using an intersection-over-union (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K label vector to 1 and all other entries to 0. If an anchor is unassigned, which may happen with overlap in [0.4, 0.5), it is ignored during training. Box regression targets are computed as the offset between each anchor and its assigned object box, or omitted if there is no assignment.				
1478	paper_95	What scale and aspect ratios is used for designing anchor boxes?	Authors have used anchor boxes spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and with 3 aspect ratios [0. 5, 1, 2].	We sweep over the number of scale and aspect ratio anchors used at each spatial position and each pyramid level in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and 3 aspect ratios [0.5, 1, 2]. Results using ResNet-50 are shown in Table 1c. A surprisingly good AP (30.3) is achieved using just one square anchor. However, the AP can be improved by nearly 4 points (to 34.0) when using 3 scales and 3 aspect ratios per location. We used this setting for all other experiments in this work.				
1479	paper_95	How classification subnet is different from regression subnet?	Even though the design of box regression subnet is similar to classification subnet, the design terminates in 4A linear outputs per spatial location.  Also both the subnets use separate parameters.	In parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. The design of the box regression subnet is identical to the classification subnet except that it terminates in 4A linear outputs per spatial location, see Figure 3 (d). For each of the A anchors per spatial location, these 4 outputs predict the relative offset between the anchor and the ground-truth box (we use the standard box parameterization from R-CNN [11]). We note that unlike most recent work, we use a class-agnostic bounding box regressor which uses fewer parameters and we found to be equally effective. The object classification subnet and the box regression subnet, though sharing a common structure, use separate parameters.				
1480	paper_95	Why the normalization wasn't done taken all anchors into account?	The normalisation is not done by taking all anchors into account because vast majority of anchors are easy negatives and receive negligible loss values under the focal loss.	We use the focal loss introduced in this work as the loss on the output of the classification subnet. As we will show in §5, we find that \gamma=2 works well in practice and the RetinaNet is relatively robust to \gamma\in[0.5,5]. We emphasize that when training RetinaNet, the focal loss is applied to all \scriptstyle\sim100k anchors in each sampled image. This stands in contrast to common practice of using heuristic sampling (RPN) or hard example mining (OHEM, SSD) to select a small set of anchors (e.g., 256) for each minibatch. The total focal loss of an image is computed as the sum of the focal loss over all \scriptstyle\sim100k anchors, normalized by the number of anchors assigned to a ground-truth box. We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss. Finally we note that \alpha, the weight assigned to the rare class, also has a stable range, but it interacts with \gamma making it necessary to select the two together (see Tables 1a and 1b). In general \alpha should be decreased slightly as \gamma is increased (for \gamma=2, \alpha=0.25 works best).				
1481	paper_95	Why aren't we using α=0.75 since the positive samples are our minority classes?	Setting the \alpha to 0. 75 gives a gain of 0. 9 in AP and for \gamma = 2. 0, \alpha = . 25 or . 5 gives the best results i.  it lowers the AP by .   This is why using \alpha = . 75 is not preferred.	Our next attempt to improve learning involved using the \alpha-balanced CE loss described in §3.1. Results for various \alpha are shown in Table 1a. Setting \alpha=.75 gives a gain of 0.9 points AP.For the experiments in Table 1b, for a fair comparison we find the best \alpha for each \gamma. We observe that lower \alpha’s are selected for higher \gamma’s (as easy negatives are down-weighted, less emphasis needs to be placed on the positives). Overall, however, the benefit of changing \gamma is much larger, and indeed the best \alpha’s ranged in just [.25,.75] (we tested \alpha\in[.01,.999]). We use \gamma=2.0 with \alpha=.25 for all experiments but \alpha=.5 works nearly as well (.4 AP lower).				
1482	paper_95	How do you compare background and foreground samples as γ changes?	The effect of changing γ on the distribution of the loss for positive examples is minor.  For negatives, however, increasing γ heavily concentrates the loss on hard examples, focusing nearly all attention away from easy negatives.	Cumulative distribution functions for positive and negative samples are shown in Figure 4. If we observe the positive samples, we see that the CDF looks fairly similar for different values of \gamma. For example, approximately 20% of the hardest positive samples account for roughly half of the positive loss, as \gamma increases more of the loss gets concentrated in the top 20% of examples, but the effect is minor.				
1483	paper_95	Did increase in Anchor Density improves AP?	Increasing anchor density does improve the AP value, but beyond 6-9 anchors, there was no further gain.	We sweep over the number of scale and aspect ratio anchors used at each spatial position and each pyramid level in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2^{k/4}, for k\leq 3) and 3 aspect ratios [0.5, 1, 2]. Results using ResNet-50 are shown in Table 1c. A surprisingly good AP (30.3) is achieved using just one square anchor. However, the AP can be improved by nearly 4 points (to 34.0) when using 3 scales and 3 aspect ratios per location. We used this setting for all other experiments in this work.Finally, we note that increasing beyond 6-9 anchors did not shown further gains. Thus while two-stage systems can classify arbitrary boxes in an image, the saturation of performance w.r.t. density implies the higher potential density of two-stage systems may not offer an advantage.				
1484	paper_96	What is the issue with intractable posterior distribution?	The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior.  The existing method, mean-field variational inference requires factorial.  Therefore, authors suggest SGVB (Stochastic Gradient Variational Bayes) estimator that can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters.  For coding or data representation tasks, efficient approximation of posterior inference is required.	Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.				
1485	paper_96	What are the steps in AEVB algorithm?	The AEVB algorithm connects between directed probabilistic models(trained with a variational objective) and auto-encoders.  It is connected between linear auto-encoders and a certain class of generative linear-Gaussian models. They use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm.  Using the SGVB estimator to optimize a recognition model allows us to perform very efficient approximate posterior inference using simple ancestral sampling.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.In this section we’ll give an example where we use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm.The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint.				
1486	paper_96	What are the two steps specified to generate data?	The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}).  (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}).	In order to solve our problem we invoked an alternative method for generating samples from q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}). The essential parameterization trick is quite simple. Let \mathbf{z} be a continuous random variable, and \mathbf{z}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) be some conditional distribution. It is then often possible to express the random variable \mathbf{z} as a deterministic variable \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}), where \boldsymbol{\epsilon} is an auxiliary variable with independent marginal p(\boldsymbol{\epsilon}), and gϕ(.)g_{\boldsymbol{\phi}}(.)italic_g start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( . ) is some vector-valued function parameterized by \boldsymbol{\phi}.Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.				
1487	paper_96	Why can’t we use sampling based solutions instead of this algorithm in case of large datasets?	It is hard to use sampling based solutions because batch optimization with so much data is too expensive.  If you want to inference in almost any model with continuous latent variables and/or parameters, sampling based solution is not applicable.  For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator which is one of sampling based solution.  But we need to deal with high dimensional data and the AEVB algorithm is useful.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.				
1488	paper_96	What are the uses of approximate posterior inference of the latent variable z given an observed value x for parameters θ?	For coding or data representation tasks, it is useful to approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} efficiently because the unobserved variables z have an interpretation as a latent representation or code.  In this paper, authors assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}).  They introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}).  Contrast to mean-field variational inference, this algorithm can compute its parameters \phi from some closed-form expectation by introducing learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}.  Given a datapoint \mathbf{x}, it produces a distribution (e.  a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated.	Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients.Let the prior over the latent variables be the centered isotropic multivariate Gaussian p_{\boldsymbol{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I}). Note that in this case, the prior lacks parameters. We let p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \mathbf{z} with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}) is in this case intractable.While there is much freedom in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure222Note that this is just a (simplifying) choice, and not a limitation of our method.:\displaystyle\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\displaystyle=\log\mathcal{N}(\mathbf{z};\boldsymbol{\mu}^{(i)},\boldsymbol{\sigma}^{2(i)}\mathbf{I})(9)where the mean and s.d. of the approximate posterior, \boldsymbol{\mu}^{(i)} and \boldsymbol{\sigma}^{(i)}, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint \mathbf{x}^{(i)} and the variational parameters \boldsymbol{\phi} (see appendix C).As explained in section 2.4, we sample from the posterior \mathbf{z}^{(i,l)}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)}) using \mathbf{z}^{(i,l)}=g_{\boldsymbol{\phi}}(\mathbf{x}^{(i)},\boldsymbol{\epsilon}^{(l)})=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)}\odot\boldsymbol{\epsilon}^{(l)} where \boldsymbol{\epsilon}^{(l)}\sim\mathcal{N}(\mathbf{0},\mathbf{I}). With \odot we signify an element-wise product.In this model both p_{\boldsymbol{\theta}}(\mathbf{z}) (the prior) and q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint \mathbf{x}^{(i)} is:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle\simeq\frac{1}{2}\sum_{j=1}^{J}\left(1+\log((\sigma_{j}^{(i)})^{2})-(\mu_{j}^{(i)})^{2}-(\sigma_{j}^{(i)})^{2}\right)+\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})\displaystyle\text{where\quad}\mathbf{z}^{(i,l)}\displaystyle=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)}\odot\boldsymbol{\epsilon}^{(l)}\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim\mathcal{N}(0,\mathbf{I})(10)As explained above and in appendix C, the decoding term \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling.The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution.For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}.From a coding theory perspective, the unobserved variables \mathbf{z} have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as a probabilistic encoder, since given a datapoint \mathbf{x} it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated. In a similar vein we will refer to p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) as a probabilistic decoder, since given a code \mathbf{z} it produces a distribution over the possible corresponding values of \mathbf{x}.In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.				
1489	paper_96	What is used as a recognition model in variational auto encoder?	The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders.  Authors introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}).  All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0. 01), and were jointly stochastically optimized using the MAP criterion.	For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint.For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}.				
1490	paper_96	Why is the recognition model also referred to as a probabilistic encoder?	A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known.  Therefore, in this paper, given a datapoint \mathbf{x} it produces a distribution (e.  a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated.	For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.In this section we’ll give an example where we use a neural network for the probabilistic encoder q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) (the approximation to the posterior of the generative model p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})) and where the parameters \boldsymbol{\phi} and \boldsymbol{\theta} are optimized jointly with the AEVB algorithm.The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.The recently proposed DARN method  [GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables.Even more recently,  [RMW14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB.From a coding theory perspective, the unobserved variables \mathbf{z} have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as a probabilistic encoder, since given a datapoint \mathbf{x} it produces a distribution (e.g. a Gaussian) over the possible values of the code \mathbf{z} from which the datapoint \mathbf{x} could have been generated. In a similar vein we will refer to p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) as a probabilistic decoder, since given a code \mathbf{z} it produces a distribution over the possible corresponding values of \mathbf{x}.				
1491	paper_96	What’s the effect of the gradient of the lower bound w.r.t. φ on the naïve Monte Carlo estimator?	The gradient of the lower bound w.  \boldsymbol{\phi} is a bit problematic.  The usual (naïve) Monte Carlo gradient estimator for this type of problem is impractical for our purposes.  Because gradient estimator exhibits exhibits very high variance.  Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.	Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients.This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \boldsymbol{\phi}. A proof is as follows. Given the deterministic mapping \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) we know that q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})\prod_{i}dz_{i}=p(\boldsymbol{\epsilon})\prod_{i}d\epsilon_{i}. Therefore111Note that for infinitesimals we use the notational convention d\mathbf{z}=\prod_{i}dz_{i}, \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}=\int p(\boldsymbol{\epsilon})f(\mathbf{z})\,d\boldsymbol{\epsilon}=\int p(\boldsymbol{\epsilon})f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}))\,d\boldsymbol{\epsilon}. It follows that a differentiable estimator can be constructed: \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}\simeq\frac{1}{L}\sum_{l=1}^{L}f(g_{\boldsymbol{\phi}}(\mathbf{x},\boldsymbol{\epsilon}^{(l)})) where \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon}). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound.The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints \log p_{\boldsymbol{\theta}}(\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(N)})=\sum_{i=1}^{N}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}), which can each be rewritten as:logp𝜽(𝐱(i))=DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳|𝐱(i)))+ℒ(𝜽,ϕ;𝐱(i))\displaystyle\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})=D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}^{(i)}))+\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) + caligraphic_L ( bold_italic_θ , bold_italic_ϕ ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )(1)The first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term \mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}) is called the (variational) lower bound on the marginal likelihood of datapoint i, and can be written as:\displaystyle\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)})\geq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})}\left[-\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})+\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})\right](2)which can also be written as:ℒ(𝜽,ϕ;𝐱(i))=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+𝔼qϕ⁢(𝐳|𝐱(i))[logp𝜽(𝐱(i)|𝐳)]\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]caligraphic_L ( bold_italic_θ , bold_italic_ϕ ; bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z ) ](3)We want to differentiate and optimize the lower bound \mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}) w.r.t. both the variational parameters \boldsymbol{\phi} and generative parameters \boldsymbol{\theta}. However, the gradient of the lower bound w.r.t. \boldsymbol{\phi} is a bit problematic. The usual (naïve) Monte Carlo gradient estimator for this type of problem is:\nabla_{\boldsymbol{\phi}}\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\right]=\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}\left[f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z})}\log q_{\boldsymbol{\phi}}(\mathbf{z})\right]\simeq\frac{1}{L}\sum_{l=1}^{L}f(\mathbf{z})\nabla_{q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)})}\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(l)}) where \mathbf{z}^{(l)}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)}). This gradient estimator exhibits exhibits very high variance (see e.g.  [BJP12]) and is impractical for our purposes.				
1492	paper_96	How to obtain SGVB estimator from variational lower bound?	Authors apply Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.  q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) to the variational lower bound (eq.  The KL-diverdnence in eq.  (3) can be integrated, such that only the reconstruction error requires estimation by sampling.  The KL-divergence term regularizes \phi, encouraging the approximate posterior to be close to the prior p_\theta(z) yielding a SGVB estimator.  Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients.The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the naïve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.				
1493	paper_96	Just like AEVB, Wake Sleep algorithm employs a recognition model that approximates the true posterior. Is this true?	Both of them employ a recognition model that approximates the true posterior.  Authors compare  performance of AEVB to the wake-sleep algorithm [HDFN95] employing the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder.  All parameters, both variational and generative, were initialized by random sampling from N (0, 0. 01), and were jointly stochastically optimized using the MAP criterion.	The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint.Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions.				
1494	paper_96	Compared to AEVB, what is the drawback of Wake Sleep algorithm?	Wake-Sleep has the same computational complexity as AEVB per datapoint.  Moreover, a drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood and its optimization is slow compared to AEVB.  Figure 3 demonstrates the AEVB algorithm to the wake-sleep algorithm, in terms of the estimated marginal likelihood, for a different number of training points.	The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.				
1495	paper_96	According to authors, there’s a connection between PCA and maximum-likelihood (ML) solution of a special case of the linear-Gaussian model. Is this true?	In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x}. \mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.	The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In  [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(\mathbf{z})=\mathcal{N}(0,\mathbf{I}) and a conditional distribution p(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\mathbf{W}\mathbf{z},\epsilon\mathbf{I}), specifically the case with infinitesimally small \epsilon.				
1496	paper_96	As the data in Frey Face dataset is continuous, how did the authors process it?	They consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.  samples of some continuous or discrete variable \mathbf{x}.  They assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}.  The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}).  (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}).	The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.				
1497	paper_96	What do we mean by hidden units?	The chosen number of hidden units is based on prior literature on auto-encoders and as the choice of dataset, hidden units affect overfitting. As the number of hidden units, we can learn latent representation and it affects the performance of applications such as image denoising, inpainting and super-resolution.	In relevant recent work on autoencoders [VLL{}^{+}10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle [Lin89]) of the mutual information between input X and latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model [VLL{}^{+}10], i.e. the negative reconstrution error.However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations [BCV13].Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants  [BCV13]. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations.Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In [SL10] a recognition model was employed for efficient learning with Deep Boltzmann Machines.These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models.The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0,1) using a sigmoidal activation function at the decoder output.Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.				
1498	paper_96	How can the SGVB be optimised?	Authors form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.  q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as Eq.  (5).  They use this technique to the variational lower bound (eq.  All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0. 01), and were jointly stochastically optimized using the MAP criterion.  Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients.Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint.We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.				
1499	paper_96	Sometimes the posterior distribution is intractable in nature. What do we mean by intractable?	In this paper, authors want to solve several inference tasks including image denoising, inpainting, and super-resolution by approximating marginal inference of the variable \mathbf{x}.  To solve these problems, they introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}).  The posterior distribution is intractable, thus in this paper, authors introduce the stragegy that can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.Let the prior over the latent variables be the centered isotropic multivariate Gaussian p_{\boldsymbol{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I}). Note that in this case, the prior lacks parameters. We let p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \mathbf{z} with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}) is in this case intractable.While there is much freedom in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), we’ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure222Note that this is just a (simplifying) choice, and not a limitation of our method.:\displaystyle\log q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\displaystyle=\log\mathcal{N}(\mathbf{z};\boldsymbol{\mu}^{(i)},\boldsymbol{\sigma}^{2(i)}\mathbf{I})(9)where the mean and s.d. of the approximate posterior, \boldsymbol{\mu}^{(i)} and \boldsymbol{\sigma}^{(i)}, are outputs of the encoding MLP, i.e. nonlinear functions of datapoint \mathbf{x}^{(i)} and the variational parameters \boldsymbol{\phi} (see appendix C).The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution.For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}.				
1500	paper_96	Is the computational complexity per data point of wake sleep algorithm same as AEVB or is it different?	Wake-Sleep has the same computational complexity as AEVB per datapoint.	The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood.An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. All parameters, both variational and generative, were initialized by random sampling from \mathcal{N}(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M=100 were used, with L=1 samples per datapoint.				
1501	paper_96	A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Is this estimator differentiable?	This trick can be used to obtain a differentiable estimator of the variational lower bound.  Authours show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound.  The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods.	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) we can reparameterize the random variable \widetilde{\mathbf{z}}\sim q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) using a differentiable transformation g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) of an (auxiliary) noise variable \boldsymbol{\epsilon}:\displaystyle\widetilde{\mathbf{z}}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x})\text{\quad with \quad}\boldsymbol{\epsilon}\sim p(\boldsymbol{\epsilon})(4)See section 2.4 for general strategies for chosing such an approriate distribution p(\boldsymbol{\epsilon}) and function g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}).We can now form Monte Carlo estimates of expectations of some function f(\mathbf{z}) w.r.t. q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) as follows:\displaystyle\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[f(\mathbf{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}^{(i)}))\right]\displaystyle\simeq\frac{1}{L}\sum_{l=1}^{L}{f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(l)},\mathbf{x}^{(i)}))}\text{\quad where \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(5)We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}):\displaystyle\widetilde{\mathcal{L}}^{A}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\displaystyle=\frac{1}{L}\sum_{l=1}^{L}\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)},\mathbf{z}^{(i,l)})-\log q_{\boldsymbol{\phi}}(\mathbf{z}^{(i,l)}|\mathbf{x}^{(i)})\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(6)Often, the KL-divergence DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing \boldsymbol{\phi}, encouraging the approximate posterior to be close to the prior p_{\boldsymbol{\theta}}(\mathbf{z}).This yields a second version of the SGVB estimator \widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\simeq\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)}), corresponding to eq. (3), which typically has less variance than the generic estimator:\displaystyle\widetilde{\mathcal{L}}^{B}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})=−DK⁢L(qϕ(𝐳|𝐱(i))||p𝜽(𝐳))+1L∑l=1L(logp𝜽(𝐱(i)|𝐳(i,l)))\displaystyle=-D_{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\boldsymbol{\theta}}(\mathbf{z}))+\frac{1}{L}\sum_{l=1}^{L}(\log p_{\boldsymbol{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}))= - italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT bold_italic_ϕ end_POSTSUBSCRIPT ( bold_z | bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) | | italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_z ) ) + divide start_ARG 1 end_ARG start_ARG italic_L end_ARG ∑ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( roman_log italic_p start_POSTSUBSCRIPT bold_italic_θ end_POSTSUBSCRIPT ( bold_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | bold_z start_POSTSUPERSCRIPT ( italic_i , italic_l ) end_POSTSUPERSCRIPT ) )\displaystyle\text{where \quad}\mathbf{z}^{(i,l)}\displaystyle=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\text{\quad and \quad}\boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon})(7)Given multiple datapoints from a dataset \mathbf{X} with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:\displaystyle\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X})\simeq\widetilde{\mathcal{L}}^{M}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^{M}\widetilde{\mathcal{L}}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})(8)where the minibatch \mathbf{X}^{M}=\{\mathbf{x}^{(i)}\}_{i=1}^{M} is a randomly drawn sample of M datapoints from the full dataset \mathbf{X} with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M=100. Derivatives \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\widetilde{\mathcal{L}}(\boldsymbol{\theta};\mathbf{X}^{M}) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients.This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \boldsymbol{\phi}. A proof is as follows. Given the deterministic mapping \mathbf{z}=g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}) we know that q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})\prod_{i}dz_{i}=p(\boldsymbol{\epsilon})\prod_{i}d\epsilon_{i}. Therefore111Note that for infinitesimals we use the notational convention d\mathbf{z}=\prod_{i}dz_{i}, \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}=\int p(\boldsymbol{\epsilon})f(\mathbf{z})\,d\boldsymbol{\epsilon}=\int p(\boldsymbol{\epsilon})f(g_{\boldsymbol{\phi}}(\boldsymbol{\epsilon},\mathbf{x}))\,d\boldsymbol{\epsilon}. It follows that a differentiable estimator can be constructed: \int q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x})f(\mathbf{z})\,d\mathbf{z}\simeq\frac{1}{L}\sum_{l=1}^{L}f(g_{\boldsymbol{\phi}}(\mathbf{x},\boldsymbol{\epsilon}^{(l)})) where \boldsymbol{\epsilon}^{(l)}\sim p(\boldsymbol{\epsilon}). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound.The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.				
1502	paper_96	Why are the parameters important? What can we do with them?	Authors introduce a practical estimator of the lower bound and its derivatives w.  the parameters.  They introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}).	How can we perform efficient approximate inference and learning with directed probabilistic modelswhose continuous latent variables and/or parameters have intractable posterior distributions?The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \nabla_{\boldsymbol{\theta},\boldsymbol{\phi}}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{X}) (see algorithm  1), plus a small weight decay term corresponding to a prior p(\boldsymbol{\theta})=\mathcal{N}(0,\mathbf{I}). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:1.Intractability: the case where the integral of the marginal likelihood p_{\boldsymbol{\theta}}(\mathbf{x})=\int p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})\,d\mathbf{z} is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x}) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), e.g. a neural network with a nonlinear hidden layer.2.A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.We are interested in, and propose a solution to, three related problems in the above scenario:1.Efficient approximate ML or MAP estimation for the parameters \boldsymbol{\theta}. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.2.Efficient approximate posterior inference of the latent variable \mathbf{z} given an observed value \mathbf{x} for a choice of parameters \boldsymbol{\theta}. This is useful for coding or data representation tasks.3.Efficient approximate marginal inference of the variable \mathbf{x}. This allows us to perform all kinds of inference tasks where a prior over \mathbf{x} is required. Common applications in computer vision include image denoising, inpainting and super-resolution.For the purpose of solving the above problems, let us introduce a recognition model q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}): an approximation to the intractable true posterior p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x}). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters \boldsymbol{\phi} are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters \boldsymbol{\phi} jointly with the generative model parameters \boldsymbol{\theta}.In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q_{\boldsymbol{\phi}}(\mathbf{z}|\mathbf{x}), but please note that the technique can be applied to the case q_{\boldsymbol{\phi}}(\mathbf{z}), i.e. where we do not condition on \mathbf{x}, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.				
1503	paper_96	What should be the size of the latent space for generative model in case of MNIST datasets for getting the best results?	For higher dimensional latent space the estimates became unreliable and authors use the MNIST dataset which is a low dimensional dataset.  For likelihood lower bound, they trained generative models (decoders) and corresponding encoders(a.  recognition models) having 500 hidden units in case of MNIST.  And for very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator.  For the encoder and decoder they used neural networks with 100 hidden units, and 3 latent variables.  for higher dimensional latent space the estimates became unreliable.	We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used.The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.				
1504	paper_96	What do we mean by i.i.d. datasets?	They show that for i.  datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.  For the case of i.  datasets, they can inference and learning efficiently.	Let us consider some dataset \mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^{N} consisting of N i.i.d. samples of some continuous or discrete variable \mathbf{x}. We assume that the data are generated by some random process, involving an unobserved continuous random variable \mathbf{z}. The process consists of two steps: (1) a value \mathbf{z}^{(i)} is generated from some prior distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{z}); (2) a value \mathbf{x}^{(i)} is generated from some conditional distribution p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}). We assume that the prior p_{\boldsymbol{\theta}^{*}}(\mathbf{z}) and likelihood p_{\boldsymbol{\theta}^{*}}(\mathbf{x}|\mathbf{z}) come from parametric families of distributions p_{\boldsymbol{\theta}}(\mathbf{z}) and p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z}), and that their PDFs are differentiable almost everywhere w.r.t. both \boldsymbol{\theta} and \mathbf{z}. Unfortunately, a lot of this process is hidden from our view: the true parameters \boldsymbol{\theta}^{*} as well as the values of the latent variables \mathbf{z}^{(i)} are unknown to us.We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results.				
1505	paper_97	How is the conditioning vector obtained?	From text prompts, the conditioning embedding vector is obtained.  The text prompt is first tokenized to provide a fixed length token identification vector, and then the language model maps this vector to produce an embedding that serves as a conditioning vector.	Diffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. Specifically, this corresponds to learning the reverse process of a fixed-length Markovian forward process. In simple terms, a conditional diffusion model \hat{\mathbf{x}}_{\theta} is trained using a squared error loss to denoise a variably-noised image \mathbf{z}_{t}\coloneqq\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}} as follows:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}\right](1)where \mathbf{x} is the ground-truth image, \mathbf{c} is a conditioning vector (e.g., obtained from a text prompt), {\bm{\epsilon}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) is a noise term and \alpha_{t},\sigma_{t},w_{t} are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t\sim\mathcal{U}([0,1]).At inference time, the diffusion model is sampled by iteratively denoising \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) using either the deterministic DDIM song2020denoising  or the stochastic ancestral sampler ho2020denoising . Intermediate points \mathbf{z}_{t_{1}},\dotsc,\mathbf{z}_{t_{T}}, where 1=t_{1}>\cdots>t_{T}=0, are generated, with decreasing noise levels. These points, \hat{\mathbf{x}}^{t}_{0}\coloneqq\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t},\mathbf{c}), are functions of the \mathbf{x}-predictions.The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. ramesh2022hierarchical  use CLIP text embeddings that are translated into image embeddings using a learned prior, while Saharia et al. saharia2022photorealistic  use a pre-trained T5-XXL language model raffel2020exploring . In our work, we use the latter.Language models like T5-XXL generate embeddings of a tokenized text prompt, and vocabulary encoding is an important pre-processing step for prompt embedding. In order to transform a text prompt \mathbf{P} into a conditioning embedding \mathbf{c}, the text is first tokenized using a tokenizer f using a learned vocabulary. Following saharia2022photorealistic , we use the SentencePiece tokenizer kudo2018sentencepiece . After tokenizing a prompt \mathbf{P} using tokenizer f we obtain a fixed-length vector f(\mathbf{P}). The language model \Gamma is conditioned on this token identifier vector to produce an embedding \mathbf{c}\coloneqq\Gamma(f(\mathbf{P})). Finally, the text-to-image diffusion model is directly conditioned on \mathbf{c}.				
1506	paper_97	Which language model is used for vocabulary encoding and why?	CLIP and T5-XXL language models are used for translating noise into image embeddings.  Authors’ used T5-XXL for their model.  Vocabulary encoding is a pre-processing step for these language models which is used to convert the prompt into a condition embedding vector.	The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. ramesh2022hierarchical  use CLIP text embeddings that are translated into image embeddings using a learned prior, while Saharia et al. saharia2022photorealistic  use a pre-trained T5-XXL language model raffel2020exploring . In our work, we use the latter.Language models like T5-XXL generate embeddings of a tokenized text prompt, and vocabulary encoding is an important pre-processing step for prompt embedding. In order to transform a text prompt \mathbf{P} into a conditioning embedding \mathbf{c}, the text is first tokenized using a tokenizer f using a learned vocabulary. Following saharia2022photorealistic , we use the SentencePiece tokenizer kudo2018sentencepiece . After tokenizing a prompt \mathbf{P} using tokenizer f we obtain a fixed-length vector f(\mathbf{P}). The language model \Gamma is conditioned on this token identifier vector to produce an embedding \mathbf{c}\coloneqq\Gamma(f(\mathbf{P})). Finally, the text-to-image diffusion model is directly conditioned on \mathbf{c}.				
1507	paper_97	What kind of output variations are possible with dreambooth?	Output variants include changing the subject's location, species, color, shape, pose, expression, material, and semantics.	Given only a few (3-5) casually captured images of a specific subject, without any textual description, our objective is to generate new images of the subject with high detail fidelity and with variations guided by text prompts.We do not impose any restrictions on input image capture settings and the subject image can have varying contexts.Examples of output variations include: changing the place where the subject is, changing a property of the subject such as color, species, or shape, and modifying the subject’s pose, expression, material, and other semantic modifications. We find that the breadth of these modifications is very large given the powerful prior of these models. A high-level overview of our method is presented in Figure 3.				
1508	paper_97	How are the input images labelled while fine tuning the model?	Authors fine-tuned all the layers that contain text embeddings and these embeddings were created from labels( "a [identifier] [class noun”) of all input photographs of the subject. 	We opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can be obtained using a classifier. We specifically use a class descriptor in the sentence in order to tether the prior of the class to our unique subject. We found that using only a unique identifier, without a class noun, as a key for our subject increased training time and decreased performance. In essence, we want to leverage the diffusion model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier. In this way, it can leverage the visual prior to generate new poses and articulations of the subject in different contexts.Since our input image set is quite small, fine-tuning the large image generation models can overfit to both the context and the appearance of the subject in the given input images (e.g., subject pose).Figure 12 (top) shows some sample generated images with naive fine-tuning where we clearly see that both the subject dog’s appearance and context are overfitted to those in the input images.There are many techniques that can be used to address these problems, such as regularization or selectively fine-tuning certain parts of the model. There is uncertainty on which parts of the model need to be frozen to both obtain good subject fidelity and semantic modification flexibility. In our experience, the best results that achieve maximum subject fidelity are achieved by fine-tuning all layers of the model. Nevertheless, this includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of language drift.				
1509	paper_97	Why do we use a class descriptor while fine tuning the model?	Visual prior to specific class can generate new poses and articulations of the subject in different contexts.  Additionally, incorporating the class descriptor speeds up training and improves results.	We opt for a simpler approach and label all input images of the subject “a [identifier] [class noun]”, where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can be obtained using a classifier. We specifically use a class descriptor in the sentence in order to tether the prior of the class to our unique subject. We found that using only a unique identifier, without a class noun, as a key for our subject increased training time and decreased performance. In essence, we want to leverage the diffusion model’s prior of the specific class and entangle it with the embedding of our subject’s unique identifier. In this way, it can leverage the visual prior to generate new poses and articulations of the subject in different contexts.				
1510	paper_97	Why shouldn’t an existing word be used as an identifier while fine tuning?	Existing words in the training set of text-to-image diffusion models have stronger priors, hence they shouldn't be employed as identifiers during fine tuning.	A naive way of constructing an identifier for our subject is to use an existing word. For example, using the words like “unique” or “special”. One problem is that existing English words tend to have a stronger prior due to occurrence in the training set of text-to-image diffusion models. We generally find increased training time and decreased performance when using such generic words to index our subject, since the model has to both learn to disentangle them from their original meaning and to re-entangle them to reference our subject. This approach can also fail by entangling the meaning of the word with the appearance of our object, for example in the extreme case if the identifier chosen is the adjective “blue” and our subject is grey, colors will be entangled at inference and we will sample a mix of grey and blue subjects (as well as mixes of both).This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model.A hazardous way of doing this is to select random characters in the English language and concatenate them to generate a rare identifier (e.g. “xxy5syt00”). In reality, the tokenizer might tokenize each letter separately, and the prior for the diffusion model is strong for these letters. Specifically, if we sample the model with such an identifier before fine-tuning we will get pictorial depictions of the letters or concepts that are linked to those letters. We often find that these tokens incur the same weaknesses as using common English words to index the subject.				
1511	paper_97	Why can’t we use a string of random letters as an identifier while fine tuning the model?	Theoretically, concatenating random characters to create an unique identifier has weak prior in both the language model and the diffusion model.  However, in practice, it has strong prior since the model tokenizes each letter independently, which increases training and reduces performance.	A naive way of constructing an identifier for our subject is to use an existing word. For example, using the words like “unique” or “special”. One problem is that existing English words tend to have a stronger prior due to occurrence in the training set of text-to-image diffusion models. We generally find increased training time and decreased performance when using such generic words to index our subject, since the model has to both learn to disentangle them from their original meaning and to re-entangle them to reference our subject. This approach can also fail by entangling the meaning of the word with the appearance of our object, for example in the extreme case if the identifier chosen is the adjective “blue” and our subject is grey, colors will be entangled at inference and we will sample a mix of grey and blue subjects (as well as mixes of both).This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model.A hazardous way of doing this is to select random characters in the English language and concatenate them to generate a rare identifier (e.g. “xxy5syt00”). In reality, the tokenizer might tokenize each letter separately, and the prior for the diffusion model is strong for these letters. Specifically, if we sample the model with such an identifier before fine-tuning we will get pictorial depictions of the letters or concepts that are linked to those letters. We often find that these tokens incur the same weaknesses as using common English words to index the subject.				
1512	paper_97	How to create rare token identifier?	In order to get a list of rare token IDs, we first search for rare tokens in the vocabulary.  The de-tokenizer is then used to invert the vocabulary, resulting in a string of characters that serves as our rare-token identifier.	In a nutshell, our approach is to find relatively rare tokens in the vocabulary, and then invert these rare tokens into text space. In order to do this, we first perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers f(\hat{\mathbf{V}}), where f is a tokenizer; a function that maps character sequences to tokens and \hat{\mathbf{V}} is the decoded text stemming from the tokens f(\hat{\mathbf{V}}). This sequence can be of variable length k with k being a hyperparameter of our method. We find that relatively short sequences of k=\{1,...,3\} work well. Then, by inverting the vocabulary using the de-tokenizer on f(\hat{\mathbf{V}}) we obtain a sequence of characters that define our unique identifier \hat{\mathbf{V}}. We observe that using uniform random sampling without replacement of tokens that correspond to 3 or fewer Unicode characters (without spaces) and using tokens in the T5-XXL tokenizer range of \{5000,...,10000\} works well.				
1513	paper_97	How to deal with overfitting due to small input set while fine tuning the text-to-img model?	For small input sets, Fine-tuning  large image generation models can overfit context and subject appearance.  Regularization or selective model fine-tuning can solve these issues.  Fine-tuning all model layers achieves maximum subject fidelity and gives the best result.	Since our input image set is quite small, fine-tuning the large image generation models can overfit to both the context and the appearance of the subject in the given input images (e.g., subject pose).Figure 12 (top) shows some sample generated images with naive fine-tuning where we clearly see that both the subject dog’s appearance and context are overfitted to those in the input images.There are many techniques that can be used to address these problems, such as regularization or selectively fine-tuning certain parts of the model. There is uncertainty on which parts of the model need to be frozen to both obtain good subject fidelity and semantic modification flexibility. In our experience, the best results that achieve maximum subject fidelity are achieved by fine-tuning all layers of the model. Nevertheless, this includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of language drift.				
1514	paper_97	What is language drift and how to deal with it?	Language drift occurs when a language model pre-trained on a large text corpus and fine-tuned for a specific task loses syntactic and semantic understanding as it improves learning the target task only.  Authors suggested that their novel autogenous class-specific prior-preserving loss solves this issue.	In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject.The phenomenon of language drift has been an observed problem in the language model literature Lee2019CounteringLD ; lu2020countering , where a language model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language as it learns to improve in the target task. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffusion models. Since our text prompt contains both the [identifier] and [class noun], when a diffusion model is fine-tuned on a small set of subject images, we observe that it slowly forgets how to generate subjects of the same class and progressively forgets the class-specific prior and can not generate different instances of the class in question. Figure 13 (middle) shows some sample generated images of “a dog” after fine-tuning the model on specific dog images. The results clearly show that the model loses the capability of generating generic dog images with naive fine-tuning.We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{"a [class noun]"})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4.	Language drift occurs when a language model pre-prepared on a large text corpus and fine-tuned for a specific task loses syntactic and semantic understanding as it improves learning the target task only.  Authors suggested that their novel autogenous class-specific prior-preserving loss solves this issue.	Tortured phrases	pre-trained -> pre-prepared	
1515	paper_97	What is prior preservation loss?	Prior preservation loss supervises the model with its own samples to keep the prior during few-shot fine-tuning. The loss equation is presented in the evidential paragraph.	We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{"a [class noun]"})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4.				
1516	paper_97	What are the applications of dreambooth?	Applications of Text-based image generation includes recontextualization & manipulation of subjects, , original art renditions, novel view synthesis and much more.	We discuss various insights related to the suggested mechanism and its analogy to related work, as well as highlight the contribution of each component via ablation studies, and compare our method with alternative baselines. We apply our approach to a myriad of text-based image generation applications including recontextualization of subjects, modification of their properties, original art renditions, and more, paving the way to a new stream of previously unassailable tasks.				
1517	paper_97	Explain Class Prior Ablation	We observe that the class prior of the erroneous class remains entangled and we cannot create new images of our subject when the model is trained in this manner.  With a longer fitting time, we can help resolve the class noun prior, but the model loses the ability to produce subjects and performance decreases.  The model struggles to learn the subject instance and associate the class prior with it without a class noun.	We show the results of using no class noun, a wrong class noun, and the correct class noun for text supervision of our subject images. Results are shown in Figure 11. We observe that the class prior of the wrong class (dog) remains entangled and we are not able to generate new images of our subject when the model is trained in this manner. Given a longer fitting time, we are able to disentangle the prior of the class noun “dog”, but at the cost of losing the ability to generate dogs with the model and ultimately with decreased performance. If we are to train without a class noun, the model has difficulty learning the subject instance and does not easily entangle the class prior with the instance. The model takes longer to converge and can generate erroneous samples.				
1518	paper_97	Which pretrained large text to image models have authors used?	Authors used pre-trained Imagen text-to-image diffusion model.	More formally, given a few images of a subject (\sim3-5), our objective is to implant the subject into the output domain of the model such that it can be synthesized with a unique identifier. To that end, we propose techniques to represent a given subject with rare token identifiers and fine-tune a pre-trained, diffusion-based text-to-image framework that operates in two steps; generating a low-resolution image from text and subsequently applying super-resolution (SR) diffusion models. We first fine-tune the low-resolution text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). In order to prevent overfitting and language drift Lee2019CounteringLD ; lu2020countering  that cause the model to associate the class name (e.g., “dog”) with the specific instance, we propose an autogenous, class-specific prior preservation loss, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject. In the second step, we fine-tune the super-resolution component with pairs of low-resolution and high-resolution versions of the input images. This allows the model to maintain high fidelity to small (but important) details of the subject. We use the pre-trained Imagen model saharia2022photorealistic  as a base model in our experiments, although our method is not constrained to any specific text-to-image diffusion model.				
1519	paper_97	Is it true that large text to image models cannot mimic and create novel rendition of images in a reference set?	This is true that large text to image models cannot mimic and create novel rendition of images in a reference set.	Recently developed large text-to-image models achieve a remarkable leap in the evolution of AI, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language saharia2022photorealistic ; ramesh2022hierarchical . One of the main advantages of such models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.While the synthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of those same subjects in different contexts. The main reason is that the expressiveness of their output domain is limited; even the most detailed textual description of an object may yield instances with different appearances. Furthermore, even models whose text embedding lies in a shared language-vision space radford2021learning  cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2).				
1520	paper_97	Why can’t we create novel rendition of reference images using the pretrained model itself?	Because the output domain of the pretrained model is limited, we cannot use it to create novel renditions of reference images.	Recently developed large text-to-image models achieve a remarkable leap in the evolution of AI, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language saharia2022photorealistic ; ramesh2022hierarchical . One of the main advantages of such models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word “dog” with various instances of dogs that can appear in different poses and contexts in an image.While the synthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of those same subjects in different contexts. The main reason is that the expressiveness of their output domain is limited; even the most detailed textual description of an object may yield instances with different appearances. Furthermore, even models whose text embedding lies in a shared language-vision space radford2021learning  cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2).				
1521	paper_97	What is the problem authors have tried to solve?	The author tried to solve subject-driven generation that is to synthesize novel depictions of the subject in different contexts.	In summary, our two main contributions in this work are:•A new problem: subject-driven generation. Given a few casually captured images of a subject, the goal is to synthesize novel renditions of the subject in different contexts, while maintaining high fidelity to its key visual features.•A new technique for fine-tuning text-to-image diffusion models in a few-shot setting, while preserving the model’s semantic knowledge on the class of the subject.				
1522	paper_97	What is the loss function used by authors?	Authors of the paper used a prior-preserving loss function.	In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject.We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{"a [class noun]"})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4.More formally, given a few images of a subject (\sim3-5), our objective is to implant the subject into the output domain of the model such that it can be synthesized with a unique identifier. To that end, we propose techniques to represent a given subject with rare token identifiers and fine-tune a pre-trained, diffusion-based text-to-image framework that operates in two steps; generating a low-resolution image from text and subsequently applying super-resolution (SR) diffusion models. We first fine-tune the low-resolution text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., “A [V] dog”). In order to prevent overfitting and language drift Lee2019CounteringLD ; lu2020countering  that cause the model to associate the class name (e.g., “dog”) with the specific instance, we propose an autogenous, class-specific prior preservation loss, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject. In the second step, we fine-tune the super-resolution component with pairs of low-resolution and high-resolution versions of the input images. This allows the model to maintain high fidelity to small (but important) details of the subject. We use the pre-trained Imagen model saharia2022photorealistic  as a base model in our experiments, although our method is not constrained to any specific text-to-image diffusion model.Our proposed loss acts as a powerful regularizer. A naively fine-tuned network can quickly overfit to the small set of subject images. In order to explore this phenomenon, we train two models for 200 epochs, one using naive fine-tuning (i.e. using the loss in Equation 1), and a network using our prior-preservation loss shown in Equation 2. We show results for different context captions in Figure 12. We observe that the regularization effect of our loss allows us to capture a wider range of poses for our subject dog without sacrificing subject fidelity. Importantly, we observe that using naive fine-tuning the dog usually lies on a fabric-type material similar to the training images, whereas this is avoided using our method.Further, we evaluate how our prior preservation loss described in Section 4.2 conserves variability in the prior and show sample results in Figure 13. We verify that a vanilla model is able to generate a large variety of dogs, while a naively fine-tuned model on the subject dog exhibits language drift and generates our subject dog given the prompt “a dog”. Our proposed loss preserves the variability of the prior and the model is able to generate new instances of our dog given a prompt of the style “a [V] dog” but also varied instances of dogs given a “a dog” prompt.				
1523	paper_97	Explain limitations of dreambooth	Authors presented numerous drawbacks, the first of which is that it cannot accurately produce the required context.  The second failure mode is context-appearance entanglement, in which the subject's appearance alters as a result of the prompted context.  Third, we observe overfitting to real images when the prompt is similar to the original setting in which the subject was observed.  Other limits discovered include the fact that some subjects are considerably easier to learn than others.	Our method has several limitations, which we demonstrate in Figure 17, grouped into three main failure modes. The first is related to not being able to accurately generate the prompted context. For example, in Figure 17 we observe that when we prompt the model with “a [V] backpack in the ISS” and “a [V] backpack on the moon” it is not able to generate the desired contexts. Possible reasons are that the generative model does not have a strong prior for these contexts, or that representing both the subject and the context together is a difficult task for the model. The second failure mode is context-appearance entanglement, where the appearance of the subject changes due to the prompted context. In Figure 17 we show examples of a backpack that changes colors due to the desired context being rare (“a [V] backpack in the Bolivian Salt Flats”) or entangling the color of the context with that of the subject (“a [V] backpack on top of blue fabric”). Third, we also observe overfitting to the real images that happens when the prompt is similar to the original setting in which the subject was seen. An example is shown in Figure 17.Other limitations observed are that some subjects are much easier to learn than others. For example, the model has a very strong prior for dogs and cats, with many learned variations. Occasionally, with subjects that are rarer or more complex, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated features on the subject, depending on the strength of the model prior, and the complexity of the semantic modification.				
1524	paper_98	What are the FID values achieved by authors using Diffusion Model on ImageNet?	They obtain state-of-the-art image generation on ImageNet 64×64.  For higher resolution ImageNet.  Table 5 shows the performance of ADM.  Metrics include FID, sFID, Prec, Rec.	For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and Saharia et al. [53] train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256\times256, but does not reach the same performance as state-of-the-art models like BigGAN-deep Nichol and Dhariwal (2021); Saharia et al. (2021), as seen in Table 5.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.				
1525	paper_98	Why are GANs so difficult to train?	GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers.  Much work has been done to achieve GAN-like sample quality with likelihood-based models and they are typically easier to scale and train than GANs.	GANs Goodfellow et al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et al. (2018); Wu et al. (2019); Karras et al. (2019b) as measured by sample quality metrics such as FID Heusel et al. (2017), Inception Score Salimans et al. (2016) and Precision Kynkäänniemi et al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et al. (2019); Nichol and Dhariwal (2021); Nash et al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et al. (2018); Miyato et al. (2018); Brock et al. (2016).While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et al. (2019); Ho et al. (2020); Nash et al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.				
1526	paper_98	How do we obtain the noise(epsilon) in a diffusion model?	Diffusion models sample from a distribution by reversing a gradual noising process.  In particular, sampling starts with noise xT and produces gradually less-noisy samples xT −1, xT −2, .  until reaching a final sample x0.  We assume that the noise ε is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.	On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise x_{T} and produces gradually less-noisy samples x_{T-1},x_{T-2},... until reaching a final sample x_{0}. Each timestep t corresponds to a certain noise level, and x_{t} can be thought of as a mixture of a signal x_{0} with some noise \epsilon where the signal to noise ratio is determined by the timestep t. For the remainder of this paper, we assume that the noise \epsilon is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations.A diffusion model learns to produce a slightly more “denoised” x_{t-1} from x_{t}. Ho et al. [25] parameterize this model as a function \epsilon_{\theta}(x_{t},t) which predicts the noise component of a noisy sample x_{t}. To train these models, each sample in a minibatch is produced by randomly drawing a data sample x_{0}, a timestep t, and noise \epsilon, which together give rise to a noised sample x_{t} (Equation 17).The training objective is then ||\epsilon_{\theta}(x_{t},t)-\epsilon||^{2}, i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26).				
1527	paper_98	Which are the metrics used by authors to compare the performance of the models?	They use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work.  Moreover, they use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage.  In Table 4, they report FID, sFID, IS, Precision, and Recall as metrics.	Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et al. (2015) latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et al. (2019a, b); Brock et al. (2018); Ho et al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et al. (2019a, b); Ho et al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et al. (2017); Brock et al. (2018), and evaluate metrics for all models using the same codebase.For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.				
1528	paper_98	What is the final improved architecture used by authors for experiments in this paper?	They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.	In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.We explore the following architectural changes:For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.	They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and down-examining, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.	Tortured phrases	downsampling -> down-examining	
1529	paper_98	Why did the authors have to scale the classifier gradients by a constant factor larger than 1?	When using a scale of 1, they observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.  Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%.  When using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.	We can safely ignore the constant term C_{4}, since it corresponds to the normalizing coefficient Z in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by \Sigma g. Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor s for the gradients, which we describe in more detail in Section 4.3.In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect.To understand the effect of scaling classifier gradients, note that s\cdot\mathop{}\!\nabla_{\!x}\log p(y|x)=\mathop{}\!\nabla_{\!x}\log\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.				
1530	paper_98	How does the trade-off between fidelity and diversity vary with the Gradient Scale?	When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.  Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%.  Using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.	We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets.In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection.Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect.To understand the effect of scaling classifier gradients, note that s\cdot\mathop{}\!\nabla_{\!x}\log p(y|x)=\mathop{}\!\nabla_{\!x}\log\frac{1}{Z}p(y|x)^{s}, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)^{s}. When s>1, this distribution becomes sharper than p(y|x), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples.Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do so on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis.				
1531	paper_98	What is IS as a measure of fidelity?	IS measures of fidelity but it has a drawback that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS.	Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS Barratt and Sharma (2018). To better capture diversity than IS, Fréchet Inception Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 Szegedy et al. (2015) latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkäänniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision.				
1532	paper_98	In terms of image synthesis, do the GANs perform better than VQ-VAE or not?	Fidelity can be higher, but GANs are not always better in terms of low diversity.  In table 5 ImageNet256x256 experiment, BigGAN-deep beats VA-VAE2 about FID, sFID, Precision but lose about Recall.	GANs Goodfellow et al. (2014) currently hold the state-of-the-art on most image generation tasks Brock et al. (2018); Wu et al. (2019); Karras et al. (2019b) as measured by sample quality metrics such as FID Heusel et al. (2017), Inception Score Salimans et al. (2016) and Precision Kynkäänniemi et al. (2019). However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models Razavi et al. (2019); Nichol and Dhariwal (2021); Nash et al. (2021). Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers Brock et al. (2018); Miyato et al. (2018); Brock et al. (2016).While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models Razavi et al. (2019); Ho et al. (2020); Nash et al. (2021); Child (2021). While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64\times64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do so using only 25 diffusion steps.Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE van den Oord et al. (2017) and VQ-VAE-2 Razavi et al. (2019) are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer Nash et al. (2021) is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE Vahdat and Kautz (2020) and VDVAE Child (2021) have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history Ackley et al. (1985); Dayan et al. (1995); Hinton (2002). Sampling from the EBM distribution is challenging, and Xie et al. [70] demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch [15] further improve upon this approach, obtaining high quality images. More recently, Gao et al. [18] incorporate diffusion steps into an energy-based model, and find that doing so improves image samples from these models.				
1533	paper_98	Why did authors recompute some of the metrics using public samples or models?	P1 demonstrates why did authors recompute some of the metrics.  There are two reasons: first, some papers compare against arbitrary subsets of the training set which are not readily available.  and second, subtle implementation differences can affect the resulting FID values.	We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work Karras et al. (2019a, b); Brock et al. (2018); Ho et al. (2020). We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers Karras et al. (2019a, b); Ho et al. (2020) compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values Parmar et al. (2021). To ensure consistent comparisons, we use the entire training set as the reference batch Heusel et al. (2017); Brock et al. (2018), and evaluate metrics for all models using the same codebase.				
1534	paper_98	What is AdaGN?	AdaGN incorporates the timestep and class embedding into each residual block after a group normalization operation [69], similar to adaptive instance norm [27] and FiLM.  They define AdaGN(h, y) = ys GroupNorm(h)+yb, where h is the intermediate activations of the residual block following the first convolution, and y = [ys, yb] is obtained from a linear projection of the timestep and class embedding.	We also experiment with a layer Nichol and Dhariwal (2021) that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation Wu and He (2018), similar to adaptive instance norm Karras et al. (2019a) and FiLM Perez et al. (2017). We define this layer as \text{AdaGN}(h,y)=y_{s}\text{ GroupNorm}(h)+y_{b}, where h is the intermediate activations of the residual block following the first convolution, and y=[y_{s},y_{b}] is obtained from a linear projection of the timestep and class embedding.				
1535	paper_99	How did the authors decide lambda, and what is the optimal value?	lambda=1. 0 is used to avoid any domain-targeted tuning.  Authors train the term and neural components independently, combing them only at inference.	This is identical to QGen, but instead of using the pure neural model, we train the hybrid model in Section 4.4 setting \lambda=1.0 for all models to avoid any domain-targeted tuning. We train the term and neural components independently, combing them only at inference.				
1538	paper_99	I agree that increasing the size of the passage embedding vectors is better, especially for long passages. What I am curious about is the motivation behind introducing the weight metirx. What if we just use CLS vector?	weight matrix preserves the original size of h_{\text{CLS}} and perform better than down-projecting to a lower dimensional of CLS vector.	We encode P as (\text{CLS, }p_{1},\ldots,p_{m},\text{ SEP}). For some datasets, a passage contains both a title T=(t_{1},...,t_{l}) and content C=(c_{1},...,c_{o}), in which case we encode the passage as (\text{CLS, }t_{1},...,t_{l},\text{SEP},c_{1},...,c_{o},\text{ SEP}).These sequences are fed to the BERT encoder.Let h_{\text{CLS}}\in\mathbb{R}^{N} be the final representation of the “CLS” token.Passage encodings p are computed by applying a linear projection, i.e., \textbf{p}=\textbf{W}*h_{\text{CLS}}, where W is a N\times N weight matrix (thus N=768), which preserves the original size of h_{\text{CLS}}. This has been shown to perform better than down-projecting to a lower dimensional vector Luan et al. (2020), especially for long passages.				
1539	paper_99	For training, did the authors intentionally use a single relevant passage or have no choice but to do that becuase the training dataset provides only one relevant passage, i.e., because of annotation scarcity?	The authors had no choice because the training dataset provides only one relevant passage, i.	In this work, we are specifically investigating the zero-shot scenario where there exists neither user issued questions nor domain specific data except the passage collection itself. We propose to address the training data scarcity issue by generating synthetic questions Zhou et al. (2017); Duan et al. (2017); Alberti et al. (2019); Nogueira et al. (2019).Leverage the fact that there are large question-answer data sources freely available from the web Shah and Pomerantz (2010); Duan et al. (2017).we first train a question generator using general domain question-answer pairs.The passage collection of a target domain is then fed into this generator to create pairs of noisy question-passage pairs, which are used to train a retrieval model (see Figure 2).In this work, we mine English question-answer pairs from community resources, primarily StackExchange444archive.org/details/stackexchange and Yahoo! Answers555webscope.sandbox.yahoo.com/catalog.php?datatype=l.Note we use stackexchange as it covers a wide range of topics, and we focus on investigating thedomain adaptability of using a question generation approach.We leave comparing question generator trained on different datasets or using different architectures to future work.				
1541	paper_99	What sentence splitter did you use for chunking?	we split its abstract into chunks with sentence boundaries preserved. A passage is constructed by concatenating the title and one chunk.  Chunk size is set so that each passage has no more than 200 wordpiece tokens.  It does not split sentences but passages.	Biomedical questions from Task B Phase A of BioASQ Tsatsaronis et al. (2015).We use BioASQ 7 and 8 test data for evaluation.The collection contains all abstracts from MEDLINE articles.Given an article, we split its abstract into chunks with sentence boundaries preserved.A passage is constructed by concatenating the title and one chunk.Chunk size is set so that each passage has no more than 200 wordpiece tokens.				
1546	paper_99	What are the rescorers they used?	A supervised neural rescorer is used.	One question we can ask is how close to the state-of-the-art in supervised passage retrieval are these zero-shot models. To test this we looked at BioASQ 8 dataset and compare to the top-participant systems.101010participants-area.bioasq.org Since BioASQ provides annotated training data, the top teams typically use supervised models with a first-stage retrieval plus rescorer architecture. For instance, the AUEB group, which is the top or near top system for BioASQ 6, 7 and 8, uses a BM25 first-stage retrieval model plus a supervised neural rescorer Brokos et al. (2018); Pappas et al. (2019).				
1547	paper_99	Is it true that the fact that the optimal performance was not seen with 100% of data indicates the strong generalization ability? I suspect that there can be tail documents that can be challenging for the system to memorize even when 100% data is used for training.	its true that the fact that the optimal performance was not seen with 100% of data indicates the strong generalization ability.  Since, the peak of retrieval accuracy is achieved when using a 20\% subset, which covers 21\% of the reference passages.  Therefore, Tail documents can not be challenging for system because the learned system does generalize.	Since our approach allows us to generate queries on every passage of the target corpus, one question is that whether retrieval system trained this way simply memorizes the target corpus or it also generalize on unseen passages. Furthermore, from an efficiency standpoint, how many synthetic training examples are required to achieve maximum performance.To answer these questions, we uniformly sample a subset of documents and then generate synthetic queries only on that subset.Results on BIOASQ 7 are shown in Figure 4, where x-axis denotes the percentage of sampled documents.We can see that retrieval accuracy improves as passage coverage increases.The peak is achieved when using a 20\% subset, which covers 21\% of the reference passages.This is not surprising because the number of frequently discussed entities/topics are typically limited, and a subset of the passages covers most of them.This result also indicates that the learned system does generalize, otherwise optimal performance would be seen with 100\% of the data.				
1549	paper_99	What are the term-based techniques they used in their experiments?	Traditional term-based methods like BM25 Robertson et al.	Traditional term-based methods like BM25 Robertson et al. (1995) are powerful zero-shot models and can outperform supervised neural models in many cases Lin (2019). Rescoring systems have shown that integrating BM25 into a neural model improves performance McDonald et al. (2018). However, for first-stage retrieval most work focuses on approximations via re-ranking Karpukhin et al. (2020); Luan et al. (2020). Here we present a technique for exact hybrid first-stage retrieval without the need for a re-ranking stage. Our method is motivated by the work of Seo et al. (2019) for sparse-dense QA.				
1550	paper_99	How much the quality of generated pseudo-queries affect retrieval performance on target domain?	larger generation models lead to improved generators.  However, there is little difference in retrieval metrics,suggesting that large domain targeted data is the more important criteria.	Another interesting question is how important is the quality of the question generator relative to retrieval performance.Below we measured generation quality (via Rouge-based metrics Lin and Hovy (2002)) versus retrieval quality for three systems. The base generator contains 12 transformer layers, the lite version only uses the first 3 layer. The large one contains 24 transformer layers and each layer with larger hidden layer size, 4096, and more attention heads, 16.Retrieval quality was measured on BIOASQ 7 and generation quality with a held out set of the community question-answer data set.Results are shown in Table 6. We can see that larger generation models lead to improved generators. However, there is little difference in retrieval metrics,suggesting that large domain targeted data is the more important criteria.				
1551	paper_99	Will the pseudo-query generator generalize well when the target corpus is significantly different from the source domain on which the generator was trained?	The pseudo-query generator generalize well when the target corpus is significantly different from the source domain on which the generator was trained.  The result of querying through subset shows increase in performance as passage coverage increases and peak I obtained with only 21%of the dataset.	Since our approach allows us to generate queries on every passage of the target corpus, one question is that whether retrieval system trained this way simply memorizes the target corpus or it also generalize on unseen passages. Furthermore, from an efficiency standpoint, how many synthetic training examples are required to achieve maximum performance.To answer these questions, we uniformly sample a subset of documents and then generate synthetic queries only on that subset.Results on BIOASQ 7 are shown in Figure 4, where x-axis denotes the percentage of sampled documents.We can see that retrieval accuracy improves as passage coverage increases.The peak is achieved when using a 20\% subset, which covers 21\% of the reference passages.This is not surprising because the number of frequently discussed entities/topics are typically limited, and a subset of the passages covers most of them.This result also indicates that the learned system does generalize, otherwise optimal performance would be seen with 100\% of the data.				
1553	paper_99	I am wondering why the authors used community QA datasets for training the question generator. How about open-domain information retrieval datasets, such as MS MARCO, which also cover diverse domains?	Using open-domain information retrieval datasets for training neural retrieval models do not transfer well especially for specialized domains.	First, we observe that general-domain question-passage pairs can be acquired from community platforms Shah and Pomerantz (2010); Duan et al. (2017) or high quality academic datasets that are publicly available Kwiatkowski et al. (2019); Bajaj et al. (2016). Such resources have been used to create open domain QA passage retrieval models. However, as shown in Guo et al. (2020) and in our later experiments, neural retrieval models trained on the general domain data often do not transfer well, especially for specialized domains.				