Citation_context	Cited_content	Label	Twist_category	Qwen8B_prediction	Qwen14B_prediction	Mistral7B_prediction	Mistral24B_prediction	Llama8B_prediction	Deepseek_prediction
A formal definition of non-differentiability has not been provided by the authors.  However, authors in this context use this idea of differentiability to explain that most automated metrics cannot provide quality estimates after a language model generates one token or a parietal output - similar to humans, they can provide quality estimates only after a language model generates a full sequence.  This non-differentiability is pointed out as a problem by the authors for the new metrics (i.  things like BLUERT or BERTScore) - it is unclear from this paper alone if previous vanilla metrics (i.  BLEU) were per token differentiable or not.  The authors are looking for ways to use these non-differentiable scores in RL frameworks, potentially as a reward function.	The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
A formal definition of non-differentiability has not been provided by the authors.  However, authors in this context use this idea of differentiability to explain that most automated metrics can provide quality estimates after a language model generates one token or a parietal output - similar to humans, they can provide quality estimates only after a language model generates a full sequence.  This non-differentiability is pointed out as a problem by the authors for the new metrics (i.  things like BLUERT or BERTScore) - it is unclear from this paper alone if previous vanilla metrics (i.  BLEU) were per token differentiable or not.  The authors are looking for ways to use these non-differentiable scores in RL frameworks, potentially as a reward function.	The ultimate aim of language technology is to interact with humans.However, most language models are trained without direct signals of human preference,with supervised target strings serving as (a sometimes crude) proxy.One option to incorporate user feedback is via human-in-the-loop, i.e., a user would be expected to provide feedback for each sample online as the model trains, but this degree of dense supervision is often prohibitive and inefficient.Automated metrics offer a promising compromise: learned models of human preference like BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020), summarization preferences (Wu et al., 2021) have significantly improved correlation with human judgment compared to earlier metrics (BLEU, METEOR, etc.), and are cheap to evaluate. But — these functions are usually not per-token differentiable: like humans, metrics can only offer quality estimates for full generations.Reinforcement Learning (RL) offers a natural path forward for optimizing non-differentiable, scalar objectives for LM-based generation when it is cast as a sequential decision-making problem. However, Goodhart’s Law222Strathern (1997) paraphrases: When a measure becomes a target, it ceases to be a good measure.looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates. Recent works have shown promising results in aligning LMs to human preferences via RL by constraining preference-based rewards to incorporate notions of fluency (Wu et al., 2021; Ouyang et al., 2022) but progress in this line of work is heavily hindered by a lack of open-source benchmarks and algorithmic implementations—resulting in perception that RL is a challenging paradigm for NLP (Choshen et al., 2020; Kreutzer et al., 2021).	Incorrect	Opposite	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The action space for language modeling is equal to the vocabulary set of language models.  Since the vocabularies are very large (i.  tens of thousands of possible tokens), the action space is also very large.  In general, locomotion in the real world can be condensed to three quantities - moving across X, Y or Z axes, or a linear combination thereof.  The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).RL for Large Action Spaces. MIXER (Ranzato et al., 2015) combined ideas from schedule sampling and REINFORCE (Williams, 1992).Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such asKG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration.Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The action space for language modeling is equal to the vocabulary set of language models.  Since the vocabularies are very large (i.  tens of thousands of possible tokens), the action space is also very large.  In general, locomotion in the real world can be condensed to five quantities - moving across X, Y or Z axes, or a linear combination thereof.  The authors mention that typical RL problems have an action space that is an order of magnitude smaller, but do not specifically explain how the action spaces for typical problems is modeled or constructed.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).RL for Large Action Spaces. MIXER (Ranzato et al., 2015) combined ideas from schedule sampling and REINFORCE (Williams, 1992).Bahdanau et al. (2016) proposed an actor-critic algorithm to address the variance/large action space problems when using REINFORCE for language generation; follow-up works such asKG-A2C (Ammanabrolu & Hausknecht, 2020), TrufLL (Martin et al., 2022), AE-DQN (Zahavy et al., 2018), and GALAD (Ammanabrolu et al., 2022) addressed similar issues by attempting to eliminate and reduce the action space during exploration.Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Actor critic models are a class of reinforcement learning algorithms.  The paper does not contain information on Q-learning or how actor critic models are different from Q-learning models.	RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments.Formally, this class of algorithms allows us to train a parameterized control policy defined as \pi_{\theta}:\mathcal{S}\rightarrow\mathcal{A}, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory \mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(\bm{s}_{t},a_{t})].Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \pi_{0} from which we initial our agent’s policy \pi_{\theta}=\pi_{0}.Similarly, the value network V_{\phi} used to estimate the value function is also initialized from \pi_{0} except for the final layer which is randomly initialized to output a single scalar value.As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})]leading to a definition of our advantage function as A_{t}^{\pi}(\bm{s},a)=Q_{t}^{\pi}(\bm{s},a)-V_{t}^{\pi}.To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).	Correct		Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
Actor critic models are a class of reinforcement learning algorithms.  The paper contains information on Q-learning and how actor critic models are different from Q-learning models.	RL4LMs supports fine-tuning and training LMs from scratch via on-policy actor-critic algorithms on language environments.Formally, this class of algorithms allows us to train a parameterized control policy defined as \pi_{\theta}:\mathcal{S}\rightarrow\mathcal{A}, a function that attempts to select an action in a given state so as to maximize long term discounted rewards over a trajectory \mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(\bm{s}_{t},a_{t})].Our benchmark experiments focus on fine-tuning a pre-trained LM denoted as \pi_{0} from which we initial our agent’s policy \pi_{\theta}=\pi_{0}.Similarly, the value network V_{\phi} used to estimate the value function is also initialized from \pi_{0} except for the final layer which is randomly initialized to output a single scalar value.As with other deep RL actor-critic algorithms, we define our value and Q-value functions as V_{t}^{\pi}=\mathbb{E}_{a_{t}\sim\pi}[\sum_{\tau=t}^{T}\gamma R(\bm{s}_{\tau},a_{\tau},{\bm{y}})],Q_{t}^{\pi}(\bm{s}_{t},a_{t})=R(\bm{s}_{t},a_{t},{\bm{y}})+\gamma\mathbb{E}_{s_{t+1}\sim P}[V_{t+1}^{\pi}(\bm{s}_{t+1})]leading to a definition of our advantage function as A_{t}^{\pi}(\bm{s},a)=Q_{t}^{\pi}(\bm{s},a)-V_{t}^{\pi}.To increase training stability, advantage is appoximated using Generalized Advantage Estimation (Schulman et al., 2015).	Incorrect	Opposite	Incorrect	Unrelated	Correct	Incorrect	Correct	Correct
The authors hypothesize that their dynamic masking function helps because it adds a new constraint that the RL algorithm has to abide by.  Additionally, since this is a dynamic function, being updated oftenly (every mu steps), it is likely that the masking function ensures that the selected top-p tokens are more relevant to the current state the RL algorithm needs to analyse and decide on.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The authors hypothesize that their dynamic masking function doesn't help because it adds a new constraint that the RL algorithm has to abide by. Additionally, since this is a dynamic function, being updated oftenly (every mu steps), it is likely that the masking function ensures that the selected top-p tokens are more relevant to the current state the RL algorithm needs to analyse and decide on.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors mention that they update the masking function every "mu" steps, but the main text of the paper itself does not appear to contain the exact value of mu itself - there is a possibility that the author's model could work with mu=1 instead (i.  update every step) instead of updating it every couple of steps (i.  mu > 1), though the authors do not explain in this paper if this were done.  However, the authors mention that one reason why NLPO outperforms PPO is probably because of this dynamic masking function that is updated occasionally - this indicates to us that mu is probably not an unbounded or very large number (if mu were very large, tending to infinity, the function would no longer be dynamic). 	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors mention that they update the masking function every "mu" steps, but the main text of the paper itself does not appear to contain the exact value of mu itself - there is a possibility that the author's model could work with mu=5 instead (i.  update every step) instead of updating it every couple of steps (i.  mu > 5), though the authors do not explain in this paper if this were done.  However, the authors mention that one reason why NLPO outperforms PPO is probably because of this dynamic masking function that is updated occasionally - this indicates to us that mu is probably not an unbounded or very large number (if mu were very large, tending to infinity, the function would no longer be dynamic). 	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The use of token-masking policy in the proposed method (NLPO) is theorized by the authors to have been a key reason on why it was able to outperform the PPO based model.  They hypothesized that their masking function acts as a dynamic constraint added to the algorithm, that is able to more effectively filter and capture relevant information about the state thanks to its' dynamic nature.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The use of token-masking policy in the proposed method (TMM) is theorized by the authors to have been a key reason on why it was able to outperform the BERT based model.  They hypothesized that their masking function acts as a dynamic constraint added to the algorithm, that is able to more effectively filter and capture relevant information about the state thanks to its' dynamic nature.	Specifically, NLPOmaintains a masking policy \pi_{\psi}: the masking policy is a copy of the current policy (\pi_{\theta}), but is updated only every \mu steps.A parameterized-invalid-mask is created from \pi_{\psi} by first selecting the top-p tokens from the vocabulary,444\pi_{\psi} could be trained with alternate sampling techniques like top-k or beam search (or even hard-coded via rules by domain experts), though we find top-p sampling to be most effective in practice. and then applying an invalid-mask to the remaining tokens—i.e. setting their probabilities to zero when sampling actions from \pi_{\theta} during training;this periodic updating policy \pi_{\psi} is inspired by off-policy Q-learning algorithms (Andrychowicz et al., 2017),providing the policy \pi_{\theta} with an additional constraint that balances between the benefits of containing more task relevant information than the KL penalty derived from \pi_{0} and the risk of reward hacking.We provide pseudocode in Algorithm 1 (green portions highlight the differences with PPO).PPO vs. NLPO.Figure 2 shows that NLPO generally outperforms PPO and supervised, especially when applied after supervised training.We hypothesize that the primary reason for NLPO’s improved performance and stability is because the masking policy provides an additional constraint for the current policy.This constraint is not based on the initial untuned policy like the KL penalty but of the policy from \mu iterations ago and likely contains more task-relevant information learned during RL training.Table 3 (and Appendix Table 8) shows how performance increases up to a point and then decreases as p in top-p sampling is increased for the masking policy, relaxing the constraint by eliminating less tokens at each step, implying that there is a balance to be found in how much the model should be constrained during RL training.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The combinatorial action space here probably refers to the set of all possible actions that a RL agent for optimizing a language model could possibly take - here, the action set consists of the entire vocabulary of the language model, which can range to tens of thousands for typical GPT/T5 models used today.  This is unlike general RL tasks, where the action space is an order of magnitude smaller.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The combinatorial action space here probably refers to the set of all possible actions that a RL agent for optimizing a language model could possibly take - here, the action set consists of only a small part of vocabulary of the language model, which can range to tens of thousands for typical GPT/T5 models used today.  This is unlike general RL tasks, where the action space is an order of magnitude smaller.	Language generation action spaces are orders of magnitude larger than what most discrete action space RL algorithms are designed for (Ranzato et al., 2015; Ammanabrolu, 2021), e.g., GPT-2/3 and T5 have a vocabulary size of 50K and 32K respectively.We hypothesize that the size of the action space is a core cause of instability when training LMs with existing RL methods.To address this issue, we introduce NLPO (Natural Language Policy Optimization), which is inspired by work on action elimination/invalid-action masking (Zahavy et al., 2018; Huang & Ontañón, 2020; Ammanabrolu & Hausknecht, 2020). NLPO, a parameterized-masked extension of PPO, learns to mask out less relevant tokens in-context as it trains. NLPO accomplishes this via top-p sampling, which restricts tokens to the smallest possible set whose cumulative probability is greater than the probability parameter p (Holtzman et al., 2018).Each environment is an NLP task: we are given a supervised dataset \mathcal{D}=\{({\bm{x}}^{i},{\bm{y}}^{i})\}_{i=1}^{N} of N examples, where {\bm{x}}\in\mathcal{X} is an language input and {\bm{y}}\in\mathcal{Y} is the target string. Generation can be viewed as a Markov Decision Process (MDP) \langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T\rangle using a finite vocabulary \mathcal{V}.Each episode in the MDP begins by sampling a datapoint ({\bm{x}},{\bm{y}}) from our dataset and ends when the current time step t exceeds the horizon T or an end of sentence (EOS) token is generated.The input {\bm{x}}=(x_{0},\cdots,x_{m}) is a task-specific prompt that is used as our initial state \bm{s}_{0}=(x_{0},\cdots,x_{m}), where \bm{s}_{0}\in\mathcal{S} and \mathcal{S} is the state space with x_{m}\in\mathcal{V}. An action in the environment a_{t}\in\mathcal{A} consists of a token from our vocabulary \mathcal{V}. The transition function P:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S} deterministically appends an action a_{t} to the end of the state \bm{s}_{t-1}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{t-1}). This continues until the end of the horizon t\leq T and we obtain a state \bm{s}_{T}=(x_{0},\cdots,x_{m},a_{0},\cdots,a_{T}).At the end of an episode a reward \mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{Y}\rightarrow\mathbb{R}^{1} that depends on the (\bm{s}_{T},{\bm{y}}) (e.g., an automated metric like PARENT Dhingra et al. (2019)) is emitted. RL4LMs provides an OpenAI gym (Brockman et al., 2016) styleAPI for an RL environmentthat simulates this LM-Based MDP formulation.Abstracting the details of the MDP environment structure allows for new tasks to be added quickly with compatibility across all implemented algorithms.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Authors best practices were to combine effects of different ways of regularization to produce interpretable images.  They first search randomly through 300 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Authors best practices were to combine effects of different ways of regularization to produce interpretable images.  They first search randomly through 500 different combinations of  hyperparameters, then they pick the best four sets of hyperparameters that are compliments to each other and then these sets would be used to visualize preferred images for different classes.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The paper's DNN uses multiple smaller Gaussian kernels iteratively as a way of regularization during the optimization process as seen in equation 2 .	Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step rθ (x) = GaussianBlur(x, θb width). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter θb every to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The paper's DNN uses multiple smaller Bayesian kernels iteratively as a way of regularization during the optimization process as seen in equation 2 .	Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step rθ (x) = GaussianBlur(x, θb width). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter θb every to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Unrelated	Incorrect	Incorrect
Authors mainly introduce four different and newly used regularizations that would help researchers in visualizing responses from different layers.  These regularizations are designed to overcome different pathologies commonly encountered by gradient descent without regularization : L2 decay to penalize large pixel values which do not naturally occur, Gaussian blur:a useful regularization to iteratively penalize high frequency information associated with generated images via gradient ascent through each optimization step, Clipping pixels with small norm or Clipping pixels with small contribution.	We investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization.Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r_{\theta}(\mathbf{x})=\mathrm{GaussianBlur}(\mathbf{x},\theta_{\mathrm{b\_width}}). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \theta_{\mathrm{b\_every}} to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).Paragraph 10 : L2 decay: A common regularization, L2 decay penalizes large values and is implemented as rθ (x) = (1−θdecay)·x. L2 decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization.Paragraph 11 :Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an x∗ that contains somewhat small, somewhat smooth values. However, x∗ will still tend to contain non-zero pixel values everywhere. Even if some pixels in x∗ show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in x∗ will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed.Paragraph 12 : Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Authors mainly introduce three different and newly used regularizations that would help researchers in visualizing responses from different layers.  These regularizations are designed to overcome different pathologies commonly encountered by gradient descent without regularization : L2 decay to penalize large pixel values which do not naturally occur, Gaussian blur:a useful regularization to iteratively penalize high frequency information associated with generated images via gradient ascent through each optimization step, Clipping pixels with small norm or Clipping pixels with small contribution.	We investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization.Gaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r_{\theta}(\mathbf{x})=\mathrm{GaussianBlur}(\mathbf{x},\theta_{\mathrm{b\_width}}). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \theta_{\mathrm{b\_every}} to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.The second tool — new regularizations that enable improved, interpretable, optimized visualizations of learned features — will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous studieshave shown that discriminative networks can easily be fooled or hacked by the addition of certain structurednoise in image space (Szegedy et al., 2013; Nguyen et al., 2014).An oft-cited reason for this property is that discriminative training leads networksto ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible imagesand then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y.Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).Paragraph 10 : L2 decay: A common regularization, L2 decay penalizes large values and is implemented as rθ (x) = (1−θdecay)·x. L2 decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization.Paragraph 11 :Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an x∗ that contains somewhat small, somewhat smooth values. However, x∗ will still tend to contain non-zero pixel values everywhere. Even if some pixels in x∗ show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in x∗ will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed.Paragraph 12 : Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
linear sweep can be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.	Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
linear sweep cannot be seen as a regular increment in the values of some regularization hyperparameter (from leftmost where there is no regularization to rightmost where strong regularization occur ) to see the variation of their effects on the corresponding activations.	Figure 3. The effects of each regularization method from Section 3 when used individually. Each of the four rows shows a linear sweep in hyperparameter space from no regularization (left) to strong regularization (right). When applied too strongly, some regularizations cause the optimization to fail (e.g. L2 decay, top row) or the images to be less interpretable (small norm and small contribution clipping, bottom two rows). For this reason, a random hyperparameter search was useful for finding joint hyperparameter settings that worked well together (see Figure 4). Best viewed electronically, zoomed in.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Calculating absolute difference between some neuron activation of an input and the activation for same input without certain pixel can be considered a way of measuring the contribution of that pixel in the total response of the neuron.  To ensure faster computation, we can estimate activation near the input with 1st order approximation (linear) and hence this leads  to total contribution estimated as the element wise product of the activation gradient and the input x (each element in this product shows how this pixel affects the total response ), we repeat this product for all different channels ,sum them all, and take absolute value  to find pixels with small contribution in either direction, positive or negative to get rid of.	Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel’s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |a_{i}(\mathbf{x})-a_{i}(\mathbf{x}_{-j})|, where \mathbf{x}_{-j} is \mathbf{x} but with the j^{th} pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing a_{i}(\mathbf{x}) around \mathbf{x}, in which case the contribution of each dimension of \mathbf{x} can be estimated as the elementwise product of \mathbf{x} and the gradient. We then sum over all three channels and take the absolute value, computing \left|\sum_{c}\mathbf{x}\circ\nabla_{\mathbf{x}}a_{i}(\mathbf{x})\right|. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r_{\theta}(\mathbf{x}) as the operation that sets pixels with contribution under the \theta_{\mathrm{c\_pct}} percentile to zero.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Calculating absolute difference between some neuron activation of an input and the activation for same input without certain pixel can be considered a way of measuring the contribution of that pixel in the partial response of the neuron.  To ensure faster computation, we can estimate activation near the input with 1st order approximation (linear) and hence this leads  to total contribution estimated as the element wise product of the activation gradient and the input x (each element in this product shows how this pixel affects the total response ), we repeat this product for all different channels ,sum them all, and take absolute value  to find pixels with small contribution in either direction, positive or negative to get rid of.	Clipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel’s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |a_{i}(\mathbf{x})-a_{i}(\mathbf{x}_{-j})|, where \mathbf{x}_{-j} is \mathbf{x} but with the j^{th} pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing a_{i}(\mathbf{x}) around \mathbf{x}, in which case the contribution of each dimension of \mathbf{x} can be estimated as the elementwise product of \mathbf{x} and the gradient. We then sum over all three channels and take the absolute value, computing \left|\sum_{c}\mathbf{x}\circ\nabla_{\mathbf{x}}a_{i}(\mathbf{x})\right|. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r_{\theta}(\mathbf{x}) as the operation that sets pixels with contribution under the \theta_{\mathrm{c\_pct}} percentile to zero.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
Dataset-centric approach requires the trained network together with some dataset  to run through the network showing high or low responses of different units while interacting with most significant images of such dataset.  This approach can also use deconvolution layers and upsampling to map and highlight the regions of an image that were responsible of the firing of the different units. "Network-centric approach" deals only with network without the need to any dataset. You can start with some initial input, compute activations through the forward path and then compute gradients while backprop.  You can then ascent or descent the input towards gradient until you reach a preferred input stimulus x* for the unit under consideration.  Working with input images, you can visualize that x* if you want. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Network-centric approachDataset-centric approach requires the trained network together with some dataset  to run through the network showing high or low responses of different units while interacting with most significant images of such dataset.  This approach can also use deconvolution layers and upsampling to map and highlight the regions of an image that were responsible of the firing of the different units. "Dataset-centric approach" deals only with network without the need to any dataset. You can start with some initial input, compute activations through the forward path and then compute gradients while backprop.  You can then ascent or descent the input towards gradient until you reach a preferred input stimulus x* for the unit under consideration.  Working with input images, you can visualize that x* if you want. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	Incorrect	Change concept	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
An example of "dataset-centric" approach can be deconvolution method which is used to highlight certain regions of some image that has the highest effects in the response of different units. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
An example of "dataset-centric" approach can be deconvolution method which is used to highlight all regions of some image that has the highest effects in the response of different units. 	Another approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fergus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
An example of such approach would be to consider a trained network, start with some initial input and compute the forward path activations.  compute gradients through backprop and then move this input towards or against the gradient direction until you have some interesting input that is of much significance in the responses of considered neurons.	Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
An example of such approach would be to consider a trained network, start without any initial input and compute the forward path activations.  compute gradients through backprop and then move this input towards or against the gradient direction until you have some interesting input that is of much significance in the responses of considered neurons.	Network-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input \mathbf{x}=\mathbf{x_{0}}, the activation a_{i}(\mathbf{x}) caused at some unit i by this input is computed, and then steps are taken in input spacealong the gradient \partial a_{i}(\mathbf{x})/\partial\mathbf{x} to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some \mathbf{x^{*}} which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, \mathbf{x^{*}} can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
'Hacks' means that they are not likely to naturally exist (non-natural looking images).  However they may even cause harmful changes in the response of the network.  Adversarial points for instance are examples of such hacks where slight increments in pixels of even correctly classified images can make them fool the network and tend to go beyond there original part of space and hence be misclassified.	These gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of “hacks” that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable “fooling examples” (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets(Goodfellow et al., 2014).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
'Hacks' means that they are not likely to naturally exist (non-natural looking images).  However they may even cause harmful changes in the response of the network.  Adversarial points for instance are examples of such hacks where slight increments in pixels of incorrectly classified images can make them fool the network and tend to go beyond there original part of space and hence be misclassified.	These gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of “hacks” that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable “fooling examples” (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets(Goodfellow et al., 2014).	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
row-major means that consecutive small grayscale images of each row reside next to each other unlike "column-major" and both are methods of storing elements in memory. 	Figure 1 shows examples of this type of plot for the \mathsf{conv5} layer.The \mathsf{conv5} layer has size 256\times13\times13, which we depict as 256 separate 13\times13 grayscale images. Each of the 256 small images contains activations in the same spatial x-y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\times16 grid in row-major order.Figure 2 shows a zoomed in view of one particular channel, \mathsf{conv5_{151}}, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.	Correct		Unrelated	Unrelated	Correct	Unrelated	Unrelated	Correct
row-major means that consecutive small RGB images of each row reside next to each other unlike "column-major" and both are methods of storing elements in memory. 	Figure 1 shows examples of this type of plot for the \mathsf{conv5} layer.The \mathsf{conv5} layer has size 256\times13\times13, which we depict as 256 separate 13\times13 grayscale images. Each of the 256 small images contains activations in the same spatial x-y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\times16 grid in row-major order.Figure 2 shows a zoomed in view of one particular channel, \mathsf{conv5_{151}}, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.	Incorrect	Change concept	Unrelated	Unrelated	Correct	Unrelated	Incorrect	Incorrect
Zero mean input data and Standardization in general improve the convergence properties of BP training, so it can help to reach desired solution fast.  Also, Authors may intend to have centered inputs so that network reduces its biasing towards certain classes or certain large or tiny response values, hence we can have reasonable values for activations and more visualizable responses from different neurons.	Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where	Correct		Incorrect	Incorrect	Correct	Incorrect	Correct	Incorrect
Zero mean input data and Standardization in general improve the convergence properties of BP training and few-shot tunning so it can help to reach desired solution fast.  Also, Authors may intend to have centered inputs so that network reduces its biasing towards certain classes or certain large or tiny response values, hence we can have reasonable values for activations and more visualizable responses from different neurons.	Our network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, \mathbf{x}, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image \mathbf{x^{*}} where	Incorrect	Invent something doesn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The reason is that convolution layers learn parameters that can extract useful information and relations from the feature map that can help it afterwards to judge and give suitable responses of what this category is.  Responses from learned detectors can resemble among some set of categories and can also differ among other set of categories.  Input -not being in the training classes- still has a feature map that different layers would respond to according to those different detectors which the network has already learned and would still give a probability vector which may not be accurate.  Hence, having noise in the input can stimulate different detectors to respond and fire different activations that would lead to changes in the probability output vector.	•One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on \mathsf{conv4} and \mathsf{conv5}. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section 4).•When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set’s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (\mathsf{fc6} and \mathsf{fc7}) also reveals a similar sensitivity to small input changes.•Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the \mathsf{conv5} layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted \mathsf{conv5_{151}} (channel number 151 on \mathsf{conv5}), is shown in Figure 2 activating for human and lion faces and in Figure 1 activating for a cat face. Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types — playgrounds, restaurant patios, living rooms, etc. — learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The reason is that convolution layers learn parameters that can extract useful information and relations from the feature map that can help it afterwards to judge and give suitable responses of what this category is.  Responses from learned detectors can resemble among some set of categories and can also differ among other set of categories.  Input -not being in the training classes- still has a feature map that different layers would respond to according to those unlearned detectors and would still give a probability vector which may not be accurate.  Hence, having noise in the input can stimulate different detectors to respond and fire different activations that would lead to changes in the probability output vector.	•One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on \mathsf{conv4} and \mathsf{conv5}. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section 4).•When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set’s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (\mathsf{fc6} and \mathsf{fc7}) also reveals a similar sensitivity to small input changes.•Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the \mathsf{conv5} layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted \mathsf{conv5_{151}} (channel number 151 on \mathsf{conv5}), is shown in Figure 2 activating for human and lion faces and in Figure 1 activating for a cat face. Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types — playgrounds, restaurant patios, living rooms, etc. — learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The paper reaches this goal by calculating each pixel norm over the 3 colour channels and zeroing out small-norm pixels according to some threshold (the percentile of all pixel norms in x).	Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an \mathbf{x^{*}} that contains somewhat small, somewhat smooth values. However, \mathbf{x^{*}} will still tend to contain non-zero pixel values everywhere. Even if some pixels in \mathbf{x^{*}} show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in \mathbf{x^{*}} will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r_{\theta}(\mathbf{x}) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \theta_{\mathrm{n\_pct}}, is specified as a percentile of all pixel norms in \mathbf{x}.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The paper reaches this goal by calculating each pixel norm over the 3 colour channels and zeroing out small-norm pixels randomly.	Clipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an \mathbf{x^{*}} that contains somewhat small, somewhat smooth values. However, \mathbf{x^{*}} will still tend to contain non-zero pixel values everywhere. Even if some pixels in \mathbf{x^{*}} show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in \mathbf{x^{*}} will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit’s activation. We wish to bias the search away from such behavior and instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r_{\theta}(\mathbf{x}) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \theta_{\mathrm{n\_pct}}, is specified as a percentile of all pixel norms in \mathbf{x}.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Correct	Correct
The authors use 300 sets of possible hyperparameter combinations for the random hyperparameter search, then choose four of them that complement each other well.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors use 100 sets of possible hyperparameter combinations for the random hyperparameter search, then choose four of them that complement each other well.	If the above regularization methods are applied individually, they aresomewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter.However, preliminary experiments uncovered that their combinedeffect produces better visualizations. To pick a reasonable set ofhyperparameters for all methods at once, we ran a randomhyperparameter search of 300 possible combinations and settled on fourthat complement each other well. The four selected combinations arelisted in Table 1 and optimized images using each are shown for the “Gorilla” class output unit in Figure 4. Of the four, some show highfrequency information, others low frequency; some contain densepixel data, and others contain only sparse outlines of importantregions.We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition canbe gleaned by considering all four at once.Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Generative structure is how the data is distributed inside the space where it lives, for example when learning to detect jaguar class, parameters encode not only the jaguar’s spots(Only to distinguish it through a rare property), but to some extent also its four legs(to learn the pattern with which the whole creature can be found).  So, discriminative parameters also contain significant “generative” structure.	However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section S1 for one hypothesis of why a strong p(x) model is needed). With the careful design or learning of a p(x) model that biases toward realism,one may be able to harnessthe large number of parameters present in a discriminately learned p(y|x) modelto generate realistic images by enforcing probability under both models simultaneously.Even with the simple, hand-coded p(x) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure 4). This implies that the discriminative parameters also contain significant “generative” structure from thetraining dataset; that is, the parameters encodenot only the jaguar’s spots, but to some extent also its four legs.With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al. (2015) shows some interesting results in this direction.While the images generated in this paper are far from being photo-realistic, they do suggest thattransferring discriminatively trained parameters to generative models — opposite the direction of the usual unsupervised pretraining approach — may be a fruitful area for further investigation.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Generative structure is how the data is distributed inside the space where it lives, for example when learning to detect jaguar class, parameters encode only the jaguar’s spots(Only to distinguish it through a rare property), without encoding its legs.  So, discriminative parameters also contain significant “generative” structure.	However, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section S1 for one hypothesis of why a strong p(x) model is needed). With the careful design or learning of a p(x) model that biases toward realism,one may be able to harnessthe large number of parameters present in a discriminately learned p(y|x) modelto generate realistic images by enforcing probability under both models simultaneously.Even with the simple, hand-coded p(x) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure 4). This implies that the discriminative parameters also contain significant “generative” structure from thetraining dataset; that is, the parameters encodenot only the jaguar’s spots, but to some extent also its four legs.With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al. (2015) shows some interesting results in this direction.While the images generated in this paper are far from being photo-realistic, they do suggest thattransferring discriminatively trained parameters to generative models — opposite the direction of the usual unsupervised pretraining approach — may be a fruitful area for further investigation.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25.  Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on COVID-19 data.  There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval.  Therefore, the performance of the retriever in out-of-domain may be worse.	However, as shown in Thakur et al. (2021b), dense retrieval methods require large amounts of training data to work well.333For reference, the popular MS MARCO dataset (Nguyen et al., 2016) has about 500k training instances; the Natural Questions dataset (Kwiatkowski et al., 2019) has more than 100k training instances.  Most importantly, dense retrieval methods are extremely sensitive to domain shifts: Models trained on MS MARCO perform rather poorly for questions for COVID-19 scientific literature (Wang et al., 2020; Voorhees et al., 2021). The MS MARCO dataset was created before COVID-19, hence, it does not include any COVID-19 related topics and models did not learn how to represent this topic well in a vector space.We use the MS MARCO passage ranking dataset Nguyen et al. (2016) as the data from the source domain. It has 8.8M passages and 532.8K query-passage pairs labeled as relevant in the training set. As Table 1 shows, a state-of-the-art dense retrieval model, achieving an MRR@10 of 33.2 points on the MS MARCO passage ranking dataset, performs poorly on the six selected domain-specific retrieval datasets when compared to simple BM25 lexical search.So far, ICT and CD have only been studied on in-domain performance, i.e. a large in-domain labeled dataset is available which is used for subsequent supervised fine-tuning. SimCSE, CT, and TSDAE have been only studied for unsupervised sentence embedding learning. As our results show in Appendix E, they do not work at all for purely unsupervised dense retrieval.If these pre-training approaches can be used for unsupervised domain adaptation for dense retrieval was so far unclear. In this work, we transfer the setup from Wang et al. (2021) to dense retrieval and first pre-train on the target corpus, followed by supervised training on labeled data from MS MARCO Nguyen et al. (2016). Performance is then measured on the target corpus.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
It is said that when evaluating a retriever trained on a source domain in an out-of-domain setting, the performance is obtained lower than BM25.  Also, dense retrievers are said to be sensitive to domain shift and models that perform well on MS MARCO do not perform well on Sentimental-analysis data.  There have been many studies on unsupervised sentence embedding learning, but it is said that they do not work well in unsupervised dense retrieval.  Therefore, the performance of the retriever in out-of-domain may be worse.	However, as shown in Thakur et al. (2021b), dense retrieval methods require large amounts of training data to work well.333For reference, the popular MS MARCO dataset (Nguyen et al., 2016) has about 500k training instances; the Natural Questions dataset (Kwiatkowski et al., 2019) has more than 100k training instances.  Most importantly, dense retrieval methods are extremely sensitive to domain shifts: Models trained on MS MARCO perform rather poorly for questions for COVID-19 scientific literature (Wang et al., 2020; Voorhees et al., 2021). The MS MARCO dataset was created before COVID-19, hence, it does not include any COVID-19 related topics and models did not learn how to represent this topic well in a vector space.We use the MS MARCO passage ranking dataset Nguyen et al. (2016) as the data from the source domain. It has 8.8M passages and 532.8K query-passage pairs labeled as relevant in the training set. As Table 1 shows, a state-of-the-art dense retrieval model, achieving an MRR@10 of 33.2 points on the MS MARCO passage ranking dataset, performs poorly on the six selected domain-specific retrieval datasets when compared to simple BM25 lexical search.So far, ICT and CD have only been studied on in-domain performance, i.e. a large in-domain labeled dataset is available which is used for subsequent supervised fine-tuning. SimCSE, CT, and TSDAE have been only studied for unsupervised sentence embedding learning. As our results show in Appendix E, they do not work at all for purely unsupervised dense retrieval.If these pre-training approaches can be used for unsupervised domain adaptation for dense retrieval was so far unclear. In this work, we transfer the setup from Wang et al. (2021) to dense retrieval and first pre-train on the target corpus, followed by supervised training on labeled data from MS MARCO Nguyen et al. (2016). Performance is then measured on the target corpus.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Using lexical matching makes it difficult to identify synonyms or to distinguish between ambiguous words.	Information Retrieval (IR) is a central component of many natural language applications. Traditionally, lexical methods (Robertson et al., 1994) have been used to search through text content. However, these methods suffer from the lexical gap (Berger et al., 2000) and are not able to recognize synonyms and distinguish between ambiguous words.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Using lexical matching makes it difficult to identify noun phrases or to distinguish between entities.	Information Retrieval (IR) is a central component of many natural language applications. Traditionally, lexical methods (Robertson et al., 1994) have been used to search through text content. However, these methods suffer from the lexical gap (Berger et al., 2000) and are not able to recognize synonyms and distinguish between ambiguous words.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Correct	Incorrect
During indexing, they use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory.  Across all experiments, only one GPU is dedicated per query for retrieval (i. , for methods with neural computations) but we use up to all four GPUs during indexing.	To evaluate the latency of neural re-ranking models in §4.2, we use a single Tesla V100 GPU that has 32 GiBs of memory on a server with two Intel Xeon Gold 6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469 GiBs of RAM. For the mostly CPU-based retrieval experiments in §4.3 and the indexing experiments in §4.5, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
During indexing, they use another server with the same CPU and system memory specifications but which has two Titan V GPUs attached, each with 8 GiBs of memory.  Across all experiments, only one GPU is dedicated per query for retrieval (i. , for methods with neural computations) but we use up to all four GPUs during indexing.	To evaluate the latency of neural re-ranking models in §4.2, we use a single Tesla V100 GPU that has 32 GiBs of memory on a server with two Intel Xeon Gold 6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469 GiBs of RAM. For the mostly CPU-based retrieval experiments in §4.3 and the indexing experiments in §4.5, we use another server with the same CPU and system memory specifications but which has four Titan V GPUs attached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
They used (MRR@10) for measuring efficiency and effectiveness .	Diving deeper into the quality–cost tradeoff between BERT and ColBERT, Figure 4 demonstrates the relationships between FLOPs and effectiveness (MRR@10) as a function of the re-ranking depth k when re-ranking the top-k results by BM25, comparing ColBERT and BERT{}_{\textnormal{base}} (our training). We conduct this experiment on MS MARCO (Dev). We note here that as the official top-1000 ranking does not provide the BM25 order (and also lacks documents beyond the top-1000 per query), the models in this experiment re-rank the Anserini (Yanget al., 2018) toolkit’s BM25 output. Consequently, both MRR@10 values at k=1000 are slightly higher from those reported in Table 1.	Correct		Correct	Correct	Correct	Correct	Correct	Incorrect
They used F1-score for measuring efficiency and effectiveness .	Diving deeper into the quality–cost tradeoff between BERT and ColBERT, Figure 4 demonstrates the relationships between FLOPs and effectiveness (MRR@10) as a function of the re-ranking depth k when re-ranking the top-k results by BM25, comparing ColBERT and BERT{}_{\textnormal{base}} (our training). We conduct this experiment on MS MARCO (Dev). We note here that as the official top-1000 ranking does not provide the BM25 order (and also lacks documents beyond the top-1000 per query), the models in this experiment re-rank the Anserini (Yanget al., 2018) toolkit’s BM25 output. Consequently, both MRR@10 values at k=1000 are slightly higher from those reported in Table 1.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Unrelated	Incorrect	Incorrect
In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall.  Moving beyond MRR@10, large gains in Recall@k for k equals to 50, 200, and 1000.  For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.	Shifting our attention to ColBERT’s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-to-end models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, we also see large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
In fact, using ColBERT in the end-to-end setup is not superior in terms of MRR@10 to re-ranking with the same model due to the decreased recall.  Moving beyond MRR@10, large gains in Recall@k for k equals to 50, 200, and 1000.  For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of re-ranking (instead of just end-to-end retrieval) with ColBERT.	Shifting our attention to ColBERT’s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-to-end models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, we also see large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the official BM25’s Recall@1000 and even all but docTTTTTquery’s Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
In contrast with this trend, ColBERT (which employs late interaction over BERT performs no worse than the original adaptation of BERT for ranking and is only marginally less effective than BERT and our training of BERT.  While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT, in particular, by over 170\times in latency and 13,900\times in FLOPs.	In contrast with this trend, ColBERT (which employs late interaction over BERT{}_{\textnormal{base}}) performs no worse than the original adaptation of BERT{}_{\textnormal{base}} for ranking by Nogueira and Cho (Nogueira and Cho, 2019; Nogueiraet al., 2019b) and is only marginally less effective than BERT{}_{\textnormal{large}} and our training of BERT{}_{\textnormal{base}} (described above). While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT{}_{\textnormal{base}}, in particular, by over 170\times in latency and 13,900\times in FLOPs. This highlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM like BERT. While ColBERT’s re-ranking latency is slightly higher than the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this difference is explained by the time it takes to gather, stack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only 13 milliseconds of its total execution time. We note that ColBERT’s latency and FLOPs can be considerably reduced by padding queries to a shorter length, using smaller vector dimensions (the MRR@10 of which is tested in §4.5), employing quantization of the document vectors, and storing the embeddings on GPU if sufficient memory exists. We leave these directions for future work.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
In contrast with this trend, ColBERT (which employs late interaction over BERT performs no worse than the original adaptation of BERT for ranking and is only marginally less effective than BERT and our training of BERT.  While highly competitive in effectiveness, ColBERT is orders of magnitude more expensive than BERT, in particular, by over 170\times in latency and 13,900\times in FLOPs.	In contrast with this trend, ColBERT (which employs late interaction over BERT{}_{\textnormal{base}}) performs no worse than the original adaptation of BERT{}_{\textnormal{base}} for ranking by Nogueira and Cho (Nogueira and Cho, 2019; Nogueiraet al., 2019b) and is only marginally less effective than BERT{}_{\textnormal{large}} and our training of BERT{}_{\textnormal{base}} (described above). While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERT{}_{\textnormal{base}}, in particular, by over 170\times in latency and 13,900\times in FLOPs. This highlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM like BERT. While ColBERT’s re-ranking latency is slightly higher than the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this difference is explained by the time it takes to gather, stack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only 13 milliseconds of its total execution time. We note that ColBERT’s latency and FLOPs can be considerably reduced by padding queries to a shorter length, using smaller vector dimensions (the MRR@10 of which is tested in §4.5), employing quantization of the document vectors, and storing the embeddings on GPU if sufficient memory exists. We leave these directions for future work.	Incorrect	change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system.  Hence, a zero-shot scenario in this context refer to cases where relevance annotations are not available and  does not refer to unavailability of query set.	However, creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system. So far, it is unclear how well existing trained neural models will perform for other text domains or textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse embeddings vs. dense embeddings, generalize to out-of-distribution data.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system.  Hence, a zero-shot scenario in this context refer to cases where query set are not available.	However, creating a large training corpus is often time-consuming and expensive and hence many retrieval systems are applied in a zero-shot setup, with no available training data to train the system. So far, it is unclear how well existing trained neural models will perform for other text domains or textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse embeddings vs. dense embeddings, generalize to out-of-distribution data.	Incorrect	change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6. 4% and 2. 8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems.  In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14. 4% and 31.	The results reveal large differences between approaches: Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators.Next, we manually added for all systems, the missing annotation (or holes) following the original annotation guidelines. During annotation, we were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total, we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all systems with this additional annotations.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 64% and 28%, indicating that the annotation pool contained the top-hits from lexical retrieval systems.  In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14. 4% and 31.	The results reveal large differences between approaches: Lexical approaches like BM25 and docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits found by these systems have not been judged by annotators.Next, we manually added for all systems, the missing annotation (or holes) following the original annotation guidelines. During annotation, we were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total, we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all systems with this additional annotations.	Incorrect	change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
According to the authors, K as a hyperparameter is only best believed as the number of user intents and does not necessarily equal the actual number of user intents.	The main goal of next item prediction task is to optimizeEq. (1).Assume that there are also K different user intents (e.g., purchasing holiday gifts, preparing for fishing activity, etc.)in a recommender system that formsthe intent variable c=\left\{c_{i}\right\}_{i=1}^{K}, thenthe probability of a user interacting with a certainitem can be rewritten as follows:(7)\begin{split}P_{\theta}(s^{u})=\mathbb{E}_{(c)}\left[P_{\theta}(s^{u},c)\right].\end{split}However, users intents are latent by definition.Because of the missing observation of variable c,we are in a ‘chicken-and-eggs’ situation thatwithout c, we cannot estimate parameter \theta,and without \theta we cannot inferwhat the value of c might be.The larger of the intent class number K means users can havemore diverseintentions.The larger value of the strength of SeqCL objective \betameans the ICL task contributes more tothe final model.The results on Yelp is shown in Figure 5.We find that: (1)ICLRec reaches itsbest performance when increasing K to 512,and then it starts to deteriorateas K become larger.When K is very small,the number of users undereach intent prototype can potentially be large.As a result, false-positive samples(i.e., users that actually have different intentsare considered as having the same intent erroneously)are introduced to the contrastive SSL,thus affecting learning.On the other hand, when K is too large,the number of users undereach intent prototype is small,the introduced false-negative sampleswill also impair contrastive SSL.In Yelp, 512 user intents summarizeusers’ distinct behaviors best.(2) A ‘sweet-spot’ of \lambda=0.5 canalso be found.It indicatesthat the ICL task can benefitthe recommendation predictionas an auxiliary task.The impact of the batch size and \beta are provided in Appendix D.Recently, many approaches have been proposed to studyusers’ intents forimproving recommendations (Wanget al., 2019b; Cenet al., 2020; Li et al., 2019; Liet al., 2021b).MCPRN (Wanget al., 2019b)designs mixture-channel purposerouting networks to adaptivelylearnusers’ different purchase purposesof each itemunder different channels (sub-sequences) for session-based recommendation.MITGNN(Liuet al., 2020a)proposes amulti-intenttranslation graph neural networkto mine users’ multiple intentsby considering the correlations of the intents.ICM-SR (Panet al., 2020)designs anintent-guided neighbor detectorto retrieve correctneighbor sessionsfor neighbor representation.Different from session-based recommendation,another line of worksfocus on modeling the sequentialdynamics of users’ interaction behaviorsin a longer time span.DSSRec (Maet al., 2020)proposes a seq2seq trainingstrategy using multiple future interactions as supervision and introducing an intent variable from her historical and future behavior sequences.The intent variable is used to capture mutual information between an individual user’s historical and future behavior sequences.Two users of similar intentsmight be far away in representation space.Unlike this work, our intent variable is learned over all users’ sequences and is used to maximize mutual information across different users with similar learned intents.ASLI (Tanjim et al., 2020)captures intentvia a temporal convolutionalnetwork with side information (e.g., user action types such asclick, add-to-favorite, etc.),and then use the learnedintents toguide SR model to predictthe next item.Instead, our methodcan learn users’ intentsbased on user interactiondata only.	Correct		Incorrect	Correct	Incorrect	Incorrect	Incorrect	Correct
According to the authors, K as a hyperparameter actually equals the actual number of user intents.	The main goal of next item prediction task is to optimizeEq. (1).Assume that there are also K different user intents (e.g., purchasing holiday gifts, preparing for fishing activity, etc.)in a recommender system that formsthe intent variable c=\left\{c_{i}\right\}_{i=1}^{K}, thenthe probability of a user interacting with a certainitem can be rewritten as follows:(7)\begin{split}P_{\theta}(s^{u})=\mathbb{E}_{(c)}\left[P_{\theta}(s^{u},c)\right].\end{split}However, users intents are latent by definition.Because of the missing observation of variable c,we are in a ‘chicken-and-eggs’ situation thatwithout c, we cannot estimate parameter \theta,and without \theta we cannot inferwhat the value of c might be.The larger of the intent class number K means users can havemore diverseintentions.The larger value of the strength of SeqCL objective \betameans the ICL task contributes more tothe final model.The results on Yelp is shown in Figure 5.We find that: (1)ICLRec reaches itsbest performance when increasing K to 512,and then it starts to deteriorateas K become larger.When K is very small,the number of users undereach intent prototype can potentially be large.As a result, false-positive samples(i.e., users that actually have different intentsare considered as having the same intent erroneously)are introduced to the contrastive SSL,thus affecting learning.On the other hand, when K is too large,the number of users undereach intent prototype is small,the introduced false-negative sampleswill also impair contrastive SSL.In Yelp, 512 user intents summarizeusers’ distinct behaviors best.(2) A ‘sweet-spot’ of \lambda=0.5 canalso be found.It indicatesthat the ICL task can benefitthe recommendation predictionas an auxiliary task.The impact of the batch size and \beta are provided in Appendix D.Recently, many approaches have been proposed to studyusers’ intents forimproving recommendations (Wanget al., 2019b; Cenet al., 2020; Li et al., 2019; Liet al., 2021b).MCPRN (Wanget al., 2019b)designs mixture-channel purposerouting networks to adaptivelylearnusers’ different purchase purposesof each itemunder different channels (sub-sequences) for session-based recommendation.MITGNN(Liuet al., 2020a)proposes amulti-intenttranslation graph neural networkto mine users’ multiple intentsby considering the correlations of the intents.ICM-SR (Panet al., 2020)designs anintent-guided neighbor detectorto retrieve correctneighbor sessionsfor neighbor representation.Different from session-based recommendation,another line of worksfocus on modeling the sequentialdynamics of users’ interaction behaviorsin a longer time span.DSSRec (Maet al., 2020)proposes a seq2seq trainingstrategy using multiple future interactions as supervision and introducing an intent variable from her historical and future behavior sequences.The intent variable is used to capture mutual information between an individual user’s historical and future behavior sequences.Two users of similar intentsmight be far away in representation space.Unlike this work, our intent variable is learned over all users’ sequences and is used to maximize mutual information across different users with similar learned intents.ASLI (Tanjim et al., 2020)captures intentvia a temporal convolutionalnetwork with side information (e.g., user action types such asclick, add-to-favorite, etc.),and then use the learnedintents toguide SR model to predictthe next item.Instead, our methodcan learn users’ intentsbased on user interactiondata only.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors propose the EM framework as it guarantees convergence	Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq. (7) via EMis to start with an initial guessof the model parameter \thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq. (7) w.r.t theparameter \theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore.To discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns users’ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent users’ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve model’s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.In this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The authors didn't propose the EM framework as it causes convergence	Later, we will show that a generalized Expectation-Maximization frameworkprovides a direction to address above problemwith a convergence guarantee.The basic idea of optimizing Eq. (7) via EMis to start with an initial guessof the model parameter \thetaand estimate the expected valuesof the missing variable c, i.e., the E-step.And once we have the values of c,we can maximize the Eq. (7) w.r.t theparameter \theta, i.e., the M step.We can repeat this iterative process until the likelihood cannot increase anymore.To discover the benefits oflatent intentsand address challenges,we propose theIntent Contrastive Learning (ICL),a general learning paradigm thatleverages the latent intentfactor into SR.It learns users’ intentdistributionsfrom all user behavior sequencesvia clustering.And it leveragesthe learnt intentsinto the SR modelvia a new contrastive SSL,whichmaximizes the agreementbetween a view of sequenceand its corresponding intent.The intent representation learning moduleand the contrastive SSL module are mutually reinforcedto train a more expressivesequence encoder.We tackle the challenge of intentmining problem byintroducing alatent variable to represent users’ intentsand learn them alternatelyalong with the SR model optimization throughan expectation-maximization (EM) frameworkto ensure convergence.We suggest fusing learnt intent informationinto SR via the proposed contrastive SSL,as it can improve model’s performance as wellas robustness.Extensive experiments conducted on four real-world datasetsfurther verify the effectiveness of the proposed learning paradigm,which improves performance and robustness,even when recommender systemsfaceheavy data sparsity issues.In this work,we propose a new learning paradigm ICL that canmodel latent intent factorsfrom user interactionsand fuse them into a sequential recommendationmodel via a newcontrastive SSL objective.ICL is formulated withinan EM framework, which guaranteesconvergence. Detailed analyses showthe superiority of ICL andexperiments conducted onfour datasets further demonstratethe effectiveness of the proposed method.	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
BUIR requires positive user-item pairs instead of negative sampling for training.	For all the datasets, BUIRid shows the substantially higher performance than the discriminative methods taking only user-id/item-id (i.e., BPR, NeuMF, CML, and SML).In particular, the sparser the training set becomes, the larger the performance improvement of BUIRid is achieved over the best baseline (denoted by Improvid).It is obvious that BUIRid is more robust to the extreme sparsity compared to the other baselines that are more likely to explicitly use “positive but unobserved” interactions as negative interactions when positive user-item interactions are more rarely observed.BUIRid is not affected by such inconsistent supervision from uncertain negative interactions because it directly optimizes the representations of users and items by using only positive interactions.As a solution to the aforementioned limitations, this paper proposes a novel OCCF framework, named as BUIR, which does not require the negative sampling at all for training the model.The main idea is, given a positive user-item interaction (u, v), to make representations for u and v similar to each other, in order to encode the preference information into the representations.However, a naive end-to-end learning framework that guides positive user-item pairs to be similar to each other without any negative supervision can easily converge to a collapsed solution – the encoder network outputs the same representations for all the users and items.First of all, the BPR framework that optimizes the cross-prediction score, q\left(f(u)\right)^{\top}f(v)+f(u)^{\top}q\left(f(v)\right), is not as effective as ours;it is even worse compared to the conventional BPR, which optimizes the inner-product score f(u)^{\top}f(v).This implies that the performance improvement of BUIR is mainly caused by our learning framework rather than its score modeling based on the predictor.In addition, even without the stochastic augmentation, the neighbor-based encoder (i.e., LGCN) based on the BUIR framework beats LGCN based on the BPR framework, which demonstrates that BUIR successfully addresses the issue of incorrect negative sampling.Lastly, our framework with the stochastic neighbor augmentation further improves the performance by taking benefits from various views of the positive user-item interactions for the optimization.This paper proposes a novel framework for learning the representations of users and items, termed as BUIR, to address the main challenges of the OCCF problem: the implicit assumption about negative interactions, and high sparsity of observed (positively-labeled) interactions.First, BUIR directly bootstraps the representations of users and items by minimizing their cross-prediction error.This makes BUIR use only partially-observed positive interactions for training the model, and accordingly, it can eliminate the need for negative sampling.In addition, BUIR is able to learn the augmented views of each positive interaction obtained from the neighborhood information, which further relieves the data sparsity issue of the OCCF problem.Through the extensive comparison with a wide range of OCCF methods, we demonstrate that BUIR consistently outperforms all the other baselines in terms of top-K recommendation.In particular, the effectiveness of BUIR becomes more significant for much sparse datasets in which the positively-labeled interactions are not enough to optimize the model as well as the assumption about negative interactions becomes less valid.Based on its great compatibility with existing user/item encoder networks, we expect that our BUIR framework can be a major solution for the OCCF problem, replacing the conventional BPR framework.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
BUIR requires negative sampling instead of positive user-item pairs for training.	For all the datasets, BUIRid shows the substantially higher performance than the discriminative methods taking only user-id/item-id (i.e., BPR, NeuMF, CML, and SML).In particular, the sparser the training set becomes, the larger the performance improvement of BUIRid is achieved over the best baseline (denoted by Improvid).It is obvious that BUIRid is more robust to the extreme sparsity compared to the other baselines that are more likely to explicitly use “positive but unobserved” interactions as negative interactions when positive user-item interactions are more rarely observed.BUIRid is not affected by such inconsistent supervision from uncertain negative interactions because it directly optimizes the representations of users and items by using only positive interactions.As a solution to the aforementioned limitations, this paper proposes a novel OCCF framework, named as BUIR, which does not require the negative sampling at all for training the model.The main idea is, given a positive user-item interaction (u, v), to make representations for u and v similar to each other, in order to encode the preference information into the representations.However, a naive end-to-end learning framework that guides positive user-item pairs to be similar to each other without any negative supervision can easily converge to a collapsed solution – the encoder network outputs the same representations for all the users and items.First of all, the BPR framework that optimizes the cross-prediction score, q\left(f(u)\right)^{\top}f(v)+f(u)^{\top}q\left(f(v)\right), is not as effective as ours;it is even worse compared to the conventional BPR, which optimizes the inner-product score f(u)^{\top}f(v).This implies that the performance improvement of BUIR is mainly caused by our learning framework rather than its score modeling based on the predictor.In addition, even without the stochastic augmentation, the neighbor-based encoder (i.e., LGCN) based on the BUIR framework beats LGCN based on the BPR framework, which demonstrates that BUIR successfully addresses the issue of incorrect negative sampling.Lastly, our framework with the stochastic neighbor augmentation further improves the performance by taking benefits from various views of the positive user-item interactions for the optimization.This paper proposes a novel framework for learning the representations of users and items, termed as BUIR, to address the main challenges of the OCCF problem: the implicit assumption about negative interactions, and high sparsity of observed (positively-labeled) interactions.First, BUIR directly bootstraps the representations of users and items by minimizing their cross-prediction error.This makes BUIR use only partially-observed positive interactions for training the model, and accordingly, it can eliminate the need for negative sampling.In addition, BUIR is able to learn the augmented views of each positive interaction obtained from the neighborhood information, which further relieves the data sparsity issue of the OCCF problem.Through the extensive comparison with a wide range of OCCF methods, we demonstrate that BUIR consistently outperforms all the other baselines in terms of top-K recommendation.In particular, the effectiveness of BUIR becomes more significant for much sparse datasets in which the positively-labeled interactions are not enough to optimize the model as well as the assumption about negative interactions becomes less valid.Based on its great compatibility with existing user/item encoder networks, we expect that our BUIR framework can be a major solution for the OCCF problem, replacing the conventional BPR framework.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Unrelated	Incorrect	Correct
Approximating the online encoder keep the target encoder from converging to the collapsed solution.	Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The online encoder doesn't keep the target encoder from converging to the collapsed solution.	Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Correct	Incorrect
Online encoders prevent models from collapsing into trivial solutions without explicitly using negative interactions for optimization.	We argue that the above collapsed solution is incurred by the si\x02multaneous optimization of 𝑢 and 𝑣 within the end-to-end learning framework of a single encoder. Hence, we instead adopt the student\x02teacher-like network [6, 29] in which only the student’s output 𝑢 (and 𝑣) is optimized to predict the target 𝑣 (and 𝑢) presented by the teacher. Specifically, BUIR directly bootstraps1 the representations of users and items by employing two distinct encoder networks, referred to as online encoder and target encoder. The high-level idea is training only the online encoder for the prediction task between 𝑢 and 𝑣, where the target for its prediction is provided by the target encoder. That is, the online encoder is optimized so that its user (and item) vectors get closer to the item (and user) vectors com\x02puted by the target encoder. At the same time, the target encoder is updated based on momentum-based moving average [6, 8, 29] to slowly approximate the online encoder, which encourages to pro\x02vide enhanced representations as the target for the online encoder. By doing so, the online encoder can capture the positive relation\x02ship between 𝑢 and 𝑣 into the representations, while preventing the model from collapsing to the trivial solution without explicitly using any negative interactions for the optimization.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Online encoders allow models collapsing into trivial solutions without explicitly using negative interactions for optimization.	We argue that the above collapsed solution is incurred by the si\x02multaneous optimization of 𝑢 and 𝑣 within the end-to-end learning framework of a single encoder. Hence, we instead adopt the student\x02teacher-like network [6, 29] in which only the student’s output 𝑢 (and 𝑣) is optimized to predict the target 𝑣 (and 𝑢) presented by the teacher. Specifically, BUIR directly bootstraps1 the representations of users and items by employing two distinct encoder networks, referred to as online encoder and target encoder. The high-level idea is training only the online encoder for the prediction task between 𝑢 and 𝑣, where the target for its prediction is provided by the target encoder. That is, the online encoder is optimized so that its user (and item) vectors get closer to the item (and user) vectors com\x02puted by the target encoder. At the same time, the target encoder is updated based on momentum-based moving average [6, 8, 29] to slowly approximate the online encoder, which encourages to pro\x02vide enhanced representations as the target for the online encoder. By doing so, the online encoder can capture the positive relation\x02ship between 𝑢 and 𝑣 into the representations, while preventing the model from collapsing to the trivial solution without explicitly using any negative interactions for the optimization.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Using predictor can optimize the representation without any negative sample.	Existing discriminative OCCF methods (Rendle et al., 2009; Hsieh et al., 2017) have tried to optimize the latent space where the user-item interactions are directly encoded into their inner product (or Euclidean distance).On the contrary, BUIR additionally uses the predictor to model their interaction, which results in the capability of encoding the high-level relationship between users and items into the representations.In conclusion, with the help of the predictor, BUIR accurately computes the user-item interaction scores as well as optimizes the representation without explicitly using negative samples.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Using predictor can optimize the representation with only a few negative samples.	Existing discriminative OCCF methods (Rendle et al., 2009; Hsieh et al., 2017) have tried to optimize the latent space where the user-item interactions are directly encoded into their inner product (or Euclidean distance).On the contrary, BUIR additionally uses the predictor to model their interaction, which results in the capability of encoding the high-level relationship between users and items into the representations.In conclusion, with the help of the predictor, BUIR accurately computes the user-item interaction scores as well as optimizes the representation without explicitly using negative samples.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
Stochastic means it use random neighborhood information of each user and item during data augmentation.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Furthermore, we introduce a stochastic data augmentation technique to relieve the data sparsity problem in our framework.Motivated by the recent success of self-supervised learning in various domains (Chenet al., 2020; Devlinet al., 2019), we exploit augmented views of an input interaction, which are generated based on the neighborhood information of each user and item (i.e., the set of the items interacted with a user, and the users interacted with an item).The stochastic augmentation is applied to positive user-item pairs when they are passed to the encoder, so as to produce the different views of the pairs.To be precise, by making our encoder use a random subset of a user’s (and item’s) neighbors for the input features, it produces a similar effect to increasing the number of positive pairs from the data itself without any human intervention.In the end, BUIR is allowed to learn various views of each positive user-item pair.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Stochastic means it use pre-selected neighborhood information of each user and item during data augmentation.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Furthermore, we introduce a stochastic data augmentation technique to relieve the data sparsity problem in our framework.Motivated by the recent success of self-supervised learning in various domains (Chenet al., 2020; Devlinet al., 2019), we exploit augmented views of an input interaction, which are generated based on the neighborhood information of each user and item (i.e., the set of the items interacted with a user, and the users interacted with an item).The stochastic augmentation is applied to positive user-item pairs when they are passed to the encoder, so as to produce the different views of the pairs.To be precise, by making our encoder use a random subset of a user’s (and item’s) neighbors for the input features, it produces a similar effect to increasing the number of positive pairs from the data itself without any human intervention.In the end, BUIR is allowed to learn various views of each positive user-item pair.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Model gets best performance when the value of parameter tau is larger or equal than 0. 9 and smaller than 1.	Implementation Details.  We implement the proposed framework and all the baselines by using PyTorch, and use the Adam optimizer to train them.For BUIR, we fix the momentum coefficient \tau to 0.995, and adopt a single linear layer for the predictor q_{\theta}.666We empirically found that these hyperparameters hardly affect the final performance of BUIR, and the sensitivity analysis on the parameters is provided in Section 4.6.The augmentation function \psi simply uses a uniform distribution for drawing a drop probability p\sim\mathcal{U}(0,1), where each user’s (item’s) neighbor is independently deleted with the probability p.Figure 6 clearly shows that the performance is hardly affected by \tau in the range of [0.9, 1.0).In other words, any values of \tau larger than 0.9 allow the target encoder to successfully provide the target representations to the online encoder, by slowly approximating the online encoder;on the contrary, BUIR cannot learn the effective representations at all in case that the target encoder is fixed (i.e., \tau=1).This observation is consistent with previous work on momentum-based moving average (Tarvainen andValpola, 2017; Heet al., 2020b; Grill et al., 2020) that showed all values of \tau between 0.9 and 0.999 can yield the best performance.Furthermore, BUIR performs the best with a single-layer predictor, because a multi-layer predictor makes it difficult to optimize the relationship between outputs of the two encoder networks.In conclusion, BUIR is more powerful even with fewer hyperparameters, compared to existing OCCF methods that include a variety of regularization terms or modeling components.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Model gets best performance when the value of parameter tau is larger or equal than 0. 5 and smaller than 0.9.	Implementation Details.  We implement the proposed framework and all the baselines by using PyTorch, and use the Adam optimizer to train them.For BUIR, we fix the momentum coefficient \tau to 0.995, and adopt a single linear layer for the predictor q_{\theta}.666We empirically found that these hyperparameters hardly affect the final performance of BUIR, and the sensitivity analysis on the parameters is provided in Section 4.6.The augmentation function \psi simply uses a uniform distribution for drawing a drop probability p\sim\mathcal{U}(0,1), where each user’s (item’s) neighbor is independently deleted with the probability p.Figure 6 clearly shows that the performance is hardly affected by \tau in the range of [0.9, 1.0).In other words, any values of \tau larger than 0.9 allow the target encoder to successfully provide the target representations to the online encoder, by slowly approximating the online encoder;on the contrary, BUIR cannot learn the effective representations at all in case that the target encoder is fixed (i.e., \tau=1).This observation is consistent with previous work on momentum-based moving average (Tarvainen andValpola, 2017; Heet al., 2020b; Grill et al., 2020) that showed all values of \tau between 0.9 and 0.999 can yield the best performance.Furthermore, BUIR performs the best with a single-layer predictor, because a multi-layer predictor makes it difficult to optimize the relationship between outputs of the two encoder networks.In conclusion, BUIR is more powerful even with fewer hyperparameters, compared to existing OCCF methods that include a variety of regularization terms or modeling components.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The online encoder is updated to minimize the error between the output and the target and updated by the gradients back-propagated from the loss, but target network is updated based on the momentum update and updated as the moving average of the online encoder .	BUIR makes use of two distinct encoder networks that have the same structure: online encoder f_{\theta} and target encoder f_{\xi}.They are parameterized by \theta and \xi, respectively.The key idea of BUIR is to train the online encoder by using outputs of the target encoder as its target, while gradually improving the target encoder as well.The main difference of BUIR from existing end-to-end learning frameworks is that f_{\theta} and f_{\xi} are updated in different ways.The online encoder is trained to minimize the error between its output and the target, whereas the target network is slowly updated based on the momentum update (Heet al., 2020b) so as to keep its output consistent.To sum up, the parameters of the online encoder and target encoder are optimized by(4)\begin{split}\theta&\leftarrow\theta-\eta\cdot\nabla_{\theta}\mathcal{L}_{\theta,\xi}\\\xi&\leftarrow\tau\cdot\xi+(1-\tau)\cdot\theta.\end{split}\eta is the learning rate for stochastic optimization, and \tau\in[0,1] is a momentum coefficient (also called as target decay) for momentum-based moving average.The online encoder f_{\theta} (and the predictor q_{\theta}) is effectively optimized by the gradients back-propagated from the loss (Equation (3)), while the target encoder f_{\xi} is updated as the moving average of the online encoder.By taking a large value of \tau, the target encoder slowly approximates the online encoder.This momentum-based update makes \xi evolve more slowly than \theta, which enables to bootstrap the representations by providing enhanced but consistent targets to the online encoders (Heet al., 2020b; Grill et al., 2020).Figure 1 illustrates the overall framework of BUIR with the simple one-hot encoders.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.Similarly to Section 3.2, the online encoder is trained by minimizing \mathcal{L}_{\theta,\xi}(\psi(u,\mathcal{V}_{u}),\psi(v,\mathcal{U}_{v})), and the target encoder is slowly updated by the momentum mechanism.After the optimization is finished, the interaction score is inferred by f_{\theta}(u,\mathcal{V}_{u}) and f_{\theta}(v,\mathcal{U}_{v}) (Equation (5)).Figure 2 shows an example of our data augmentation which injects a certain level of perturbations to the neighbors.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The online encoder is updated to minimize the error between the output and the target and updated by user-indicated gradients, but target network is updated based on the momentum update and updated as the moving average of the online encoder .	BUIR makes use of two distinct encoder networks that have the same structure: online encoder f_{\theta} and target encoder f_{\xi}.They are parameterized by \theta and \xi, respectively.The key idea of BUIR is to train the online encoder by using outputs of the target encoder as its target, while gradually improving the target encoder as well.The main difference of BUIR from existing end-to-end learning frameworks is that f_{\theta} and f_{\xi} are updated in different ways.The online encoder is trained to minimize the error between its output and the target, whereas the target network is slowly updated based on the momentum update (Heet al., 2020b) so as to keep its output consistent.To sum up, the parameters of the online encoder and target encoder are optimized by(4)\begin{split}\theta&\leftarrow\theta-\eta\cdot\nabla_{\theta}\mathcal{L}_{\theta,\xi}\\\xi&\leftarrow\tau\cdot\xi+(1-\tau)\cdot\theta.\end{split}\eta is the learning rate for stochastic optimization, and \tau\in[0,1] is a momentum coefficient (also called as target decay) for momentum-based moving average.The online encoder f_{\theta} (and the predictor q_{\theta}) is effectively optimized by the gradients back-propagated from the loss (Equation (3)), while the target encoder f_{\xi} is updated as the moving average of the online encoder.By taking a large value of \tau, the target encoder slowly approximates the online encoder.This momentum-based update makes \xi evolve more slowly than \theta, which enables to bootstrap the representations by providing enhanced but consistent targets to the online encoders (Heet al., 2020b; Grill et al., 2020).Figure 1 illustrates the overall framework of BUIR with the simple one-hot encoders.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.Similarly to Section 3.2, the online encoder is trained by minimizing \mathcal{L}_{\theta,\xi}(\psi(u,\mathcal{V}_{u}),\psi(v,\mathcal{U}_{v})), and the target encoder is slowly updated by the momentum mechanism.After the optimization is finished, the interaction score is inferred by f_{\theta}(u,\mathcal{V}_{u}) and f_{\theta}(v,\mathcal{U}_{v}) (Equation (5)).Figure 2 shows an example of our data augmentation which injects a certain level of perturbations to the neighbors.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
According to the authors, assuming unobserved user-item pairs negative will lead to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.	Nevertheless, the negative sampling approach has critical limitations in the following aspects.First, the underlying assumption about negative interactions becomes less valid as user-item interactions get sparser.This is because as fewer positive interactions are observed, the number of ”positive but unobserved” interactions increases, which consequently makes it even harder to sample correct negative ones.Such uncertainty of supervision eventually degrades the performance for top-K recommendation.Second, the convergence speed and the final performance depend on the specific choice of distributions for negative sampling.For example, sampling negative pairs from a non-uniform distribution (Rendle andFreudenthaler, 2014; Dinget al., 2019) (e.g., the multinomial distribution which models the probability of each interaction being actually negative) can improve the final performance, but inevitably incurs high computational costs, especially when a lot of users and items should be considered.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
According to the authors, assuming unobserved user-item pairs negative will not lead to limited performance since there are some cases of positive but unobserved, and the number of this case is increased.	Nevertheless, the negative sampling approach has critical limitations in the following aspects.First, the underlying assumption about negative interactions becomes less valid as user-item interactions get sparser.This is because as fewer positive interactions are observed, the number of ”positive but unobserved” interactions increases, which consequently makes it even harder to sample correct negative ones.Such uncertainty of supervision eventually degrades the performance for top-K recommendation.Second, the convergence speed and the final performance depend on the specific choice of distributions for negative sampling.For example, sampling negative pairs from a non-uniform distribution (Rendle andFreudenthaler, 2014; Dinget al., 2019) (e.g., the multinomial distribution which models the probability of each interaction being actually negative) can improve the final performance, but inevitably incurs high computational costs, especially when a lot of users and items should be considered.	Incorrect	Opposite	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
To prevent the problem of collapsed sollution, the authors update target encoder and online encoder differently.	Pointing out that the contrastive methods need to carefully treat the negative instances during the training for effectiveness and efficiency, the most recent work proposed a bootstrapping-based self-supervised learning framework (Grill et al., 2020; Chen and He, 2021), which is capable of avoiding the collapsed solution without the help of negative instances.Inspired by bootstrapping methods in deep reinforcement learning (Mnihet al., 2015; Mnih et al., 2016), it directly bootstraps the representation of images by using two neural networks that iteratively learn from each other.This approach achieves the state-of-the-art performance for various downstream tasks in computer vision, and also shows better robustness to the choice of data augmentations used for self-supervision.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
To prevent the problem of collapsed sollution, the authors avoid updating target encoder and online encoder differently.	Pointing out that the contrastive methods need to carefully treat the negative instances during the training for effectiveness and efficiency, the most recent work proposed a bootstrapping-based self-supervised learning framework (Grill et al., 2020; Chen and He, 2021), which is capable of avoiding the collapsed solution without the help of negative instances.Inspired by bootstrapping methods in deep reinforcement learning (Mnihet al., 2015; Mnih et al., 2016), it directly bootstraps the representation of images by using two neural networks that iteratively learn from each other.This approach achieves the state-of-the-art performance for various downstream tasks in computer vision, and also shows better robustness to the choice of data augmentations used for self-supervision.Bypassing the collapsed solution.  It is obvious that the loss in Equation (3) admits the collapsed solution with respect to \theta and \xi, which means both the encoders generate the same representations for all users and items.For this reason, the conventional end-to-end learning strategy, which optimizes both f_{\theta} and f_{\xi} to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution.In contrast, our proposed framework updates each of the encoders in different ways.From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder.That is, the direction of updating the target encoder (\theta-\xi) totally differs from that of updating the online encoder (-\nabla_{\theta}\mathcal{L}_{\theta,\xi}),and this effectively keeps both the encoders from converging to the collapsed solution.Several recent work on bootstrapping-based representation learning (Grill et al., 2020; Chen and He, 2021) empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.	Incorrect	Opposite	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors show augmented views of positive interactions can lead the performance improvement, especially in sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The authors show augmented views of positive interactions can lead the performance improvement, especially in less sparser datasets by showing the experimental result of stochastic data augmentation achieved a big improvement compared to the case of using the fixed neighborhood information as encoder inputs.	In Figure 4, our stochastic data augmentation (i.e., P>0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., P=0) as encoder inputs.This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users’ preference on items even in much sparse dataset.Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability.This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance.To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-K recommendation among a wide range of OCCF methods.In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions.In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Incorrect
Through experiments, the authors demonstrated that the performance of the model (i. , MRR@10) decreased without knowledge propagation and that it was comparable to vanilla ERNIE, which demonstrated that multi-hop neighbors were essential for ranking performance.  This result can be attributed to how using multi-hope neighbors allows for knowledge to propagate between query and passage.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.By applying a K-layer GMN in each layer of the knowledge injector, the output entity representation \hat{\mathbf{E}}_{e_{h}}^{(K)} can ensemble knowledge from all the K-hop neighbors. As described in Section 4.1.2 that all the paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} between \mathbf{q} and \mathbf{p} is within K hops, the GMN module can attentively propagate knowledge along the paths from entities in \mathbf{p} to those in \mathbf{q}, and vice versa, which can enrich the semantics of the entities that benefit the relevance modeling.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Through experiments, the authors demonstrated that the performance of the model (i. , MRR@10) decreased without knowledge propagation and that it was comparable to fine-tuned ERNIE, which demonstrated that multi-hop neighbors were essential for ranking performance.  This result can be attributed to how using multi-hope neighbors allows for knowledge to propagate between query and passage.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.By applying a K-layer GMN in each layer of the knowledge injector, the output entity representation \hat{\mathbf{E}}_{e_{h}}^{(K)} can ensemble knowledge from all the K-hop neighbors. As described in Section 4.1.2 that all the paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} between \mathbf{q} and \mathbf{p} is within K hops, the GMN module can attentively propagate knowledge along the paths from entities in \mathbf{p} to those in \mathbf{q}, and vice versa, which can enrich the semantics of the entities that benefit the relevance modeling.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
This work proposes an aggregation module that employs a PLM and a Graph Neural Network (GMN) to model the interaction between explicit and implicit knowledge.  The PLM encodes text to obtain word representations (i. , implicit knowledge), and the Graph Neural Network (GMN) encodes knowledge meta-graphs to obtain entity representations (i. , explicit knowledge).  This module aggregates the word and entity representations to aggregate the implicit and explicit knowledge.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Overall, our contributions can be summarized as follows:•It is the first attempt to solve the knowledge enhanced PLMs problem for passage re-ranking. The key motivation lies in that bridging the semantic gap between the query and passage with the help of both kinds of knowledge.•We design a novel knowledge graph distillation method. It refines a reliable knowledge graph from the existing one globally and constructs a knowledge meta graph based on the refined graph locally.•We propose a novel aggregation of PLM and graph neural network framework to model the interaction between explicit knowledge and implicit knowledge.•Experimental results show the effectiveness of KERM on both general and domain specific data, achieving state-of-the-art performance for passage re-ranking. We also conduct a comprehensive study for the effects of each module in our method. The code is available at https://github.com/DQ0408 /KERM.	Correct		Incorrect	Correct	Correct	Correct	Incorrect	Correct
This work proposes an aggregation module that employs a LLM and a Graph Neural Network (GMN) to model the interaction between explicit and implicit knowledge.  The LLM encodes text to obtain word representations (i. , implicit knowledge), and the Graph Neural Network (GMN) encodes knowledge meta-graphs to obtain entity representations (i. , explicit knowledge).  This module aggregates the word and entity representations to aggregate the implicit and explicit knowledge.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Overall, our contributions can be summarized as follows:•It is the first attempt to solve the knowledge enhanced PLMs problem for passage re-ranking. The key motivation lies in that bridging the semantic gap between the query and passage with the help of both kinds of knowledge.•We design a novel knowledge graph distillation method. It refines a reliable knowledge graph from the existing one globally and constructs a knowledge meta graph based on the refined graph locally.•We propose a novel aggregation of PLM and graph neural network framework to model the interaction between explicit knowledge and implicit knowledge.•Experimental results show the effectiveness of KERM on both general and domain specific data, achieving state-of-the-art performance for passage re-ranking. We also conduct a comprehensive study for the effects of each module in our method. The code is available at https://github.com/DQ0408 /KERM.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
This work conducted ablation studies to investigate the contribution of each component in the performance of KERM.  By testing different settings for the knowledge injector, this work found that performance decreases without knowledge interaction and also without knowledge propagation.  By testing the model without global or local distillation, they also demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation.  These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.	Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
This work conducted ablation studies to investigate the contribution of each component in the performance of KERM.  By testing different settings for the knowledge injector, this work found that performance doesn't change without knowledge interaction and also without knowledge propagation.  However, by testing the model without global or local distillation, they demonstrated that performance decreases without global distillation and efficiency decreases without either global or local distillation.  These experiments demonstrate that each component of KERM contributes to passage re-ranking performance quantitatively.	Here we compare ranking performances of KERM and other PLMs based re-rankers on the first two widely used query sets. Moreover, ablation studies for each component of KERM are also explored. All experimental results were reported under the same BM25 setting.Table 3 shows the performance comparisons between different settings of knowledge injector, which is statistically significant. From this table, we can observe the following phenomena. (1) MRR@10 of KERM without interaction and propagation process decreases at least 1\% respectively. This indicates both knowledge interaction and propagation processes play an indispensable role in ranking performance. (2) The performance of KERM without propagation is comparable to vanilla ERNIE. Not only query and passage entities, but also their multi-hop neighbors are essential for the ranking performance. (3) MRR@10 of KERM without knowledge interaction drops the most. It suggests the simple and straightforward way to aggregate knowledge graph with text does not work in the passage re-ranking scenario. The text and knowledge graph need to be refined with each other mutually in the interaction, which will be further analyzed in detail as follows.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking.  By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without global distillation, and that time efficiency decreased the most without local distillation.  Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.	Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet (Speeret al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage.Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever.For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
This work proposes using knowledge graph distillation as it can help retain only informative knowledge needed for passage re-ranking.  By investigating the effect of global and local distillation separately, this work found that the MRR@10 score and efficiency decreased slightly without local distillation, and that time efficiency decreased the most without global distillation.  Therefore, this work demonstrates that both global and local distillation of knowledge graphs is useful for re-ranking tasks in terms of performance and efficiency.	Existing knowledge graphs are usually incomplete and noisy. It is unsuitable for direct introduction of them to the current model. Specially, there is no knowledge base particularly for passage re-ranking task. For example, ConceptNet (Speeret al., 2017) is a general knowledge graph that contains common sense knowledge, where the information might not be useful for our passage re-ranking task. Therefore, it is critical for us to propose a knowledge graph distillation process from both global and local perspectives.Knowledge graph distillation is performed in both global and local perspectives. To explore their roles in the ranking performance, we remove the graph pruning globally and sentence selection locally respectively, keep other settings unchanged, and derive KERM without graph pruning and sentence selection respectively. From results on TREC 2019 DL in Table 5, observations are listed as below. (1) Without global graph pruning, MRR@10 and the average edge score, calculated through Eq.(3), decrease the most, and the time efficiency drops slightly. This indicates the original knowledge graph exists noise data that affect performance. (2) Without sentence selection, the time efficiency drops the most and the average edge score decreases slightly, which proves that not every sentence in a passage has a positive effect on semantic matching. Overall, knowledge graph distillation is significant to KERM.The main goal of this paper is to reasonably introduce external knowledge graph to PLMs for passage re-ranking. We first design a novel knowledge meta graph construction method to distill reliable and query related knowledge from a general and noisy knowledge graph. The knowledge meta graph bridges the semantic gap between each query and passage. Then we propose a knowledge injector layer for mutually updating text and knowledge representations, which transformers word to entity representations for graph meta network, vice versa. Knowledge Enhanced Ranking Model is pretrained with Masked Language Model (MLM) Sentence Relation Prediction (SRP) [38] tasks, and fine-tuned with cross entropy loss function for passage re-ranking task. Experimental results on public benchmark datasets show the effectiveness of the proposed method compared with state-of-the-art baselines without external knowledge due to its first attempt. The role of each module in KERM is also comprehensively analyzed. Since this work was limited to the one-to-one meta-graph of a query-passage pair built online, continued efforts are needed to make knowledge enhancement more efficient for both retrieval and re-ranking stage.Despite that the knowledge graph distillation in our method is empirically shown to be effective for the final performance, the implementation of graph pruning and meta-graph construction is still based on simple heuristics. A more promising way of formulating a useful meta-graph is to jointly learn a graph generator with the reranker in an end-to-end fashion, which enables more flexibility.Besides, it is currently infeasible to exploit the external knowledge in the retrieval stage, which needs to exhaustively build massive meta-graphs for a large scale of candidates. A further study could focus on how to use external knowledge in PLM based retriever.For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
MARCO-Passage collection is a large-scale publicly available corpus and two query sets derived from this corpus are used in the paper: MSMARCO-TRAIN and MSMARCO-DEV.	We use a large-scale public available corpus, i.e., MSMARCO-Passage collection (Nguyen et al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL (Craswell et al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed 111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work (Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section 6.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
MARCO-Passage collection is a large-scale publicly available corpus and only one query set derived from this corpus is used in the paper: MSMARCO-TRAIN.	We use a large-scale public available corpus, i.e., MSMARCO-Passage collection (Nguyen et al., 2016), as our passage collection. This collection contains approximately 8.8 million passages extracted from 3.2 million web documents covering multiple fields. We train our model on the MSMARCO-TRAIN query set of 502,939 queries and evaluate KERM on three query sets. Table 1 provides the detailed information of these query sets.The first test set is MSMARCO-DEV, which includes 6,980 sparsely-judged queries mixed with multiple domains. Each query has an average of 1.1 relevant passages with binary relevance label.The second test set is TREC 2019 DL (Craswell et al., 2020), which contains 43 densely-judged queries with fine-grained relevance labels, i.e., irrelevant, relevant, highly relevant and perfectly relevant. On average, a query has 95.4 relevant passages, and most queries have more than 10 relevant passages. With fine-grained labels and multiple relevant passages per query, TREC 2019 DL can be used to reflect the fine-grained ranking performance between relevant passages.To evaluate KERM on specific domains, we further introduce Ohsumed 111http://disi.unitn.it/moschitti/corpora.htm query set, which contains 63 queries on bio-medical domain.The collection of Ohsumed is constructed from the first 20,000 passages in Mesh categories of the year 1991.Following the previous work (Joachims, 1998), the test collection including 10,000 passages are utilized for performance comparison on Ohsumed query set.Each query has an average of 50.9 relevant passages with three graded relevance labels. In section 6.4, we demonstrate that the quality of external knowledge constructed by KERM in such domain could be more useful.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The knowledge graph is distilled globally by taking an existing knowledge graph and pruning unreliable or noise relations based on TransE embeddings.  The graph is then distilled locally by extracting entities from both the given query and passage, and constructing a query-document bipartite entity graph based on the extracted entities and their k-hop neighbors.	For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The knowledge graph is distilled globally by taking an existing knowledge graph and pruning unreliable or noise relations based on BERT embeddings.  The graph is then distilled locally by extracting entities from both the given query and passage, and constructing a query-document bipartite entity graph based on the extracted entities and their k-hop neighbors.	For knowledge graph distillation, we propose a novel pipeline to establish knowledge meta graphs, which only retain informative knowledge for passage re-ranking. Specifically, we first distill a graph globally for passage re-ranking scenario from an existing knowledge graph by pruning some unreliable or noisy relations based on TransE embedding. Then for a specific query-passage pair, we extract entities from both the query and passage, and construct a query-document bipartite entity graph based on query and passage entities and their k-hop neighbors, namely knowledge meta graph. Challenge 1. could be addressed in this distillation process.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain.  For example, in ConceptNet, the entity “hepatitis” has relations with both “infectious disease” and “adult”.  To the concept “hepatitis”,  the concept “adults” is more general than “infectious disease” and thus the relationship between “hepatitis” and “infectious disease” is more reliable and informative.	•Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.•Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.Fig. 2 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig. 2, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Unreliable relations in a knowledge graph involve trivial factual triplets that do not bring substantial information gain.  For example, in ConceptNet, the entity “hepatitis” has relations with both “infectious disease” and “adult”.  To the concept “hepatitis”,  the concept “adult” is more general than “infectious disease” and thus the relationship between “hepatitis” and “adult” is more reliable and informative.	•Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge.•Challenge 2.The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult.How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.Fig. 2 shows a real case of our global graph pruning method on ConceptNet, i.e., a general knowledge graph. In this case, the entity hepatitis has various relations to disease, infectious disease, adult, etc. From the distance of nodes in Fig. 2, we can clearly observe that the knowledge hepatitis is an infectious disease is more reliable and informative than hepatitis is located at adult. To hepatitis, the concept adult is more general than infectious disease. This indicates that our pruning method can effectively eliminate less informative knowledge.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The Graph Meta Network (GMN) refines knowledge in a meta-graph.  A meta-graph is a graph that is constructed by constructing multi-hop paths between the entities in a query and a passage using the knowledge from a global graph.  The meaning for “meta” in both graph meta network (GMN) and meta-graph is not explicitly defined in this paper.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage. Thus, we further leverage the knowledge in the global graph G to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling. More specifically, for a given query-passage pair (i.e., (q, p)), we propose to construct a bipartite meta-graph that connects those entities in the q and those in p.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The Graph Meta Network (GMN) refines knowledge in a meta-graph.  A meta-graph is a graph that is constructed by constructing multi-hop paths between the entities in a query and a passage without using the knowledge from a global graph.  The meaning for “meta” in both graph meta network (GMN) and meta-graph is not explicitly defined in this paper.	Knowledge propagation via meta-graph.It is worth noting that, the above-defined knowledge injection process only leverages knowledge embeddings learned by TransE on the global graph \overline{\mathcal{G}}. Particularly, it lacks considering the knowledge that bridges the semantics between query and passage. To this end, we introduce a Graph Meta Network (GMN) module that refines knowledge with the constructed meta-graph \mathbf{G}_{\mathbf{q},\mathbf{p}}, The multi-hop paths of \mathbf{G}_{\mathbf{q},\mathbf{p}} allow the knowledge to be propagated between query and passage, which can enhance the relevance signal to be captured by the model, and thus alleviate the semantic gap.For knowledge aggregation, we design a novel interaction module between text and knowledge graph to combine the implicit and explicit knowledge. To derive implicit knowledge from text, we employ PLM as text encoder. To be aligned with implicit knowledge, knowledge meta graph is encoded with a multi-layer graph neural network (i.e. k-hop), namely Graph Meta Network (GMN). Each transformer layer outputs word representations. Each graph meta network layer outputs entity representations. Both word and entity representations are aggregated as the input of the following transformer and GMN layer, respectively in a novelly designed module, namely knowledge injector. Therefore through knowledge aggregation, implicit knowledge from text corpus and explicit knowledge from existing knowledge graph can mutually boost each other to achieve a better re-ranking performance, in which the issues in Challenge 2. could be mitigated.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage. Thus, we further leverage the knowledge in the global graph G to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling. More specifically, for a given query-passage pair (i.e., (q, p)), we propose to construct a bipartite meta-graph that connects those entities in the q and those in p.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
This work mentions using the Paddle Graph Learning (PGL) framework from the deep learning framework PaddlePaddle.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
This work mentions using the Paddle Entity Learning (PEL) framework from the deep learning framework PaddlePaddle.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.	Incorrect	Change concept	Incorrect	Incorrect	Incorrect	Unrelated	Incorrect	Incorrect
ConceptNet is a general knowledge graph and, in this work, the authors merged relation types in the graph to construct a multi-relational graph with 17 relation types.	We use ConceptNet (Speeret al., 2017), a general knowledge graph as our external knowledge base \mathcal{G}. Following KagNet (Linet al., 2019), we merge relation types to increase graph density and construct a multi-relational graph with 17 relation types, including atlocation, causes, createdby, etc.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
ConceptNet is a general knowledge graph and, in this work, the authors merged relation types in the graph to construct a multi-relational graph with 19 relation types.	We use ConceptNet (Speeret al., 2017), a general knowledge graph as our external knowledge base \mathcal{G}. Following KagNet (Linet al., 2019), we merge relation types to increase graph density and construct a multi-relational graph with 17 relation types, including atlocation, causes, createdby, etc.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Unrelated	Incorrect
Compared to conventional Next Sentence Prediction (NSP), Sentence Relation Prediction (SRP) aims to predict whether a given sentence is the next sentence, previous sentence relation, or no relation with another sentence.	Knowledge-enhanced pre-training.Following previous studies (Nogueiraet al., 2019a; Yanet al., 2021; Kim and Ko, 2021), we conduct continual pre-training on MSMARCO corpus to warm up the parameters of GMN module.We apply Masked Language Model (MLM) (Devlinet al., 2018) and Sentence Relation Prediction (SRP) (Wang et al., 2019) as the pre-training tasks in KERM.Compared to conventional Next Sentence Prediction (NSP) (Devlinet al., 2018), the task of SRP is to predict whether a given sentence is the next sentence, previous sentence relation or no relation with another sentence. To incorporate knowledge during the pre-training stage, we construct a meta-graph for each sentence pair, and apply the knowledge aggregation process as introduced above.The pre-training loss is defined as\mathcal{L}_{p}=\mathcal{L}_{MLM}+\mathcal{L}_{SRP}.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Compared to conventional Next Sentence Prediction (NSP), Sentence Relation Prediction (SRP) aims to predict only whether a given sentence is the next sentence, without predicting previous sentence relation, or no relation with another sentence.	Knowledge-enhanced pre-training.Following previous studies (Nogueiraet al., 2019a; Yanet al., 2021; Kim and Ko, 2021), we conduct continual pre-training on MSMARCO corpus to warm up the parameters of GMN module.We apply Masked Language Model (MLM) (Devlinet al., 2018) and Sentence Relation Prediction (SRP) (Wang et al., 2019) as the pre-training tasks in KERM.Compared to conventional Next Sentence Prediction (NSP) (Devlinet al., 2018), the task of SRP is to predict whether a given sentence is the next sentence, previous sentence relation or no relation with another sentence. To incorporate knowledge during the pre-training stage, we construct a meta-graph for each sentence pair, and apply the knowledge aggregation process as introduced above.The pre-training loss is defined as\mathcal{L}_{p}=\mathcal{L}_{MLM}+\mathcal{L}_{SRP}.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
BM25 and DPR are both examples of retrievers used in large-scale passage collection.  BM25 is described as a traditional sparse retriever and DPR leverages PLM to empower the retriever by a single vector.	The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPR (Karpukhin et al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERT (Zhanet al., 2020), ColBERT (Khattab andZaharia, 2020), COIL (Gaoet al., 2021) and Interactor (Yeet al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1 (Qu et al., 2021) and RocketQAv2 (Ren et al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICT (Leeet al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. Condenser (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.Given a query q, passage re-ranking aims at ordering a set of 𝜘 passages, i.e., P =  p𝜅  𝜘 𝜅=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {𝑤𝑝 } |p| 𝑝=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {𝑤𝑞 } |q| 𝑞=1 . Note that a passage p consists of 𝑇 sentences p = {s𝜏 } 𝑇 𝜏=1.	Correct		Correct	Incorrect	Correct	Incorrect	Correct	Correct
BM25 and DPR are both examples of retrievers used in large-scale passage collection.  BM25 leverages PLM to empower the retriever by a single vector, and DPR is described as a traditional sparse retriever.	The low-dimensional dense representations for query and passage are computed by PLMs based retrievers from the dual-encoder architecture. Afterward, the candidate passage set could be retrieved efficiently via approximate nearest neighbor algorithms.Existing studies could be categorized into two parts:(1) By optimizing the matching stage: DPR (Karpukhin et al., 2020) is the first study to leverage PLM to empower the retriever by a single vector. Other researches, such asRepBERT (Zhanet al., 2020), ColBERT (Khattab andZaharia, 2020), COIL (Gaoet al., 2021) and Interactor (Yeet al., 2022), obtain multiple vectors for query and passage for matching.(2) By optimizing the representation learning module: RocketQAv1 (Qu et al., 2021) and RocketQAv2 (Ren et al., 2021) boost the representation learning of retriever by leveraging the power of cross-encoder in a cascade or joint manner. Other studies boost the representation learning by designed IR-oriented pre-training tasks.ICT (Leeet al., 2019) treats sentences as pseudo-queries and matched them to the passage they originate from. Condenser (Gao and Callan, 2021) utilizes a novel pre-training task, which can produces an information-rich representation to condense an input sequence.We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.Given a query q, passage re-ranking aims at ordering a set of 𝜘 passages, i.e., P =  p𝜅  𝜘 𝜅=1 , which is usually retrieved from a large-scale passage collection by a retriever, e.g. BM25 [48], DPR [16] etc. In particular, a passage is a sequence of words p = {𝑤𝑝 } |p| 𝑝=1 , where |p| is the length of passage p. Similarly, a query is a sequence of words q = {𝑤𝑞 } |q| 𝑞=1 . Note that a passage p consists of 𝑇 sentences p = {s𝜏 } 𝑇 𝜏=1.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Correct	Incorrect
RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder.  While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i. ,  a distillation procedure).	Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
RocketQAv1 trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful dual-encoder to empower the cross-encoder.  While it inherits the parameters from RocketQAv1, RocketQAv2 extends the first version through a novel approach that jointly trains the dense passage retriever and passage re-ranker, and by using a large PLM for data augmentation and denoising (i. ,  a distillation procedure).	Existing PLMs based re-rankers typically improve ranking performance from two aspects: (1) By optimizing the ranking procedure: monoBERT (Nogueira and Cho, 2019) is the first work that re-purposed BERT as a passage re-ranker and achieves state-of-the-art results. duoBERT (Nogueiraet al., 2019a) integrates monoBERT in a multistage ranking architecture and adopts a pairwise classification approach to passage relevance computation. UED (Yanet al., 2021) proposes a cascade pre-training manner that can jointly enhance the retrieval stage through passage expansion with a pre-trained query generator and thus elevate the re-ranking stage with a pre-trained transformer encoder. The two stages can facilitate each other in a unified pre-training framework. H-ERNIE (Chuet al., 2022) proposes a multi-granularity PLM for web search.(2) By designing rational distillation procedure: LM Distill + Fine-Tuning (Gaoet al., 2020) explores a variety of distillation methods to equip a smaller re-ranker with both general-purpose language modeling knowledge learned in pre-training and search- specific relevance modeling knowledge learned in fine-tuning, and produces a faster re-ranker with better ranking performance. CAKD (Hofstätter et al., 2020) proposes a cross-architecture knowledge distillation procedure with a Margin-MSE loss, which can distill knowledge from multiple teachers at the same time. RocketQAv1 (Qu et al., 2021) trains dual-encoder and cross-encoder in a cascade manner, which leverages the powerful cross-encoder to empower the dual-encoder. RocketQAv2 (Ren et al., 2021) proposes a novel approach that jointly trains the dense passage retriever and passage re-ranker. The parameters of RocketQAv2 are inherited from RocketQAv1. Besides, RocketQAv2 utilizes a large PLM for data augmentation and denoising, which can also be regarded as a distillation procedure. Notably, these two types of studies anticipate more insightful information to be captured by the advanced ranking and training procedures, while neglecting the limitations of implicit knowledge extracted from noisy and heterogeneous data. Therefore, in this paper, we proposed the first knowledge-enhanced PLM based re-ranker, which thoughtfully leverages explicit external knowledge that improve the effectiveness of the model.We include several PLMs based re-rankers in our evaluation, including the state-of-the-art:•monoBERT (Nogueira and Cho, 2019): The first study that re-purposes BERT as a re-ranker and achieves state-of-the-art results.•duoBERT (Nogueiraet al., 2019a):This work proposes a pairwise classification approach using BERT, which obtains the ability to be more sensitive to semantics through greater computation.•UED (Yanet al., 2021): A unified pre-training framework that jointly refines re-ranker and query generator. For a fair comparison, we only use the re-ranker in UED without passage expansion.•LM Distill+Fine-Tuning (LDFT) (Gaoet al., 2020):A variety of distillation methods are compared in this paper. The experimental results indicate that a proper distillation procedure (i.e. first distill the language model, and then fine-tune on the ranking task) could produce a faster re-ranker with better ranking performance.•CAKD (Hofstätter et al., 2020): This work proposes a cross-architecture knowledge distillation procedure with Margin-MSE loss, which can distill knowledge from multiple teachers.•RocketQAv1 (Qu et al., 2021): This work mainly focuses on the training of PLM based retriever, where the re-ranker is an intermediate product of its training process.•RocketQAv2 (Ren et al., 2021): Based on RocketQAv1, this work proposes a novel approach that jointly trains the PLM based retriever and re-ranker.To compare the performance of different methods, we resort to two ranking metrics.For MSMARCO-DEV, We adopt Mean Reciprocal Rank (i.e., MRR@10).For TREC 2019 DL, we use Mean Average Precision, i.e., MAP@10 and MAP@30.For Ohsumed, both Mean Reciprocal Rank and Mean Average Precision (i.e., MRR@10 and MAP@10) are employed for comprehensive performance analysis in queries requiring in-depth domain knowledge.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
In their experiments, the authors showed that all of the models performed poorly on the bio-medical domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset.  This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain.  These results suggest that further training on bio-medical data could increase performance.	However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways (Nogueiraet al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous (Liuet al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate.Results are obtained from Table 6. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs’ pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
In their experiments, the authors showed that all of the models performed poorly on the computer-science domain due to the textual data of the domain not being covered widely in the PLMs’ pretraining dataset.  This lack of data can cause the PLM to struggle to reveal and capture knowledge specific to that domain.  These results suggest that further training on bio-medical data could increase performance.	However, implicit knowledge still has some inherent weaknesses, which limits the applicability of PLMs based re-rankers. First,queries and passages are usually created by different persons and have different expression ways (Nogueiraet al., 2019b), such as word usage and language style.Worse still, the data distributions of search queries and web contents are highly heterogeneous (Liuet al., 2021), where various specialized domains (e.g., bio-medical) may only have few training examples in a general corpus. Domain-specific knowledge can hardly be revealed and captured by the model, and thus the processing of domain-specific queries is often inaccurate.Results are obtained from Table 6. (1) Poor ranking performances of all models on bio-medical domain indicates that it is more challenging in the data scarcity scenario, where textual data is not covered widely in the PLMs’ pretraining datasets. (2) Compared with ERNIE, KERM has a higher relative improvement in bio-medical domain than general domain. This demonstrates that the incorporation of knowledge graph is more useful for a data scarcity domain. To verify this idea, we compare the size of knowledge meta graph used for different domains as follows.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Incorrect
Large-scale pre-trained language models (PLMs) have been found to be successful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus.  PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.	Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Large-scale pre-trained language models (PLMs) have been found to be unsuccessful for passage re-ranking due to their ability to learn semantic relevance in the latent space from massive textual corpus.  PLMs obtain this ability from their expressive transformer architecture and the pretrain-then-finetune paradigm.	Passage Re-ranking is a crucial stage in modern information retrieval systems, which aims to reorder a small set of candidate passages to be presented to users. To put the most relevant passages on top of a ranking list, a re-ranker is usually designed with powerful capacity in modeling semantic relevance, which attracted a wealth of research studies in the past decade (Guo et al., 2020). Recently,large-scale pre-trained language models (PLMs), e.g. BERT (Devlinet al., 2018), ERNIE (Sun et al., 2019) and RoBERTa (Liu et al., 2019), have dominated many natural language processing tasks, and have also achieved remarkable success on passage re-ranking.For example, PLM based re-rankers (MacAvaney et al., 2019; Liet al., 2020; Dong and Niu, 2021; Donget al., 2022) have achieved state-of-the-art performance, which takes the concatenation of query-passage pair as input, and applies multi-layer full-attention to model their semantic relevance. Their superiority can be attributed to the expressive transformer structure and the pretrain-then-finetune paradigm, which allow the model to learn useful implicit knowledge (i.e., semantic relevance in the latent space) from massive textual corpus (Fan et al., 2021).	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Entities that exactly match entities in E are selected from q and s* to construct the meta-graph.  Also, entities that are sub-sequences of other recognized entities are omitted.  This process assumes that entities are identified in the query and passage.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Entities that exactly match entities in E are selected from q and s* to construct the meta-graph.  Also, entities that are sub-sequences of other recognized entities are omitted.  This process confirms that entities are identified in the query and passage.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Through the experiments, this work demonstrated that the KERM model was able to significantly improve on the performance of its backbone model, ERNIE.  The authors posit that this is due to how KERM explicitly introduces external knowledge which can improve semantic matching performance.  This suggests that KERM models with other backbone models will be able to improve on the performance of their backbone models.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.(4) Compared with \textrm{ERNIE}_{\textrm{base}} we trained, \textrm{KERM}_{\textrm{base}} shows a significant improvement on both two query sets. This indicates the explicit introduction of external knowledge can alleviate the semantic gap and heterogeneity between query and passage, and improve the semantic matching performance.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Through the experiments, this work demonstrated that the KERM model was able to significantly improve on the performance of its backbone model, ERNIE, Bert_large and GPT3.  The authors posit that this is due to how KERM explicitly introduces external knowledge which can improve semantic matching performance.  This suggests that KERM models with other backbone models will be able to improve on the performance of their backbone models.	We use the traditional sparse retriever BM25 (Yanget al., 2017) as our first stage method. All experiments are conducted under the same BM25 setting with 1000 retrieved candidates. We conduct experiments with the deep learning framework PaddlePaddle (Maet al., 2019) on up to 4 NVIDIA Tesla A100 GPUs (with 40G RAM). For the GMN module, we use Paddle Graph Learning (PGL) 222https://github.com/PaddlePaddle/PGL, an efficient and flexible graph learning framework based on PaddlePaddle. For training, we used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5 for text encoder and 1e-4 for knowledge injector. The model is trained up to 5 epochs with a batch size of 640 and 240 for base and large models respectively.In our experiments, the PLM small, base and large models have 6, 12 and 24 Transformer layers respectively.The text encoder has 9 layers and 21 layers for base and large model respectively, and the knowledge injector both has 3 layers in our experiment. The dropout rates are set to 0.1. The ratio of the positive to the hard negative is set to 1:19.All transformer layers in KERM’s backbone are initialized from ERNIE-2.0 base (Sunet al., 2020b), which is a BERT-like model pre-trained with a continual pre-training framework on multiple tasks. We perform Knowledge-enhanced pre-training on MARCO passage collection to warm up the parameters in knowledge injector, which has 60,000 iterations under the batch size of 256.For a fair comparison, the same pre-training without knowledge enhancement is also conducted on \textrm{ERNIE}_{\textrm{base}} re-ranker and all models in ablation studies.(4) Compared with \textrm{ERNIE}_{\textrm{base}} we trained, \textrm{KERM}_{\textrm{base}} shows a significant improvement on both two query sets. This indicates the explicit introduction of external knowledge can alleviate the semantic gap and heterogeneity between query and passage, and improve the semantic matching performance.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
While approaches like CokeBERT and CoLake integrate sophisticated knowledge into PLMs through knowledge graphs, they did not focus specifically on using knowledge to empower PLMs for re-ranking tasks.	Existing KE-PLMs can be categorized by the granularity of knowledge they incorporate from knowledge graph (KG), as text-based knowledge, entity knowledge and KG meta-graphs.To integrate text-based knowledge, RAG (Lewiset al., 2020) and KIF (Fanet al., 2020) first retrieve top-k documents from Wikipedia using KNN-based retrieval, and the PLM model is employed to generate the output conditioned on these retrieved documents. Entity-level information can be highly useful for a variety of natural language understanding tasks. Hence, many existing KE-PLMs target this type of simple yet powerful knowledge. ERNIE(BAIDU) (Sun et al., 2019) introduces a new pre-training strategy of language model which masking phrases or entities in order to implicitly learn both synaptic and semantic knowledge from these units. ERNIE(THU) (Zhanget al., 2019) integrates informative entity representations in the knowledge module into the underlying layers of the semantic module based on the alignments between text and entity to equip the model with the ability of knowledge awareness. As knowledge graphs provide richer information than simply entity, more and more researchers start to explore integration of more sophisticated knowledge, such as meta-graphs in KG. CokeBERT (Su et al., 2021) proposes a novel semantic-driven Graph Neural Network (GNN) to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text.CoLake (Sunet al., 2020a) also uses GNN to aggregate information from the constructed meta-graph in both pre-training and inference. CoLake converts the meta-graph into token sequence and appends it to input sequence for PLMs, which is distinctive to CokeBERT. Although extensive research has been proposed up to now to address the knowledge-aware problem, none exists which constrained on how to use knowledge to empower PLMs particularly for re-ranking tasks.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage.Thus, we further leverage the knowledge in the global graph \overline{\mathcal{G}} to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling.More specifically, for a given query-passage pair (i.e., (\mathbf{q},\mathbf{p})), we propose to construct a bipartite meta-graph that connects those entities in the \mathbf{q} and those in \mathbf{p}.	Correct		Incorrect	Correct	Incorrect	Incorrect	Incorrect	Correct
While approaches like CokeBERT, SBERT, DeBERTa and CoLake integrate sophisticated knowledge into PLMs through knowledge graphs, they did not focus specifically on using knowledge to empower PLMs for re-ranking tasks.	Existing KE-PLMs can be categorized by the granularity of knowledge they incorporate from knowledge graph (KG), as text-based knowledge, entity knowledge and KG meta-graphs.To integrate text-based knowledge, RAG (Lewiset al., 2020) and KIF (Fanet al., 2020) first retrieve top-k documents from Wikipedia using KNN-based retrieval, and the PLM model is employed to generate the output conditioned on these retrieved documents. Entity-level information can be highly useful for a variety of natural language understanding tasks. Hence, many existing KE-PLMs target this type of simple yet powerful knowledge. ERNIE(BAIDU) (Sun et al., 2019) introduces a new pre-training strategy of language model which masking phrases or entities in order to implicitly learn both synaptic and semantic knowledge from these units. ERNIE(THU) (Zhanget al., 2019) integrates informative entity representations in the knowledge module into the underlying layers of the semantic module based on the alignments between text and entity to equip the model with the ability of knowledge awareness. As knowledge graphs provide richer information than simply entity, more and more researchers start to explore integration of more sophisticated knowledge, such as meta-graphs in KG. CokeBERT (Su et al., 2021) proposes a novel semantic-driven Graph Neural Network (GNN) to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text.CoLake (Sunet al., 2020a) also uses GNN to aggregate information from the constructed meta-graph in both pre-training and inference. CoLake converts the meta-graph into token sequence and appends it to input sequence for PLMs, which is distinctive to CokeBERT. Although extensive research has been proposed up to now to address the knowledge-aware problem, none exists which constrained on how to use knowledge to empower PLMs particularly for re-ranking tasks.Different from existing knowledge-enhanced PLMs for other NLP tasks, our aim for the re-ranking task is particularly on the relevance modeling between query and passage.Thus, we further leverage the knowledge in the global graph \overline{\mathcal{G}} to construct “bridges” between query and passage, which alleviates the semantic gap and improves semantic modeling.More specifically, for a given query-passage pair (i.e., (\mathbf{q},\mathbf{p})), we propose to construct a bipartite meta-graph that connects those entities in the \mathbf{q} and those in \mathbf{p}.	Incorrect	Invent something didn't mentioned	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
This work’s approach aims at focusing mostly on informative factors.  For example, the key sentence selection module focused on extracting only the most relevant sentences and the target entity recognition module focused on identifying only the most informative entities.  Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.• Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. • Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
This work’s approach aims at focusing mostly on informative factors.  For example, the key sentence selection module focused on extracting only the most informative sentences and the target entity recognition module focused on identifying only the relevant entities.  Further, this work argues that, to use knowledge graphs for re-ranking tasks, it is important that the graphs contain triplets with substantial information gain.	(1)Key sentence selection. The actual information need of a user usually concentrates on a small part of a relevant passage (Guo et al., 2020). To this end, we mimic human judgment and only focus on the sentence of each passage that is the most related to a query (Zou et al., 2021).In particular, we define the relevance score between a query q and a sentence \textbf{s}_{i} as(7)Rel_{qs}(\textbf{q},\textbf{s}_{i})=\frac{\sum_{q=1}^{|\textbf{q}|}\textbf{E}(w_{q})}{|\textbf{q}|}\cdot\frac{\sum_{s=1}^{|\textbf{s}_{i}|}\textbf{E}(w_{s})}{|\textbf{s}_{i}|}.For the sake of efficiency, we initialize \textbf{E}(w) from Word2Vec (Mikolovet al., 2013) embedding.Based on Eq.(7), we select the most relevant sentence \textbf{s}^{*} in p to build the meta-graph for \mathbf{q} and \mathbf{p}.(2)Target entity recognition.Next, we select the entities in q and \textbf{s}^{*} to construct the meta-graph. Specifically, we only consider the entities that exactly match in \mathcal{E}. Meanwhile, we omit those entity phrases that are sub-sequences of other recognized entities.For example, in the query "what causes low liver enzymes", both "liver" and "liver enzyme" are entities, but the entity "liver enzyme" is more informative to be recognized as the target entity, and "liver" should be omitted.(3)Path discovery. Finally, given the target entities of q and \textbf{s}^{*} (denoted as \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}, respectively), we perform Breadth First Search (BFS) on \overline{\mathcal{G}} to discover the paths within K-hop between \phi_{\mathbf{q}} and \phi_{\mathbf{s}^{*}}. Note that we only keep the within-K-hop paths that might be the most useful for the downstream re-ranking task. Meanwhile, the knowledge could be complemented from the K-hop paths.• Challenge 1. Existing knowledge graph are not constructed for re-ranking task. They usually contain trivial factual triples, which can hardly bring information gain. The inappropriate selection of external knowledge could even jeopardize the re-ranker performance. How to utilize existing knowledge graph to re-ranking task is remain a challenge. • Challenge 2. The explicit knowledge and implicit knowledge are highly heterogeneous due to the different sources, which makes the aggregation of the two difficult. How to mutually refine each other and effectively aggregate explicit knowledge into implicit knowledge to alleviate the semantic gap between query and passage is still a challenge.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors mention that FPGA often have under 10 MB memory. (For example, the Xilinx Vertex-7 FPGA has a maximum of 8. 5 MB of on-chip memory and no off-chip memory or storage)	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The authors mention that FPGA often have under 15 MB memory. (For example, the Xilinx Vertex-7 FPGA has a maximum of 12.5 MB of on-chip memory and no off-chip memory or storage)	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Correct	Incorrect
Tesla autopilot system uses a convolutional neural network to detect objects on its way.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	Correct		Unrelated	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Tesla, Xiaomi and Huawei autopilot systems all use a convolutional neural network to detect objects on its way.	Much of the recent research on deep convolutional neural networks (CNNs) has focused on increasing accuracy on computer vision datasets.For a given accuracy level, there typically exist multiple CNN architectures that achieve that accuracy level.Given equivalent accuracy, a CNN architecture with fewer parameters has several advantages:\bulletMore efficient distributed training.Communication among servers is the limiting factor to the scalability of distributed CNN training.For distributed data-parallel training, communication overhead is directly proportional to the number of parameters in the model Iandola et al. (2016).In short, small models train faster due to requiring less communication.\bulletLess overhead when exporting new models to clients. For autonomous driving, companies such as Tesla periodically copy new models from their servers to customers’ cars. This practice is often referred to as an over-the-air update. Consumer Reports has found that the safety of Tesla’s Autopilot semi-autonomous driving functionality has incrementally improved with recent over-the-air updates Consumer Reports (2016). However, over-the-air updates of today’s typical CNN/DNN models can require large data transfers. With AlexNet, this would require 240MB of communication from the server to the car. Smaller models require less communication, making frequent updates more feasible.\bulletFeasible FPGA and embedded deployment. FPGAs often have less than 10MB111For example, the Xilinx Vertex-7 FPGA has a maximum of 8.5 MBytes (i.e. 68 Mbits) of on-chip memory and does not provide off-chip memory. of on-chip memory and no off-chip memory or storage. For inference, a sufficiently small model could be stored directly on the FPGA instead of being bottlenecked by memory bandwidth Qiu et al. (2016), while video frames stream through the FPGA in real time.Further, when deploying CNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model could be stored directly on-chip, and smaller models may enable the ASIC to fit on a smaller die.	Incorrect	Invent something didn't mentioned	Unrelated	Incorrect	Correct	Unrelated	Incorrect	Incorrect
a module can be thought of as a block of some several layers may be of different filter sizes and dimensions to perform some specific functionality.  Many such modules are then combined to form a complete network.  For example, Inception modules, which are comprised of a number of different dimensionalities of filters, like 1x1 and 3x3, sometimes 5x5, 1x3 and 3x1.	With the trend of designing very deep CNNs, it becomes cumbersome to manually select filter dimensions for each layer.To address this, various higher level building blocks, or modules, comprised of multiple convolution layers with a specific fixed organization have been proposed.For example, the GoogLeNet papers propose Inception modules, which are comprised of a number of different dimensionalities of filters, usually including 1x1 and 3x3, plus sometimes 5x5 Szegedy et al. (2014) and sometimes 1x3 and 3x1 Szegedy et al. (2015).Many such modules are then combined, perhaps with additional ad-hoc layers, to form a complete network.We use the term CNN microarchitecture to refer to the particular organization and dimensions of the individual modules.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
a module can be thought of as a block of some several layers that must be of different filter sizes and dimensions to perform some specific functionality.  Many such modules are then combined to form a complete network.  For example, Inception modules, which are comprised of a number of different dimensionalities of filters, like 1x1 and 3x3, sometimes 5x5, 1x3 and 3x1.	With the trend of designing very deep CNNs, it becomes cumbersome to manually select filter dimensions for each layer.To address this, various higher level building blocks, or modules, comprised of multiple convolution layers with a specific fixed organization have been proposed.For example, the GoogLeNet papers propose Inception modules, which are comprised of a number of different dimensionalities of filters, usually including 1x1 and 3x3, plus sometimes 5x5 Szegedy et al. (2014) and sometimes 1x3 and 3x1 Szegedy et al. (2015).Many such modules are then combined, perhaps with additional ad-hoc layers, to form a complete network.We use the term CNN microarchitecture to refer to the particular organization and dimensions of the individual modules.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
An example of DSE approach can be  Bayesian optimization, simulated annealing, randomized search or genetic algorithms and all tend to develop automated approaches to find NN architectures exhibiting higher accuracy.	Neural networks (including deep and convolutional NNs) have a large design space, with numerous options for microarchitectures, macroarchitectures, solvers, and other hyperparameters.It seems natural that the community would want to gain intuition about how these factors impact a NN’s accuracy (i.e. the shape of the design space).Much of the work on design space exploration (DSE) of NNs has focused on developing automated approaches for finding NN architectures that deliver higher accuracy.These automated DSE approaches include bayesian optimization Snoek et al. (2012), simulated annealing Ludermir et al. (2006), randomized search Bergstra & Bengio (2012), and genetic algorithms Stanley & Miikkulainen (2002).To their credit, each of these papers provides a case in which the proposed DSE approach produces a NN architecture that achieves higher accuracy compared to a representative baseline.However, these papers make no attempt to provide intuition about the shape of the NN design space.Later in this paper, we eschew automated approaches – instead, we refactor CNNs in such a way that we can do principled A/B comparisons to investigate how CNN architectural decisions influence model size and accuracy.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
An example of DSE approach can be  Bayesian optimization, simulated annealing, randomized search or genetic algorithms. Among them, only Bayesian optimization tends to develop automated approaches to find NN architectures exhibiting higher accuracy.	Neural networks (including deep and convolutional NNs) have a large design space, with numerous options for microarchitectures, macroarchitectures, solvers, and other hyperparameters.It seems natural that the community would want to gain intuition about how these factors impact a NN’s accuracy (i.e. the shape of the design space).Much of the work on design space exploration (DSE) of NNs has focused on developing automated approaches for finding NN architectures that deliver higher accuracy.These automated DSE approaches include bayesian optimization Snoek et al. (2012), simulated annealing Ludermir et al. (2006), randomized search Bergstra & Bengio (2012), and genetic algorithms Stanley & Miikkulainen (2002).To their credit, each of these papers provides a case in which the proposed DSE approach produces a NN architecture that achieves higher accuracy compared to a representative baseline.However, these papers make no attempt to provide intuition about the shape of the NN design space.Later in this paper, we eschew automated approaches – instead, we refactor CNNs in such a way that we can do principled A/B comparisons to investigate how CNN architectural decisions influence model size and accuracy.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
Downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map.  If early layers  have large strides, then most layers will have small activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have large activation maps.	Strategy 3. Downsample late in the network so that convolution layers have large activation maps.In a convolutional network, each convolution layer produces an output activation map with a spatial resolution that is at least 1x1 and often much larger than 1x1.The height and width of these activation maps are controlled by: (1) the size of the input data (e.g. 256x256 images) and (2) the choice of layers in which to downsample in the CNN architecture.Most commonly, downsampling is engineered into CNN architectures by setting the (stride > 1) in some of the convolution or pooling layers (e.g. Szegedy et al. (2014); Simonyan & Zisserman (2014); Krizhevsky et al. (2012)).If early333In our terminology, an “early” layer is close to the input data. layers in the network have large strides, then most layers will have small activation maps.Conversely, if most layers in the network have a stride of 1, and the strides greater than 1 are concentrated toward the end444In our terminology, the “end” of the network is the classifier. of the network, then many layers in the network will have large activation maps.Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.Indeed, K. He and H. Sun applied delayed downsampling to four different CNN architectures, and in each case delayed downsampling led to higher classification accuracy He & Sun (2015).	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
As we can see, downsampling aim to collect summary about statistics of different regions of some feature map, and this can be addressed with stride >1 in convolution or pooling layers which affects the size of activation map.  If early layers  have large strides, then most layers will have large activation maps and if most layers in the network have a stride of 1, and the strides greater than 1 are later in the network, then many layers in the network will have small activation maps.	Strategy 3. Downsample late in the network so that convolution layers have large activation maps.In a convolutional network, each convolution layer produces an output activation map with a spatial resolution that is at least 1x1 and often much larger than 1x1.The height and width of these activation maps are controlled by: (1) the size of the input data (e.g. 256x256 images) and (2) the choice of layers in which to downsample in the CNN architecture.Most commonly, downsampling is engineered into CNN architectures by setting the (stride > 1) in some of the convolution or pooling layers (e.g. Szegedy et al. (2014); Simonyan & Zisserman (2014); Krizhevsky et al. (2012)).If early333In our terminology, an “early” layer is close to the input data. layers in the network have large strides, then most layers will have small activation maps.Conversely, if most layers in the network have a stride of 1, and the strides greater than 1 are concentrated toward the end444In our terminology, the “end” of the network is the classifier. of the network, then many layers in the network will have large activation maps.Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.Indeed, K. He and H. Sun applied delayed downsampling to four different CNN architectures, and in each case delayed downsampling led to higher classification accuracy He & Sun (2015).	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Authors used a mix of 1x1 and 3x3 filters in the expand layer of the fire module to reduce the number of parameters while still getting benefits from the desired properties of having reasonable scope of the input receptive field and extracting correlations and useful information by applying the 3*3 filters of the CNN.  To have a small number of parameters in a CNN, we need to decrease the number of input channels to the 3x3 filters and here comes the role of 1*1 filters, while the 3x3 filters are used to capture larger spatial features (Assuming only 3*3 and 1*1 kernels).  This way, the model get its wide fame of achieving a high level of accuracy with fewer parameters than other networks.	Strategy 2. Decrease the number of input channels to 3x3 filters.Consider a convolution layer that is comprised entirely of 3x3 filters.The total quantity of parameters in this layer is (number of input channels) * (number of filters) * (3*3).So, to maintain a small total number of parameters in a CNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), but also to decrease the number of input channels to the 3x3 filters.We decrease the number of input channels to 3x3 filters using squeeze layers, which we describe in the next section. We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Authors used a mix of 1x1 and 3x3 filters in the expand layer of the fire module to increase the number of parameters while still getting benefits from the desired properties of having reasonable scope of the input receptive field and extracting correlations and useful information by applying the 3*3 filters of the CNN.	Strategy 2. Decrease the number of input channels to 3x3 filters.Consider a convolution layer that is comprised entirely of 3x3 filters.The total quantity of parameters in this layer is (number of input channels) * (number of filters) * (3*3).So, to maintain a small total number of parameters in a CNN, it is important not only to decrease the number of 3x3 filters (see Strategy 1 above), but also to decrease the number of input channels to the 3x3 filters.We decrease the number of input channels to 3x3 filters using squeeze layers, which we describe in the next section. We define the Fire module as follows.A Fire module is comprised of: a squeeze convolution layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters; we illustrate this in Figure 1.The liberal use of 1x1 filters in Fire modules is an application of Strategy 1 from Section 3.1.We expose three tunable dimensions (hyperparameters) in a Fire module: s_{1x1}, e_{1x1}, and e_{3x3}.In a Fire module, s_{1x1} is the number of filters in the squeeze layer (all 1x1), e_{1x1} is the number of 1x1 filters in the expand layer, and e_{3x3} is the number of 3x3 filters in the expand layer.When we use Fire modules we set s_{1x1} to be less than (e_{1x1} + e_{3x3}), so the squeeze layer helps to limit the number of input channels to the 3x3 filters, as per Strategy 2 from Section 3.1.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
According to the authors,  the effectiveness of SqueezeNet's model compression does not decrease with a smaller CNN compared to the baseline	In addition, these results demonstrate that Deep Compression Han et al. (2015a) not only works well on CNN architectures with many parameters (e.g. AlexNet and VGG), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture.Deep Compression compressed SqueezeNet by 10×10\times10 × while preserving the baseline accuracy.In summary: by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510×510\times510 × reduction in model size with no decrease in accuracy compared to the baseline.	Correct		Incorrect	Correct	Correct	Incorrect	Correct	Incorrect
According to the authors,  the effectiveness of SqueezeNet's model compression decreases with a smaller CNN compared to the baseline	In addition, these results demonstrate that Deep Compression Han et al. (2015a) not only works well on CNN architectures with many parameters (e.g. AlexNet and VGG), but it is also able to compress the already compact, fully convolutional SqueezeNet architecture.Deep Compression compressed SqueezeNet by 10×10\times10 × while preserving the baseline accuracy.In summary: by combining CNN architectural innovation (SqueezeNet) with state-of-the-art compression techniques (Deep Compression), we achieved a 510×510\times510 × reduction in model size with no decrease in accuracy compared to the baseline.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
size after taking these considerations would be a 0. 66 MB model 363× smaller than 32-bit AlexNet with equivalent accuracy to AlexNet.	It appears that we have surpassed the state-of-the-art results from the model compression community:even when using uncompressed 32-bit values to represent the model, SqueezeNet has a 1.4×1.4\times1.4 × smaller model size than the best efforts from the model compression community while maintaining or exceeding the baseline accuracy.Until now, an open question has been: are small models amenable to compression, or do small models “need” all of the representational power afforded by dense floating-point values?To find out, we applied Deep Compression Han et al. (2015a) to SqueezeNet, using 33% sparsity666Note that, due to the storage overhead of storing sparse matrix indices, 33% sparsity leads to somewhat less than a 3×3\times3 × decrease in model size. and 8-bit quantization.This yields a 0.66 MB model (363×363\times363 × smaller than 32-bit AlexNet) with equivalent accuracy to AlexNet.Further, applying Deep Compression with 6-bit quantization and 33% sparsity on SqueezeNet, we produce a 0.47MB model (510×510\times510 × smaller than 32-bit AlexNet) with equivalent accuracy.Our small model is indeed amenable to compression.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
size after taking these considerations would be a 0.47MB model 363× smaller than 32-bit AlexNet with equivalent accuracy to AlexNet.	It appears that we have surpassed the state-of-the-art results from the model compression community:even when using uncompressed 32-bit values to represent the model, SqueezeNet has a 1.4×1.4\times1.4 × smaller model size than the best efforts from the model compression community while maintaining or exceeding the baseline accuracy.Until now, an open question has been: are small models amenable to compression, or do small models “need” all of the representational power afforded by dense floating-point values?To find out, we applied Deep Compression Han et al. (2015a) to SqueezeNet, using 33% sparsity666Note that, due to the storage overhead of storing sparse matrix indices, 33% sparsity leads to somewhat less than a 3×3\times3 × decrease in model size. and 8-bit quantization.This yields a 0.66 MB model (363×363\times363 × smaller than 32-bit AlexNet) with equivalent accuracy to AlexNet.Further, applying Deep Compression with 6-bit quantization and 33% sparsity on SqueezeNet, we produce a 0.47MB model (510×510\times510 × smaller than 32-bit AlexNet) with equivalent accuracy.Our small model is indeed amenable to compression.	Incorrect	Change number	Correct	Correct	Correct	Correct	Incorrect	Incorrect
To investigate the effect of the squeeze ratio on model size, models were trained from scratch so that one can make comparisons for these separate models.	In these experiments, we use SqueezeNet (Figure 2) as a starting point.As in SqueezeNet, these experiments use the following metaparameters: base_{e}=128, incr_{e}=128, pct_{3x3}=0.5, and freq=2.We train multiple models, where each model has a different squeeze ratio (SR)777Note that, for a given model, all Fire layers share the same squeeze ratio. in the range [0.125, 1.0].In Figure 3(a), we show the results of this experiment, where each point on the graph is an independent model that was trained from scratch.SqueezeNet is the SR=0.125 point in this figure.888Note that we named it SqueezeNet because it has a low squeeze ratio (SR). That is, the squeeze layers in SqueezeNet have 0.125x the number of filters as the expand layers.From this figure, we learn that increasing SR beyond 0.125 can further increase ImageNet top-5 accuracy from 80.3% (i.e. AlexNet-level) with a 4.8MB model to 86.0% with a 19MB model.Accuracy plateaus at 86.0% with SR=0.75 (a 19MB model), and setting SR=1.0 further increases model size without improving accuracy.	Correct		Incorrect	Correct	Correct	Correct	Incorrect	Correct
To investigate the effect of the squeeze ratio on model size, models were fine-tuned so that one can make comparisons for these separate models.	In these experiments, we use SqueezeNet (Figure 2) as a starting point.As in SqueezeNet, these experiments use the following metaparameters: base_{e}=128, incr_{e}=128, pct_{3x3}=0.5, and freq=2.We train multiple models, where each model has a different squeeze ratio (SR)777Note that, for a given model, all Fire layers share the same squeeze ratio. in the range [0.125, 1.0].In Figure 3(a), we show the results of this experiment, where each point on the graph is an independent model that was trained from scratch.SqueezeNet is the SR=0.125 point in this figure.888Note that we named it SqueezeNet because it has a low squeeze ratio (SR). That is, the squeeze layers in SqueezeNet have 0.125x the number of filters as the expand layers.From this figure, we learn that increasing SR beyond 0.125 can further increase ImageNet top-5 accuracy from 80.3% (i.e. AlexNet-level) with a 4.8MB model to 86.0% with a 19MB model.Accuracy plateaus at 86.0% with SR=0.75 (a 19MB model), and setting SR=1.0 further increases model size without improving accuracy.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
This was one of the experimental investigations that was interesting.  Also, complex bypass adds more parameters which increases the number of parameters trained for the same task on the same data(it may have some small overfitting side effect ).	The choice of connections across multiple layers or modules is an emerging area of CNN macroarchitectural research.Residual Networks (ResNet) He et al. (2015b) and Highway Networks Srivastava et al. (2015) each propose the use of connections that skip over multiple layers, for example additively connecting the activations from layer 3 to the activations from layer 6.We refer to these connections as bypass connections.The authors of ResNet provide an A/B comparison of a 34-layer CNN with and without bypass connections; adding bypass connections delivers a 2 percentage-point improvement on Top-5 ImageNet accuracy.	Correct		Incorrect	Unrelated	Incorrect	Unrelated	Incorrect	Incorrect
This was one of the experimental investigations that was interesting.  Also, complex bypass adds less parameters which decreases the number of parameters trained for the same task on the same data(it may have some small overfitting side effect ).	The choice of connections across multiple layers or modules is an emerging area of CNN macroarchitectural research.Residual Networks (ResNet) He et al. (2015b) and Highway Networks Srivastava et al. (2015) each propose the use of connections that skip over multiple layers, for example additively connecting the activations from layer 3 to the activations from layer 6.We refer to these connections as bypass connections.The authors of ResNet provide an A/B comparison of a 34-layer CNN with and without bypass connections; adding bypass connections delivers a 2 percentage-point improvement on Top-5 ImageNet accuracy.	Incorrect	Opposite	Incorrect	Unrelated	Incorrect	Unrelated	Incorrect	Incorrect
The authors they tried multiples of the initial anchor sizes specified by the 9 clusters.  The clusters as specified at the cell D58.	Anchor box x,y offset predictions. We tried using the normal anchor box prediction mechanism where you predict the x,y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn’t work very well.Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, t_{x}, t_{y}, t_{w}, t_{h}. If the cell is offset from the top left corner of the image by (c_{x},c_{y}) and the bounding box prior has width and height p_{w}, p_{h}, then the predictions correspond to:	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors they tried multiples of the initial anchor sizes specified by the 4 clusters.  The clusters as specified at the cell D58.	Anchor box x,y offset predictions. We tried using the normal anchor box prediction mechanism where you predict the x,y offset as a multiple of the box width or height using a linear activation. We found this formulation decreased model stability and didn’t work very well.Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15]. The network predicts 4 coordinates for each bounding box, t_{x}, t_{y}, t_{w}, t_{h}. If the cell is offset from the top left corner of the image by (c_{x},c_{y}) and the bounding box prior has width and height p_{w}, p_{h}, then the predictions correspond to:	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
residuals = skip connections, which means that DarkNet-53 uses skip connections.	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!	Correct		Correct	Incorrect	Correct	Incorrect	Unrelated	Correct
No skip connections were used in DarkNet-53.	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
YOLOv3 performs poorly because of struggles to get the perfect bounding box alignment with the objects.	However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
RetinaNet performs poorly because ot struggles to get the perfect bounding box alignment with the objects.	However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.	Incorrect	Change concept	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors hypothesize that YOLOv3 may already be robust to the problem which the focal loss is trying to solve because it has spearate objectness predictions and conditional class predictions.  That is why adding the focal loss did not improve the performance of YOLOv3.	Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren’t totally sure.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The authors hypothesize that YOLOv3 may already be robust to the problem which the focal loss is trying to solve because it does not have spearate objectness predictions and conditional class predictions.  That is why adding the focal loss did not improve the performance of YOLOv3.	Focal loss. We tried using focal loss. It dropped our mAP about 2 points. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the class predictions? Or something? We aren’t totally sure.	Incorrect	Opposite	Incorrect	Correct	Incorrect	Incorrect	Incorrect	Incorrect
A sarcastic comment means a concern for authors that Google, Facebook, and similar corporations use these kind of models to harvest and use our personal information.  A similar sarcastic comment regarding military.  The authors should be responsible for their work and consider possible consequences to the world.	But maybe a better question is: “What are we going to do with these detectors now that we have them?” A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and definitely won’t be used to harvest your personal information and sell it to…. wait, you’re saying that’s exactly what it will be used for?? Oh.Well the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait…..111The author is funded by the Office of Naval Research and Google.I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. We owe the world that much.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
A sarcastic comment means a concern for authors that Amazon, Apple, and similar corporations use these kind of models to harvest and use our personal information. A similar sarcastic comment regarding the government. The authors should be responsible for their work and consider possible consequences to the world.	But maybe a better question is: “What are we going to do with these detectors now that we have them?” A lot of the people doing this research are at Google and Facebook. I guess at least we know the technology is in good hands and definitely won’t be used to harvest your personal information and sell it to…. wait, you’re saying that’s exactly what it will be used for?? Oh.Well the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait…..111The author is funded by the Office of Naval Research and Google.I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park [13], or tracking their cat as it wanders around their house [19]. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. We owe the world that much.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Correct
Some of the limitations of YOLOv3, based on the information given in the paper are: it is still quite a bit behind other models like RetinaNet in the "COCO's weired average mAP" metric (COCO average AP between 95 IOU metric), performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object, it has comparatively worse performance on medium and larger size objects. 	YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3×3\times3 × faster. It is still quite a bit behind other models like RetinaNet in this metric though.However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.YOLOv3 is a good detector. It’s fast, it’s accurate. It’s not as great on the COCO average AP between .5 and .95 IOU metric. But it’s very good on the old detection metric of .5 IOU.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Some of the limitations of YOLOv3, based on the information given in the paper are: it is still quite a bit behind other models like RetinaNet in the "COCO's weired average mAP" metric (COCO average AP between 95 IOU metric), performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object, it has comparatively worse performance on medium and larger size objects, and it fails completely on objects smaller than 10x10 pixels.	YOLOv3 is pretty good! See table 3. In terms of COCOs weird average mean AP metric it is on par with the SSD variants but is 3×3\times3 × faster. It is still quite a bit behind other models like RetinaNet in this metric though.However, when we look at the “old” detection metric of mAP at IOU=.5 (or AP{}_{50} in the chart) YOLOv3 is very strong. It is almost on par with RetinaNet and far above the SSD variants. This indicates that YOLOv3 is a very strong detector that excels at producing decent boxes for objects. However, performance drops significantly as the IOU threshold increases indicating YOLOv3 struggles to get the boxes perfectly aligned with the object.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.YOLOv3 is a good detector. It’s fast, it’s accurate. It’s not as great on the COCO average AP between .5 and .95 IOU metric. But it’s very good on the old detection metric of .5 IOU.	Incorrect	Invent something doesn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Darknet-53 is better than ResNet-101 and 1. 5\times1. 5 × faster.	Each network is trained with identical settings and tested at 256\times 256, single crop accuracy. Run times are measured on a Titan X at 256\times 256. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5×1.5\times1.5 × faster. Darknet-53 has similar performance to ResNet-152 and is 2×2\times2 × faster.Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That’s mostly because ResNets have just way too many layers and aren’t very efficient.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
ResNet-101 is better than Darknet-53 and 1. 5\times1. 5 × faster.	Each network is trained with identical settings and tested at 256\times 256, single crop accuracy. Run times are measured on a Titan X at 256\times 256. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5×1.5\times1.5 × faster. Darknet-53 has similar performance to ResNet-152 and is 2×2\times2 × faster.Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That’s mostly because ResNets have just way too many layers and aren’t very efficient.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors use binary cross-entropy loss.	Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The authors use Softmax over all class probabilities.	Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	Incorrect	Change concept	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
YOLOv3 now struggles more with medium and larger size objects, i. , performs worse than before.  On the other hand, it is more succesful for smaller objects.	In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
YOLOv3 now struggles more with smaller objects, i. , performs worse than before.  On the other hand, it is more succesful for medium and larger size objects.	In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
YOLOv3 is faster and better than YOLO.  It has more layers.  The authors also tried some small tricks and experiments which further improved the overall performance.	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!When we plot accuracy vs speed on the AP{}_{50} metric (see figure 5) we see YOLOv3 has significant benefits over other detection systems. Namely, it’s faster and better.So here’s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that’s better than the other ones. We’ll just take you through the whole system from scratch so you can understand it all.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
YOLOv3 is faster and better than YOLO. It has significantly more (over 50) layers. The authors also tried some small tricks and experiments which further improved the overall performance.	We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3\times 3 and 1\times 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it…. wait for it….. Darknet-53!When we plot accuracy vs speed on the AP{}_{50} metric (see figure 5) we see YOLOv3 has significant benefits over other detection systems. Namely, it’s faster and better.So here’s the deal with YOLOv3: We mostly took good ideas from other people. We also trained a new classifier network that’s better than the other ones. We’ll just take you through the whole system from scratch so you can understand it all.	Incorrect	Change number	Correct	Correct	Correct	Correct	Incorrect	Correct
By using multi-scaled prediction, YOLOv3 has improved performance for small objects.  Also, the subsequent scales benefit from previous scales and the previous features from earlier layers.	We perform the same design one more time to predict boxes for the final scale. Thus our predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
By using multi-scaled prediction, YOLOv3 has improved performance for medium and larger size objects.  Also, the subsequent scales benefit from previous scales and the previous features from earlier layers.	We perform the same design one more time to predict boxes for the final scale. Thus our predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new multi-scale predictions we see YOLOv3 has relatively high AP{}_{S} performance. However, it has comparatively worse performance on medium and larger size objects. More investigation is needed to get to the bottom of this.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Binary cross-entropy is used for the class predictions.  Logistic activation is used and is better than the linear activation.	Linear x,y predictions instead of logistic. We tried using a linear activation to directly predict the x,y offset instead of the logistic activation. This led to a couple point drop in mAP.Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	Correct		Correct	Correct	Correct	Correct	Incorrect	Incorrect
Binary cross-entropy is used for the class predictions. Logistic activation is used and is better than the linear activation, as it prevents gradient vanishing during training.	Linear x,y predictions instead of logistic. We tried using a linear activation to directly predict the x,y offset instead of the logistic activation. This led to a couple point drop in mAP.Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.	Incorrect	Invent something doesn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
5 different types of experiments are performed to test the proposed models.  They are Generalization over time scales, Experiments on MNIST, Experiments on Natural Image Patches, Out-of-domain Inputs, and Visualizing Features.	Generalization over time scalesIn the next experiment, we test if the model can work at time scales that aredifferent than what it was trained on. We take a one hidden layer unconditionedComposite Model trained on moving MNIST digits. The model has 2048 LSTM unitsand looks at a 64 \times 64 input. It was trained on input sequences of 10frames to reconstruct those 10 frames as well as predict 10 frames into thefuture. In order to test if the future predictor is able to generalize beyond 10frames, we let the model run for 100 steps into the future.Fig. 7(a) shows the pattern of activity in the LSTM units of thefuture predictorpathway for a randomly chosen test input. It shows the activity at each of thethree sigmoidal gates (input, forget, output), the input (after the tanhnon-linearity, before being multiplied by the input gate), the cell state andthe final output (after being multiplied by the output gate). Even though theunits are ordered randomly along the vertical axis, we can see that the dynamicshas a periodic quality to it. The model is able to generate persistent motionfor long periods of time. In terms of reconstruction, the model only outputsblobs after the first 15 frames, but the motion is relatively well preserved.More results, including long range future predictions over hundreds of time steps can see been athttp://www.cs.toronto.edu/~nitish/unsupervised_video.To show that setting up a periodic behaviour is not trivial,Fig. 7(b) shows the activity from a randomly initialized futurepredictor. Here, the LSTM state quickly converges and the outputs blur completely.Experiments on MNIST; We first trained our models on a dataset of moving MNIST digits. In this dataset, each video was 20 frames long and consisted of two digits moving inside a 64 × 64 patch. The digits were chosen randomly from the training set and placed initially at random locations inside the patch.Experiments on Natural Image Patches; Next, we tried to see if our models can also work with natural image patches. For this, we trained the models on sequences of 32 × 32 natural image patches extracted from the UCF-101 dataset. In this case, we used linear output units and the squared error loss function.Out-of-domain Inputs; Next, we test this model’s ability to deal with out-of domain inputs. For this, we test the model on sequences of one and three moving digits. The model was trained on sequences of two moving digits, so it has never seen inputs with just one digit or three digits.Visualizing Features; Next, we visualize the features learned by this model. Fig. 9 shows the weights that connect each input frame to the encoder LSTM. There are four sets of weights. One set of weights connects the frame to the input units. There are three other sets, one corresponding to each of the three gates (input, forget and output). Each weight has a size of 64 × 64.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
4 different types of experiments are performed to test the proposed models. They are Generalization over time scales, Experiments on MNIST, Experiments on Natural Image Patches, and Visualizing Features.	Generalization over time scalesIn the next experiment, we test if the model can work at time scales that aredifferent than what it was trained on. We take a one hidden layer unconditionedComposite Model trained on moving MNIST digits. The model has 2048 LSTM unitsand looks at a 64 \times 64 input. It was trained on input sequences of 10frames to reconstruct those 10 frames as well as predict 10 frames into thefuture. In order to test if the future predictor is able to generalize beyond 10frames, we let the model run for 100 steps into the future.Fig. 7(a) shows the pattern of activity in the LSTM units of thefuture predictorpathway for a randomly chosen test input. It shows the activity at each of thethree sigmoidal gates (input, forget, output), the input (after the tanhnon-linearity, before being multiplied by the input gate), the cell state andthe final output (after being multiplied by the output gate). Even though theunits are ordered randomly along the vertical axis, we can see that the dynamicshas a periodic quality to it. The model is able to generate persistent motionfor long periods of time. In terms of reconstruction, the model only outputsblobs after the first 15 frames, but the motion is relatively well preserved.More results, including long range future predictions over hundreds of time steps can see been athttp://www.cs.toronto.edu/~nitish/unsupervised_video.To show that setting up a periodic behaviour is not trivial,Fig. 7(b) shows the activity from a randomly initialized futurepredictor. Here, the LSTM state quickly converges and the outputs blur completely.Experiments on MNIST; We first trained our models on a dataset of moving MNIST digits. In this dataset, each video was 20 frames long and consisted of two digits moving inside a 64 × 64 patch. The digits were chosen randomly from the training set and placed initially at random locations inside the patch.Experiments on Natural Image Patches; Next, we tried to see if our models can also work with natural image patches. For this, we trained the models on sequences of 32 × 32 natural image patches extracted from the UCF-101 dataset. In this case, we used linear output units and the squared error loss function.Out-of-domain Inputs; Next, we test this model’s ability to deal with out-of domain inputs. For this, we test the model on sequences of one and three moving digits. The model was trained on sequences of two moving digits, so it has never seen inputs with just one digit or three digits.Visualizing Features; Next, we visualize the features learned by this model. Fig. 9 shows the weights that connect each input frame to the encoder LSTM. There are four sets of weights. One set of weights connects the frame to the input units. There are three other sets, one corresponding to each of the three gates (input, forget and output). Each weight has a size of 64 × 64.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Future Predictor, Composite Model, Conditional Future Predictor, Composite Model with Conditional Future Predictor are the variants of LSTM encoder-decoder models are used in this study.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.Future prediction results are summarized in Table 2. For MNIST we compute the cross entropy of the predictions with respect to the ground truth, both of which are 64 × 64 patches. For natural image patches, we compute the squared loss. We see that the Composite Model always does a better job of predicting the future compared to the Future Predictor. This indicates that having the autoencoder along with the future predictor to force the model to remember more about the inputs actually helps predict the future better. Next, we can compare each model with its conditional variant. Here, we find that the conditional models perform better, as was also noted in Fig. 5.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Future Predictor, Composite Model, Conditional Future Predictor, Composite Model with Conditional Future Predictor are the variants of GRU encoder-decoder models are used in this study.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.Future prediction results are summarized in Table 2. For MNIST we compute the cross entropy of the predictions with respect to the ground truth, both of which are 64 × 64 patches. For natural image patches, we compute the squared loss. We see that the Composite Model always does a better job of predicting the future compared to the Future Predictor. This indicates that having the autoencoder along with the future predictor to force the model to remember more about the inputs actually helps predict the future better. Next, we can compare each model with its conditional variant. Here, we find that the conditional models perform better, as was also noted in Fig. 5.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The supervised task is action recognition and unsupervised tasks are representation reconstruction, which can be inferred from P4.	In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitative evaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	Correct		Incorrect	Incorrect	Correct	Incorrect	Unrelated	Incorrect
The supervised task is representation reconstruction and unsupervised tasks are action recognition, which can be inferred from P4.	In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitative evaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Recurrent neural networks using the Long Short Term Memory (LSTM) architectures have been used for supervised sequence learning tasks.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Recurrent neural networks using the Gated Recurrent Unit (GRU) architectures have been used for supervised sequence learning tasks.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Since LSTM based encoder/decoder method successfully worked for real time sequential nature application, it is a good method.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Since LSTM based encoder/decoder method struggled with real time sequential nature application, it is not a suitable method.	Understanding temporal sequences is important for solving many problems in theAI-set. Recently, recurrent neural networks using the Long Short Term Memory(LSTM) architecture (Hochreiter & Schmidhuber, 1997) have been used successfully to perform various supervisedsequence learning tasks, such as speech recognition (Graves & Jaitly, 2014), machinetranslation (Sutskever et al., 2014; Cho et al., 2014), and caption generation for images(Vinyals et al., 2014). They have also been applied on videos for recognizingactions and generating natural language descriptions (Donahue et al., 2014). Ageneral sequence to sequence learning framework was described by Sutskever et al. (2014)in which a recurrent network is used to encode a sequence into a fixed lengthrepresentation, and then another recurrent network is used to decode a sequenceout of that representation. In this work, we apply and extend this framework tolearn representations of sequences of images. We choose to work in theunsupervised setting where we only have access to a dataset of unlabelledvideos.	Incorrect	Opposite	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
The authors extended identical LSTM classifier framework as baseline to learn representation of image sequences.	The baseline for comparing these models is an identical LSTM classifier but with randomly initialized weights. All classifiers used dropout regularization, where we dropped activations as they were communicated across layers but not through time within the same LSTM as proposed in Zaremba et al. (2014). We emphasize that this is a very strong baseline and does significantly better than just using single frames. Using dropout was crucial in order to train good baseline models especially with very few training examples.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors extended identical LSTM classifier framework as baseline to learn representation of image sequences, which was then fine-tuned on a larger, unlabeled dataset.	The baseline for comparing these models is an identical LSTM classifier but with randomly initialized weights. All classifiers used dropout regularization, where we dropped activations as they were communicated across layers but not through time within the same LSTM as proposed in Zaremba et al. (2014). We emphasize that this is a very strong baseline and does significantly better than just using single frames. Using dropout was crucial in order to train good baseline models especially with very few training examples.	Incorrect	Invent something doesn't mentioned	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Labelling videos is a tedious job and that makes supervise training very expensive.  Compare to that unsupervised model can take advantage of all videos available that doesn't need labelling.  That's why the authors prefer to learn video representations through unsupervised models.	Supervised learning has been extremely successful in learning good visualrepresentations that not only produce good results at the task they are trainedfor, but also transfer well to other tasks and datasets. Therefore, it isnatural to extend the same approach to learning video representations. This hasled to research in 3D convolutional nets (Ji et al., 2013; Tran et al., 2014), different temporalfusion strategies (Karpathy et al., 2014) and exploring different ways ofpresenting visual information to convolutional nets (Simonyan & Zisserman, 2014a).However, videos are much higher dimensional entities compared to single images.Therefore, it becomes increasingly difficult to do credit assignment and learn longrange structure, unless we collect much more labelled data or do a lot offeature engineering (for example computing the right kinds of flow features) tokeep the dimensionality low. The costly work of collecting more labelled dataand the tedious work of doing more clever engineering can go a long way insolving particular problems, but this is ultimately unsatisfying as a machinelearning solution. This highlights the need for using unsupervised learning tofind and represent structure in videos. Moreover, videos have a lot ofstructure in them (spatial and temporal regularities) which makes themparticularly well suited as a domain for building unsupervised learning models.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Labelling videos is a tedious job and that makes supervise training very expensive. Compare to that self-supervised model can take advantage of all videos available that doesn't need labelling. That's why the authors prefer to learn video representations through self-supervised models.	Supervised learning has been extremely successful in learning good visualrepresentations that not only produce good results at the task they are trainedfor, but also transfer well to other tasks and datasets. Therefore, it isnatural to extend the same approach to learning video representations. This hasled to research in 3D convolutional nets (Ji et al., 2013; Tran et al., 2014), different temporalfusion strategies (Karpathy et al., 2014) and exploring different ways ofpresenting visual information to convolutional nets (Simonyan & Zisserman, 2014a).However, videos are much higher dimensional entities compared to single images.Therefore, it becomes increasingly difficult to do credit assignment and learn longrange structure, unless we collect much more labelled data or do a lot offeature engineering (for example computing the right kinds of flow features) tokeep the dimensionality low. The costly work of collecting more labelled dataand the tedious work of doing more clever engineering can go a long way insolving particular problems, but this is ultimately unsatisfying as a machinelearning solution. This highlights the need for using unsupervised learning tofind and represent structure in videos. Moreover, videos have a lot ofstructure in them (spatial and temporal regularities) which makes themparticularly well suited as a domain for building unsupervised learning models.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Image patches and high-level percepts are the two types of inputs used in the proposed model.	The inputs to the model can, in principle, be any representation of individualvideo frames. However, for the purposes of this work, we limit our attention to two kinds of inputs. The first is image patches. For this we use natural imagepatches as well as a dataset of moving MNIST digits. The second ishigh-level “percepts” extracted by applying a convolutional net trained onImageNet. These percepts are the states of last (and/or second-to-last) layers ofrectified linear hidden states from a convolutional neural net model.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Image patches and low-level features are the two types of inputs used in the proposed model.	The inputs to the model can, in principle, be any representation of individualvideo frames. However, for the purposes of this work, we limit our attention to two kinds of inputs. The first is image patches. For this we use natural imagepatches as well as a dataset of moving MNIST digits. The second ishigh-level “percepts” extracted by applying a convolutional net trained onImageNet. These percepts are the states of last (and/or second-to-last) layers ofrectified linear hidden states from a convolutional neural net model.	Incorrect	Change concept	Correct	Unrelated	Correct	Incorrect	Incorrect	Incorrect
Since LSTM based auto-encoder models control the learning an identity mapping, it forced learn good features.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used to decode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Since LSTM based auto-encoder models fail to control the learning of an identity mapping, it prevented them from learning good features.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used to decode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	Incorrect	Opposite	Correct	Correct	Correct	Correct	Incorrect	Correct
The two factors that control the model from learning an identity mapping or prevent overfitting are fixed number of hidden units and forceful decode of the input representation recursively.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used todecode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The two factors that control the model from learning an identity mapping or prevent overfitting are a dynamic hidden unit allocation and forceful decode of the input representation recursively.	Why should this learn good features?The state of the encoder LSTM after the last input has been read is therepresentation of the input video. The decoder LSTM is being asked toreconstruct back the input sequence from this representation. In order to do so,the representation must retain information about the appearance of the objectsand the background as well as the motion contained in the video.However, an important question for any autoencoder-style model is what preventsit from learning an identity mapping and effectively copying the input to theoutput. In that case all the information about the input would still be presentbut the representation will be no better than the input. There are two factorsthat control this behaviour. First, the fact that there are only a fixed numberof hidden units makes it unlikely that the model can learn trivial mappings forarbitrary length input sequences. Second, the same LSTM operation is used todecode the representation recursively. This means that the same dynamics must beapplied on the representation at any stage of decoding. This further preventsthe model from learning an identity mapping.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Correct	Correct
Ranzato model predict only the next frame but LSTM future predictor model predicts a long sequence into the future.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
LSTM future predictor model predict only the next frame but Ranzato model predicts a long sequence into the future.	Another natural unsupervised learning task for sequences is predicting thefuture. This is the approach used in language models for modeling sequences ofwords. The design of the Future Predictor Model is same as that of theAutoencoder Model, except that the decoder LSTM in this case predicts frames ofthe video that come after the input sequence (Fig. 3).Ranzato et al. (2014) use a similar model but predict only the next frame at eachtime step. This model, on the other hand, predicts a long sequence into thefuture. Here again we can consider two variants of the decoder – conditionaland unconditioned.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Correct	Incorrect
Since representation is another form of input, that's why it doesn't need label for any purpose.	When designing any unsupervised learning model, it is crucial to have the right inductive biases and choose the right objective function so that the learning signal points the model towards learning useful features. In this paper, we use the LSTM Encoder-Decoder framework to learn video representations. The key inductive bias here is that the same operation must be applied at each time step to propagate information to the next step. This enforces the fact that the physics of the world remains the same, irrespective of input. The same physics acting on any state, at any time, must produce the next state. Our model works as follows. The Encoder LSTM runs through a sequence of frames to come up with a representation. This representation is then decoded through another LSTM to produce a target sequence. We consider different choices of the target sequence. One choice is to predict the same sequence as the input. The motivation is similar to that of autoencoders – we wish to capture all that is needed to reproduce the input but at the same time go through the inductive biases imposed by the model. Another option is to predict the future frames. Here the motivation is to learn a representation that extracts all that is needed to extrapolate the motion and appearance beyond what has been observed. These two natural choices can also be combined. In this case, there are two decoder LSTMs – one that decodes the representation into the input sequence and another that decodes the same representation to predict the future.	Correct		Incorrect	Unrelated	Correct	Unrelated	Incorrect	Incorrect
Since the model is trained using a self-supervised objective, that's why it doesn't need labels for its training purpose.	When designing any unsupervised learning model, it is crucial to have the right inductive biases and choose the right objective function so that the learning signal points the model towards learning useful features. In this paper, we use the LSTM Encoder-Decoder framework to learn video representations. The key inductive bias here is that the same operation must be applied at each time step to propagate information to the next step. This enforces the fact that the physics of the world remains the same, irrespective of input. The same physics acting on any state, at any time, must produce the next state. Our model works as follows. The Encoder LSTM runs through a sequence of frames to come up with a representation. This representation is then decoded through another LSTM to produce a target sequence. We consider different choices of the target sequence. One choice is to predict the same sequence as the input. The motivation is similar to that of autoencoders – we wish to capture all that is needed to reproduce the input but at the same time go through the inductive biases imposed by the model. Another option is to predict the future frames. Here the motivation is to learn a representation that extracts all that is needed to extrapolate the motion and appearance beyond what has been observed. These two natural choices can also be combined. In this case, there are two decoder LSTMs – one that decodes the representation into the input sequence and another that decodes the same representation to predict the future.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Unrelated	Incorrect	Incorrect
Since in conditional decoder have access to last few frames, often it find a easy way to pick up a correlated frame but not necessary an optimized one.  That is why it is difficult to optimize.	There is also an argument against using a conditional decoder from theoptimization point-of-view. There are strong short-range correlations invideo data, for example, most of the content of a frame is same as the previousone. If the decoder was given access to the last few frames while generating aparticular frame at training time, it would find it easy to pick up on thesecorrelations. There would only be a very small gradient that tries to fix up theextremely subtle errors that require long term knowledge about the inputsequence. In an unconditioned decoder, this input is removed and the model isforced to look for information deep inside the encoder.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Since in conditional decoder have access to last few frames, often it find a difficult way to pick up a correlated frame but an optimized one. That is why it is easy to optimize.	There is also an argument against using a conditional decoder from theoptimization point-of-view. There are strong short-range correlations invideo data, for example, most of the content of a frame is same as the previousone. If the decoder was given access to the last few frames while generating aparticular frame at training time, it would find it easy to pick up on thesecorrelations. There would only be a very small gradient that tries to fix up theextremely subtle errors that require long term knowledge about the inputsequence. In an unconditioned decoder, this input is removed and the model isforced to look for information deep inside the encoder.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The author talk both advantage and disadvantage of conditional and unconditional decoder blocks.  They also provided a strong argument in favor of using a conditional decoder but clearly no winner is mentioned.	For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The author talk both advantage and disadvantage of conditional and unconditional decoder blocks.  They also provided a strong argument in favor of using an unconditional decoder.	For each of these two models, we can consider two possibilities - one in whichthe decoder LSTM is conditioned on the last generated frame and the other inwhich it is not. In the experimental section, we explore these choicesquantitatively. Here we briefly discuss arguments for and against a conditionaldecoder. A strong argument in favour of using a conditional decoder is that itallows the decoder to model multiple modes in the target sequence distribution.Without that, we would end up averaging the multiple modes in the low-levelinput space. However, this is an issue only if we expect multiple modes in thetarget sequence distribution. For the LSTM Autoencoder, there is only onecorrect target and hence a unimodal target distribution. But for the LSTM FuturePredictor there is a possibility of multiple targets given an input because evenif we assume a deterministic universe, everything needed to predict the futurewill not necessarily be observed in the input.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Correct
UCF-101 and HMDB-51 datasets are used for supervised learning.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of 6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips. Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the superviseddatasets (UCF-101 and HMDB-51) for unsupervised training. However, we found thatusing them did not give any significant advantage over just using the YouTubevideos.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Kinetics-400 and HMDB-51 datasets are used for supervised learning.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of 6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips. Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the superviseddatasets (UCF-101 and HMDB-51) for unsupervised training. However, we found thatusing them did not give any significant advantage over just using the YouTubevideos.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
UCF-101, HMDB-51 and YouTube videos datasets are used for supervised learning.	To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips.Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the supervised datasets (UCF-101 and HMDB-51) for unsupervised training. However, we found that using them did not give any significant advantage over just using the YouTubevideos.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
YouTube videos datasets are used for supervised learning.	To train the unsupervised models, we used a subset of the Sports-1M dataset(Karpathy et al., 2014), that contains 1 million YouTube clips.Even though this dataset is labelled for actions, we didnot do any supervised experiments on it because of logistical constraints withworking with such a huge dataset. We instead collected 300 hours of video byrandomly sampling 10 second clips from the dataset. It is possible to collectbetter samples if instead of choosing randomly, we extracted videos where a lot ofmotion is happening and where there are no shot boundaries. However, we did notdo so in the spirit of unsupervised learning, and because we did not want tointroduce any unnatural bias in the samples. We also used the supervised datasets (UCF-101 and HMDB-51) for unsupervised training. However, we found that using them did not give any significant advantage over just using the YouTubevideos.	Incorrect	Change concept	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
The UCF-101 dataset contains 13,320 videos with an average length of 6. 2 seconds.  The HMDB-51 dataset contains 5100 videos with mean length of the videos is 3. 2 seconds.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The UCF-101 dataset contains 13,320 videos with an average length of 3.2 seconds.  The HMDB-51 dataset contains 5100 videos with mean length of the videos is 6.2 seconds.	We use the UCF-101 and HMDB-51 datasets for supervised tasks.The UCF-101 dataset (Soomro et al., 2012) contains 13,320 videos with an average length of6.2 seconds belonging to 101 different action categories. The dataset has 3standard train/test splits with the training set containing around 9,500 videosin each split (the rest are test).The HMDB-51 dataset (Kuehne et al., 2011) contains 5100 videos belonging to 51 differentaction categories. Mean length of the videos is 3.2 seconds. This also has 3train/test splits with 3570 videos in the training set and rest in test.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The improvement in classification by using unsupervised learning was not as big as we expected, we still managed to yield an additional improvement over a strong baseline.  If the unsupervised learning model comes up with useful representations then the classifier perform better, especially when there are only a few labelled examples.  Based on the above evidence, it can be safely said that features learned by unsupervised learning improved the performance of supervised learning tasks.	Fig. 12 compares three models - single frame classifier(logistic regression), baseline LSTM classifier and the LSTM classifierinitialized with weights from the Composite Model as the number of labelledvideos per class is varied. Note that having one labelled video means havingmany labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. Forexample, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get aconsiderable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification byusing unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenuesfor improvements later.Next, we compare the models using performance on a supervised task.Table 3 shows the performance on actionrecognition achieved by finetuning different unsupervised learning models.Besides running the experiments on the full UCF-101 and HMDB-51 datasets, we also ran theexperiments on small subsets of these to better highlight the case where we havevery few training examples. We find that all unsupervised models improve over thebaseline LSTM which is itself well-regularized by using dropout. The Autoencodermodel seems to perform consistently better than the Future Predictor. TheComposite model which combines the two does better than either one alone.Conditioning on the generated inputs does not seem to give a clearadvantage over not doing so. The Composite Model with a conditional futurepredictor works the best, although its performance is almost same as that of theComposite Model.We proposed models based on LSTMs that can learn good video representations. Wecompared them and analyzed their properties through visualizations. Moreover, wemanaged to get an improvement on supervised tasks. The best performing model wasthe Composite Model that combined an autoencoder and a future predictor.Conditioning on generated outputs did not have a significant impact on theperformance for supervised tasks, however it made the future predictions lookslightly better. The model was able to persistently generate motion well beyondthe time scales it was trained for. However, it lost the precise object featuresrapidly after the training time scale. The features at the input and outputlayers were found to have some interesting properties.In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitativeevaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The features learned by unsupervised learning did not improve the performance of supervised learning tasks.	Fig. 12 compares three models - single frame classifier(logistic regression), baseline LSTM classifier and the LSTM classifierinitialized with weights from the Composite Model as the number of labelledvideos per class is varied. Note that having one labelled video means havingmany labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. Forexample, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get aconsiderable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification byusing unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenuesfor improvements later.Next, we compare the models using performance on a supervised task.Table 3 shows the performance on actionrecognition achieved by finetuning different unsupervised learning models.Besides running the experiments on the full UCF-101 and HMDB-51 datasets, we also ran theexperiments on small subsets of these to better highlight the case where we havevery few training examples. We find that all unsupervised models improve over thebaseline LSTM which is itself well-regularized by using dropout. The Autoencodermodel seems to perform consistently better than the Future Predictor. TheComposite model which combines the two does better than either one alone.Conditioning on the generated inputs does not seem to give a clearadvantage over not doing so. The Composite Model with a conditional futurepredictor works the best, although its performance is almost same as that of theComposite Model.We proposed models based on LSTMs that can learn good video representations. Wecompared them and analyzed their properties through visualizations. Moreover, wemanaged to get an improvement on supervised tasks. The best performing model wasthe Composite Model that combined an autoencoder and a future predictor.Conditioning on generated outputs did not have a significant impact on theperformance for supervised tasks, however it made the future predictions lookslightly better. The model was able to persistently generate motion well beyondthe time scales it was trained for. However, it lost the precise object featuresrapidly after the training time scale. The features at the input and outputlayers were found to have some interesting properties.In order to evaluate the learned representations we qualitatively analyze thereconstructions and predictions made by the model. For a more quantitativeevaluation, we use these LSTMs as initializations for the supervised task ofaction recognition. If the unsupervised learning model comes up with usefulrepresentations then the classifier should be able to perform better, especiallywhen there are only a few labelled examples. We find that this is indeed thecase.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
As the number of training videos increases the performance of supervised and unsupervised tasks increases.	Fig. 12 compares three models - single frame classifier (logistic regression), baseline LSTM classifier and the LSTM classifier initialized with weights from the Composite Model as the number of labelled videos per class is varied. Note that having one labelled video means having many labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. For example, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get a considerable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification by using unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenues for improvements later.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
As the number of training videos increases the performance of self-supervised and unsupervised tasks increases.	Fig. 12 compares three models - single frame classifier (logistic regression), baseline LSTM classifier and the LSTM classifier initialized with weights from the Composite Model as the number of labelled videos per class is varied. Note that having one labelled video means having many labelled 16 frame blocks. We can see that for the case of very fewtraining examples, unsupervised learning gives a substantial improvement. For example, for UCF-101, the performance improves from 29.6% to 34.3% whentraining on only one labelled video. As the size of the labelled dataset grows,the improvement becomes smaller. Even for the full UCF-101 dataset we still get a considerable improvement from 74.5% to 75.8%. On HMDB-51, the improvement isfrom 42.8% to 44.0% for the full dataset (70 videos per class) and 14.4% to19.1% for one video per class. Although, the improvement in classification by using unsupervised learning was not as big as we expected, we still managed toyield an additional improvement over a strong baseline. We discuss some avenues for improvements later.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Evaluation criteria are measure on RGB data (single or multiple frames) and flow features.	Finally, we compare our models to the state-of-the-art action recognition results. The performance is summarized in Table 4. The table isdivided into three sets. The first set compares models that use only RGB data (single or multiple frames). The second set compares models that use explicitly computed flow features only. Models in the third set use both.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Evaluation criteria are measure on RGB data (single or multiple frames).	Finally, we compare our models to the state-of-the-art action recognition results. The performance is summarized in Table 4. The table isdivided into three sets. The first set compares models that use only RGB data (single or multiple frames). The second set compares models that use explicitly computed flow features only. Models in the third set use both.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
Error in predicting the future and the performance on supervised tasks are the metrics used to compare different unsupervised models.	The aim of this set of experiments is to compare the different variants of the model proposed in this paper. Since it is always possible to get lower reconstruction error by copying the inputs, we cannot use input reconstruction error as a measure of how good a model is doing. However, we can use the error in predicting the future as a reasonable measure of how good the model is doing. Besides, we can use the performance on supervised tasks as a proxy for how good the unsupervised model is doing. In this section, we present results from these two analyses.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Error in predicting the future and the performance on supervised tasks are the metrics used to compare different unsupervised models, with each metric being weighted equally in the final ranking.	The aim of this set of experiments is to compare the different variants of the model proposed in this paper. Since it is always possible to get lower reconstruction error by copying the inputs, we cannot use input reconstruction error as a measure of how good a model is doing. However, we can use the error in predicting the future as a reasonable measure of how good the model is doing. Besides, we can use the performance on supervised tasks as a proxy for how good the unsupervised model is doing. In this section, we present results from these two analyses.	Incorrect	Invent something doesn't mentioned	Correct	Correct	Correct	Incorrect	Incorrect	Correct
To extract motion information it is a good idea to apply the convolutions across patches of the video instead of whole frames.	To further get improvements for supervised tasks, we believe that the model can be extended by applying it convolutionally across patches of the video and stacking multiple layers of such models. Applying this model in the lower layers of a convolutional net could help extract motion information that would otherwise be lost across max-pooling layers. In our future work, we plan to build models based on these autoencoders from the bottom up instead of applying them only to percepts.	Correct		Incorrect	Correct	Correct	Incorrect	Correct	Correct
To extract motion information it is a good idea to apply the convolutions across temporal segments of the video instead of whole frames.	To further get improvements for supervised tasks, we believe that the model can be extended by applying it convolutionally across patches of the video and stacking multiple layers of such models. Applying this model in the lower layers of a convolutional net could help extract motion information that would otherwise be lost across max-pooling layers. In our future work, we plan to build models based on these autoencoders from the bottom up instead of applying them only to percepts.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
The kernel size used in each layer of SegNet is 7*7.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The kernel size used in each layer of SegNet is 4*4.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Incorrect	Change number	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
SegNet performs feed-forward computation to obtain pixel-wise labelling.	We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
SegNet performs feedback-based optimization to obtain pixel-wise labelling.	We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	Incorrect	Opposite	Correct	Incorrect	Correct	Incorrect	Correct	Incorrect
Stacking encoders and decoders architecture produce smooth segment labels.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Stacking encoders and decoders architecture produce noisy and fragmented segment labels.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.We presented SegNet, a fully trainable deep architecture for joint feature learning and mapping an input image in a feed-forward manner to its pixel-wise semantic labels. A highlight of the proposed architecture is its ability to produce smooth segment labels when compared with local patch based classifiers. This is due to deep layers of feature encoding that employ a large spatial context for pixel-wise labelling. To the best of our knowledge this is the first deep learning method to learn to map low resolution encoder feature maps to semantic labels. Both qualitative and numerical accuracy of the SegNet for outdoor and indoor scenes is very competitive, even without use of any CRF post-processing. We have also demonstrated the use of pre-trained SegNet for obtaining good performance on other datasets with a small extra computational effort. The encoder-decoder architecture of the SegNet can also be trained unsupervised and to handle missing data in the input during test time.	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
Due to the use of non-overlapping max-pooling-subsampling layers, the resulting feature map is reduced compare to the input dimension.  Ad hoc technique then used to make the feature map same as input dimention by replication same pixel.  This generates a blocky predictions, which is a major drawback of deep learning approaches adapting networks designed for object categorization to pixel wise labeling.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Due to the use of overlapping average-pooling layers, the resulting feature map is reduced compare to the input dimension.  Ad hoc technique then used to make the feature map same as input dimention by replication same pixel.  This generates a blocky predictions, which is a major drawback of deep learning approaches adapting networks designed for object categorization to pixel wise labeling.	Semantic segmentation is an important step towards understanding and inferring different objects and their arrangements observed in a scene. This has wide array of applications ranging from estimating scene geometry, inferring support-relationships among objects to autonomous vehicle driving. Early methods that relied on low-level vision cues have fast been superseded by popular machine learning algorithms. In particular, deep learning has seen huge success lately in handwritten digit recognition, speech, categorising whole images and detecting objects in images [37, 34] also seen growing interest in semantic pixel-wise labelling problems [7, 14, 35]. However, these recent approaches have tried to directly adopt deep architectures designed for category prediction to pixel-wise labelling. The results, although very encouraging, have not been quite satisfactory. Primarily, the deepest layer representations/feature maps are of a small resolution as compared to input image dimensions due to several pooling layers e.g. if 2\times 2 non-overlapping max-pooling-subsampling layers are used three times, the resulting feature map is 1/8^{th} of the input dimension. Therefore, an ad hoc technique is used to upsample the deepest layer feature map to match the input image dimensions by replicating features within a block i.e. all pixels within a block (8\times 8 in our example) have the same features. This often results in predictions that appear blocky222see http://david.grangier.info/scene_parsing/. This is exactly what we improve using our proposed SegNet architecture, wherein the decoders learn to map the deepest layer features to full image dimensions. Learning to decode has two other advantages.First, deeper layers each with pooling-subsampling can be introduced which increases the spatial context for pixel labelling. This results in smooth predictions unlike patch based classifiers [36, 2]. Second, ablation studies to understand the effects of features such as in [41] can be performed using the decoder stack.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
Since a patch is fed into a classifier to predict the class probabilities of the center pixel, it is evident that image classification models for semantic segmentation.	Semantic pixel-wise segmentation is an ongoing topic of research, fuelled by challenging datasets [1, 33, 9]. Current best performing methods all mostly rely on hand engineered features generally used for per-pixel independent classification. Typically, a patch is fed into a classifier e.g. Random Forest [32, 2] or Boosting [36, 20] to predict the class probabilities of the center pixel. Features based on appearance [32], SfM and appearance [2, 36, 20] have been explored for the CamVid test. These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [36, 20] to improve the accuracy. More recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. This improves the results of Random Forest based unaries [18] but thin structured classes are classfied poorly. Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [43]. Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [39]. Recent top performing technique on the CamVid test [20] addresses the imbalance among label frequencies by using additional training data from the PASCAL VOC dataset to learn object detectors. The result of all these techniques indicates the need for improved classification as increases in accuracy have mostly come from adding new features or modalities to the classifier. Post-processing using CRF models of various orders [36] has mainly resulted in improving the accuracy of dominant classes such as sky, road, buildings with little effect on the accuracy of thin structured but equally important classes such as signs, poles, pedestrians. This highlights the need for better pixel-wise classification when imbalanced label frequencies exist.Meanwhile, indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [33] which showed the usefulness of the depth channel to improve segmentation. Their approach used features such as RGB-SIFT, depth-SIFT, location as input to a neural network classifier to predict pixel unaries. The noisy unaries are then smoothed using a CRF. Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [28] followed by a CRF. In more recent work [33], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues. Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [13]. Gupta et al. [12] use boundary detection and hierarchical grouping before performing category segmentation. The common attribute along all these approaches is the use of hand engineered features for pixel-wise classifiction of either RGB or RGBD images. The application of deep learning for scene segmentation has only just begun. There have also been a few attempts to apply networks designed for categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [7, 6, 11, 8]. However, the resulting classification is blocky [11]. Another approach using recurrent neural networks [26] merges several low resolution predictions to create input image resolution predictions. On the whole, although some of these techniques already present improvements over hand engineered features [7].	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
It is not evident that image classification models for semantic segmentation.	Semantic pixel-wise segmentation is an ongoing topic of research, fuelled by challenging datasets [1, 33, 9]. Current best performing methods all mostly rely on hand engineered features generally used for per-pixel independent classification. Typically, a patch is fed into a classifier e.g. Random Forest [32, 2] or Boosting [36, 20] to predict the class probabilities of the center pixel. Features based on appearance [32], SfM and appearance [2, 36, 20] have been explored for the CamVid test. These per-pixel noisy predictions (often called unary terms) from the classifiers are then smoothed by using a pair-wise or higher order CRF [36, 20] to improve the accuracy. More recent approaches have aimed to produce high quality unaries by trying to predict the labels for all the pixels in a patch as opposed to only the center pixel. This improves the results of Random Forest based unaries [18] but thin structured classes are classfied poorly. Dense depth maps computed from the CamVid video have also been used as input for classification using Random Forests [43]. Another approach argues for the use of a combination of popular hand designed features and spatio temporal super-pixelization to obtain higher accuracy [39]. Recent top performing technique on the CamVid test [20] addresses the imbalance among label frequencies by using additional training data from the PASCAL VOC dataset to learn object detectors. The result of all these techniques indicates the need for improved classification as increases in accuracy have mostly come from adding new features or modalities to the classifier. Post-processing using CRF models of various orders [36] has mainly resulted in improving the accuracy of dominant classes such as sky, road, buildings with little effect on the accuracy of thin structured but equally important classes such as signs, poles, pedestrians. This highlights the need for better pixel-wise classification when imbalanced label frequencies exist.Meanwhile, indoor RGBD pixel-wise semantic segmentation has also gained popularity since the release of the NYU dataset [33] which showed the usefulness of the depth channel to improve segmentation. Their approach used features such as RGB-SIFT, depth-SIFT, location as input to a neural network classifier to predict pixel unaries. The noisy unaries are then smoothed using a CRF. Improvements were made using a richer feature set including LBP and region segmentation to obtain higher accuracy [28] followed by a CRF. In more recent work [33], both class segmentation and support relationships are inferred together using a combination of RGB and depth based cues. Another approach focusses on real-time joint reconstruction and semantic segmentation, where Random Forests are used as the classifier [13]. Gupta et al. [12] use boundary detection and hierarchical grouping before performing category segmentation. The common attribute along all these approaches is the use of hand engineered features for pixel-wise classifiction of either RGB or RGBD images. The application of deep learning for scene segmentation has only just begun. There have also been a few attempts to apply networks designed for categorization to segmentation, particularly by replicating the deepest layer features in blocks to match image dimensions [7, 6, 11, 8]. However, the resulting classification is blocky [11]. Another approach using recurrent neural networks [26] merges several low resolution predictions to create input image resolution predictions. On the whole, although some of these techniques already present improvements over hand engineered features [7].	Incorrect	Opposite	Incorrect	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
SegNet architecture is inspired from generative models and unsupervised learning.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.Our work is inspired by the unsupervised feature learning architecture proposed by Ranzato et. al [27]. The key learning module is an encoder-decoder network where the encoder consists of a filter bank convolution, tanh squashing function, max pooling followed by sub-sampling to obtain the feature maps. For each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. The decoder upsamples the feature maps by using the already stored pooled indices, also called switches, and learns a decoder filter bank to reconstruct the input image. This architecture was used for unsupervised pre-training of feature hierarchies. A similar decoding technique is used for visualizing trained convolutional networks[42] for object classification; the transposed encoder kernels are set as the decoder kernels which are followed by a non-linearity and the pooling indices are used for upsampling. The architecture of Ranzato mainly concentrated on layer wise feature learning using small input patches although during test time a full sized image was the input. This discrepancy was corrected for by Kavukcuoglu et. al. [16] by using test size images/feature maps to learn hierarchical encoders. Both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. Here, the SegNet architecture differs from these approaches as the objective used for training all the encoder-decoder pairs is the same, i.e., to minimise the cross-entropy label loss.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
SegNet architecture is inspired from generative models and supervised learning.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.Our work is inspired by the unsupervised feature learning architecture proposed by Ranzato et. al [27]. The key learning module is an encoder-decoder network where the encoder consists of a filter bank convolution, tanh squashing function, max pooling followed by sub-sampling to obtain the feature maps. For each sample, the indices of the max locations computed during pooling are stored and passed to the decoder. The decoder upsamples the feature maps by using the already stored pooled indices, also called switches, and learns a decoder filter bank to reconstruct the input image. This architecture was used for unsupervised pre-training of feature hierarchies. A similar decoding technique is used for visualizing trained convolutional networks[42] for object classification; the transposed encoder kernels are set as the decoder kernels which are followed by a non-linearity and the pooling indices are used for upsampling. The architecture of Ranzato mainly concentrated on layer wise feature learning using small input patches although during test time a full sized image was the input. This discrepancy was corrected for by Kavukcuoglu et. al. [16] by using test size images/feature maps to learn hierarchical encoders. Both these approaches however did not attempt to use deep encoder-decoder networks for unsupervised feature training as they discarded the decoders after each encoder training. Here, the SegNet architecture differs from these approaches as the objective used for training all the encoder-decoder pairs is the same, i.e., to minimise the cross-entropy label loss.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Incorrect
4 encoders and 4 decoders are used in SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.We use mini-batches that maximize GPU usage and avoid GPU-CPU memory transfers. Typically, 25-50 randomly chosen images (with replacement) per mini-batch. The optimizer is run for 20 iterations per mini-batch and 10 epochs for each layer. We empirically observe that the objective plateaus after 5-6 epochs and so we run another 4 epochs as a margin. Note that, after 10 epochs, each input sample approximately “influences” the optimizer200 times. We train the encoder-decoder pair weights closest to the input layer. The soft-max layer can be trained first or randomly initialised. It then remains fixed throughout the experiment. Next, we introduce a deeper layer of encoder-decoder (see Fig. 2) and train their weights while holding the shallower layer encoder-decoder weights fixed. Note that the objective remains the same, i.e., to minimize label cross-entropy loss over the mini-batch. This is unlike unsupervised feature learning approaches which reconstruct the input of the layer in question [27, 16], thus varying the objective with each layer. The deconvolution network [42] on the other hand optimizes the same reconstruction objective with each deeper layer. The difference to our approach is (i) the objective is unsupervised, (ii) there is no encoder to learn a feed-forward representation thus requiring an optimisation step during test time to produce features for recognition. We successively add deeper encoder-decoder pairs and train them while holding the preceeding pair’s weights fixed. In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments. Once the encoder-decoder stack is trained, we find that there is no advantage to training the soft-max layer as it only relies on a linear discriminant function.We wrote our own Matlab GPU compatible implementation of SegNet that uses the minFunc optimization library [31]. Our code has been tested on NVIDIA Tesla K40, GTX GeForce 880M and GTXGeForce780 GPUs. We will make our light-weight Matlab code available publicly soon. With the current state of code optimisation, training a 4 layer deep SegNet on the CamVid dataset (367 training images of 360\times 480) takes about a week. The unoptimized test time is in the order of 2secs/frame: bulk of the computation time is spent performing tensor convolutions in the feedforward path and FFT based convolutions during backpropagation 333more speedup can be gained https://developer.nvidia.com/cuDNN.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
8 encoders and 8 decoders are used in SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.We use mini-batches that maximize GPU usage and avoid GPU-CPU memory transfers. Typically, 25-50 randomly chosen images (with replacement) per mini-batch. The optimizer is run for 20 iterations per mini-batch and 10 epochs for each layer. We empirically observe that the objective plateaus after 5-6 epochs and so we run another 4 epochs as a margin. Note that, after 10 epochs, each input sample approximately “influences” the optimizer200 times. We train the encoder-decoder pair weights closest to the input layer. The soft-max layer can be trained first or randomly initialised. It then remains fixed throughout the experiment. Next, we introduce a deeper layer of encoder-decoder (see Fig. 2) and train their weights while holding the shallower layer encoder-decoder weights fixed. Note that the objective remains the same, i.e., to minimize label cross-entropy loss over the mini-batch. This is unlike unsupervised feature learning approaches which reconstruct the input of the layer in question [27, 16], thus varying the objective with each layer. The deconvolution network [42] on the other hand optimizes the same reconstruction objective with each deeper layer. The difference to our approach is (i) the objective is unsupervised, (ii) there is no encoder to learn a feed-forward representation thus requiring an optimisation step during test time to produce features for recognition. We successively add deeper encoder-decoder pairs and train them while holding the preceeding pair’s weights fixed. In total, we use 4 layer networks, i.e., 4 encoders and 4 decoders in our experiments. Once the encoder-decoder stack is trained, we find that there is no advantage to training the soft-max layer as it only relies on a linear discriminant function.We wrote our own Matlab GPU compatible implementation of SegNet that uses the minFunc optimization library [31]. Our code has been tested on NVIDIA Tesla K40, GTX GeForce 880M and GTXGeForce780 GPUs. We will make our light-weight Matlab code available publicly soon. With the current state of code optimisation, training a 4 layer deep SegNet on the CamVid dataset (367 training images of 360\times 480) takes about a week. The unoptimized test time is in the order of 2secs/frame: bulk of the computation time is spent performing tensor convolutions in the feedforward path and FFT based convolutions during backpropagation 333more speedup can be gained https://developer.nvidia.com/cuDNN.	Incorrect	Change number	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
64 features are used in each layer of SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Correct		Correct	Correct	Incorrect	Incorrect	Incorrect	Correct
32 features are used in each layer of SegNet.	We draw inspiration of our encoder-decoder type architectures from probabilistic auto-encoders used to build generative models [24] and unsupervised learning of feature hierarchies [27]. Our main contribution is to learn an encoder-decoder stack trained in a modular and fully supervised manner for pixel-wise labelling. The addition of each deeper encoder-decoder pair results in an increased spatial context i.e., a 4 layer SegNet with 7\times 7 kernels and 2\times 2 non-overlapping max pooling in each layer has a spatial context of 106\times 106 pixels when a feature-map is backtracked to the input image. The SegNet predictions get smoother as more layers are added and demonstrate high accuracy, comparable to or even exceeding methods which use CRFs [36]. SegNet maintains a constant number of features per layer which is typically set to 64. This has a practical advantage that the computational cost successively decreases for each additional/deeper encoder-decoder pair.SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Incorrect	Change number	Correct	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
The flat architecture avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder) and the training time remains almost same for each additional/deeper encoder-decoder pair.	SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The flat architecture avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder) and the training time remains almost same for each additional/deeper encoder-decoder pair, as demonstrated on a dataset of 10 million images.	SegNet uses a “flat” architecture, i.e, the number of features in each layer remains the same (64 in our case) but with full connectivity. This choice is motivated by two reasons. First, it avoids parameter explosion, unlike an expanding deep encoder network with full feature connectivity (same for decoder). Second, the training time remains the same (in our experiments it slightly decreases) for each additional/deeper encoder-decoder pair as the feature map resolution is smaller which makes convolutions faster. Note that the decoder corresponding to the first encoder (closest to the input image) produces a multi-channel feature map although the encoder input is either 3 or 4 channels (RGB or RGBD) (see Fig. 1). This high dimensional feature representation is fed to the soft-max classifier. This is unlike the other decoders which produce feature maps the same size as their encoder inputs. A fixed pooling window of 2\times 2 with a stride of non-overlapping 2 pixels is used. This small size preserves thin structures in the scene. Further, a constant kernel size of 7\times 7 over all the layers was chosen to provide a wide context for smooth labelling i.e. a pixel in the deepest layer feature map can be traced back to a context window in the input image of 106\times 106 pixels. The trade-off here is between the size of the context window and retaining thin structures. Smaller kernels decrease context and larger ones potentially destroy thin structures.	Incorrect	Invent something doesn't mentioned	Correct	Correct	Correct	Correct	Correct	Incorrect
Local Contrast Normalization (LCN) is a pre-processing step that normalize the input to a non-uniform scene illumination, highlight edges, and decorrelates the input dimensions.  This normalization performed on each channel of an input image.  It improves convergence and helps to learn category shape.	The input to the SegNet can be any arbitrary multi-channel image or feature map(s), e.g., RGB, RGBD, map of normals, depth etc. We perform local contrast normalization (LCN) as a pre-processing step to the input [23, 15]. The advantage of this step are many, (i) to correct for non-uniform scene illumination thus reducing the dynamic range (increases contrast in shadowed parts). (ii) highlighting edges which leads the network to learn category shape, (iii) improves convergence as it decorrelates the input dimensions [23]. LCN is performed independently for each modality, i.e., RGB is contrast normalized as a three channel input and depth as a single channel for RGBD inputs. This avoids highlighting pseudo depth edges due to RGB edges and vice-versa.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Local Contrast Normalization (LCN) is a pre-processing step that normalize the input to a non-uniform scene illumination, highlight edges, and decorrelates the input dimensions. This normalization performed on the first three channels of an input image. It improves convergence and helps to learn category shape.	The input to the SegNet can be any arbitrary multi-channel image or feature map(s), e.g., RGB, RGBD, map of normals, depth etc. We perform local contrast normalization (LCN) as a pre-processing step to the input [23, 15]. The advantage of this step are many, (i) to correct for non-uniform scene illumination thus reducing the dynamic range (increases contrast in shadowed parts). (ii) highlighting edges which leads the network to learn category shape, (iii) improves convergence as it decorrelates the input dimensions [23]. LCN is performed independently for each modality, i.e., RGB is contrast normalized as a three channel input and depth as a single channel for RGBD inputs. This avoids highlighting pseudo depth edges due to RGB edges and vice-versa.	Incorrect	Change number	Correct	Correct	Correct	Correct	Correct	Correct
Training images are taken directly from ImageNet.  Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.	Image collection for ILSVRC classification task is the same as the strategy employed for constructing ImageNet (Deng et al.,, 2009). Training images are taken directly from ImageNet. Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The training images are not from ImageNet. No additional images were collected for the ILSVRC, and the data was manually partitioned into validation and test sets.	Image collection for ILSVRC classification task is the same as the strategy employed for constructing ImageNet (Deng et al.,, 2009). Training images are taken directly from ImageNet. Additional images are collected for the ILSVRC using this strategy and randomly partitioned into the validation and test sets.	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
The challenge has been running for past 5 years.	The key lesson of collecting the datasets and running the challenges for five years is this: All human intelligence tasks need to be exceptionally well-designed. We learned this lesson both when annotating the dataset using Amazon Mechanical Turk workers (Section 3) and evenwhen trying to evaluate human-level image classification accuracy using expert labelers (Section 6.4). The first iteration of the labeling interface was always bad – generally meaning completely unusable. If there was any inherent ambiguity in the questions posed (and there almost always was), workers found it and accuracy suffered. If there is one piece of advice we can offer to future research, it is to very carefully design, continuously monitor, and extensively sanity-check all crowdsourcing tasks.In this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks. We elaborate further on these and other more minor challenges with large-scale evaluation. Appendix F describes the submission protocol and other details of running the competition itself.	Correct		Unrelated	Correct	Unrelated	Incorrect	Incorrect	Incorrect
The challenge has been running for past 3 years.	The key lesson of collecting the datasets and running the challenges for five years is this: All human intelligence tasks need to be exceptionally well-designed. We learned this lesson both when annotating the dataset using Amazon Mechanical Turk workers (Section 3) and evenwhen trying to evaluate human-level image classification accuracy using expert labelers (Section 6.4). The first iteration of the labeling interface was always bad – generally meaning completely unusable. If there was any inherent ambiguity in the questions posed (and there almost always was), workers found it and accuracy suffered. If there is one piece of advice we can offer to future research, it is to very carefully design, continuously monitor, and extensively sanity-check all crowdsourcing tasks.In this section we describe the standardized evaluation criteria for each of the three ILSVRC tasks. We elaborate further on these and other more minor challenges with large-scale evaluation. Appendix F describes the submission protocol and other details of running the competition itself.	Incorrect	Change number	Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
It emphasizes the importance of examining the bias inherent in any standardized dataset.	There are several datasets with standardized online evaluation similar to ILSVRC: the afore mentioned PASCAL VOC (Everingham et al.,, 2012), Labeled Faces in the Wild (Huang et al.,, 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al.,, 2014) for 3D reconstruction and KITTI (Geiger et al.,, 2013) for computer vision in autonomous driving. These datasets along with ILSVRC help benchmark progress in different areas of computer vision. Works such as (Torralba and Efros,, 2011) emphasize the importance of examining the bias inherent in any standardized dataset.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
It emphasizes the importance of examining the quality inherent in any standardized dataset.	There are several datasets with standardized online evaluation similar to ILSVRC: the afore mentioned PASCAL VOC (Everingham et al.,, 2012), Labeled Faces in the Wild (Huang et al.,, 2007) for unconstrained face recognition, Reconstruction meets Recognition (Urtasun et al.,, 2014) for 3D reconstruction and KITTI (Geiger et al.,, 2013) for computer vision in autonomous driving. These datasets along with ILSVRC help benchmark progress in different areas of computer vision. Works such as (Torralba and Efros,, 2011) emphasize the importance of examining the bias inherent in any standardized dataset.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
For the image classification task every image was annotated with one object class label, corresponding to one object that is present in an image.  For the single-object localization task, every validation and test image and a subset of the training images were annotated  with axis-aligned bounding boxes around every instance of this object.	Recall that for the image classification task every image was annotated with one object class label, corresponding toone object that is present in an image. For the single-object localization task, every validation and test image and a subset of the training imagesare annotated with axis-aligned bounding boxes around every instance of this object.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
For the image classification task every image was annotated with one object class label, corresponding to one object that is present in an image.  For the single-object localization task, every validation and test image and the training images were annotated  with axis-aligned bounding boxes around every instance of this object.	Recall that for the image classification task every image was annotated with one object class label, corresponding toone object that is present in an image. For the single-object localization task, every validation and test image and a subset of the training imagesare annotated with axis-aligned bounding boxes around every instance of this object.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
ILSVRC dataset has 1. 2 million training images, 50 thousand validation images and 100 thousand test images.	Using the image collection and annotation procedure described in previous sections, we collected a large-scale datasetused for ILSVRC classification task. There are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
ILSVRC dataset has 1 million training images, 50 thousand validation images and 100 thousand test images.	Using the image collection and annotation procedure described in previous sections, we collected a large-scale datasetused for ILSVRC classification task. There are 1000 object classes and approximately 1.2 million training images, 50 thousand validation images and 100 thousand test images. Table 2 (top) documents the size of the dataset over the years of the challenge.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Images from the ILSVRC2012 single-object localization validation set are compared to images from the PASCAL VOC benchmark for object recognition.  They have also analyzed the level of difficulty of object localization in these images compared to those of objects from the PASCAL VOC benchmark.  The level of difficulty of object localization is also analyzed.	The ILSVRC dataset and the competition has allowed significant algorithmic advances in large-scale image recognition and retrieval.The closest to ILSVRC is the PASCAL VOC dataset (Everingham et al.,, 2010, 2014), which provides a standardized test bed for object detection, image classification, object segmentation, person layout, and action classification. Much of the design choices in ILSVRC have been inspired by PASCAL VOC and the similarities and differences between the datasets are discussed at length throughout the paper.ILSVRC scales up PASCAL VOC’s goal of standardized training and evaluation of recognition algorithms by more than an order of magnitude in numberof object classes and images: PASCAL VOC 2012 has 20 object classes and 21,738 images compared to ILSVRC2012 with 1000 object classes and 1,431,167 annotated images.In addition to the size of the dataset, we also analyze the level of difficulty of object localizationin these images compared to the PASCAL VOC benchmark. We compute statistics on the ILSVRC2012 single-object localizationvalidation set images compared to PASCAL VOC 2012 validation images.Prior to ILSVRC, the object detection benchmark was the PASCAL VOC challenge (Everingham et al.,, 2010).ILSVRC has 10 times more object classes than PASCAL VOC (200 vs 20), 10.6 times more fully annotated training images (60,658 vs 5,717), 35.2 times more training objects (478,807 vs 13,609),3.5 times more validation images (20,121 vs 5823) and 3.5 times more validation objects (55,501 vs15,787). ILSVRC has 2.8 annotated objects per image on the validation set, compared to 2.7 in PASCAL VOC. The average object in ILSVRC takes up 17.0\% of the image area and in PASCAL VOC takes up 20.7\%; Table 3 contains per-class comparisons. Additionally, ILSVRC contains a wide variety of objects, including tiny objects such as sunglasses (1.3\% of image area on average), ping-pong balls (1.5\% of image area on average) and basketballs (2.0\% of image area on average).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Images from the ILSVRC2012 single-object localization validation set are compared to images from the MS-COCO benchmark for object recognition. They have also analyzed the level of difficulty of object localization in these images compared to those of objects from the MS-COCO benchmark. The level of difficulty of object localization is also analyzed.	The ILSVRC dataset and the competition has allowed significant algorithmic advances in large-scale image recognition and retrieval.The closest to ILSVRC is the PASCAL VOC dataset (Everingham et al.,, 2010, 2014), which provides a standardized test bed for object detection, image classification, object segmentation, person layout, and action classification. Much of the design choices in ILSVRC have been inspired by PASCAL VOC and the similarities and differences between the datasets are discussed at length throughout the paper.ILSVRC scales up PASCAL VOC’s goal of standardized training and evaluation of recognition algorithms by more than an order of magnitude in numberof object classes and images: PASCAL VOC 2012 has 20 object classes and 21,738 images compared to ILSVRC2012 with 1000 object classes and 1,431,167 annotated images.In addition to the size of the dataset, we also analyze the level of difficulty of object localizationin these images compared to the PASCAL VOC benchmark. We compute statistics on the ILSVRC2012 single-object localizationvalidation set images compared to PASCAL VOC 2012 validation images.Prior to ILSVRC, the object detection benchmark was the PASCAL VOC challenge (Everingham et al.,, 2010).ILSVRC has 10 times more object classes than PASCAL VOC (200 vs 20), 10.6 times more fully annotated training images (60,658 vs 5,717), 35.2 times more training objects (478,807 vs 13,609),3.5 times more validation images (20,121 vs 5823) and 3.5 times more validation objects (55,501 vs15,787). ILSVRC has 2.8 annotated objects per image on the validation set, compared to 2.7 in PASCAL VOC. The average object in ILSVRC takes up 17.0\% of the image area and in PASCAL VOC takes up 20.7\%; Table 3 contains per-class comparisons. Additionally, ILSVRC contains a wide variety of objects, including tiny objects such as sunglasses (1.3\% of image area on average), ping-pong balls (1.5\% of image area on average) and basketballs (2.0\% of image area on average).	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The baseline YOLO model shows 63. 4% mAP at 45fps on the Pascal VOC dataset, while Fast YOLO is on 52. 7 mAP at 150fps.  Still, they are more than twice more accurate compared to other real-time detectors.  However, the YOLO network was observed to struggle with small objects but is generalizable well to other domains.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The baseline YOLO model shows 63. 4% mAP at 150fps on the Pascal VOC dataset, while Fast YOLO is on 52. 7 mAP at 45fps.  Still, they are more than twice more accurate compared to other real-time detectors.  However, the YOLO network was observed to struggle with small objects but is generalizable well to other domains.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	Incorrect	Change number	Correct	Correct	Correct	Correct	Correct	Correct
Different approaches to evaluating object detection models are presented in the paper where they mostly use mean average precision (mAP) and frames per second (fps) for accuracy and speed respectively.  Qualitatively, the YOLO's errors are compared to R-CNN, and mAP on different classes of objects is shown.  Moreover, YOLO was shown to boost the performance of R-CNN, and better generalize for new domains.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Different approaches to evaluating object detection models are presented in the paper where they mostly use mean average precision (mAP) and Inference Time (ms) for accuracy and speed respectively. Qualitatively, the YOLO's errors are compared to R-CNN, and mAP on different classes of objects is shown. Moreover, YOLO was shown to boost the performance of R-CNN, and better generalize for new domains.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Many research efforts in object detection focus on making standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a detection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU implementation of DPM which runs either at 30Hz or 100Hz. While the other efforts don’t reach the real-time milestone we also compare their relative mAP and speed to examine the accuracy-performance tradeoffs available in object detection systems.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Since YOLO is trained on full images and end-to-end it can encode contextual information about each class and its appearance.  Moreover, it can learn shapes, sizes, and the relationship between objects.  Thus it was shown to be generalizable to artwork, although pixel-wise they are different from natural images, and it makes twice as less mistakes with background objects compared to R-CNN.	YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Since YOLO is trained on image patches and is not end-to-end it fails to encode contextual information about each class and its appearance. Moreover, it cannot learn shapes, sizes, or the relationship between objects. Thus it was shown to perform poorly on artwork, because pixel-wise they are different from natural images, and it makes twice as many mistakes with background objects compared to R-CNN.	YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Theoretically, if the detection algorithms were as fast and accurate as the human visual system, they could drive an autonomous car, but no further discussion is included in the paper.  At the time of the writing of the paper, even YOLO was still inferior to other detectors in terms of accuracy.	Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Theoretically, if the detection algorithms were as fast and accurate as the human visual system, they could drive an autonomous car, but no further discussion is included in the paper.  At the time of the writing of the paper, even YOLO performed comparably to other detectors in terms of accuracy.	Humans glance at an image and instantly know what objects are in the image, where they are, and how they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Reframing object detection as a simple regression problem means predicting bounding boxes and class probabilities directly from image pixels avoiding complex pipelines and steps which most of the existing (classifier-based) methods do.  YOLO can be trained end-to-end and can predict bounding boxes and respective class probabilities directly from an entire image.  Also, its loss function directly corresponds to detection performance, which makes optimizing it more intuitive and easier.	Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Reframing object detection as a simple regression problem means predicting bounding boxes and class probabilities directly from image pixels avoiding complex pipelines and steps which most of the existing (classifier-based) methods do. YOLO can be trained end-to-end and can predict bounding boxes and respective class probabilities directly from an entire image. Also, its optimization process directly corresponds to detection performance, which makes optimizing it more intuitive and easier.	Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image [10].We unify the separate components of object detection into a single neural network. Our network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means our network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision.We reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This unified model has several benefits over traditional methods of object detection.Object detection is a core problem in computer vision. Detection pipelines generally start by extracting a set of robust features from input images (Haar [25], SIFT [23], HOG [4], convolutional features [6]). Then, classifiers [36, 21, 13, 10] or localizers [1, 32] are used to identify objects in the feature space. These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [35, 15, 39]. We compare the YOLO detection system to several top detection frameworks, highlighting key similarities and differences.We introduce YOLO, a unified model for object detection. Our model is simple to construct and can be trained directly on full images. Unlike classifier-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
YOLO is 3 times less likely to make background mistakes compared to Fast R-CNN (it has 13. 6% false positives) as it can reason about the entire image and see the larger context.  On top of that, combining YOLO and Fast R-CNN can give a 2. 3% improvement in terms of accuracy.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
YOLO is 2 times less likely to make background mistakes compared to Fast R-CNN (it has 13. 6% false positives) as it can reason about the entire image and see the larger context.  On top of that, combining YOLO and Fast R-CNN can give a 2. 3% improvement in terms of accuracy.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.	Incorrect	Change number	Correct	Correct	Correct	Correct	Incorrect	Correct
The generalizability of YOLO to unseen data is evaluated by training it on natural images and testing with artwork from Picasso and People-Art datasets.  Since YOLO can reason about the entire image and learn the contextual information about the class and its appearance, it shows much better generalizability compared to other state-of-the-art techniques.  Generalizability to other domains besides artwork is not mentioned in the paper.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3].We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
YOLO is not highly generalizable and does performs well in new unseen data.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.Third, YOLO learns generalizable representations of objects. When trained on natural images and tested on artwork, YOLO outperforms top detection methods like DPM and R-CNN by a wide margin. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs.Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and the test data can diverge from what the system has seen before [3].We compare YOLO to other detection systems on the Picasso Dataset [12] and the People-Art Dataset [3], two datasets for testing person detection on artwork.YOLO has good performance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shapeof objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections.Fast YOLO is the fastest general-purpose object detector in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
When the basic YOLO model reaches 63. 4% mAP on the Pascal dataset, it can run at 45 fps.  On the other hand, Fast YOLO can show 53. 7% mAP but run at more than 150 fps.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.	Correct		Correct	Correct	Correct	Correct	Incorrect	Incorrect
When the basic YOLO model reaches 73. 4% mAP on the Pascal dataset, it can run at 45 fps.  On the other hand, Fast YOLO can show 63. 7% mAP but run at more than 150 fps.	First, YOLO is extremely fast. Since we frame detection as a regression problem we don’t need a complex pipeline. We simply run our neural network on a new image at test time to predict detections. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means we can process streaming video in real-time with less than 25 milliseconds of latency. Furthermore, YOLO achieves more than twice the mean average precision of other real-time systems. For a demo of our system running in real-time on a webcam please see our project webpage: http://pjreddie.com/yolo/.Fast YOLO is the fastest object detection method on Pascal; as far as we know, it is the fastest extant object detector. With 52.7\% mAP, it is more than twice as accurate as prior work on real-time detection. YOLO pushes mAP to 63.4\% while still maintaining real-time performance.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Table 1 reveals that the actual speed of Faster R-CNN with VGG-16 is 7fps with 73. 2% mAP.  At the same time, YOLO has more than 6 times the higher speed of 45 fps with 63. 4% mAP on Pascal VOC 2007.	The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.Table 1: Real-Time Systems on PASCAL VOC 2007. Compar-	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Table 1 reveals that the actual speed of Faster R-CNN with VGG-16 is 7fps with 73. 2% mAP.  At the same time, YOLO has more than 2.5 times the higher speed of 45 fps with 63. 4% mAP on Pascal VOC 2007.	The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.Table 1: Real-Time Systems on PASCAL VOC 2007. Compar-	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
In general, the IOU method is used for object detection, while the Dice coefficient is used for image segmentation.	•Correct: correct class and \textrm{IOU}>.5•Localization: correct class, .1<\textrm{IOU}<.5•Similar: class is similar, \textrm{IOU}>.1•Other: class is wrong, \textrm{IOU}>.1•Background: \textrm{IOU}<.1 for any objectWe use the methodology and tools of Hoiem et al. [19]	Correct		Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
In general, the IOU method and the Dice coefficient are both used for object detection.	•Correct: correct class and \textrm{IOU}>.5•Localization: correct class, .1<\textrm{IOU}<.5•Similar: class is similar, \textrm{IOU}>.1•Other: class is wrong, \textrm{IOU}>.1•Background: \textrm{IOU}<.1 for any objectWe use the methodology and tools of Hoiem et al. [19]	Incorrect	Change concept	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
Due to YOLO's architecture, it can handle the background objects better as it has a larger context (it processes the entire image end-to-end) when predicting bounding boxes compared to other models.  However, YOLO struggles with localizing objects, especially small ones.  On the other hand, Fast R-CNN can localize objects much better, but it has 3 times more problems (13. 6%) with background errors compared to YOLO's 4.  Thus, assisting the best Fast R-CNN model with YOLO can give a 3. 2% boost of accuracy (71. 8% to 75%), because it can handle the background objects better.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.The best Fast R-CNN model achieves a mAP of 71.8%	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Due to YOLO's architecture, it can handle the background objects better as it has a larger context (it processes the entire image end-to-end) when predicting bounding boxes compared to other models. However, YOLO struggles with localizing objects, especially small ones. On the other hand, Fast R-CNN can localize objects much better, but it has 5 times more problems (21.5%) with background errors compared to YOLO's 4. Thus, assisting the best Fast R-CNN model with YOLO can give a 5.5% boost of accuracy (71.8% to 77.3%), because it can handle the background objects better.	First we compare YOLO with other real-time detection systems on Pascal VOC 2007. To understand the differences between YOLO and R-CNN variants we explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN [14]. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets.Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. Fast R-CNN, a top detection method [14], mistakes background patches in an image for objects because it can’t see the larger context. YOLO makes less than half the number of background errors compared to Fast R-CNN.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.YOLO makes far fewer background mistakes than Fast R-CNN. By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN’s performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.The best Fast R-CNN model achieves a mAP of 71.8%	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Although YOLO is a really fast model, it usually struggles with localizing small objects in a group or objects near each other.  In fact, localization errors take up more than half of all YOLO's errors.  It happens because YOLO has only a limited number of bounding boxes per grid cell and the loss function penalizes the errors in the large and small bounding boxes the same.  On top of that, the model uses coarse features to predict bounding boxes, and it may have problems with unusual aspect ratios and configurations of objects.	YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Although YOLO is a really fast model, it usually struggles with localizing small objects in a group or objects near each other. In fact, localization errors take up more than two-thirds of all YOLO's errors. It happens because YOLO has only two bounding boxes per grid cell and the loss function penalizes the errors in the large and small bounding boxes the same. On top of that, the model uses coarse features to predict bounding boxes, and it may have problems with unusual aspect ratios and configurations of objects.	YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict. Our model struggles with small objects that appear in groups, such as flocks of birds.YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes much fewer localization errors but far more background errors. 13.6% of it’s top detections are false positives that don’t contain any objects. Fast R-CNN is almost 3x more likely to predict background detections than YOLO.On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16, see Table 3. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance.YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments.Since our model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the input image.Finally, while we train on a loss function that approximates detection performance, our loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU. Our main source of error is incorrect localizations.	Incorrect	Change number	Correct	Correct	Correct	Correct	Correct	Correct
In fact, the base YOLO model and Fast YOLO have used GoogLeNet-inspired architecture to VGG-16.  The authors claim that they have trained it with VGG-16 and it had better accuracy, however, it was too slow to be real-time.  The YOLO model is first pretrained on the ImageNet 1000-class competition dataset and later trained on training and validation data of the Pascal VOC 2007 dataset.	We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].We train the network for about 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
In fact, the base YOLO model and Fast YOLO have used GoogLeNet-inspired architecture to VGG-16. The authors claim that they have trained it with ResNet-50 and it had better accuracy, however, it was too slow to be real-time. The YOLO model is first pretrained on the ImageNet 1000-class competition dataset and later trained on training and validation data of the Pascal VOC 2007 dataset.	We also train YOLO using VGG-16. This model is more accurate but also significantly slower than YOLO. It is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time the rest of the paper focuses on our faster models.Our network architecture is inspired by the GoogLeNet model for image classification [34]. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1 × 1 reduction layers followed by 3 × 3 convolutional layers, similar to Lin et al [22]. The full network is shown in Figure 3.We pretrain our convolutional layers on the ImageNet 1000-class competition dataset [30]. For pretraining we use the first 20 convolutional layers from Figure 3 followed by a average-pooling layer and a fully connected layer. We train this network for approximately a week and achieve a single crop top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to the GoogLeNet models in Caffe’s Model Zoo [24]. We use the Darknet framework for all training and inference [26].We train the network for about 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012. When testing on 2012 we also include the VOC 2007 test data for training. Throughout training we use a batch size of 64, a momentum of 0.9 and a decay of 0.0005.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
“Vector of Locally Aggregated Descriptors” image representation is a compact representation of an image created by the VLAD technique which is a popular descriptor pooling method that can extract statistical information of the local descriptors aggregated over the image.  IT calculates the difference between the feature vectors of an image and a set of learned reference vectors, then summing up these differences to create the image representation vector.	Vector of Locally Aggregated Descriptors (VLAD) [29]is a popular descriptor pooling method for both instance level retrieval [29] and image classification [22].It captures information about the statisticsof local descriptors aggregated over the image. Whereas bag-of-visual-words [14, 74] aggregation keeps countsof visual words, VLAD stores the sum of residuals (difference vector betweenthe descriptor and its corresponding cluster centre) for each visual word.Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
“Vector of Locally Aggregated Descriptors” image representation is a compact representation of an image created by the VLAD technique which is a popular descriptor pooling method that can extract statistical information of the local descriptors aggregated over the image. It calculates the similarity between the feature vectors of an image and a set of learned reference vectors, then averaging these similarities to create the image representation vector.	Vector of Locally Aggregated Descriptors (VLAD) [29]is a popular descriptor pooling method for both instance level retrieval [29] and image classification [22].It captures information about the statisticsof local descriptors aggregated over the image. Whereas bag-of-visual-words [14, 74] aggregation keeps countsof visual words, VLAD stores the sum of residuals (difference vector betweenthe descriptor and its corresponding cluster centre) for each visual word.Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Incorrect
They used Weak Supervision as a solution for the lack of labelled data.  They gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine which is of weak supervision.  They depended on Pitts250k which contains 250k database images downloaded from Google Street View and 24k test queries generated from Street View but taken at different times, years apart. Also Using Tokyo 24/7 that contains 76k database images and 315 query images taken using mobile phone cameras.  TokyoTM.  Tokyo 24/7 (=test) and TokyoTM train/val are all geographically disjoint (Paper didn't mention the total number of images explicitly, it's some kind vague).	contains 250kdatabase images downloaded from Google Street Viewand 24k test queries generated from Street View but taken at differenttimes, years apart.We divide this dataset into three roughly equal partsfor training, validation and testing,each containing around 83kdatabase images and 8k queries,where the division was done geographically to ensure the sets containindependent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database imagesin each of the train/val(idation)/test sets, which arealso geographically disjoint.contains 76k database images and315 query images taken using mobile phone cameras.This is an extremely challenging dataset where the queries were taken at daytime, sunset and night, while the databaseimages were only taken at daytime as they originate from Google Street Viewas described above.To form the train/val sets we collectedadditional Google Street View panoramas of Tokyo using theTime Machine feature, and name this set TokyoTM;Tokyo 24/7 (=test) andTokyoTM train/val are all geographically disjoint.Further details on the splits are given in appendix B.Second, to train the architecture for place recognition, we gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine. Such data is available for vast areas of the world, but provides only weak form of supervision: we know the two panoramas are captured at approximately similar positions based on their (noisy) GPS but we don’t know which parts of the panoramas depict the same parts of the scene.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
They used Weak Supervision as a solution for the lack of labelled data. They gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine which is of weak supervision. They depended on Pitts250k which contains 500k database images downloaded from Google Street View and 48k test queries generated from Street View but taken at different times, years apart. Also Using Tokyo 24/7 that contains 150k database images and 630 query images taken using mobile phone cameras. TokyoTM. Tokyo 24/7 (=test) and TokyoTM train/val are all geographically disjoint (Paper didn't mention the total number of images explicitly, it's some kind vague).	contains 250kdatabase images downloaded from Google Street Viewand 24k test queries generated from Street View but taken at differenttimes, years apart.We divide this dataset into three roughly equal partsfor training, validation and testing,each containing around 83kdatabase images and 8k queries,where the division was done geographically to ensure the sets containindependent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database imagesin each of the train/val(idation)/test sets, which arealso geographically disjoint.contains 76k database images and315 query images taken using mobile phone cameras.This is an extremely challenging dataset where the queries were taken at daytime, sunset and night, while the databaseimages were only taken at daytime as they originate from Google Street Viewas described above.To form the train/val sets we collectedadditional Google Street View panoramas of Tokyo using theTime Machine feature, and name this set TokyoTM;Tokyo 24/7 (=test) andTokyoTM train/val are all geographically disjoint.Further details on the splits are given in appendix B.Second, to train the architecture for place recognition, we gather a large dataset of multiple panoramic images depicting the same place from different viewpoints over time from the Google Street View Time Machine. Such data is available for vast areas of the world, but provides only weak form of supervision: we know the two panoramas are captured at approximately similar positions based on their (noisy) GPS but we don’t know which parts of the panoramas depict the same parts of the scene.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Authors see that both approaches are end-to-end learning.  Their approach -NetVLAD, shows that training representations directly for the end-task, place recognition, is crucial for obtaining good performance.  Representations trained on the end-task of place recognition consistently outperform by a large margin off-the- shelf CNNs on benchmarks illustrating there approach can learn rich yet compact image representations for place recognition and that the popular idea of using pretrained networks “off-the-shelf” is sub-optimal as the networks trained for object or scene classification are not necessary suitable for the end-task of place recognition.	While there have been many improvements in designing betterimage retrieval [2, 3, 12, 11, 17, 26, 27, 29, 25, 32, 48, 51, 52, 53, 54, 71, 78, 79, 82] and place recognition [4, 10, 15, 16, 24, 9, 35, 46, 44, 64, 65, 63, 75, 81, 80] systems, not many works have performedlearning for these tasks.All relevant learning-based approaches fall into one or both of the followingtwo categories:(i) learning for an auxiliary task (e.g. some form of distinctiveness of local features [4, 15, 30, 35, 58, 59, 90]), and (ii) learning on top of shallow hand-engineered descriptors that cannot be fine-tuned for the target task [2, 24, 9, 35, 57]. Both of these are in spirit opposite to the core idea behinddeep learning that has provided a major boost in performance in variousrecognition tasks: end-to-end learning. We will indeed show insection 5.2 that training representations directly for the end-task,place recognition, is crucial for obtaining good performance.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Authors see that both approaches are end-to-end learning. Their approach -NetVLAD, shows that training representations directly for the end-task, object recognition, is crucial for obtaining good performance. Representations trained on the end-task of object recognition consistently outperform by a large margin off-the-shelf CNNs on benchmarks illustrating their approach can learn rich yet compact image representations for object recognition and that the popular idea of using pretrained networks “off-the-shelf” is sub-optimal.	While there have been many improvements in designing betterimage retrieval [2, 3, 12, 11, 17, 26, 27, 29, 25, 32, 48, 51, 52, 53, 54, 71, 78, 79, 82] and place recognition [4, 10, 15, 16, 24, 9, 35, 46, 44, 64, 65, 63, 75, 81, 80] systems, not many works have performedlearning for these tasks.All relevant learning-based approaches fall into one or both of the followingtwo categories:(i) learning for an auxiliary task (e.g. some form of distinctiveness of local features [4, 15, 30, 35, 58, 59, 90]), and (ii) learning on top of shallow hand-engineered descriptors that cannot be fine-tuned for the target task [2, 24, 9, 35, 57]. Both of these are in spirit opposite to the core idea behinddeep learning that has provided a major boost in performance in variousrecognition tasks: end-to-end learning. We will indeed show insection 5.2 that training representations directly for the end-task,place recognition, is crucial for obtaining good performance.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Their architecture managed to improve over current state-of-the-art compact image representations on standard image retrieval benchmarks by large margin on available datasets, obtaining an mAP of 63. 5%, 73. 5% and 79. 9% on Oxford 5k, Paris 6k, Holidays, respectively.  which is a +20% relative improvement on Oxford 5k.  Their proposed representations learnt end-to-end, outperformed the pretrained image representations and off-the-shelf CNN descriptors.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.We show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Their architecture managed to improve over current state-of-the-art compact image representations on standard image retrieval benchmarks by large margin on available datasets, obtaining an mAP of 63.5%, 73.5% and 79.9% on Oxford 5k, Paris 6k, Holidays, respectively. which is a +20% relative improvement on Oxford 5k. Their proposed representations learnt end-to-end, outperformed the pretrained image representations and off-the-shelf CNN descriptors, and also demonstrated superior robustness to occlusions and lighting variations.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.We show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.	Incorrect	Invent something doesn't mentioned	Correct	Correct	Correct	Correct	Correct	Correct
L2-norm for each column of the representation matrix, converted into a vector, and finally L2-normalized over the new vector.	Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
L1-norm for each column of the representation matrix, converted into a vector, and finally L1-normalized over the new vector.	Formally, given N D-dimensional local image descriptors \{\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\} as input,and K cluster centres (“visual words”) \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} as VLAD parameters,the output VLAD image representation V is K\times D-dimensional.For convenience we will write V as a K\times D matrix, but this matrixis converted into a vector and, after normalization, used asthe image representation. The (j,k) element of V is computedas follows:V(j,k)=\sum_{i=1}^{N}a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})\left(x_{i}(j)-c_{k}(j)\right),(1)where x_{i}(j) and c_{k}(j) are the j-th dimensions of the i-th descriptor and k-th cluster centre, respectively.a_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) denotes the membership ofthe descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to k-th visual word, i.e. it is 1 if cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}is the closest cluster to descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} and 0 otherwise.Intuitively, each D-dimensional column k of V records the sum ofresiduals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}) of descriptors which are assigned to cluster \mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}.The matrix V is then L2-normalized column-wise(intra-normalization [3]),converted into a vector,and finally L2-normalized in its entirety [29].	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Correct	Incorrect
The original VLAD method uses hand-crafted features and applies the VLAD technique to them by concatenating multiple VLADs.  On the other hand, NetVLAD layer uses a CNN to extract features and applies the VLAD technique in a single layer by learning the aggregation weights of the residuals (xi − ck) in different parts of the descriptor space.  The NetVLAD layer has three independent sets of parameters, {wk}, {bk} and {ck}, that enables greater flexibility and adaptability to the CNN features than the original VLAD method which uses only {ck}.	In order to profit from years of wisdom produced in image retrieval,we propose to mimic VLAD in a CNN frameworkand design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representationtrainable end-to-end on the target task (in our case place recognition).To construct a layer amenable to training via backpropagation,it is required that the layer’s operation is differentiable withrespect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable, which we describe next.By expanding the squares in (2), it is easy to see that the terme^{-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\rVert^{2}} cancels between the numerator and the denominatorresulting in a soft-assignment of the following form\bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})=\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}},(3)where vector \mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}=2\alpha\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k} and scalar b_{k}=-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\rVert^{2}.The final form of the NetVLAD layer is obtained byplugging the soft-assignment (3) into the VLAD descriptor (1) resulting inV(j,k)=\sum_{i=1}^{N}\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}}\left(x_{i}(j)-c_{k}(j)\right),(4)where\{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} are sets of trainable parameters for each cluster k.Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k})in different parts of the descriptor space weighted by the soft-assignment \bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) of descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to cluster k.Note however, that the NetVLAD layer has three independentsets of parameters \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\}, compared to just\{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} of the original VLAD. This enables greater flexibility than the original VLAD,as explained in figure 3.Decoupling \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k},b_{k}\} from \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} has been proposed in[3] as a means to adapt the VLAD to a new dataset.All parameters of NetVLAD are learnt for the specific task in an end-to-end manner.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The original VLAD method uses CNN-learned features and applies the VLAD technique to them by concatenating multiple VLADs. On the other hand, NetVLAD layer uses hand-crafted features and applies the VLAD technique in a single layer without learning the aggregation weights of the residuals (xi − ck) in different parts of the descriptor space. The NetVLAD layer has only one set of parameters, {ck}, that reduces flexibility and adaptability to the CNN features compared to the original VLAD method which uses multiple parameter sets.	In order to profit from years of wisdom produced in image retrieval,we propose to mimic VLAD in a CNN frameworkand design a trainable generalized VLAD layer, NetVLAD. The result is a powerful image representationtrainable end-to-end on the target task (in our case place recognition).To construct a layer amenable to training via backpropagation,it is required that the layer’s operation is differentiable withrespect to all its parameters and the input.Hence, the key challenge is to make the VLAD pooling differentiable, which we describe next.By expanding the squares in (2), it is easy to see that the terme^{-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}\rVert^{2}} cancels between the numerator and the denominatorresulting in a soft-assignment of the following form\bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i})=\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}},(3)where vector \mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}=2\alpha\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k} and scalar b_{k}=-\alpha\lVert\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\rVert^{2}.The final form of the NetVLAD layer is obtained byplugging the soft-assignment (3) into the VLAD descriptor (1) resulting inV(j,k)=\sum_{i=1}^{N}\frac{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k}}}{\sum_{k^{\prime}}{e^{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k^{\prime}}^{T}\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}+b_{k^{\prime}}}}}\left(x_{i}(j)-c_{k}(j)\right),(4)where\{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} are sets of trainable parameters for each cluster k.Similarly to the original VLAD descriptor, the NetVLAD layer aggregates the first order statistics of residuals (\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}-\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k})in different parts of the descriptor space weighted by the soft-assignment \bar{a}_{k}(\mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i}) of descriptor \mathchoice{\mbox{\boldmath$\displaystyle\bf x$}}{\mbox{\boldmath$\textstyle\bf x$}}{\mbox{\boldmath$\scriptstyle\bf x$}}{\mbox{\boldmath$\scriptscriptstyle\bf x$}}_{i} to cluster k.Note however, that the NetVLAD layer has three independentsets of parameters \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k}\}, \{b_{k}\} and \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\}, compared to just\{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} of the original VLAD. This enables greater flexibility than the original VLAD,as explained in figure 3.Decoupling \{\mathchoice{\mbox{\boldmath$\displaystyle\bf w$}}{\mbox{\boldmath$\textstyle\bf w$}}{\mbox{\boldmath$\scriptstyle\bf w$}}{\mbox{\boldmath$\scriptscriptstyle\bf w$}}_{k},b_{k}\} from \{\mathchoice{\mbox{\boldmath$\displaystyle\bf c$}}{\mbox{\boldmath$\textstyle\bf c$}}{\mbox{\boldmath$\scriptstyle\bf c$}}{\mbox{\boldmath$\scriptscriptstyle\bf c$}}_{k}\} has been proposed in[3] as a means to adapt the VLAD to a new dataset.All parameters of NetVLAD are learnt for the specific task in an end-to-end manner.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
typo in the original composition, should be "the entire (Pitts250k) dataset" rather than "the entire (Pitts30k) dataset"	contains 250k database images downloaded from Google Street View and 24k test queries generated from Street View but taken at different times, years apart. We divide this dataset into three roughly equal parts for training, validation and testing, each containing around 83k database images and 8k queries, where the division was done geographically to ensure the sets contain independent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database images in each of the train/val(idation)/test sets, which are also geographically disjoint.	Correct		Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
They use the entire (Pitts250k) dataset and divide it into three equal parts for training, validation and testing, each containing around 83k database images and 8k queries which are geographically disjoint.	contains 250k database images downloaded from Google Street View and 24k test queries generated from Street View but taken at different times, years apart. We divide this dataset into three roughly equal parts for training, validation and testing, each containing around 83k database images and 8k queries, where the division was done geographically to ensure the sets contain independent images.To facilitate faster training, for some experiments,a smaller subset (Pitts30k) is used, containing 10k database images in each of the train/val(idation)/test sets, which are also geographically disjoint.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Correct
As authors have considered : Google Street View Time Machine was a novel source (at that time) for learning an image representation for place recognition .	We propose to exploit a new source of data – Google Street View Time Machine –which provides multiple street-level panoramic images taken at different times at close-by spatial locations on the map.As will be seen in section 5.2, this novel data source is precious for learning an image representation for place recognition. As shown in figure 4, the same locations are depicted at different times and seasons, providing the learning algorithm with crucial information it can use to discover which features are useful or distracting, and what changes should the image representation be invariant to, in order to achieve good place recognition performance.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
As authors have considered : Google Street View Time Machine was a commonly used source for learning an image representation for place recognition.	We propose to exploit a new source of data – Google Street View Time Machine –which provides multiple street-level panoramic images taken at different times at close-by spatial locations on the map.As will be seen in section 5.2, this novel data source is precious for learning an image representation for place recognition. As shown in figure 4, the same locations are depicted at different times and seasons, providing the learning algorithm with crucial information it can use to discover which features are useful or distracting, and what changes should the image representation be invariant to, in order to achieve good place recognition performance.	Incorrect	Opposite	Correct	Correct	Correct	Correct	Correct	Correct
Authors reported of using mAP to compare their approach with image retrieval benchmark.  Mean average precision is used in information retrieval and computer vision tasks to evaluate the performance of a model through its ability to retrieve relevant images from a dataset.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
Authors reported of using top-1 accuracy to compare their approach with image retrieval benchmark. Top-1 accuracy is used in information retrieval and computer vision tasks to evaluate the performance of a model through its ability to retrieve relevant images from a dataset.	We use our best performing network (VGG-16, f_{VLAD} with whitening down to 256-D)trained completely on Pittsburgh, to extract image representationsfor standard object and image retrieval benchmarks.Our representation sets the state-of-the-art for compact image representations (256-D)by a large margin on all three datasets, obtaining an mAP of63.5%, 73.5% and 79.9% onOxford 5k [53], Paris 6k [54], Holidays [26], respectively;for example, this is a +20% relative improvement on Oxford 5k.Appendix Ccontains more detailed results.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Unrelated	Incorrect	Incorrect
Authors mentioned that they have cropped the CNN at the last convolutional layer and view it as a dense descriptor extractor as they found it work well in experiments i. ,instance retrieval and texture recognition.  However, this point doesn't have enough discussion in the paper but generally speaking, cropping at the end, this way, may obtain good levels of abstraction and compact vector representations as going deeper and deeper, while cropping at middle may not extract the desired features and also, may not be  dense enough to complete this task with good performance.	In order to learn the representation end-to-end, we designa CNN architecture that mimics this standard retrieval pipeline in an unifiedand principled manner with differentiable modules.For step (i), we crop the CNNat the last convolutional layer and view itas a dense descriptor extractor.This has been observed to work well for instance retrieval[6, 7, 62] and texture recognition [13].Namely, the output of the last convolutional layer is aH\times W\times D map which can be considered as a set of D-dimensionaldescriptors extracted at H\times W spatial locations.For step (ii) we design a new pooling layer inspired by the Vector of Locally Aggregated Descriptors (VLAD) [29]that pools extracted descriptors into a fixed image representation and its parameters are learnable via back-propagation.We call this new pooling layer “NetVLAD” layer and describe it in the next section.	Correct		Correct	Correct	Correct	Correct	Correct	Incorrect
Authors mentioned that they have cropped the CNN at the middle convolutional layer and view it as a dense descriptor extractor as they found it work well in experiments i. ,instance retrieval and texture recognition.  However, this point doesn't have enough discussion in the paper but generally speaking, cropping at the end, this way, may obtain good levels of abstraction and compact vector representations as going deeper and deeper, while cropping at middle may not extract the desired features and also, may not be dense enough to complete this task with good performance.	In order to learn the representation end-to-end, we designa CNN architecture that mimics this standard retrieval pipeline in an unifiedand principled manner with differentiable modules.For step (i), we crop the CNNat the last convolutional layer and view itas a dense descriptor extractor.This has been observed to work well for instance retrieval[6, 7, 62] and texture recognition [13].Namely, the output of the last convolutional layer is aH\times W\times D map which can be considered as a set of D-dimensionaldescriptors extracted at H\times W spatial locations.For step (ii) we design a new pooling layer inspired by the Vector of Locally Aggregated Descriptors (VLAD) [29]that pools extracted descriptors into a fixed image representation and its parameters are learnable via back-propagation.We call this new pooling layer “NetVLAD” layer and describe it in the next section.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Multi-orientation pooling is a learning strategy in which the rotations around vertical axis are combined with the elevation rotations, although I am not sure what are elevation rotations.	Both of our new networks are sensitive to shape orientation, i.e., they capture different information at different orientations. To capture a more holistic sense of a 3D object, we add an orientation pooling stage that aggregates information from different orientations.Similar to Su-MVCNN [32] which aggregates information from multiple view inputs through a view-pooling layer and follow-on fully connected layers, we sample 3D input from different orientations and aggregate them in a multi-orientation volumetric CNN (MO-VCNN) as shown in Fig 5. At training time, we generate different rotations of the 3D model by changing both azimuth and elevation angles, sampled randomly. A volumetric CNN is firstly trained on single rotations. Then we decompose the network to \text{CNN}_{1} (lower layers) and \text{CNN}_{2} (higher layers) to construct a multi-orientation version. The MO-VCNN’s weights are initialized by a previously trained volumetric CNN with \text{CNN}_{1}’s weights fixed during fine-tuning. While a common practice is to extract the highest level features (features before the last classification linear layer) of multiple orientations, average/max/concatenate them, and train a linear SVM on the combined feature, this is just a special case of the MO-VCNN.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.	Correct		Correct	Incorrect	Correct	Incorrect	Incorrect	Correct
Multi-orientation pooling is a learning strategy in which the rotations around vertical axis are combined with the illumination variations.	Both of our new networks are sensitive to shape orientation, i.e., they capture different information at different orientations. To capture a more holistic sense of a 3D object, we add an orientation pooling stage that aggregates information from different orientations.Similar to Su-MVCNN [32] which aggregates information from multiple view inputs through a view-pooling layer and follow-on fully connected layers, we sample 3D input from different orientations and aggregate them in a multi-orientation volumetric CNN (MO-VCNN) as shown in Fig 5. At training time, we generate different rotations of the 3D model by changing both azimuth and elevation angles, sampled randomly. A volumetric CNN is firstly trained on single rotations. Then we decompose the network to \text{CNN}_{1} (lower layers) and \text{CNN}_{2} (higher layers) to construct a multi-orientation version. The MO-VCNN’s weights are initialized by a previously trained volumetric CNN with \text{CNN}_{1}’s weights fixed during fine-tuning. While a common practice is to extract the highest level features (features before the last classification linear layer) of multiple orientations, average/max/concatenate them, and train a linear SVM on the combined feature, this is just a special case of the MO-VCNN.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The reasoning is that low-frequency information in 3D seems to be quite discriminative, because the authors use the resolution of only 30x30x30, which is really low resolution in any case.  The only explanation of why the method is still working is that low-frequency information is discriminative.	As shown in Fig 2, even with similar level of object detail, the volumetric CNN (green) is 4.8\% worse than the multi-view CNN (blue). That is, there is still significant room to improve the architecture of volumetric CNNs. This discovery motivates our efforts in Sec 4 to improve volumetric CNNs. Additionally, low-frequency information in 3D seems to be quite discriminative for object classification—it is possible to achieve 89.5\% accuracy (blue) at a resolution of only 30\times 30\times 30. This discovery motivates our efforts in Sec 5 to improve multi-view CNNs with a 3D multi-resolution approach.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The reasoning is that low-frequency information in 3D seems to be quite discriminative, because the authors use the resolution of only 10x10x10, which is really low resolution in any case.  The only explanation of why the method is still working is that low-frequency information is discriminative.	As shown in Fig 2, even with similar level of object detail, the volumetric CNN (green) is 4.8\% worse than the multi-view CNN (blue). That is, there is still significant room to improve the architecture of volumetric CNNs. This discovery motivates our efforts in Sec 4 to improve volumetric CNNs. Additionally, low-frequency information in 3D seems to be quite discriminative for object classification—it is possible to achieve 89.5\% accuracy (blue) at a resolution of only 30\times 30\times 30. This discovery motivates our efforts in Sec 5 to improve multi-view CNNs with a 3D multi-resolution approach.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The auxiliary tasks are closely related to the main tasks but are difficult to overfit to keep the learning from early convergence even when the main task is overfitted.  The property of the auxiliary tasks is that they are supposed to be challenging by using only only partial subvolumes for the predictions.  The auxiliary tasks better exploit the discrimnative power of local regions because they do not use additional knowledge about the semantics of the object.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.We observe significant overfitting when we train the volumetric CNN proposed by [33] in an end-to-end fashion (see supplementary). When the volumetric CNN overfits to the training data, it has no incentive to continue learning. We thus introduce auxiliary tasks that are closely correlated with the main task but are difficult to overfit, so that learning continues even if our main task is overfitted. These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local subvolume of the input.Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. This design is different from the classic multi-task learning setting of hetergenous auxiliary tasks, which inevitably requires collecting additional annotations (e.g., conducting both object classification and detection [9]).We provide a detailed analysis over factors that influence the performance of volumetric CNNs, including network architecture and volumn resolution. Based upon our analysis, we strive to improve the performance of volumetric CNNs.We propose two volumetric CNN network architectures that signficantly improve state-of-the-art of volumetric CNNs on 3D shape classification. This result has also closed the gap between volumetric CNNs and multi-view CNNs, when they are provided with 3D input discretized at 30\times 30\times 30 3D resolution.The first network introduces auxiliary learning tasks by classifying part of an object, which help to scrutize details of 3D objects more deeply.The second network uses long anisotropic kernels to probe for long-distance interactions. Combining data augmentation with a multi-orientation pooling, we observe significant performance improvement for both networks.We also conduct extensive experiments to study the influence of volume resolution, which sheds light on future directions of improving volumetric CNNs.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The auxiliary tasks are closely related to the main tasks but are difficult to overfit to keep the learning from early convergence even when the main task is overfitted. The property of the auxiliary tasks is that they are supposed to be challenging by using only partial subvolumes for the predictions. The auxiliary tasks better exploit the representational power of local regions because they do not use additional knowledge about the semantics of the object.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.We observe significant overfitting when we train the volumetric CNN proposed by [33] in an end-to-end fashion (see supplementary). When the volumetric CNN overfits to the training data, it has no incentive to continue learning. We thus introduce auxiliary tasks that are closely correlated with the main task but are difficult to overfit, so that learning continues even if our main task is overfitted. These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local subvolume of the input.Without complete knowledge of the object, the auxiliary tasks are more challenging, and can thus better exploit the discriminative power of local regions. This design is different from the classic multi-task learning setting of hetergenous auxiliary tasks, which inevitably requires collecting additional annotations (e.g., conducting both object classification and detection [9]).We provide a detailed analysis over factors that influence the performance of volumetric CNNs, including network architecture and volumn resolution. Based upon our analysis, we strive to improve the performance of volumetric CNNs.We propose two volumetric CNN network architectures that signficantly improve state-of-the-art of volumetric CNNs on 3D shape classification. This result has also closed the gap between volumetric CNNs and multi-view CNNs, when they are provided with 3D input discretized at 30\times 30\times 30 3D resolution.The first network introduces auxiliary learning tasks by classifying part of an object, which help to scrutize details of 3D objects more deeply.The second network uses long anisotropic kernels to probe for long-distance interactions. Combining data augmentation with a multi-orientation pooling, we observe significant performance improvement for both networks.We also conduct extensive experiments to study the influence of volume resolution, which sheds light on future directions of improving volumetric CNNs.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The anisotropic probing kernel is designed specifically to be able to capture long-range interactions between 3D points of the objects.  In particular, the kernel is elongated and captures only voxels of the same height and along the probing direction.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The isotropic probing kernel is designed specifically to be able to capture long-range interactions between 3D points of the objects. In particular, the kernel is elongated and captures only voxels of the same height and along the probing direction.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Similar to 2D multi-resolution filtering approaches, the 3D multi-resolution approaches capture information at multiple scales.  The main difference is that the 3D filtering approach respects the distances in 3D.	Although the multi-view CNN presented by [32] produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation.We introduce multi-resolution 3D filtering to capture information at multiple scales. We perform sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as they are view-invariant. In particular, this helps regularize out potential noise or irregularities in real-world scanned data (relative to synthetic training data), enabling robust performance on real-world scans. Note that our 3D multi-resolution filtering is different from classical 2D multi-resolution approaches, since the 3D filtering respects the distance in 3D.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Similar to 2D multi-resolution filtering approaches, the 3D multi-resolution approaches capture information at a single scale.  The main difference is that the 3D filtering approach respects the distances in 3D.	Although the multi-view CNN presented by [32] produces compelling results, we are able to improve its performance through a multi-resolution extension with improved data augmentation.We introduce multi-resolution 3D filtering to capture information at multiple scales. We perform sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as they are view-invariant. In particular, this helps regularize out potential noise or irregularities in real-world scanned data (relative to synthetic training data), enabling robust performance on real-world scans. Note that our 3D multi-resolution filtering is different from classical 2D multi-resolution approaches, since the 3D filtering respects the distance in 3D.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Incorrect
The authors follow previous works, such as VoxNet [24[, 3DShapeNets [33], and MVCNN [32] that also use ModelNet test set to evaluate their approaches.  In order to be able to compare with them and provide more quantitative results, this paper also evaluates on ModelNet's test set.  Additionally, as described in Table 1 (paragraph P7), the authors use ModelNet to provide additional evaluations and ablative analyses, which is more suitable to be done on synthetic data than RGB-D data.	We use ModelNet [33] for our training and testing datasets. ModelNet currently contains 127,915 3D CAD models from 662 categories. ModelNet40, a subset including 12,311 models from 40 categories, is well annotated and can be downloaded from the web. The authors also provide a training and testing split on the website, in which there are 9,843 training and 2,468 test models444VoxNet [24] uses the train/test split provided on the website and report average class accuracy on the 2,468 test split. 3DShapeNets [33] and MVCNN [32] use another train/test split comprising the first 80 shapes of each category in the “train” folder (or all shapes if there are fewer than 80) and the first 20 shapes of each category in the “test” folder, respectively.. We use this train/test split for our experiments.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors follow previous works, such as VoxNet [24[, 3DShapeNets [33], and MVCNN [32] that also use ModelNet test set to evaluate their approaches.  In order to be able to compare with them and provide more quantitative results, this paper only evaluates on ModelNet's dev set.  Additionally, as described in Table 1 (paragraph P7), the authors use ModelNet to provide additional evaluations and ablative analyses, which is more suitable to be done on synthetic data than RGB-D data.	We use ModelNet [33] for our training and testing datasets. ModelNet currently contains 127,915 3D CAD models from 662 categories. ModelNet40, a subset including 12,311 models from 40 categories, is well annotated and can be downloaded from the web. The authors also provide a training and testing split on the website, in which there are 9,843 training and 2,468 test models444VoxNet [24] uses the train/test split provided on the website and report average class accuracy on the 2,468 test split. 3DShapeNets [33] and MVCNN [32] use another train/test split comprising the first 80 shapes of each category in the “train” folder (or all shapes if there are fewer than 80) and the first 20 shapes of each category in the “test” folder, respectively.. We use this train/test split for our experiments.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors improve upon previous augmentation strategies and provide analyses to compare each combination of the augmentation strategy (azimuth rotation (AZ), AZ + translation, and AZ + elevation rotation), and conclude that the latter gives the best results.	Compared with 2D image datasets, currently available 3D shape datasets are limited in scale and variation. To fully exploit the design of our networks, we augment the training data with different azimuth and elevation rotations.This allows the first network to cover local regions at different orientations, and the second network to relate distant points at different relative angles.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors improve upon previous augmentation strategies and provide analyses to compare each combination of the augmentation strategy (azimuth rotation (AZ), AZ + translation, and AZ + elevation rotation), and conclude that the previous gives the best results.	Compared with 2D image datasets, currently available 3D shape datasets are limited in scale and variation. To fully exploit the design of our networks, we augment the training data with different azimuth and elevation rotations.This allows the first network to cover local regions at different orientations, and the second network to relate distant points at different relative angles.Compared to 3DShapeNets [33] which only augments data by rotating around vertical axis, our experiment shows that orientation pooling combined with elevation rotation can greatly increase performance.We use the same volumetric CNN model, the end-to-end learning verion of 3DShapeNets [33], to train and test on three variations of augmented data (Table 1). Similar trend is observed for other volumetric CNN variations.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The similarities between volumetric and multi-view representation are:- when stored as tensors, both representations can easily be used to train convolutional neural networks	Two representations of generic 3D shapes are popularly used for object classification, volumetric and multi-view (Fig 1). The volumetric representation encodes a 3D shape as a 3D tensor of binary or real values. The multi-view representation encodes a 3D shape as a collection of renderings from multiple viewpoints. Stored as tensors, both representations can easily be used to train convolutional neural networks, i.e., volumetric CNNs and multi-view CNNs.Intuitively, a volumetric representation should encode as much information, if not more, than its multi-view counterpart. However, experiments indicate that multi-view CNNs produce superior performance in object classification. Fig 2 reports the classification accuracy on the ModelNet40 dataset by state-of-the-art volumetric/multi-view architectures111We train models by replicating the architecture of [33] for volumetric CNNs and [32] for multi-view CNNs. All networks are trained in an end-to-end fashion. All methods are trained/tested on the same split for fair comparison. The reported numbers are average instance accuracy. See Sec 6 for details.. A volumetric CNN based on voxel occupancy (green) is 7.3\% worse than a multi-view CNN (yellow).We investigate this performance gap in order to ascertain how to improve volumetric CNNs. The gap seems to be caused by two factors: input resolution and network architecture differences. The multi-view CNN down-samples each rendered view to 227\times 227 pixels (Multi-view Standard Rendering in Fig 1); to maintain a similar computational cost, the volumetric CNN uses a 30\times 30\times 30 occupancy grid (Volumetric Occupancy Grid in Fig 1)222Note that 30\times 30\times 30\approx 227\times 227.. As shown in Fig 1, the input to the multi-view CNN captures more detail.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The similarities between volumetric and multi-view representation are:- when stored as tensors, both representations can't easily be used to train convolutional neural networks	Two representations of generic 3D shapes are popularly used for object classification, volumetric and multi-view (Fig 1). The volumetric representation encodes a 3D shape as a 3D tensor of binary or real values. The multi-view representation encodes a 3D shape as a collection of renderings from multiple viewpoints. Stored as tensors, both representations can easily be used to train convolutional neural networks, i.e., volumetric CNNs and multi-view CNNs.Intuitively, a volumetric representation should encode as much information, if not more, than its multi-view counterpart. However, experiments indicate that multi-view CNNs produce superior performance in object classification. Fig 2 reports the classification accuracy on the ModelNet40 dataset by state-of-the-art volumetric/multi-view architectures111We train models by replicating the architecture of [33] for volumetric CNNs and [32] for multi-view CNNs. All networks are trained in an end-to-end fashion. All methods are trained/tested on the same split for fair comparison. The reported numbers are average instance accuracy. See Sec 6 for details.. A volumetric CNN based on voxel occupancy (green) is 7.3\% worse than a multi-view CNN (yellow).We investigate this performance gap in order to ascertain how to improve volumetric CNNs. The gap seems to be caused by two factors: input resolution and network architecture differences. The multi-view CNN down-samples each rendered view to 227\times 227 pixels (Multi-view Standard Rendering in Fig 1); to maintain a similar computational cost, the volumetric CNN uses a 30\times 30\times 30 occupancy grid (Volumetric Occupancy Grid in Fig 1)222Note that 30\times 30\times 30\approx 227\times 227.. As shown in Fig 1, the input to the multi-view CNN captures more detail.	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
The authors choose CAD models and RGB-D data for several reasons.  First, to demonstrate that, while learning only the synthetic CAD models they are still able to generalize to real-world RGB-D reconstructions.  Second, the RGB-D dataset is exclusively proposed in this paper, and it is purposely difficult (contains occlusions and reconstruction noise).  Third, by using multiple sources of data, they demonstrate that the model is robust to different data types.	We provide a new real-world scanning dataset benchmark, comprising 243 objects of 12 categories; the geometry is captured with an ASUS Xtion Pro and a dense reconstruction is obtained using the publicly-available VoxelHashing framework [25].For each scan, we have performed a coarse, manual segmentation of the object of interest.In addition, each scan is aligned with the world-up vector.While there are existing datasets captured with commodity range sensors – e.g., [29, 34, 31] – this is the first containing hundreds of annotated models from dense 3D reconstructions.The goal of this dataset is to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a single RGB-D frame but still with many occlusions. This dataset is used as a test set.In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.Wu et al. [33] lift 2.5D to 3D with their 3DShapeNets approach by categorizing each voxel as free space, surface or occluded, depending on whether it is in front of, on, or behind the visible surface (i.e., the depthvalue) from the depth map.The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks.Their method is particularly relevant in the context of this work, as they are the first to apply CNNs on a 3D representation.A similar approach is VoxNet [24], which also uses binary voxel grids and a corresponding 3D CNN architecture.The advantage of these approaches is that it can process different sources of 3D data, including LiDAR point clouds, RGB-D point clouds, and CAD models; we likewise follow this direction.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors choose CAD models and RGB-D data for several reasons.  First, to demonstrate that, while learning only the synthetic CAD models they are still able to generalize to real-world RGB-D reconstructions.  Second, the RGB-D dataset is exclusively proposed in this paper, and it is purposely difficult (contains occlusions and reconstruction noise).  However, the authors didn't use multiple sources of data to demonstrate that the model is robust to different data types.	We provide a new real-world scanning dataset benchmark, comprising 243 objects of 12 categories; the geometry is captured with an ASUS Xtion Pro and a dense reconstruction is obtained using the publicly-available VoxelHashing framework [25].For each scan, we have performed a coarse, manual segmentation of the object of interest.In addition, each scan is aligned with the world-up vector.While there are existing datasets captured with commodity range sensors – e.g., [29, 34, 31] – this is the first containing hundreds of annotated models from dense 3D reconstructions.The goal of this dataset is to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a single RGB-D frame but still with many occlusions. This dataset is used as a test set.In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.Wu et al. [33] lift 2.5D to 3D with their 3DShapeNets approach by categorizing each voxel as free space, surface or occluded, depending on whether it is in front of, on, or behind the visible surface (i.e., the depthvalue) from the depth map.The resulting representation is a 3D binary voxel grid, which is the input to a CNN with 3D filter banks.Their method is particularly relevant in the context of this work, as they are the first to apply CNNs on a 3D representation.A similar approach is VoxNet [24], which also uses binary voxel grids and a corresponding 3D CNN architecture.The advantage of these approaches is that it can process different sources of 3D data, including LiDAR point clouds, RGB-D point clouds, and CAD models; we likewise follow this direction.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors train on synthetic data for several reasons.  One of the reasons is that the proposed method can better adapt from synthetic to real data than previous methods.  Other than the evidential information, it is easier to collect a large amount of synthetic data compared to real data, especially for training purposes.	In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors didn't train on synthetic data for several reasons.  One of the reasons is that the proposed method can not adapt from synthetic to real data than previous methods.  Other than the evidential information, it is more difficult to collect a large amount of synthetic data compared to real data, especially for training purposes.	In addition to providing extensive experiments on 3D CAD model datasets, we also introduce a dataset of real-world 3D data, constructed using dense 3D reconstruction taken with [25]. Experiments show that our networks can better adapt from synthetic data to this real-world data than previous methods.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The anisotropic probing kernels can be seen as a special type of convolutional layer.  These kernels are elongated in 3D and can thus encode long-range interactions between the points.  They are an alternative to using standard computer graphics rendering.  Using anisotropic probing kernels helps to capture the global structure of the 3D volume.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Key to this network is the use of an elongated anisotropic kernel which helps capture the global structure of the 3D volume.As illustrated in Fig 4, the neural network has two modules: an anisotropic probing module and a network in network module.The anisotropic probing module contains three convolutional layers of elongated kernels, each followed by a nonlinear ReLU layer.Note that both the input and output of each layer are 3D tensors.In contrast to traditional isotropic kernels, an anisotropic probing module has the advantage of aggregating long-range interactions in the early feature learning stage with fewer parameters. As a comparison, with traditional neural networks constructed from isotropic kernels, introducing long-range interactions at an early stage can only be achieved through large kernels, which inevitably introduce many more parameters.After anisotropic probing, we use an adapted NIN network [23] to address the classification problem.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.From Table 2, first, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show superior performance, indicating the effectiveness of our design;second, multi-orientation pooling increases performance for all network variations. This is especially significant for the anisotropic probing network, since each orientation usually only carries partial information of the object.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The anisotropic probing kernels can be seen as a special type of convolutional layer.  These kernels are elongated in 2D and can thus encode long-range interactions between the points.  They are an alternative to using standard computer graphics rendering.  Using anisotropic probing kernels helps to capture the global structure of the 2D volume.	We propose two network variations that significantly improve state-of-the-art CNNs on 3D volumetric data.The first network is designed to mitigate overfitting by introducing auxiliary training tasks, which are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial subvolumes. Therefore, no additional annotation efforts are needed. The second network is designed to mimic multi-view CNNs, as they are strong in 3D shape classification. Instead of using rendering routines from computer graphics, our network projects a 3D shape to 2D by convolving its 3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between points. An image CNN is then appended to classify the 2D projection. Note that the training of the projection module and the image classification module is end-to-end. This emulation of multi-view CNNs achieves similar performance to them, using only standard layers in CNN.Key to this network is the use of an elongated anisotropic kernel which helps capture the global structure of the 3D volume.As illustrated in Fig 4, the neural network has two modules: an anisotropic probing module and a network in network module.The anisotropic probing module contains three convolutional layers of elongated kernels, each followed by a nonlinear ReLU layer.Note that both the input and output of each layer are 3D tensors.In contrast to traditional isotropic kernels, an anisotropic probing module has the advantage of aggregating long-range interactions in the early feature learning stage with fewer parameters. As a comparison, with traditional neural networks constructed from isotropic kernels, introducing long-range interactions at an early stage can only be achieved through large kernels, which inevitably introduce many more parameters.After anisotropic probing, we use an adapted NIN network [23] to address the classification problem.Our anistropic probing network is capable of capturing internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to capture any 3D structure, due to its relationship with the Radon transform.The two networks proposed above are both sensitive to model orientation. In the subvolume supervision method, different model orientations define different local subvolumes; in the anisotropic probing method, only voxels of the same height and along the probing direction can have interaction in the early feature extraction stage.Thus it is helpful to augment the training data by varying object orientation and combining predictions through orientation pooling.From Table 2, first, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show superior performance, indicating the effectiveness of our design;second, multi-orientation pooling increases performance for all network variations. This is especially significant for the anisotropic probing network, since each orientation usually only carries partial information of the object.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
A highway network is a layer that uses an information highway layer, and a plain network is a general layer.  In highway networks, increasing layer depth does not affect performance, but in plain networks, it can.  One layer of the plain network is made up of normal computation units, whereas the highway network is made up of block units.	Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the i^{th} unit computes y_{i}=H_{i}(\mathbf{x}), a highway network consists of multiple blocks such that the i^{th} block computes a block state H_{i}(\mathbf{x}) and transform gate output T_{i}(\mathbf{x}). Finally, it produces the block output y_{i}=H_{i}(\mathbf{x})*T_{i}(\mathbf{x})+x_{i}*(1-T_{i}(\mathbf{x})), which is connected to the next layer.222Our pilot experiments on training very deep networks were successful with a more complex block design closely resembling an LSTM block “unrolled in time”. Here we report results only for a much simplified form.To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
A highway network is a layer that uses an information highway layer, and a plain network is a general layer.  In highway networks, increasing layer depth affects performance, but in plain networks, it can.  One layer of the plain network is made up of normal computation units, whereas the highway network is made up of block units.	Thus, depending on the output of the transform gates, a highway layer can smoothly vary its behavior between that of H and that of a layer which simply passes its inputs through. Just as a plain layer consists of multiple computing units such that the i^{th} unit computes y_{i}=H_{i}(\mathbf{x}), a highway network consists of multiple blocks such that the i^{th} block computes a block state H_{i}(\mathbf{x}) and transform gate output T_{i}(\mathbf{x}). Finally, it produces the block output y_{i}=H_{i}(\mathbf{x})*T_{i}(\mathbf{x})+x_{i}*(1-T_{i}(\mathbf{x})), which is connected to the next layer.222Our pilot experiments on training very deep networks were successful with a more complex block design closely resembling an LSTM block “unrolled in time”. Here we report results only for a much simplified form.To support the hypothesis that highway networks do not suffer from increasing depth, we conducted a series of rigorous optimization experiments, comparing them to plain networks with normalized initialization [16, 17].The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	Incorrect	Change concept	Unrelated	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Although highway networks do not perform well at best, they do not break down significantly when stacked deeply.  Also, there is freedom in setting the number of depths, and it can be learned well with vanilla SGD.  In addition, meaningful outputs come out from all layers and information can be handed over dynamically.	The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input.This begs the question: does this behaviour manifest itself in trained networks or do they just learn a static routing that applies to all inputs similarly.A partial answer can be found by looking at the mean transform gate activity (second column) and the single example transform gate outputs (third column) in Figure 2.Especially for the CIFAR-100 case, most transform gates are active on average, while they show very selective activity for the single example.This implies that for each sample only a few blocks perform transformation but different blocks are utilized by different samples.We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \approx 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.Very deep highway networks, on the other hand, can directly be trained with simple gradient descent methods due to their specific architecture. This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential.The additional parameters required by the gating mechanism help in routing information through the use of multiplicative connections, responding differently to different inputs, unlike fixed “skip” connections.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Although highway networks do not perform well at best, they do not break down significantly when stacked deeply.  Also, there is freedom in setting the number of depths, and it can't be trained well with vanilla SGD.  In addition, meaningful outputs come out from all layers and information can be handed over dynamically.	The training curves for the best performing networks for each depth are shown in Figure 1. As expected, 10 and 20-layer plain networks exhibit very good performance (mean loss <1e^{-4}), which significantly degrades as depth increases, even though network capacity increases.Highway networks do not suffer from an increase in depth, and 50/100 layer highway networks perform similar to 10/20 layer networks. The 100-layer highway network performed more than 2 orders of magnitude better compared to a similarly-sized plain network.It was also observed that highway networks consistently converged significantly faster than plain ones.One possible advantage of the highway architecture over hard-wired shortcut connections is that the network can learn to dynamically adjust the routing of the information based on the current input.This begs the question: does this behaviour manifest itself in trained networks or do they just learn a static routing that applies to all inputs similarly.A partial answer can be found by looking at the mean transform gate activity (second column) and the single example transform gate outputs (third column) in Figure 2.Especially for the CIFAR-100 case, most transform gates are active on average, while they show very selective activity for the single example.This implies that for each sample only a few blocks perform transformation but different blocks are utilized by different samples.We see a different picture for the CIFAR-100 dataset (right) with performance degrading noticeably when removing any of the first \approx 40 layers.This suggests that for complex problems a highway network can learn to utilize all of its layers, while for simpler problems like MNIST it will keep many of the unneeded layers idle. Such behavior is desirable for deep networks in general, but appears difficult to obtain using plain networks.Very deep highway networks, on the other hand, can directly be trained with simple gradient descent methods due to their specific architecture. This property does not rely on specific non-linear transformations, which may be complex convolutional or recurrent transforms, and derivation of a suitable initialization scheme is not essential.The additional parameters required by the gating mechanism help in routing information through the use of multiplicative connections, responding differently to different inputs, unlike fixed “skip” connections.A possible objection is that many layers might remain unused if the transform gates stay closed. Our experiments show that this possibility does not affect networks adversely—deep and narrow highway networks can match/exceed the accuracy of wide and shallow maxout networks, which would not be possible if layers did not perform useful computations. Additionally, we can exploit the structure of highways to directly evaluate the contribution of each layer as shown in Figure 4. For the first time, highway networks allow us to examine how much computation depth is needed for a given problem, which can not be easily done with plain networks.Our primary contribution is to show that extremely deep highway networks can be trained directly using stochastic gradient descent (SGD), in contrast to plain networks which become hard to optimize as depth increases (Section 3.1).Deep networks with limited computational budget (for which a two-stage training procedure mentioned above was recently proposed [25]) can also be directly trained in a single stage when converted to highway networks. Their ease of training is supported by experimental results demonstrating that highway networks also generalize well to unseen data.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Contrary to the authors' expectations, most of the biases were said to be reduced during training.  In CIFAR-100, it is said that the biases increase according to the depth of the gradient.  Authors explain that this is because strong negative biases at low depths are not used to close the gate.	The transform gate biases of the two networks were initialized to -2 and -4 respectively.It is interesting to note that contrary to our expectations most biases decreased further during training.For the CIFAR-100 network the biases increase with depth forming a gradient.Curiously this gradient is inversely correlated with the average activity of the transform gates, as seen in the second column.This indicates that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective.This behavior is also suggested by the fact that the transform gate activity for a single example (column 3) is very sparse.The effect is more pronounced for the CIFAR-100 network, but can also be observed to a lesser extent in the MNIST network.	Correct		Correct	Correct	Correct	Correct	Incorrect	Incorrect
Contrary to the authors' expectations, most of the biases were said to be increased during training.  In CIFAR-100, it is said that the biases increase according to the depth of the gradient.  Authors explain that this is because strong negative biases at low depths are not used to close the gate.	The transform gate biases of the two networks were initialized to -2 and -4 respectively.It is interesting to note that contrary to our expectations most biases decreased further during training.For the CIFAR-100 network the biases increase with depth forming a gradient.Curiously this gradient is inversely correlated with the average activity of the transform gates, as seen in the second column.This indicates that the strong negative biases at low depths are not used to shut down the gates, but to make them more selective.This behavior is also suggested by the fact that the transform gate activity for a single example (column 3) is very sparse.The effect is more pronounced for the CIFAR-100 network, but can also be observed to a lesser extent in the MNIST network.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors are talking about features that were learned using the attention mechanism.  The model focuses on such features, which can include color, scale, or spatial information, when it processes an image for classification.  For example, the attention mechanism can learn that blue pixels in the background of the image from the sky are not important for image classification, and the model will consequently reduce the contribution of those pixels to the final classification result.	Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures [34, 16, 23, 40]. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10].Soft attention developed in recent work [3, 17] can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module [17] achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale [3] uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors are talking about features that were not learned using the attention mechanism.  The model can't focuses on such features, which can include color, scale, or spatial information, when it processes an image for classification.  For example, the attention mechanism can learn that blue pixels in the background of the image from the sky are not important for image classification, and the model will consequently reduce the contribution of those pixels to the final classification result.	Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures [34, 16, 23, 40]. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10].Soft attention developed in recent work [3, 17] can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module [17] achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale [3] uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Incorrect	Unrelated	Incorrect
The attention masks successfully learn meaningful information from the dataset and their usage resulted in state-of-the-art results, which indicates that the attention mechanism does learn more discriminative features.	The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The attention masks can't successfully learn meaningful information from the dataset and their usage resulted in state-of-the-art results, which indicates that the attention mechanism does not learn more discriminative features.	The Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.1, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.The experiment results are shown in Table 1, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention [3] or spatial attention [17], which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.	Incorrect	Opposite	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
The main metrics used to compare different methods were Top-1 and Top-5 error, test error on the CIFAR datasets, the number of parameters and the number of FLOPs.  The mean absolute response of output features of each stage was also used to compare their method with ResNet.	In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100 [19], and ImageNet [5].Our experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.After that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.In the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.We also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We compare ResNet-164 network with Attention-92 network under different noise levels.The Table 5 shows the results.The test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.In addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.These results suggest that our Residual Attention Network can perform well even trained with high level noise data.When the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.In this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.We compare our Residual Attention Network with state-of-the-art methods including ResNet [11] and Wide ResNet [39] on CIFAR-10 and CIFAR-100 datasets.The results are shown in Table 6.Our Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.Note that Attention-92 network achieves 4.99\% test error on CIFAR-10 and 21.71\% test error on CIFAR-100 compared with 5.46\% and 24.33\% test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.In addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet [5] image classification dataset with significant reduction of computation (69% forward FLOPs).In this experiment, we explore the efficiency of proposed Residual Attention Network.We compare Attention-56 with ResNet-152 [10].The ResNet-152 has 50 trunk Residual Units and 60.2\times 10^{6} parameters compared with 18 trunk Residual Units and 31.9\times 10^{6} parameters in Attention-56.We evaluate our model using single crop scheme on the ImageNet validation set and show results in Table 7.The Attention-56 network outperforms ResNet-152 by a large margin with a 0.4\% reduction on top-1 error and a 0.26\% reduction on top-5 error.More importantly, Attention-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.When the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.For Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1 [32] by a margin with a 0.94% reduction on top-1 error and a 0.21% reduction on top-5 error.The results show that our method can be applied on different network structures.We compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.Table 7 shows the results.Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is 0.6\%.Note that the ResNet-200 network contains 32\% more parameters than Attention-92.The computational complexity of Attention-92 shown in the Table 7 suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.Above results suggest that our model enjoys high efficiency and good performance.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The main metrics used to compare different methods were Top-1 and Top-5 error, test error on the CIFAR datasets, MNIST dataset and the ImageNet datasets, the number of parameters and the number of FLOPs.  The mean absolute response of output features of each stage was also used to compare their method with ResNet.	In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100 [19], and ImageNet [5].Our experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.After that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.In the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.We also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We compare ResNet-164 network with Attention-92 network under different noise levels.The Table 5 shows the results.The test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.In addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.These results suggest that our Residual Attention Network can perform well even trained with high level noise data.When the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.In this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.We compare our Residual Attention Network with state-of-the-art methods including ResNet [11] and Wide ResNet [39] on CIFAR-10 and CIFAR-100 datasets.The results are shown in Table 6.Our Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.Note that Attention-92 network achieves 4.99\% test error on CIFAR-10 and 21.71\% test error on CIFAR-100 compared with 5.46\% and 24.33\% test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.In addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet [5] image classification dataset with significant reduction of computation (69% forward FLOPs).In this experiment, we explore the efficiency of proposed Residual Attention Network.We compare Attention-56 with ResNet-152 [10].The ResNet-152 has 50 trunk Residual Units and 60.2\times 10^{6} parameters compared with 18 trunk Residual Units and 31.9\times 10^{6} parameters in Attention-56.We evaluate our model using single crop scheme on the ImageNet validation set and show results in Table 7.The Attention-56 network outperforms ResNet-152 by a large margin with a 0.4\% reduction on top-1 error and a 0.26\% reduction on top-5 error.More importantly, Attention-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.When the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.For Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1 [32] by a margin with a 0.94% reduction on top-1 error and a 0.21% reduction on top-5 error.The results show that our method can be applied on different network structures.We compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.Table 7 shows the results.Our Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is 0.6\%.Note that the ResNet-200 network contains 32\% more parameters than Attention-92.The computational complexity of Attention-92 shown in the Table 7 suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.Above results suggest that our model enjoys high efficiency and good performance.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
If naive stacking, too many attention modules will cause a drastic performance drop as the mask values will converge to 0.  However, the model in the paper uses their own stacking method, which avoids the downfall of naive stacking.  The only consequence when using the paper's stacking method is that the model will require more parameters and FLOPs.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100 (20.67% error), and challenging ImageNet dataset (0.6% top-1 accuracy improvement) with only 46\% trunk depth and 69\% forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.(1) Stacked network structure: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.(2) Attention Residual Learning: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
If naive stacking, too many attention modules will cause a drastic performance drop as the mask values will converge to 0.  However, the model in the paper uses their own stacking method, which avoids the downfall of naive stacking.  When using the paper's stacking method, the model won't require more parameters and FLOPs.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.1 shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.We train these networks using different mechanisms and summarize the results in the Table 3.As shown in Table 3, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method.The performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with “naive attention learning” method suffers obvious degradation with increased number of Attention Module.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, i.e. ResNet on CIFAR-10 (3.90% error), CIFAR-100 (20.67% error), and challenging ImageNet dataset (0.6% top-1 accuracy improvement) with only 46\% trunk depth and 69\% forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.(1) Stacked network structure: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.(2) Attention Residual Learning: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Because of the mask values being between 0 and 1, and the fact that naive stacking attention modules means using a dot product on the resulting masks, naive stacking causes a performance drop as the dot product of several modules will converge towards 0.  The attention residual learning mechanism changes this by making the lower bound of the mask values the original features instead of 0.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.We propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output H of Attention Module asH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)(3)M(x) ranges from [0,1], with M(x) approximating 0, H(x) will approximate original features F(x). We call this method attention residual learning.Our stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as H_{i,c}(x)=x+F_{i,c}(x), where F_{i,c}(x) approximates the residual function. In our formulation, F_{i,c}(x) indicates the features generated by deep convolutional networks. The key lies on our mask branches M(x). They work as feature selectors which enhance good features and suppress noises from trunk features.In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch’s feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.1, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Because of the mask values being between 0 and 1, and the fact that naive stacking attention modules means using a dot product on the resulting masks, naive stacking causes a performance drop as the dot product of several modules will converge towards 0.  The attention residual learning mechanism changes this by making the lower bound of the mask values 0 instead of the original features.	However, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.We propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output H of Attention Module asH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)(3)M(x) ranges from [0,1], with M(x) approximating 0, H(x) will approximate original features F(x). We call this method attention residual learning.Our stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as H_{i,c}(x)=x+F_{i,c}(x), where F_{i,c}(x) approximates the residual function. In our formulation, F_{i,c}(x) indicates the features generated by deep convolutional networks. The key lies on our mask branches M(x). They work as feature selectors which enhance good features and suppress noises from trunk features.In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch’s feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.1, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.To understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.As shown in the Fig. 4, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.The Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.The attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.Therefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.In the rest of the experiments, we apply this technique to train our networks.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The bottom-up top-down feedforward structure is a combination of a bottom-up fast feedforward process that creates low resolution features maps to quickly collect global information, and a top-down attention feedback process that uses the global information along with the original feature maps to create features for inference.	However, recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10]. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG [27], Inception [33] and residual learning [10] are proposed to train very deep neural networks. Stochastic depth [14], Batch Normalization [15] and Dropout [28] exploit regularization for convergence and avoiding overfitting and degradation.The design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, i.e. segmentation [22, 25, 1] and human pose estimation [24]. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection [22] is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network [24] fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.Following previous attention mechanism idea in DBN [21], our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.(3) Bottom-up top-down feedforward attention: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation [24] and image segmentation [22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network [24] in its intention of guiding feature learning.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The bottom-up top-down feedforward structure is a combination of a bottom-up fast feedforward process that creates high resolution features maps to slowly collect global information, and a top-down attention feedback process that uses the global information along with the original feature maps to create features for inference.	However, recent advances of image classification focus on training feedforward convolutional neural networks using “very deep” structure [27, 33, 10]. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG [27], Inception [33] and residual learning [10] are proposed to train very deep neural networks. Stochastic depth [14], Batch Normalization [15] and Dropout [28] exploit regularization for convergence and avoiding overfitting and degradation.The design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, i.e. segmentation [22, 25, 1] and human pose estimation [24]. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection [22] is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network [24] fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.Following previous attention mechanism idea in DBN [21], our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.(3) Bottom-up top-down feedforward attention: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation [24] and image segmentation [22, 25, 1]. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network [24] in its intention of guiding feature learning.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The local convolutions' soft mask only consists of three Residual units, which remain the same size.  However, the encoder-decoder structure consists of downsampling and upsampling layers.	We conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.The Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.Results are shown in Table 4.The Attention-Encoder-Decoder-56 network achieves lower test error 5.52\% compared with Attention-Local-Conv-56 network 6.48\% with a considerable margin 0.94\%. The result suggests that the soft attention optimization process will benefit from multi-scale information.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The local convolutions' soft mask only consists of three Residual units, which remain the same size.  However, the encoder-decoder structure consists of only downsampling layers.	We conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.The Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.Results are shown in Table 4.The Attention-Encoder-Decoder-56 network achieves lower test error 5.52\% compared with Attention-Local-Conv-56 network 6.48\% with a considerable margin 0.94\%. The result suggests that the soft attention optimization process will benefit from multi-scale information.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Deeper networks exhibit better performance as they introduce more non-linearities and converge towards better local optima.  But, adding more layers increase both computation time and the number of parameters.  This could cause the network to be prone to overfitting.  Therefore, kernel sizes were reduced such that the number of parameters were similar to the original shallow networks, helping with generalization by reducing overfitting while still receiving the positive effect of having more layers.	Deeper networks have greater discriminative power due to the additional non-linearities and better quality of local optima (Choromanska et al. (2015)). However, convolutions with 3D kernels are computationally expensive in comparison to the 2D variants, which hampers the addition of more layers. Additionally, 3D architectures have a larger number of trainable parameters, with each layer adding C_{l}C_{l-1}\prod_{i=\{x,y,z\}}{\bm{\kappa}_{l}^{(i)}} weights to the model. C_{l} is the number of FMs in layer l and \bm{\kappa}_{l}^{\{x,y,z\}} the size of its kernel in the respective spatial dimension. Overall this makes the network increasingly prone to over-fitting.In order to build a deeper 3D architecture, we adopt the sole use of small 3^{3} kernels that are faster to convolve with and contain less weights. This design approach was previously found beneficial for classification of natural images (Simonyan and Zisserman (2014)) but its effect is even more drastic on 3D networks. When compared to common kernel choices of 5^{3} (Zikic et al. (2014); Urban et al. (2014); Prasoon et al. (2013)) and in our baseline CNN, the smaller 3^{3} kernels reduce the element-wise multiplications by a factor of approximately 5^{3}/3^{3}\approx 4.6 while reducing the number of trainable parameters by the same factor. Thus deeper network variants that are implicitly regularised and more efficient can be designed by simply replacing each layer of common architectures with more layers that use smaller kernels (Fig. 4).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Deeper networks exhibit better performance as they introduce more non-linearities and converge towards better local optima.  But, adding more layers increase both computation time and the number of parameters.  This could cause the network to be prone to overfitting.  Therefore, kernel sizes were reduced such that the number of parameters were much fewer to the original shallow networks, helping with generalization by reducing overfitting while still receiving the positive effect of having more layers.	Deeper networks have greater discriminative power due to the additional non-linearities and better quality of local optima (Choromanska et al. (2015)). However, convolutions with 3D kernels are computationally expensive in comparison to the 2D variants, which hampers the addition of more layers. Additionally, 3D architectures have a larger number of trainable parameters, with each layer adding C_{l}C_{l-1}\prod_{i=\{x,y,z\}}{\bm{\kappa}_{l}^{(i)}} weights to the model. C_{l} is the number of FMs in layer l and \bm{\kappa}_{l}^{\{x,y,z\}} the size of its kernel in the respective spatial dimension. Overall this makes the network increasingly prone to over-fitting.In order to build a deeper 3D architecture, we adopt the sole use of small 3^{3} kernels that are faster to convolve with and contain less weights. This design approach was previously found beneficial for classification of natural images (Simonyan and Zisserman (2014)) but its effect is even more drastic on 3D networks. When compared to common kernel choices of 5^{3} (Zikic et al. (2014); Urban et al. (2014); Prasoon et al. (2013)) and in our baseline CNN, the smaller 3^{3} kernels reduce the element-wise multiplications by a factor of approximately 5^{3}/3^{3}\approx 4.6 while reducing the number of trainable parameters by the same factor. Thus deeper network variants that are implicitly regularised and more efficient can be designed by simply replacing each layer of common architectures with more layers that use smaller kernels (Fig. 4).	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
According to the authors, despite BigDeep+ having a similar capacity to DeepMedic, the mean validation accuracy of BigDeep+ converges to a lower accuracy than that of DeepMedic.  The same applies to the mean DSC for the two models.  Therefore, it can be inferred that BigDeep+ is suffering from overfitting on the training data.	Figure 8 shows the improvement DeepMedic achieves over the single pathway model Deep+. In Fig. 9 we show two representative visual examples of this improvement when using the multi-scale CNN. Finally, we confirm that the performance increase can be accounted to the additional context and not the additional capacity of DeepMedic. To this end, we build a big single-scale model by doubling the FMs at each of the 9-layers of Deep+ and adding two hidden layers. This 11-layers deep and wide model, referred to as “BigDeep+”, has the same number of parameters as DeepMedic. The performance of the model is not improved, while showing signs of over-fitting.	Correct		Incorrect	Correct	Correct	Correct	Incorrect	Correct
According to the authors, despite BigDeep+ having a similar capacity to DeepMedic, the mean validation accuracy of BigDeep+ converges to a higher accuracy than that of DeepMedic.  The same applies to the mean DSC for the two models.  Therefore, it can be inferred that BigDeep+ is not suffering from overfitting on the training data.	Figure 8 shows the improvement DeepMedic achieves over the single pathway model Deep+. In Fig. 9 we show two representative visual examples of this improvement when using the multi-scale CNN. Finally, we confirm that the performance increase can be accounted to the additional context and not the additional capacity of DeepMedic. To this end, we build a big single-scale model by doubling the FMs at each of the 9-layers of Deep+ and adding two hidden layers. This 11-layers deep and wide model, referred to as “BigDeep+”, has the same number of parameters as DeepMedic. The performance of the model is not improved, while showing signs of over-fitting.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
It is implied that the annotations were done by experts at the Neurosciences Critical Care Unit at Addenbrooke's Hospital, Cambridge, UK.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×mm\timesitalic_m italic_m ×1mm×mm\timesitalic_m italic_m ×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×mm\timesitalic_m italic_m ×0.7mm×mm\timesitalic_m italic_m ×5mm), and Gradient-Echo (GE) (0.86mm×mm\timesitalic_m italic_m ×0.86mm×mm\timesitalic_m italic_m ×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case  because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm^{3} resolution, with dimensions 193\times229\times193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	Correct		Incorrect	Correct	Correct	Unrelated	Incorrect	Correct
It is implied that the annotations were done by computer science students at the Neurosciences Critical Care Unit at Addenbrooke's Hospital, Cambridge, UK.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×mm\timesitalic_m italic_m ×1mm×mm\timesitalic_m italic_m ×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×mm\timesitalic_m italic_m ×0.7mm×mm\timesitalic_m italic_m ×5mm), and Gradient-Echo (GE) (0.86mm×mm\timesitalic_m italic_m ×0.86mm×mm\timesitalic_m italic_m ×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case  because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm^{3} resolution, with dimensions 193\times229\times193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	Incorrect	Change concept	Incorrect	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
The paper cites Jarrett et al.  as the reason why they chose zero-mean normalization techniques.  It can be inferred that they did not test this claim themselves.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×1mm×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×0.7mm×5mm), and Gradient-Echo (GE) (0.86mm×0.86mm×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm3 resolution, with dimensions 193×229×193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	Correct		Incorrect	Correct	Incorrect	Incorrect	Incorrect	Correct
The paper cites Jarrett et al.  as the reason why they didn't chose zero-mean normalization techniques.  It can be inferred that they did not test this claim themselves.	Sixty-six patients with moderate-to-severe TBI who required admission to the Neurosciences Critical Care Unit at Addenbrooke’s Hospital, Cambridge, UK, underwent imaging using a 3-Tesla Siemens Magnetom TIM Trio within the first week of injury. Ethical approval was obtained from the Local Research Ethics Committee (LREC 97/290) and written assent via consultee agreement was obtained for all patients. The structural MRI sequences that are used in this work are isotropic MPRAGE (1mm×1mm×1mm), axial FLAIR, T2 and Proton Density (PD) (0.7mm×0.7mm×5mm), and Gradient-Echo (GE) (0.86mm×0.86mm×5mm). All visible lesions were manually annotated on the FLAIR and GE sequences with separate labeling for each lesion type. In nine patients the presence of hyperintense white matter lesions that were felt to be chronic in nature were also annotated. Artifacts, for example, signal loss secondary to intraparenchymal pressure probes, were also noted. For the purpose of this study we focus on binary segmentation of all abnormalities within the brain tissue. Thus, we merged all classes that correspond to intra-cerebral abnormalities into a single “lesion” label. Extra-cerebral pathologies such as epidural and subdural hematoma were treated as background. We excluded two datasets because of corrupted FLAIR images, two cases because no lesions were found and one case because of a major scanning artifact corrupting the images. This results in a total of 61 cases used for quantitative evaluation. Brain masks were obtained using the ROBEX tool (Iglesias et al. (2011)). All images were resampled to an isotropic 1mm3 resolution, with dimensions 193×229×193 and affinely registered (Studholme et al. (1999)) to MNI space using the atlas by Grabner et al. (2006). No bias field correction was used as preliminary results showed that this can negatively affect lesion appearance. Image intensities were normalized to have zero-mean and unit variance, as it has been reported that this improves CNN results (Jarrett et al. (2009)).	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Unrelated	Incorrect	Incorrect
The authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are worse than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.	Table 3 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked first, indicating superior performance on this challenging task among 14 submissions. Table 5 shows our results, along with the other two top entries (Feng et al. (2015); Halme et al. (2015)). Among the other participating methods was the CNN of Havaei et al. (2015) with 3 layers of 2D convolutions. That method perfomed less well on this challenging task (Maier et al. (2017)). This points out the advantage offered by 3D context, the large field of view of DeepMedic thanks to multi-scale processing and the representational power of deeper networks. It is important to note the decrease of performance in comparison to the training set. All methods performed worse on the data coming from the second clinical center, including the method of Feng et al. (2015) that is not machine-learning based. This highlights a general difficulty with current approaches when applied on multi-center data.Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.The performance of our system on the training data is shown in Table 4. Significant improvement is achieved by the structural regularisation offered by the CRF, although it could be partially accounted for by overfitting the training data during the CRF’s configuration. Examples for visual inspection are shown in Fig. 13.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors have shown that the performance of DeepMedic in terms of DSC, precision, sensitivity, ASSD, and Haussdorf for the BRATS and ISLES test datasets are better than the performance of DeepMedic when trained with the BRATS and ISLES training datasets.	Table 3 shows the results of our method on the BRATS test data. Results of other submissions are not accessible. The decrease in performance is possibly due to the the inclusion of test images that vary significantly from the training data, such as cases acquired in clinical centers that did not provide any of the training images, something that was confirmed by the organisers. Note that performance gains obtained with the CRF are larger in this case. This indicates not only that its configuration has not overfitted to the training database but also that the CRF is robust to factors of variation between acquisition sites, which complements nicely the more sensitive CNN.For the testing phase of the challenge we formed an ensemble of three networks, coupled with the fully connected CRF. Our submission ranked first, indicating superior performance on this challenging task among 14 submissions. Table 5 shows our results, along with the other two top entries (Feng et al. (2015); Halme et al. (2015)). Among the other participating methods was the CNN of Havaei et al. (2015) with 3 layers of 2D convolutions. That method perfomed less well on this challenging task (Maier et al. (2017)). This points out the advantage offered by 3D context, the large field of view of DeepMedic thanks to multi-scale processing and the representational power of deeper networks. It is important to note the decrease of performance in comparison to the training set. All methods performed worse on the data coming from the second clinical center, including the method of Feng et al. (2015) that is not machine-learning based. This highlights a general difficulty with current approaches when applied on multi-center data.Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.The performance of our system on the training data is shown in Table 4. Significant improvement is achieved by the structural regularisation offered by the CRF, although it could be partially accounted for by overfitting the training data during the CRF’s configuration. Examples for visual inspection are shown in Fig. 13.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors show that the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors.  They also show a relatively unsuccessful case where oversegmentation occurs, but even in this example, the hierarchy of the tumor is preserved.	Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.	Correct		Incorrect	Correct	Correct	Unrelated	Correct	Incorrect
The authors show that the model understands that the sequence of layers goes from oedema to non-enhancing core to enhancing core to necrotic core, preserving the hierarchical structure of tumors.  They also show a relatively unsuccessful case where oversegmentation occurs. In this example, the hierarchy of the tumor is not preserved.	Quantitative results from the application of the DeepMedic, the CRF and an ensemble of three similar networks on the training data are presented in Table 2. The latter two offer an improvement, albeit fairly small since the performance of DeepMedic is already rather high in this task. Also shown are results from previous works, as reported on the online evaluation platform. Various settings may vary among submissions, such as the pre-processing pipeline or the number of folds used for cross-validation. Still it appears that our system performs favourably compared to previous state-of-the-art, including the semi-automatic system of Bakas et al. (2015) (bakas1) who won the latest challenge and the method of Pereira et al. (2015) (peres1), which is based on grade-specific 2D CNNs and requires visual inspection of the tumor and identification of the grade by the user prior to segmentation. Examples of segmentations obtained with our method are shown in Fig. 12. DeepMedic behaves very well in preserving the hierarchical structure of the tumor, which we account to the large context processed by our multi-scale network.	Incorrect	Change concept	Unrelated	Incorrect	Correct	Unrelated	Correct	Incorrect
The authors mention that they reduced the amount of regularization techniques as they consider the BRATS database to be large.	Network configuration and training: We modify the DeepMedic architecture to handle multi-class problems by extending the classification layer to five feature maps (four tumor classes plus background). The rest of the configuration remains unchanged. We enrich the dataset with sagittal reflections. Opposite to the experiments on TBI, we do not employ the intensity perturbation and dropout on convolutional layers, because the network should not require as much regularisation with this large database. The network is trained on image segments extracted with equal probability centred on the whole tumor and healthy tissue. The distribution of the classes captured by our training scheme is provided in C.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors mention that they increased the amount of regularization techniques as they consider the BRATS database to be small.	Network configuration and training: We modify the DeepMedic architecture to handle multi-class problems by extending the classification layer to five feature maps (four tumor classes plus background). The rest of the configuration remains unchanged. We enrich the dataset with sagittal reflections. Opposite to the experiments on TBI, we do not employ the intensity perturbation and dropout on convolutional layers, because the network should not require as much regularisation with this large database. The network is trained on image segments extracted with equal probability centred on the whole tumor and healthy tissue. The distribution of the classes captured by our training scheme is provided in C.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Injecting the cross-attention maps of the input image enabled the authors to preserve the original composition and structure. The average attention maps are plotted, and pixels are more attracted to words that describe them, e.  pixels of the bear in the image are correlated with the word "bear". 	Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities. More specifically, injecting the cross-attention maps of the input image \mathcal{I} enables us to preserve the original composition and structure. In section 3.1, we review how cross-attention is used, and in section 3.2 we describe how to exploit the cross-attention for editing. For additional background on diffusion models, please refer to appendix A.We return to our key observation — the spatial layout and geometry of the generated image depend on the cross-attention maps. This interaction between pixels and text is illustrated in fig. 4, where the average attention maps are plotted. As can be seen, pixels are more attracted to the words that describe them, e.g., pixels of the bear are correlated with the word “bear”. Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method.Interestingly, we can see that the structure of the image is already determined in the early steps of the diffusion process.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Injecting the cross-attention maps of the input image didn't enable the authors to preserve the original composition and structure. The average attention maps are plotted, and pixels are more attracted to words that describe them, e.  pixels of the bear in the image are correlated with the word "bear". 	Our key observation is that the structure and appearances of the generated image depend not only on the random seed, but also on the interaction between the pixels to the text embedding through the diffusion process. By modifying the pixel-to-text interaction that occurs in cross-attention layers, we provide Prompt-to-Prompt image editing capabilities. More specifically, injecting the cross-attention maps of the input image \mathcal{I} enables us to preserve the original composition and structure. In section 3.1, we review how cross-attention is used, and in section 3.2 we describe how to exploit the cross-attention for editing. For additional background on diffusion models, please refer to appendix A.We return to our key observation — the spatial layout and geometry of the generated image depend on the cross-attention maps. This interaction between pixels and text is illustrated in fig. 4, where the average attention maps are plotted. As can be seen, pixels are more attracted to the words that describe them, e.g., pixels of the bear are correlated with the word “bear”. Note that averaging is done for visualization purposes, and attention maps are kept separate for each head in our method.Interestingly, we can see that the structure of the image is already determined in the early steps of the diffusion process.	Incorrect	Opposite	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The overall composition is reflected by the attenion maps, which can be injected during the diffusion process at controled time-step, which allows the necessary freedom for adapting the new prompt.	Since the attention reflects the overall composition, we can inject the attention maps M that were obtained from the generation with the original prompt \mathcal{P}, into a second generation with the modified prompt \mathcal{P}^{*}. This allows the synthesis of an edited image \mathcal{I}^{*} that is not only manipulated according to the edited prompt, but also preserves the structure of the input image \mathcal{I}. This example is a specific instance of a broader set of attention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing a general framework, followed by the details of the specific editing operations.Edit(M_{t},M_{t}^{*},t):=\begin{cases}M_{t}^{*}&\quad\text{if}\;t<\tau\\M_{t}&\quad\text{otherwise.}\\\end{cases}where \tau is a timestamp parameter that determines until which step the injection is applied.Note that the composition is determined in the early steps of the diffusion process. Therefore, by limiting the number of injection steps, we can guide the composition of the newly generated image while allowing the necessary geometry freedom for adapting to the new prompt. An illustration is provided in section 4. Another natural relaxation for our algorithm is to assign a different number of injection timestamps for the different tokens in the prompt. In case the two words are represented using a different number of tokens, the maps can be duplicated/averaged as necessary using an alignment function as described in the next paragraph.Our key idea is that we can edit images by injecting the cross-attention maps during the diffusion process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps. To apply our method to various creative editing applications, we show several methods to control the cross-attention maps through a simple and semantic interface (see fig. 1). The first is to change a single token’s value in the prompt (e.g., “dog” to “cat”), while fixing the cross-attention maps, to preserve the scene composition. The second is to globally edit an image, e.g., change the style, by adding new words to the prompt and freezing the attention on previous tokens, while allowing new attention to flow to the new tokens. The third is to amplify or attenuate the semantic effect of a word in the generated image.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The overall composition is reflected by the attenion maps, which can be injected anytime, which allows the necessary freedom for adapting the new prompt.	Since the attention reflects the overall composition, we can inject the attention maps M that were obtained from the generation with the original prompt \mathcal{P}, into a second generation with the modified prompt \mathcal{P}^{*}. This allows the synthesis of an edited image \mathcal{I}^{*} that is not only manipulated according to the edited prompt, but also preserves the structure of the input image \mathcal{I}. This example is a specific instance of a broader set of attention-based manipulations leading to different types of intuitive editing. We, therefore, start by proposing a general framework, followed by the details of the specific editing operations.Edit(M_{t},M_{t}^{*},t):=\begin{cases}M_{t}^{*}&\quad\text{if}\;t<\tau\\M_{t}&\quad\text{otherwise.}\\\end{cases}where \tau is a timestamp parameter that determines until which step the injection is applied.Note that the composition is determined in the early steps of the diffusion process. Therefore, by limiting the number of injection steps, we can guide the composition of the newly generated image while allowing the necessary geometry freedom for adapting to the new prompt. An illustration is provided in section 4. Another natural relaxation for our algorithm is to assign a different number of injection timestamps for the different tokens in the prompt. In case the two words are represented using a different number of tokens, the maps can be duplicated/averaged as necessary using an alignment function as described in the next paragraph.Our key idea is that we can edit images by injecting the cross-attention maps during the diffusion process, controlling which pixels attend to which tokens of the prompt text during which diffusion steps. To apply our method to various creative editing applications, we show several methods to control the cross-attention maps through a simple and semantic interface (see fig. 1). The first is to change a single token’s value in the prompt (e.g., “dog” to “cat”), while fixing the cross-attention maps, to preserve the scene composition. The second is to globally edit an image, e.g., change the style, by adding new words to the prompt and freezing the attention on previous tokens, while allowing new attention to flow to the new tokens. The third is to amplify or attenuate the semantic effect of a word in the generated image.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
By using RNN-based-role-shift caption model consists of two LSTM layers.  the model generates the word "yt", by taking two inputs to the model which are 1- Semantic structure sequence, and 2- corresponding proposal feature sequence.  then at each time step the model focus on one specific sub-role and its grounded region set. 	Given the semantic structure sequence \mathcal{S}=(s^{b}_{1},...,s^{b}_{K}) and corresponding proposal feature sequence \mathcal{R}=(\bm{r}_{1},...,\bm{r}_{K}), we utilize a two-layer LSTM to generate the final caption \bm{y}. At each time step, the model fouces on one specific sub-role \bm{s}^{b}_{t} and its grounded region set \bm{r}_{t}, and then generates the word y_{t}. Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: p(g_{t}|\mathcal{S},\mathcal{R}) for controlling the shift of sub-roles, and p(y_{t}|\mathcal{S},\mathcal{R}) to predict the distribution of a word.In order to generate sentences with respect to the designated VSRs, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, \eg, Arg0{}_{\text{reader}} – read – Arg1{}_{\text{thing}} – LOC in Figure 1 (c). Finally, we combine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
By using RNN-based-role-shift caption model consists of two LSTM layers.  the model generates the word "yt", by taking only one input to the model which is Semantic structure sequence.  then at each time step the model focus on one specific sub-role and its grounded region set. 	Given the semantic structure sequence \mathcal{S}=(s^{b}_{1},...,s^{b}_{K}) and corresponding proposal feature sequence \mathcal{R}=(\bm{r}_{1},...,\bm{r}_{K}), we utilize a two-layer LSTM to generate the final caption \bm{y}. At each time step, the model fouces on one specific sub-role \bm{s}^{b}_{t} and its grounded region set \bm{r}_{t}, and then generates the word y_{t}. Therefore, we take inspirations from previous CIC methods [16, 10], and predict two distributions simultaneously: p(g_{t}|\mathcal{S},\mathcal{R}) for controlling the shift of sub-roles, and p(y_{t}|\mathcal{S},\mathcal{R}) to predict the distribution of a word.In order to generate sentences with respect to the designated VSRs, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, \eg, Arg0{}_{\text{reader}} – read – Arg1{}_{\text{thing}} – LOC in Figure 1 (c). Finally, we combine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Authors verify their work using a conventions evaluation metrics in prior CIC works.  As their quantitative results report in Table 1, you can observe that author's framework can achieve the best performance over almost all metrics and benchmarks.  and as for the visualized evaluation, you can observe in Figure 5 that the author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions.  and according to the semantic structures, the captioning model can generate near-perfect descriptions.	Settings. To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works [16, 10, 78], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines666All baselines use the same visual regions as models with VSRs.: 1) C-LSTM: It is a Controllable LSTM model [63]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) C-UpDn: It is a Controllable UpDn model [3], which uses an adaptive attention to generate the captions. 3) SCT [16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) Ours w/o verb: We ablate our model by removing the verb information in both the SSP and captioning model. 5) Ours (oracle verb): It is an ideal situation, where the captioning model directly outputs the oracle format of the verb when the attending role is the verb.Quantitative Results. The quantitative results are reported in Table 1. From Table 1, we can observe that our framework can achieve the best performance over almost all metrics and benchmarks. By comparing the two different proposal settings (i.e., GSRL and GT), we can find that the accuracy of GSRL is a major bottleneck of the whole framework. Meanwhile, the ablative model (Ours w/o verb) can only achieve slightly better performance than baseline SCT and much worse performance than our full model, which reflects the importance of the verb in semantic structure learning and caption generation.Visualizations. In Figure 5, we illustrate some examples of the generated captions. We can observe that our framework always learns a human-like semantic structure based on the VSR and grounded visual regions (e.g., Arg1thing – sit – Arg2position – LOC – MNR). According to the semantic structures, the captioning model can generate near-perfect descriptions. As a by-product, a well-trained SSP can automatically produce several verb-specific semantic structures for a set of user-interested roles, and we show some examples in Figure 6. For each verb and role set, we illustrate the top two structures by using beam search. Particularly, we are surprised to find that we can even learn some structures that never appear in original datasets (the blue tick ones).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Authors verify their work using a conventions evaluation metrics in prior CIC works.  As their quantitative results report in Table 1, you can observe that author's framework can achieve the similar performance with metrics and benchmarks.  and as for the visualized evaluation, you can observe in Figure 5 that the author's framework always learns a human-like semantic structure based on the VSR and grounded visual regions.  and according to the semantic structures, the captioning model can generate near-perfect descriptions.	Settings. To evaluate the controllability of proposed framework, we followed the conventions of prior CIC works [16, 10, 78], and utilized the VSR aligned with ground truth captions as the control signals. Specifically, we compared the proposed framework with several carefully designed baselines666All baselines use the same visual regions as models with VSRs.: 1) C-LSTM: It is a Controllable LSTM model [63]. Given the features of all grounded visual regions, it first averages all region features, and then uses an LSTM to generate the captions. 2) C-UpDn: It is a Controllable UpDn model [3], which uses an adaptive attention to generate the captions. 3) SCT [16]: It regards the set of visual regions as a control signal, and utilizes a chunk-shift captioning model to generate the captions. 4) Ours w/o verb: We ablate our model by removing the verb information in both the SSP and captioning model. 5) Ours (oracle verb): It is an ideal situation, where the captioning model directly outputs the oracle format of the verb when the attending role is the verb.Quantitative Results. The quantitative results are reported in Table 1. From Table 1, we can observe that our framework can achieve the best performance over almost all metrics and benchmarks. By comparing the two different proposal settings (i.e., GSRL and GT), we can find that the accuracy of GSRL is a major bottleneck of the whole framework. Meanwhile, the ablative model (Ours w/o verb) can only achieve slightly better performance than baseline SCT and much worse performance than our full model, which reflects the importance of the verb in semantic structure learning and caption generation.Visualizations. In Figure 5, we illustrate some examples of the generated captions. We can observe that our framework always learns a human-like semantic structure based on the VSR and grounded visual regions (e.g., Arg1thing – sit – Arg2position – LOC – MNR). According to the semantic structures, the captioning model can generate near-perfect descriptions. As a by-product, a well-trained SSP can automatically produce several verb-specific semantic structures for a set of user-interested roles, and we show some examples in Figure 6. For each verb and role set, we illustrate the top two structures by using beam search. Particularly, we are surprised to find that we can even learn some structures that never appear in original datasets (the blue tick ones).	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Subjective control signals are harder to control the generation process effectively and precisely.	Early CIC works mainly focus on subjective control signals, such as sentiments [41], emotions [42, 22], and personality [14, 54], \ie, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on objective control signals. More specifically, they can be coarsely classified into two categories: 1) Content-controlled: the control signals are about the contents of interest which need to be described. As the example shown in Figure 1 (a), given the region set () as a control signal, we hope that the generated caption can cover all regions (\ie, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations [27], object regions [16, 35], scene graphs [10, 78], and mouse trace [48]. 2) Structure-controlled: the control signals are about the semantic structures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [79] of the sentence (cf. Figure 1 (b)) are some typical structure-controlled signals.	Correct		Incorrect	Correct	Correct	Correct	Incorrect	Correct
Subjective control signals are easier to control the generation process effectively and precisely.	Early CIC works mainly focus on subjective control signals, such as sentiments [41], emotions [42, 22], and personality [14, 54], \ie, the linguistic styles of sentences. Although these stylized captioning models can eventually produce style-related captions, they remain hard to control the generation process effectively and precisely. To further improve the controllability, recent CIC works gradually put a more emphasis on objective control signals. More specifically, they can be coarsely classified into two categories: 1) Content-controlled: the control signals are about the contents of interest which need to be described. As the example shown in Figure 1 (a), given the region set () as a control signal, we hope that the generated caption can cover all regions (\ie, man, wave, and surfboard). So far, various types of content-controlled signals have been proposed, such as visual relations [27], object regions [16, 35], scene graphs [10, 78], and mouse trace [48]. 2) Structure-controlled: the control signals are about the semantic structures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [79] of the sentence (cf. Figure 1 (b)) are some typical structure-controlled signals.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Authors used BlEU, METOR, ROUGE, CIDEr, and SPICE to evaluate quality based generated captions, And used Accuracy-based,  Diversity-based metrics to evaluate diversity based generation captions.	Evaluation Metrics. To evaluate the quality of the generated captions, we use five accuracy-based metrics, including BLEU-4 (B4) [45], METEOR (M) [5], ROUGE (R) [34], CIDEr-D (C) [61], and SPICE (S) [2]. Particularly, we evaluate the generated captions against the single ground truth caption. We also propose a new recall-based metric to evaluate whether the roles of the generated sentence are consistent with the ground truth caption (\ie, VSR). It measures the recall rate of the verb, semantic roles, and ordered role pairs, which are denoted as R{}_{\text{V}}, R{}_{\text{SR1}} and R{}_{\text{SR2}}, respectively.Evaluation Metrics. We used two types of metrics to evaluate the diverse captions: 1) Accuracy-based: we followed the conventions of the previous works [16, 20, 65] and reported the best-1 accuracy, \ie, the generated caption with the maximum score for each metric is chosen. Analogously, we evaluate the generated captions against the single ground truth caption. 2) Diversity-based: we followed [10] and used two metrics which only focus on the language similarity: Div-n (D-n) [4, 20] and self-CIDEr (s-C) [66].	Correct		Correct	Correct	Correct	Correct	Incorrect	Incorrect
Authors used BlEU, METOR, ROUGE, CIDEr, F1 and SPICE to evaluate quality based generated captions, And used Accuracy-based,  Diversity-based metrics to evaluate diversity based generation captions.	Evaluation Metrics. To evaluate the quality of the generated captions, we use five accuracy-based metrics, including BLEU-4 (B4) [45], METEOR (M) [5], ROUGE (R) [34], CIDEr-D (C) [61], and SPICE (S) [2]. Particularly, we evaluate the generated captions against the single ground truth caption. We also propose a new recall-based metric to evaluate whether the roles of the generated sentence are consistent with the ground truth caption (\ie, VSR). It measures the recall rate of the verb, semantic roles, and ordered role pairs, which are denoted as R{}_{\text{V}}, R{}_{\text{SR1}} and R{}_{\text{SR2}}, respectively.Evaluation Metrics. We used two types of metrics to evaluate the diverse captions: 1) Accuracy-based: we followed the conventions of the previous works [16, 20, 65] and reported the best-1 accuracy, \ie, the generated caption with the maximum score for each metric is chosen. Analogously, we evaluate the generated captions against the single ground truth caption. 2) Diversity-based: we followed [10] and used two metrics which only focus on the language similarity: Div-n (D-n) [4, 20] and self-CIDEr (s-C) [66].	Incorrect	Invent something didn't mentioned	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
A set of object proposals is extracted with an object detector from an image.  as authors utilized a Faster R-NN with ResNet-101 to obtain all proposals for each image.  noting that for COCO Entities, authors group the proposals by their detected class labels, and for FLickr30K Entities, they directly regard each proposal as a proposal set.	Given an image \bm{I}, we first utilize an object detector [50] to extract a set of object proposals \mathcal{B}. Each proposal \bm{b}_{i}\in\mathcal{B} is associated with a visual feature \bm{f}_{i} and a class label c_{i}\in\mathcal{C}. Then, we group all these proposals into N disjoint sets, \ie, \mathcal{B}=\{\mathcal{B}_{1},...,\mathcal{B}_{N}\}333Due to different annotation natures of specific CIC datasets, we group proposals by different principles. Details are shown in Section 4.2., and each proposal set \mathcal{B}_{i} consists of one or more proposals. In this GSRL step, we need to refer each sub-role in the \mathcal{VSR} to a proposal set in \mathcal{B}. Specifically, we calculate the similarity score a_{ij} between semantic role s_{i} and proposal set \mathcal{B}_{j} by:\displaystyle\bm{q}_{i}=\left[\bm{e}^{g}_{v};\bm{e}^{g}_{s_{i}};\bm{\bar{f}}\right],\quad a_{ij}=F_{a}(\bm{q}_{i},\bm{\bar{f}_{j}}),(4)where \bm{e}^{g}_{v} and \bm{e}^{g}_{s_{i}} are the word embedding features of verb v and semantic role s_{i}, \bm{\bar{f}} and \bm{\bar{f}_{j}} represent the average-pooled visual features of proposal set \mathcal{B} and \mathcal{B}_{j}, [;] is a concatenation operation, and F_{a} is a learnable similarity function444For conciseness, we leave the details in the supplementary material. .Proposal Generation and Grouping. We utilize a Faster R-CNN [50] with ResNet-101 [24] to obtain all proposals for each image. Especially, we use the model released by [3], which is finetuned on VG dataset [29]. For COCO Entities, since the “ground truth” annotations for each noun phrase are the proposals with the same class, we group the proposals by their detected class labels. But for Flickr30K Entities, we directly regard each proposal as a proposal set.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
A set of object proposals is extracted with an object detector from an image.  as authors utilized a Faster CNN with ResNet-101 to obtain all proposals for each image.  noting that for COCO Entities, authors group the proposals by their detected class labels, and for FLickr30K Entities, they directly regard each proposal as a proposal set.	Given an image \bm{I}, we first utilize an object detector [50] to extract a set of object proposals \mathcal{B}. Each proposal \bm{b}_{i}\in\mathcal{B} is associated with a visual feature \bm{f}_{i} and a class label c_{i}\in\mathcal{C}. Then, we group all these proposals into N disjoint sets, \ie, \mathcal{B}=\{\mathcal{B}_{1},...,\mathcal{B}_{N}\}333Due to different annotation natures of specific CIC datasets, we group proposals by different principles. Details are shown in Section 4.2., and each proposal set \mathcal{B}_{i} consists of one or more proposals. In this GSRL step, we need to refer each sub-role in the \mathcal{VSR} to a proposal set in \mathcal{B}. Specifically, we calculate the similarity score a_{ij} between semantic role s_{i} and proposal set \mathcal{B}_{j} by:\displaystyle\bm{q}_{i}=\left[\bm{e}^{g}_{v};\bm{e}^{g}_{s_{i}};\bm{\bar{f}}\right],\quad a_{ij}=F_{a}(\bm{q}_{i},\bm{\bar{f}_{j}}),(4)where \bm{e}^{g}_{v} and \bm{e}^{g}_{s_{i}} are the word embedding features of verb v and semantic role s_{i}, \bm{\bar{f}} and \bm{\bar{f}_{j}} represent the average-pooled visual features of proposal set \mathcal{B} and \mathcal{B}_{j}, [;] is a concatenation operation, and F_{a} is a learnable similarity function444For conciseness, we leave the details in the supplementary material. .Proposal Generation and Grouping. We utilize a Faster R-CNN [50] with ResNet-101 [24] to obtain all proposals for each image. Especially, we use the model released by [3], which is finetuned on VG dataset [29]. For COCO Entities, since the “ground truth” annotations for each noun phrase are the proposals with the same class, we group the proposals by their detected class labels. But for Flickr30K Entities, we directly regard each proposal as a proposal set.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
It was found that models with RL refinement are less affected by length normalization "α" and coverage penalty "β", authors explain this to the fact that during RL refinement, models can't learn to pay attention to the full source sentence to not under-translate or over-translate. 	Table 2 shows the impact of \alpha and \beta onthe BLEU score when decoding the WMT’14 English-to-French development set.The model used here for experiments is trained using the ML objectiveonly (without RL refinement). As can be seen from the results, havingsome length normalization and coverage penalty improves BLEU scoreconsiderably (from 30.3 to 31.4).We find that length normalization (\alpha) and coverage penalty(\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.The results of RL fine-tuning on the best En\rightarrowFr andEn\rightarrowDe models are presented inTable 6, which show that fine-tuning themodels with RL can improve BLEU scores. On WMT En\rightarrowFr,model refinement improves BLEU score by close to 1 point. On En\rightarrowDe,RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU pointsimprovement on the development set. The results presented inTable 6 are the average of 8 independent models.We also note that there is an overlap between the wins from the RL refinement and the decoderfine-tuning (i.e., the introduction of length normalization and coverage penalty).On a less fine-tuned decoder (e.g., if the decoder does beam search bylog-probability only), the win from RL would have been bigger (as is evidentfrom comparing results in Table 2 andTable 3).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
It was found that models with RL refinement are less affected by length normalization "α" and coverage penalty "β", authors explain this to the fact that during RL refinement, models can't learn to pay attention to the full source sentence to not under-translate or over-translate. 	Table 2 shows the impact of \alpha and \beta onthe BLEU score when decoding the WMT’14 English-to-French development set.The model used here for experiments is trained using the ML objectiveonly (without RL refinement). As can be seen from the results, havingsome length normalization and coverage penalty improves BLEU scoreconsiderably (from 30.3 to 31.4).We find that length normalization (\alpha) and coverage penalty(\beta) are less effective for models with RLrefinement. Table 3 summarizes ourresults. This is understandable, as during RL refinement, the modelsalready learn to pay attention to the full source sentence to notunder-translate or over-translate, which would result in a penalty on theBLEU (or GLEU) scores.The results of RL fine-tuning on the best En\rightarrowFr andEn\rightarrowDe models are presented inTable 6, which show that fine-tuning themodels with RL can improve BLEU scores. On WMT En\rightarrowFr,model refinement improves BLEU score by close to 1 point. On En\rightarrowDe,RL-refinement slightly hurts the test performance even though we observe about 0.4 BLEU pointsimprovement on the development set. The results presented inTable 6 are the average of 8 independent models.We also note that there is an overlap between the wins from the RL refinement and the decoderfine-tuning (i.e., the introduction of length normalization and coverage penalty).On a less fine-tuned decoder (e.g., if the decoder does beam search bylog-probability only), the win from RL would have been bigger (as is evidentfrom comparing results in Table 2 andTable 3).	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The weakness of conventional phrase-based translation systems over Neural Machine Translation are their brittle design choices especially when it's trained on very large-scale datasets, large scale, production quality and it lacks the ability to learn directly in an end-to-end fashion.	Neural Machine Translation(NMT) [41, 2] has recently beenintroduced as a promising approach with the potential of addressingmany shortcomings of traditional machine translation systems.The strength of NMT lies in its ability to learn directly, in anend-to-end fashion, the mapping from input text to associated output text.Its architecture typically consists of two recurrent neural networks (RNNs), oneto consume the input text sequence and one to generate translated output text.NMT is often accompanied by an attention mechanism [2]which helps it cope effectively with long input sequences.An advantage of Neural Machine Translation is that it sidesteps manybrittle design choices in traditional phrase-based machinetranslation [26]. In practice, however, NMT systemsused to be worse in accuracy than phrase-based translation systems,especially when training on very large-scale datasets as used for the verybest publicly available translation systems.Three inherent weaknesses of Neural Machine Translation are responsible for thisgap: its slower training and inference speed, ineffectiveness in dealing withrare words, and sometimesfailure to translate all words in the source sentence. Firstly, it generallytakes a considerable amount of time and computational resources totrain an NMT system on a large-scale translation dataset, thus slowing the rateof experimental turnaround time and innovation. For inference they are generallymuch slower than phrase-based systems due to the large number of parametersused.Secondly, NMT lacks robustness in translating rare words. Though thiscan be addressed in principle by training a “copy model” to mimic atraditional alignment model [31], or by using theattention mechanism to copy rare words [37], these approaches areboth unreliable at scale, since the quality of the alignments varies acrosslanguages, and the latent alignments produced by the attentionmechanism are unstable when the network is deep. Also, simple copyingmay not always be the best strategy to cope with rare words, for example whena transliteration is more appropriate. Finally,NMT systems sometimes produce output sentencesthat do not translate all parts of the input sentence – in otherwords, they fail to completely “cover” the input, which can result insurprising translations.Since then, many novel techniques have been proposed to furtherimprove NMT: using an attention mechanism to deal with rarewords [37], a mechanism to model translationcoverage [42], multi-task and semi-supervised training toincorporate more data [14, 29], a characterdecoder [9], a characterencoder [11], subwordunits [38] also to deal with rare word outputs,different kinds of attentionmechanisms [30], and sentence-levelloss minimization [39, 34].While the translation accuracy of these systems has been encouraging, systematiccomparison with large scale, production quality phrase-based translation systemshas been lacking.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The weakness of conventional phrase-based translation systems over Neural Machine Translation are their brittle design choices especially when it's trained on very small-scale datasets, small scale, production quality and it lacks the ability to learn directly in an end-to-end fashion.	Neural Machine Translation(NMT) [41, 2] has recently beenintroduced as a promising approach with the potential of addressingmany shortcomings of traditional machine translation systems.The strength of NMT lies in its ability to learn directly, in anend-to-end fashion, the mapping from input text to associated output text.Its architecture typically consists of two recurrent neural networks (RNNs), oneto consume the input text sequence and one to generate translated output text.NMT is often accompanied by an attention mechanism [2]which helps it cope effectively with long input sequences.An advantage of Neural Machine Translation is that it sidesteps manybrittle design choices in traditional phrase-based machinetranslation [26]. In practice, however, NMT systemsused to be worse in accuracy than phrase-based translation systems,especially when training on very large-scale datasets as used for the verybest publicly available translation systems.Three inherent weaknesses of Neural Machine Translation are responsible for thisgap: its slower training and inference speed, ineffectiveness in dealing withrare words, and sometimesfailure to translate all words in the source sentence. Firstly, it generallytakes a considerable amount of time and computational resources totrain an NMT system on a large-scale translation dataset, thus slowing the rateof experimental turnaround time and innovation. For inference they are generallymuch slower than phrase-based systems due to the large number of parametersused.Secondly, NMT lacks robustness in translating rare words. Though thiscan be addressed in principle by training a “copy model” to mimic atraditional alignment model [31], or by using theattention mechanism to copy rare words [37], these approaches areboth unreliable at scale, since the quality of the alignments varies acrosslanguages, and the latent alignments produced by the attentionmechanism are unstable when the network is deep. Also, simple copyingmay not always be the best strategy to cope with rare words, for example whena transliteration is more appropriate. Finally,NMT systems sometimes produce output sentencesthat do not translate all parts of the input sentence – in otherwords, they fail to completely “cover” the input, which can result insurprising translations.Since then, many novel techniques have been proposed to furtherimprove NMT: using an attention mechanism to deal with rarewords [37], a mechanism to model translationcoverage [42], multi-task and semi-supervised training toincorporate more data [14, 29], a characterdecoder [9], a characterencoder [11], subwordunits [38] also to deal with rare word outputs,different kinds of attentionmechanisms [30], and sentence-levelloss minimization [39, 34].While the translation accuracy of these systems has been encouraging, systematiccomparison with large scale, production quality phrase-based translation systemshas been lacking.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
character-delimited models takes characters as input and outputs characters, the words spitted into constituent characters, resulting typically in a few hundred basic characters including special characters appeared in the data.  While in word-delimited models OOv words are collapsed into a single UNK symbols.	A second approach we use is the mixed word/character model.As in a word model, we keep a fixed-size word vocabulary.However, unlike in a conventional word model where OOV words are collapsedinto a single UNK symbol, we convert OOV words into the sequence of itsconstituent characters.Special prefixes are prepended to the characters, to 1) show the location ofthe characters in a word, and 2) to distinguish them from normal in-vocabularycharacters. There are threeprefixes: <B>,<M>, and <E>, indicating beginning of the word, middleof the word and end of the word, respectively. For example, let’s assume theword Miki is not in the vocabulary. It will be preprocessed into asequence of special tokens: <B>M <M>i <M>k <E>i. The process isdone on both the source and the target sentences. During decoding, theoutput may also contain sequences of special tokens. With theprefixes, it is trivial to reverse the tokenization to the original words aspart of a post-processing step.The mixed word-character model is similar to the word model, except theout-of-vocabulary (OOV) words are converted into sequences ofcharacters with special delimiters around them as described in section4.2 in more detail. Inour experiments, the vocabulary size for the mixed word-charactermodel is 32K. For the pure character model, we simply split all wordsinto constituent characters, resulting typically in a few hundred basiccharacters (including special symbols appearing in the data). For thewordpiece models, we train 3 different models with vocabulary sizes of8K, 16K, and 32K.The pure character model (char input, char output) works surprisinglywell on this task, not much worse than the best wordpiece models in BLEUscore. However, these models are rather slow to train and slow to use as thesequences are much longer.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
character-delimited models takes characters as input and outputs characters, the words spitted into constituent characters, resulting typically in a few hundred basic characters including special characters appeared in the data.  While in word-delimited models OOv words are collapsed into a single SEP symbols.	A second approach we use is the mixed word/character model.As in a word model, we keep a fixed-size word vocabulary.However, unlike in a conventional word model where OOV words are collapsedinto a single UNK symbol, we convert OOV words into the sequence of itsconstituent characters.Special prefixes are prepended to the characters, to 1) show the location ofthe characters in a word, and 2) to distinguish them from normal in-vocabularycharacters. There are threeprefixes: <B>,<M>, and <E>, indicating beginning of the word, middleof the word and end of the word, respectively. For example, let’s assume theword Miki is not in the vocabulary. It will be preprocessed into asequence of special tokens: <B>M <M>i <M>k <E>i. The process isdone on both the source and the target sentences. During decoding, theoutput may also contain sequences of special tokens. With theprefixes, it is trivial to reverse the tokenization to the original words aspart of a post-processing step.The mixed word-character model is similar to the word model, except theout-of-vocabulary (OOV) words are converted into sequences ofcharacters with special delimiters around them as described in section4.2 in more detail. Inour experiments, the vocabulary size for the mixed word-charactermodel is 32K. For the pure character model, we simply split all wordsinto constituent characters, resulting typically in a few hundred basiccharacters (including special symbols appearing in the data). For thewordpiece models, we train 3 different models with vocabulary sizes of8K, 16K, and 32K.The pure character model (char input, char output) works surprisinglywell on this task, not much worse than the best wordpiece models in BLEUscore. However, these models are rather slow to train and slow to use as thesequences are much longer.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Quantization models can perform slightly have lower results on neural organization models, however in this paper authors performed some constraints during training so that's quantizable with minimal impact on the output of the model, the quantized model even performed slightly better than none-quantized training and they suggest it could be due to regularization roles those constraints had during training.	In this section, we present our approach to speed up inference withquantized arithmetic. Our solution is tailored towards the hardwareoptions available at Google. To reduce quantization errors, additionalconstraints are added to our model during training so that it is quantizablewith minimal impact on the output of the model. That is, once amodel is trained with these additional constraints, it can be subsequentlyquantized without loss to translation quality. Our experimental results suggestthat those additional constraints do not hurt model convergence nor the qualityof a model once it has converged.It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\delta,\delta] and softmax logits into[-\gamma,\gamma]. \gamma is fixed to be at 25.0, while thevalue for \delta is gradually annealed from a generous bound of\delta=8.0 at the beginning of training, to a rather stringent boundof \delta=1.0 towards the end of training. At inference time,\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT’14 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.Our solution strikes a good balance between efficiency andaccuracy. Since the computationally expensive operations (the matrixmultiplications) are done using 8-bit integer operations, ourquantized inference is quite efficient. Also, since error-sensitiveaccumulator values are stored using 16-bit integers, our solution isvery accurate and is robust to quantization errors.Table 1 shows that decoding using reducedprecision arithmetics on the TPU suffers a very minimal loss of 0.0072 onlog perplexity, and no loss on BLEU at all. This result matchesprevious work reporting that quantizing convolutional neuralnetwork models can retain most of the model quality.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Quantization models can perform slightly have lower results on neural network models, however in this paper authors performed some constraints during training so that's quantizable with minimal impact on the output of the model, the quantized model even performed slightly better than none-quantized training and they suggest it could be due to regularization roles those constraints had during training.	In this section, we present our approach to speed up inference withquantized arithmetic. Our solution is tailored towards the hardwareoptions available at Google. To reduce quantization errors, additionalconstraints are added to our model during training so that it is quantizablewith minimal impact on the output of the model. That is, once amodel is trained with these additional constraints, it can be subsequentlyquantized without loss to translation quality. Our experimental results suggestthat those additional constraints do not hurt model convergence nor the qualityof a model once it has converged.It is worth emphasizing that during training of the model we use full-precisionfloating point numbers. The only constraints we add to the modelduring training are the clipping of the RNN accumulator values into[-\delta,\delta] and softmax logits into[-\gamma,\gamma]. \gamma is fixed to be at 25.0, while thevalue for \delta is gradually annealed from a generous bound of\delta=8.0 at the beginning of training, to a rather stringent boundof \delta=1.0 towards the end of training. At inference time,\delta is fixed at 1.0. Those additional constraints do not degrademodel convergence nor the decoding quality of the model when it hasconverged. In Figure 4, we compare the lossvs. steps for an unconstrained model (the blue curve) and a constrainedmodel (the red curve) on WMT’14 English-to-French. We can see thatthe loss for the constrained model is slightly better, possibly due toregularization roles those constraints play.Our solution strikes a good balance between efficiency andaccuracy. Since the computationally expensive operations (the matrixmultiplications) are done using 8-bit integer operations, ourquantized inference is quite efficient. Also, since error-sensitiveaccumulator values are stored using 16-bit integers, our solution isvery accurate and is robust to quantization errors.Table 1 shows that decoding using reducedprecision arithmetics on the TPU suffers a very minimal loss of 0.0072 onlog perplexity, and no loss on BLEU at all. This result matchesprevious work reporting that quantizing convolutional neuralnetwork models can retain most of the model quality.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Authors used only the decoder-RNN output from the past decoding time step in the bottom decoder layer to obtain recurrent attention context which is sent directly to all the remaining decoder layers.	Our attention module is similar to [2]. Morespecifically, let \mathbf{y}_{i-1} be the decoder-RNN output fromthe past decoding time step (in our implementation, we use the output fromthe bottom decoder layer). Attention context \mathbf{a}_{i}for the current time step is computed according to the following formulas:st=A⁢t⁢t⁢e⁢n⁢t⁢i⁢o⁢n⁢F⁢u⁢n⁢c⁢t⁢i⁢o⁢n⁢(𝐲i−1,𝐱t)∀t,1≤t≤Mpt=exp⁡(st)/∑t=1Mexp⁡(st)∀t,1≤t≤M𝐚i=∑t=1Mpt.𝐱t\begin{split}s_{t}&=AttentionFunction(\mathbf{y}_{i-1},\mathbf{x}_{t})\quad\forall t,\quad 1\leq t\leq M\\p_{t}&=\exp(s_{t})/\sum_{t=1}^{M}\exp(s_{t})\quad\quad\forall t,\quad 1\leq t\leq M\\\mathbf{a}_{i}&=\sum_{t=1}^{M}p_{t}.\mathbf{x}_{t}\end{split}start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n ( bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) / ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(4)where AttentionFunction in our implementation is a feed forward network withone hidden layer.Figure 1: The model architecture of GNMT, Google’s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Authors used the decoder-RNN output combined from the past decoding time step in the bottom decoder layer, as well as the embeddings from the encoder to obtain recurrent attention context which is sent directly to all the remaining decoder layers.	Our attention module is similar to [2]. Morespecifically, let \mathbf{y}_{i-1} be the decoder-RNN output fromthe past decoding time step (in our implementation, we use the output fromthe bottom decoder layer). Attention context \mathbf{a}_{i}for the current time step is computed according to the following formulas:st=A⁢t⁢t⁢e⁢n⁢t⁢i⁢o⁢n⁢F⁢u⁢n⁢c⁢t⁢i⁢o⁢n⁢(𝐲i−1,𝐱t)∀t,1≤t≤Mpt=exp⁡(st)/∑t=1Mexp⁡(st)∀t,1≤t≤M𝐚i=∑t=1Mpt.𝐱t\begin{split}s_{t}&=AttentionFunction(\mathbf{y}_{i-1},\mathbf{x}_{t})\quad\forall t,\quad 1\leq t\leq M\\p_{t}&=\exp(s_{t})/\sum_{t=1}^{M}\exp(s_{t})\quad\quad\forall t,\quad 1\leq t\leq M\\\mathbf{a}_{i}&=\sum_{t=1}^{M}p_{t}.\mathbf{x}_{t}\end{split}start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n italic_F italic_u italic_n italic_c italic_t italic_i italic_o italic_n ( bold_y start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL start_CELL = roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) / ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_exp ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∀ italic_t , 1 ≤ italic_t ≤ italic_M end_CELL end_ROW start_ROW start_CELL bold_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL = ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW(4)where AttentionFunction in our implementation is a feed forward network withone hidden layer.Figure 1: The model architecture of GNMT, Google’s Neural Machine Translation system. On the left is the encoder network, on the right is the decoder network, in the middle is the attention module. The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers can start computing, each on a separate GPU. To retain as much parallelism as possible during running the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context, which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the encoder and decoder networks, or have them run on a separate set of dedicated GPUs.	Incorrect	Invent something didn't mentioned	Correct	Correct	Correct	Incorrect	Correct	Correct
Authors used 8 LSTM layers for the encoder, and 8 LSTM layers for the decoder with residual connections for both networks, each layer has 1024 node.	This work presents the design and implementation of GNMT, a production NMTsystem at Google, that aims toprovide solutions to the above problems. In our implementation, therecurrent networks are Long Short-Term Memory (LSTM)RNNs [23, 17]. Our LSTM RNNs have 8layers, with residual connections between layers to encourage gradientflow [21]. For parallelism, we connect the attention fromthe bottom layer of the decoder network to the top layer of theencoder network. To improve inference time, we employ low-precisionarithmetic for inference, which is further accelerated by specialhardware (Google’s Tensor Processing Unit, or TPU). To effectivelydeal with rare words, we use sub-word units (also known as“wordpieces”) [35] for inputs and outputs inour system. Using wordpieces gives a good balance between theflexibility of single characters and the efficiency of full words fordecoding, and also sidesteps the need for special treatment of unknownwords. Our beam search technique includes a length normalization procedure todeal efficiently with the problem of comparing hypotheses of differentlengths during decoding, and a coverage penalty to encourage the modelto translate all of the provided input.\begin{split}\mathbf{c}_{t}^{i},\mathbf{m}_{t}^{i}&=\mathrm{LSTM}_{i}(\mathbf{c}_{t-1}^{i},\mathbf{m}_{t-1}^{i},\mathbf{x}_{t}^{i-1};\mathbf{W}^{i})\\\mathbf{x}_{t}^{i}&=\mathbf{m}_{t}^{i}+\mathbf{x}_{t}^{i-1}\\\mathbf{c}_{t}^{i+1},\mathbf{m}_{t}^{i+1}&=\mathrm{LSTM}_{i+1}(\mathbf{c}_{t-1}^{i+1},\mathbf{m}_{t-1}^{i+1},\mathbf{x}_{t}^{i};\mathbf{W}^{i+1})\end{split}(6)Residual connections greatly improve the gradient flow in the backwardpass, which allows us to train very deep encoder and decodernetworks. In most of our experiments, we use 8 LSTM layers for the encoderand decoder, though residual connections can allow us to trainsubstantially deeper networks (similar to what was observedin [45]).In all experiments, our models consist of 8 encoder layers and 8 decoder layers.(Since the bottom encoder layer is actually bi-directional, in total there are9 logically distinct LSTM passes in the encoder.)The attention network is a simple feedforward network with one hidden layer with 1024 nodes.All of the models use 1024 LSTM nodes per encoder and decoder layers.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Authors used 12 LSTM layers for the encoder, and 12 LSTM layers for the decoder with residual connections for both networks, each layer has 1024 node.	This work presents the design and implementation of GNMT, a production NMTsystem at Google, that aims toprovide solutions to the above problems. In our implementation, therecurrent networks are Long Short-Term Memory (LSTM)RNNs [23, 17]. Our LSTM RNNs have 8layers, with residual connections between layers to encourage gradientflow [21]. For parallelism, we connect the attention fromthe bottom layer of the decoder network to the top layer of theencoder network. To improve inference time, we employ low-precisionarithmetic for inference, which is further accelerated by specialhardware (Google’s Tensor Processing Unit, or TPU). To effectivelydeal with rare words, we use sub-word units (also known as“wordpieces”) [35] for inputs and outputs inour system. Using wordpieces gives a good balance between theflexibility of single characters and the efficiency of full words fordecoding, and also sidesteps the need for special treatment of unknownwords. Our beam search technique includes a length normalization procedure todeal efficiently with the problem of comparing hypotheses of differentlengths during decoding, and a coverage penalty to encourage the modelto translate all of the provided input.\begin{split}\mathbf{c}_{t}^{i},\mathbf{m}_{t}^{i}&=\mathrm{LSTM}_{i}(\mathbf{c}_{t-1}^{i},\mathbf{m}_{t-1}^{i},\mathbf{x}_{t}^{i-1};\mathbf{W}^{i})\\\mathbf{x}_{t}^{i}&=\mathbf{m}_{t}^{i}+\mathbf{x}_{t}^{i-1}\\\mathbf{c}_{t}^{i+1},\mathbf{m}_{t}^{i+1}&=\mathrm{LSTM}_{i+1}(\mathbf{c}_{t-1}^{i+1},\mathbf{m}_{t-1}^{i+1},\mathbf{x}_{t}^{i};\mathbf{W}^{i+1})\end{split}(6)Residual connections greatly improve the gradient flow in the backwardpass, which allows us to train very deep encoder and decodernetworks. In most of our experiments, we use 8 LSTM layers for the encoderand decoder, though residual connections can allow us to trainsubstantially deeper networks (similar to what was observedin [45]).In all experiments, our models consist of 8 encoder layers and 8 decoder layers.(Since the bottom encoder layer is actually bi-directional, in total there are9 logically distinct LSTM passes in the encoder.)The attention network is a simple feedforward network with one hidden layer with 1024 nodes.All of the models use 1024 LSTM nodes per encoder and decoder layers.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Authors refer to translation domain knowledge, as they refer to Luong's et al.  (2015) Neural Machine Translation as it reads through all source words until the end of a sentence, then starts translation by emitting one target word at a time as illustrated in Figure 1.	Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Authors refer to translation domain knowledge, as they refer to Luong's et al.  (2015) Neural Machine Translation as it reads through certain source words until the end of a sentence, then starts translation by emitting one target word at a time as illustrated in Figure 1.	Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Correct	Correct
Large Neural network NMT has the ability to generalize well to very long word sequences so that it doesn't have to store gigantic phrase tables and language models, which results to having a small memory footprint.	Neural Machine Translation (NMT) achieved state-of-the-art performances inlarge-scale translation tasks such as from English to French [Luong et al., 2015] andEnglish to German [Jean et al., 2015]. NMT is appealing since it requires minimaldomain knowledge and is conceptually simple. The model by ?) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT [Koehn et al., 2003].	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Large Neural network NMT doesn't have the ability to generalize well to very long word sequences so it has to store gigantic phrase tables and language models, which results to having a small memory footprint.	Neural Machine Translation (NMT) achieved state-of-the-art performances inlarge-scale translation tasks such as from English to French [Luong et al., 2015] andEnglish to German [Jean et al., 2015]. NMT is appealing since it requires minimaldomain knowledge and is conceptually simple. The model by ?) reads through all the source words until the end-of-sentence symbol <eos> is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT [Koehn et al., 2003].	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Soft attention model's weights are placed "softly" over all patches in the source image.  while Hard attention models selects one patch of the image to attend at a time.  it's also, none-differentiable, requires more complicated techniques and less expensive at inference time. 	This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by ?) to tackle the image captiongeneration task. In their work, soft attention refers to the global attentionapproach in which weights are placed “softly” over all patches in the sourceimage. The hard attention, on the other hand, selects one patchof the image to attend to at a time. While less expensive at inference time, thehard attention model is non-differentiable and requires more complicatedtechniques such as variance reduction or reinforcement learning to train.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
Soft attention model's weights are placed "softly" over all patches in the source image.  while Hard attention models selects one patch of the image to attend at a time.  it's also, none-differentiable, requires less complicated techniques and more expensive at inference time. 	This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by ?) to tackle the image captiongeneration task. In their work, soft attention refers to the global attentionapproach in which weights are placed “softly” over all patches in the sourceimage. The hard attention, on the other hand, selects one patchof the image to attend to at a time. While less expensive at inference time, thehard attention model is non-differentiable and requires more complicatedtechniques such as variance reduction or reinforcement learning to train.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Previous work included vanilla RNN, LSTM and GRU in the decoder architecture.  as Sutskever and Luon stacked multiple layers of RNN with LSTM hidden unit for the decoder and encoder.  and Cho, Bahdanau and Jeal all adopted GRU.	Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state \mbox{\boldmath{$h$}}_{t} at the top layer of a stacking LSTM. The goal is then to derive a context vector \mbox{\boldmath{$c$}}_{t} that captures relevant source-side information to help predict the current target word y_{t}. While these models differ in how the context vector \mbox{\boldmath{$c$}}_{t} is derived, they share the same subsequent steps.?) used an RNN with the standard hidden unit for the decoder and aconvolutional neural network for encoding the source sentence representation. Onthe other hand, both ?) and ?) stackedmultiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit forboth the encoder and the decoder. ?), ?), and?) all adopted a different version of the RNN with anLSTM-inspired hidden unit, the gated recurrent unit (GRU), for bothcomponents.444They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.In more detail, one can parameterize the probability of decoding each word y_{j} as:p\left(y_{j}|y_{<j},\mbox{\boldmath{$s$}}\right)=\operatorname{softmax}\left(g\left(\mbox{\boldmath{$h$}}_{j}\right)\right)(2)with g being the transformation function that outputs a vocabulary-sizedvector.555One can provide g with other inputs such as the currentlypredicted word y_{j} as in [Bahdanau et al., 2015]. Here, \mbox{\boldmath{$h$}}_{j} is the RNN hiddenunit, abstractly computed as:\mbox{\boldmath{$h$}}_{j}=f(\mbox{\boldmath{$h$}}_{j-1},\mbox{\boldmath{$s$}}),(3)where f computes the current hidden state given the previous hidden state andcan be either a vanilla RNN unit, a GRU, or an LSTM unit. In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation s is only used once to initialize the decoder hidden state. On the other hand, in [Bahdanau et al., 2015, Jean et al., 2015] and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Previous work included vanilla RNN, CNN, LSTM and GRU in the decoder architecture.  as Sutskever and Luon stacked multiple layers of RNN with LSTM hidden unit for the decoder and encoder.  and Cho, Bahdanau and Jeal all adopted GRU.	Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state \mbox{\boldmath{$h$}}_{t} at the top layer of a stacking LSTM. The goal is then to derive a context vector \mbox{\boldmath{$c$}}_{t} that captures relevant source-side information to help predict the current target word y_{t}. While these models differ in how the context vector \mbox{\boldmath{$c$}}_{t} is derived, they share the same subsequent steps.?) used an RNN with the standard hidden unit for the decoder and aconvolutional neural network for encoding the source sentence representation. Onthe other hand, both ?) and ?) stackedmultiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit forboth the encoder and the decoder. ?), ?), and?) all adopted a different version of the RNN with anLSTM-inspired hidden unit, the gated recurrent unit (GRU), for bothcomponents.444They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.In more detail, one can parameterize the probability of decoding each word y_{j} as:p\left(y_{j}|y_{<j},\mbox{\boldmath{$s$}}\right)=\operatorname{softmax}\left(g\left(\mbox{\boldmath{$h$}}_{j}\right)\right)(2)with g being the transformation function that outputs a vocabulary-sizedvector.555One can provide g with other inputs such as the currentlypredicted word y_{j} as in [Bahdanau et al., 2015]. Here, \mbox{\boldmath{$h$}}_{j} is the RNN hiddenunit, abstractly computed as:\mbox{\boldmath{$h$}}_{j}=f(\mbox{\boldmath{$h$}}_{j-1},\mbox{\boldmath{$s$}}),(3)where f computes the current hidden state given the previous hidden state andcan be either a vanilla RNN unit, a GRU, or an LSTM unit. In [Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Cho et al., 2014, Luong et al., 2015], the source representation s is only used once to initialize the decoder hidden state. On the other hand, in [Bahdanau et al., 2015, Jean et al., 2015] and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next.	Incorrect	Invent something didn't mentioned	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, and the size of it equals the number of time steps on the source side.	The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \mbox{\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \mbox{\boldmath{$h$}}_{t} with each source hidden state \mbox{\boldmath{$\bar{h}$}}_{s}:\displaystyle\mbox{\boldmath{$a$}}_{t}(s)\displaystyle=\operatorname{align}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})(7)\displaystyle=\frac{\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\right)}{\sum_{s^{\prime}}\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s^{\prime}})\right)}Here, \operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\!=\!\begin{cases}\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it dot}}\\\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$W_{a}$}}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it general}}\\\mbox{\boldmath{$v$}}_{a}^{\top}\tanh\left(\mbox{\boldmath{$W_{a}$}}[\mbox{\boldmath{$h$}}_{t};\mbox{\boldmath{$\bar{h}$}}_{s}]\right)&\mbox{{\it concat}}\end{cases}Figure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h¯s. A global context vector ct is then computed as the weighted average, according to at, over all the source states.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
a variable-length alignment is a vector derived by comparing the current target hidden state with each source hidden state, but the size of it doesn't equal to the number of time steps on the source side.	The idea of a global attentional model is to consider all the hidden states ofthe encoder when deriving the context vector c_{t}. In this model type, avariable-length alignment vector \mbox{\boldmath{$a$}}_{t}, whose size equals the number of timesteps on the source side, is derived by comparing the current target hiddenstate \mbox{\boldmath{$h$}}_{t} with each source hidden state \mbox{\boldmath{$\bar{h}$}}_{s}:\displaystyle\mbox{\boldmath{$a$}}_{t}(s)\displaystyle=\operatorname{align}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})(7)\displaystyle=\frac{\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\right)}{\sum_{s^{\prime}}\exp\left(\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s^{\prime}})\right)}Here, \operatorname{score} is referred as a content-based function for which we consider three differentalternatives:\operatorname{score}(\mbox{\boldmath{$h$}}_{t},\mbox{\boldmath{$\bar{h}$}}_{s})\!=\!\begin{cases}\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it dot}}\\\mbox{\boldmath{$h$}}_{t}^{\top}\mbox{\boldmath{$W_{a}$}}\mbox{\boldmath{$\bar{h}$}}_{s}&\mbox{{\it general}}\\\mbox{\boldmath{$v$}}_{a}^{\top}\tanh\left(\mbox{\boldmath{$W_{a}$}}[\mbox{\boldmath{$h$}}_{t};\mbox{\boldmath{$\bar{h}$}}_{s}]\right)&\mbox{{\it concat}}\end{cases}Figure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states h¯s. A global context vector ct is then computed as the weighted average, according to at, over all the source states.	Incorrect	Opposite	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The author's model consists of 4 layers of LSTM, each has 100 cells, and 1000-dimensional embeddings.	When training our NMT systems, following [Bahdanau et al., 2015, Jean et al., 2015], we filter outsentence pairs whose lengths exceed 50 words and shuffle mini-batches as weproceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and1000-dimensional embeddings. We follow [Sutskever et al., 2014, Luong et al., 2015] in trainingNMT with similar settings: (a) our parameters are uniformly initialized in[-0.1,0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learningrate schedule is employed – we start with a learning rate of 1; after 5 epochs,we begin to halve the learning rate every epoch, (d) our mini-batch size is 128,and (e) the normalized gradient is rescaled whenever its norm exceeds 5.Additionally, we also use dropout with probability 0.2 for our LSTMs as suggested by[Zaremba et al., 2015]. For dropout models, we train for 12 epochs and start halvingthe learning rate after 8 epochs. For localattention models, we empirically set the window size D=10.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The author's model consists of 8 layers of LSTM, each has 110 cells, and 1000-dimensional embeddings.	When training our NMT systems, following [Bahdanau et al., 2015, Jean et al., 2015], we filter outsentence pairs whose lengths exceed 50 words and shuffle mini-batches as weproceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and1000-dimensional embeddings. We follow [Sutskever et al., 2014, Luong et al., 2015] in trainingNMT with similar settings: (a) our parameters are uniformly initialized in[-0.1,0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learningrate schedule is employed – we start with a learning rate of 1; after 5 epochs,we begin to halve the learning rate every epoch, (d) our mini-batch size is 128,and (e) the normalized gradient is rescaled whenever its norm exceeds 5.Additionally, we also use dropout with probability 0.2 for our LSTMs as suggested by[Zaremba et al., 2015]. For dropout models, we train for 12 epochs and start halvingthe learning rate after 8 epochs. For localattention models, we empirically set the window size D=10.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Local attention method had sharper alignment weights than global one, that's due to it's designed to only focus on a subset of words each time.	We visualize the alignment weights produced by our different attention models in Figure 7. The visualization of the local attention model is much sharper than that of the global one. This contrast matches our expectation that local attention is designed to only focus on a subset of words each time.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Local attention method had sharper alignment weights than global one, that's due to it's designed to focus on an entire set of words each time.	We visualize the alignment weights produced by our different attention models in Figure 7. The visualization of the local attention model is much sharper than that of the global one. This contrast matches our expectation that local attention is designed to only focus on a subset of words each time.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Leveraging frame rate conditioning, authours enable an additional augmentation method to tackle the limited volume of available videos at training timee, and provides additional control on the generated video at inference time by a varying number of frames-per-second.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Leveraging frame rate conditioning, authours enable an additional augmentation method to tackle the limited volume of available videos at training timee, and provides additional control on the generated video at inference time by a varying number of frames-per-minute.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
For a more thorough evaluation than existing literature in T2V, the authors collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts and filtered out prompts that were incomplete, too abstract, or offensive and then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.  It is used for zero-shot T2V human evaluation.	Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
For a more thorough evaluation than existing literature in T2V, the authors collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 500 prompts and filtered out prompts that were incomplete, too abstract, or offensive and then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories.  It is used for zero-shot T2V human evaluation.	Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
There is no alinged text and only the videos are used.  The authors use only public datasets (and no paired text for videos).  A text description describes an image frame in video so it has limitations to associate between text and phenomenon in video.  It needs to depict more detailed stories, is left for future work.  Moreover, for all of experiments they applied extrapolation network↑F with frame skip 5 to upsample a 16 frame video to 76 frames.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
There is no alinged text and only the videos are used.  The authors use only public datasets (and no paired text for videos).  A text description describes an image frame in video so it has limitations to associate between text and phenomenon in video.  It needs to depict more detailed stories, is left for future work.  Moreover, for all of experiments they applied extrapolation network↑F with frame skip 6 to upsample a 15 frame video to 76 frames.	Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Datasets.To train the image models, we use a 2.3B subset of the dataset from (Schuhmann et al., ) where the text is English. We filter out sample pairs with NSFW images 333We used this model: https://github.com/GantMan/nsfw_model, toxic words in the text, or images with a watermark probability larger than 0.5.We use WebVid-10M (Bain et al., 2021) and a 10M subset from HD-VILA-100M (Xue et al., 2022) 444These 100M clips are sourced from 3.1M videos. We randomly downloaded 3 clips per video to form our HD-VILA-10M subset. to train our video generation models.Note that only the videos (no aligned text) are used. The decoder \operatorname{D}^{t} and the interpolation model is trained on WebVid-10M. \operatorname{SR}_{l}^{t} is trained on both WebVid-10M and HD-VILA-10M.While prior work (Hong et al., 2022; Ho et al., 2022) have collected private text-video pairs for T2V generation, we use only public datasets (and no paired text for videos). We conduct automatic evaluation on UCF-101 (Soomro et al., 2012) and MSR-VTT (Xu et al., 2016) in a zero-shot setting.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
It is hard to collect datasets because a similarly sized (text, video) dataset cannot be easily collected.  For human evaluation, they employ some annotators and filtered out according to their criteria.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
It is not hard to collect datasets because a similarly sized (text, video) dataset can be easily collected.  For human evaluation, they employ some annotators and filtered out according to their criteria.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Incorrect	Opposite	Incorrect	Incorrect	Unrelated	Incorrect	Incorrect	Incorrect
They train a new masked frame interpolation and extrapolation network ↑F , capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.  Additionally, the spatial super-resolution models enable to increase a higher (controllable) frame rate.  Therefore, using the extrapolation network ↑F, it can possible to extend the video length from 16 frames to 76 frames.	In order to expand the two-dimensional (2D) conditional network into the temporal dimension, we modify the two key building blocks that now require not just spatial but also temporal dimensions in order to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2), discussed in the following two subsections. Other layers, such as fully-connected layers, do not require specific handling when adding an additional dimension, as they are agnostic to structured spatial and temporal information.Temporal modifications are made in most U-Net-based diffusion networks: the spatiotemporal decoder \operatorname{D^{t}} now generating 16 RGB frames, each of size 64\times 64, the newly added frame interpolation network \uparrow_{F}, increasing the effective frame rate by interpolating between the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks \operatorname{SR}_{l}^{t}.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.In addition to the spatiotemporal modifications discussed in Sec. 3.2, we train a new masked frame interpolation and extrapolation network \uparrow_{F}, capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.In order to increase the frame rate within memory and compute constraints, wefine-tune a spatiotemporal decoder \operatorname{D^{t}} on the task of masked frame interpolation, by zero-padding the masked input frames, enabling video upsampling. When fine-tuning on masked frame interpolation, we add an additional 4 channels to the input of the U-Net: 3 channels for the RGB masked video input and an additional binary channel indicating which frames are masked. We fine-tune with variable frame-skips and fps conditioning to enable multiple temporal upsample rates at inference time. We denote \uparrow_{F} as the operator that expands the given video tensor through masked frame interpolation. For all of our experiments we applied \uparrow_{F} with frame skip 5 to upsample a 16 frame video to 76 frames ((16-1)\times5+1). Note that we can use the same architecture for video extrapolation or image animation by masking frames at the beginning or end of a video.Using function-preserving transformations, we extend the spatial layers at the model initialization stage, to include temporal information.The extended spatial-temporal network includes new attention modules that learn temporal world dynamics from a collection of videos. This procedure significantly accelerates the T2V training process by instantaneously transferring the knowledge from a previously trained T2I network to a new T2V one. To enhance the visual quality, we train spatial super-resolution models as well as frame interpolation models. This increases the resolution of the generated videos, as well as enables a higher (controllable) frame rate.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
They train a new masked frame interpolation and extrapolation network ↑F , capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.  Additionally, the spatial super-resolution models enable to increase a higher (controllable) frame rate.  Therefore, using the extrapolation network ↑F, it can possible to extend the video length from 16 frames to 86 frames.	In order to expand the two-dimensional (2D) conditional network into the temporal dimension, we modify the two key building blocks that now require not just spatial but also temporal dimensions in order to generate videos: (i) Convolutional layers (Sec. 3.2.1), and (ii) attention layers (Sec. 3.2.2), discussed in the following two subsections. Other layers, such as fully-connected layers, do not require specific handling when adding an additional dimension, as they are agnostic to structured spatial and temporal information.Temporal modifications are made in most U-Net-based diffusion networks: the spatiotemporal decoder \operatorname{D^{t}} now generating 16 RGB frames, each of size 64\times 64, the newly added frame interpolation network \uparrow_{F}, increasing the effective frame rate by interpolating between the 16 generated frames (as depicted in Fig. 2), and the super-resolution networks \operatorname{SR}_{l}^{t}.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.In addition to the spatiotemporal modifications discussed in Sec. 3.2, we train a new masked frame interpolation and extrapolation network \uparrow_{F}, capable of increasing the number of frames of the generated video either by frame interpolation for a smoother generated video, or by pre/post frame extrapolation for extending the video length.In order to increase the frame rate within memory and compute constraints, wefine-tune a spatiotemporal decoder \operatorname{D^{t}} on the task of masked frame interpolation, by zero-padding the masked input frames, enabling video upsampling. When fine-tuning on masked frame interpolation, we add an additional 4 channels to the input of the U-Net: 3 channels for the RGB masked video input and an additional binary channel indicating which frames are masked. We fine-tune with variable frame-skips and fps conditioning to enable multiple temporal upsample rates at inference time. We denote \uparrow_{F} as the operator that expands the given video tensor through masked frame interpolation. For all of our experiments we applied \uparrow_{F} with frame skip 5 to upsample a 16 frame video to 76 frames ((16-1)\times5+1). Note that we can use the same architecture for video extrapolation or image animation by masking frames at the beginning or end of a video.Using function-preserving transformations, we extend the spatial layers at the model initialization stage, to include temporal information.The extended spatial-temporal network includes new attention modules that learn temporal world dynamics from a collection of videos. This procedure significantly accelerates the T2V training process by instantaneously transferring the knowledge from a previously trained T2I network to a new T2V one. To enhance the visual quality, we train spatial super-resolution models as well as frame interpolation models. This increases the resolution of the generated videos, as well as enables a higher (controllable) frame rate.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
To prevent flickering artifacts, they sustain hallucinating information to be consistent across frames.  They use the same noise initialization for each frame to encourage consistent detail hallucination.  For future works, they explain about thier several technical limitations such as learning association between text and phenomenon.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
To prevent flickering artifacts, they sustain hallucinating information to be consistent across frames.  They use different noise initialization for each frame to encourage consistent detail hallucination.  For future works, they explain about thier several technical limitations such as learning association between text and phenomenon.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
In the limited volume of available videos at training time, conditioning on a varying number of frames-per-second, enables an additional augmentation method, and provides additional control on the generated video at inference time.  In human evaluation experiments, it shows that raters choose this method more realistic motion more than the half of the time.  It is observed that this method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial	Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
In the limited volume of available videos at training time, conditioning on a varying number of frames-per-second, enables an additional augmentation method, and provides additional control on the generated video at inference time.  In human evaluation experiments, it shows that raters choose this method more realistic motion more than the half of the time.  It is observed that this method excels when there are little differences between frames where having real-world knowledge of how objects move is crucial.	Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
They collect 300 text prompts for human evaluation and the prompts include 5 categories.  For quantitative results, Make-A-Video outperform CogVideo in both Chinese and English settings that it can infer that Make-AVideo has significantly better generalization capabilities than prior work.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
They collect 300 text prompts for human evaluation and the prompts include 3 categories.  For quantitative results, Make-A-Video outperform CogVideo in both Chinese and English settings that it can infer that Make-AVideo has significantly better generalization capabilities than prior work.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.Automatic Evaluation on MSR-VTT. In addition to GODIVA and NÜWA that report on MSR-VTT, we also perform inference on the officially released CogVideo model with both Chinese and English inputs for comparison. For CogVideo and Make-A-Video, we only generate one sample for each prompt in a zero-shot setting. We only generate videos that are at 16\times 256\times 256 as the evaluation models do not expect higher resolutions and frame rate.The results are shown in Table 1. Make-A-Video’s zero-shot performance is much better than GODIVA and NÜWA which are trained on MSR-VTT. We also outperform CogVideo in both Chinese and English settings. Thus, Make-A-Video has significantly better generalization capabilities than prior work.Automatic Evaluation on UCF-101. UCF-101 is a popular benchmark to evaluate video generation and has been recently used in T2V models. CogVideo performed finetuning of their pretrained model for class-conditional video generation. VDM (Ho et al., 2022) performed unconditional video generation and trained from scratch on UCF-101. We argue that both settings are not ideal and is not a direct evaluation of the T2V generation capabilities. Moreover, the FVD evaluation model expects the videos to be 0.5 second (16 frames), which is too short to be used for video generation in practice. Nevertheless, in order to compare to prior work, we conducted evaluation on UCF-101 in both zero-shot and finetuning settings.As shown in Table 2, Make-A-Video’s zero-shot performance is already competitive than other approaches that are trained on UCF-101, and is much better than CogVideo, which indicates that Make-A-Video can generalize better even to such a specific domain. Our finetuning setting achieves state-of-the-art results with a significant reduction in FVD, which suggests that Make-A-Video can generate more coherent videos than prior work.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Unsupervised learning enables networks to learn from orders of magnitude more data.  This large quantity of data is important to learn representations of more subtle, less common concepts in the world.  Unsupervised learning has long had great success in advancing the field of natural language processing (NLP).	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.	Correct		Incorrect	Correct	Correct	Correct	Correct	Correct
Unsupervised learning enables networks to learn from orders of magnitude more data.  This large quantity of data is important to learn representations of less subtle, more common concepts in the world.  Unsupervised learning has long had great success in advancing the field of natural language processing (NLP).	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
A model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by the authors' temporal diffusion-based method.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Correct		Incorrect	Correct	Correct	Incorrect	Correct	Correct
A model that has only seen text describing images is not effective at generating short videos, as demonstrated by the authors' temporal diffusion-based method.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.Learning from the world around us is one of the greatest strengths of human intelligence. Just as we quickly learn to recognize people, places, things, and actions through observation, generative systems will be more creative and useful if they can mimic the way humans learn. Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data. The presented work has shown how labeled images combined effectively with unlabeled video footage can achieve that.As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Incorrect	Opposite	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
Pseudo-3D convolutional layers facilitates information sharing between the spatial and temporal axes, without succumbing to the heavy computational load of 3D conv layers.  Additionally, conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.Motivated by separable convolutions (Chollet, 2017), we stack a 1D convolution following each 2D convolutional (conv) layer, as shown in Fig. 3. This facilitates information sharing between the spatial and temporal axes, without succumbing to the heavy computational load of 3D conv layers. In addition, it creates a concrete partition between the pre-trained 2D conv layers and the newly initialized 1D conv layers, allowing us to train the temporal convolutions from scratch, while retaining the previously learned spatial knowledge in the spatial convolutions’ weights.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Pseudo-3D convolutional layers can't facilitates information sharing between the spatial and temporal axes without succumbing to the heavy computational load of 3D conv layers.  Additionally, conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.	Note that super resolution involves hallucinating information. In order to not have flickering artifacts, the hallucination must be consistent across frames. As a result, our \operatorname{SR}_{l}^{t} module operates across spatial and temporal dimensions. In qualitative inspection we found this to significantly outperform per-frame super resolution. It is challenging to extend \operatorname{SR}_{h} to the temporal dimension due to memory and compute constraints, as well as a scarcity of high resolution video data. So \operatorname{SR}_{h} operates only along the spatial dimensions. But to encourage consistent detail hallucination across frames, we use the same noise initialization for each frame.Motivated by separable convolutions (Chollet, 2017), we stack a 1D convolution following each 2D convolutional (conv) layer, as shown in Fig. 3. This facilitates information sharing between the spatial and temporal axes, without succumbing to the heavy computational load of 3D conv layers. In addition, it creates a concrete partition between the pre-trained 2D conv layers and the newly initialized 1D conv layers, allowing us to train the temporal convolutions from scratch, while retaining the previously learned spatial knowledge in the spatial convolutions’ weights.Frame rate conditioning. In addition to the T2I conditionings, similar to CogVideo (Hong et al., 2022), we add an additional conditioning parameter fps, representing the number of frames-per-second in a generated video. Conditioning on a varying number of frames-per-second, enables an additional augmentation method to tackle the limited volume of available videos at training time, and provides additional control on the generated video at inference time.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Modeling videos require expensive computational complexity that it is challenging in high-quality video data collection.  Thus, large-scale paired text-video is expensive as well.  Because of the limitations, the progress of T2V generation lags behind.	As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.Leveraging Image Priors for Video Generation.Due to the complexity of modeling videos and the challenges in high-quality video data collection, it is natural to consider leveraging image priors for videos to simplifying the learning process. After all, an image is a video with a single frame (Bain et al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates video generation as the task of finding a trajectory in the latent space of a pre-trained and fixed image generation model. In T2V generation, NÜWA (Wu et al., 2021b) combines image and video datasets in a multitask pre-training stage to improve model generalization for fine-tuning. CogVideo (Hong et al., 2022) uses a pre-trained and fixed T2I model for T2V generation with only a small number of trainable parameters to reduce memory usage during training. But the fixed autoencoder and T2I models can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable joint image and video generation. However, they sample random independent images from random videos as their source of images, and do not leverage the massive text-image datasets.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Modeling videos require expensive computational complexity that it is challenging in general video data collection.  Thus, large-scale paired text-video is expensive as well.  Because of the limitations, the progress of T2V generation lags behind.	As with all large-scale models trained on data from the web, our models have learnt and likely exaggerated social biases, including harmful ones. Our T2I generation model was trained on data that removed NSFW content and toxic words. All our data (image as well as videos) is publicly available, adding a layer of transparency to our models, and making it possible for the community to reproduce our work.Text-to-Video Generation.While there is remarkable progress in T2I generation, the progress of T2V generation lags behind largely due to two main reasons: the lack of large-scale datasets with high-quality text-video pairs, and the complexity of modeling higher-dimensional video data. Early works (Mittal et al., 2017; Pan et al., 2017; Marwah et al., 2017; Li et al., 2018; Gupta et al., 2018; Liu et al., 2019b) are mainly focused on video generation in simple domains, such as moving digits or specific human actions. To our knowledge, Sync-DRAW (Mittal et al., 2017) is the first T2V generation approach that leverages a VAE with recurrent attention. (Pan et al., 2017) and (Li et al., 2018) extend GANs from image generation to T2V generation.Leveraging Image Priors for Video Generation.Due to the complexity of modeling videos and the challenges in high-quality video data collection, it is natural to consider leveraging image priors for videos to simplifying the learning process. After all, an image is a video with a single frame (Bain et al., 2021). In unconditional video generation, MoCoGAN-HD (Tian et al., 2021) formulates video generation as the task of finding a trajectory in the latent space of a pre-trained and fixed image generation model. In T2V generation, NÜWA (Wu et al., 2021b) combines image and video datasets in a multitask pre-training stage to improve model generalization for fine-tuning. CogVideo (Hong et al., 2022) uses a pre-trained and fixed T2I model for T2V generation with only a small number of trainable parameters to reduce memory usage during training. But the fixed autoencoder and T2I models can be restrictive for T2V generation. The architecture of VDM (Ho et al., 2022) can enable joint image and video generation. However, they sample random independent images from random videos as their source of images, and do not leverage the massive text-image datasets.Make-A-Video differs from previous works in several aspects. First, our architecture breaks the dependency on text-video pairs for T2V generation.This is a significant advantage compared to prior work, that has to be restricted to narrow domains (Mittal et al., 2017; Gupta et al., 2018; Ge et al., 2022; Hayes et al., 2022), or require large-scale paired text-video data (Hong et al., 2022; Ho et al., 2022).Second, we fine-tune the T2I model for video generation, gaining the advantage of adapting the model weights effectively, compared to freezing the weights as in CogVideo (Hong et al., 2022).Third, motivated from prior work on efficient architectures for video and 3D vision tasks (Ye et al., 2019; Qiu et al., 2017; Xie et al., 2018), our use of pseudo-3D convolution (Qiu et al., 2017) and temporal attention layers not only better leverage a T2I architecture, it also allows for better temporal information fusion compared to VDM (Ho et al., 2022).	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
They employed annotators to make prompts and filtered out them correctly.  Evaluation is done about video quality and faithfulness.  For each comparison, 5 different annotators are employed.  They report FVD and IS on 10K samples and generate samples that follow the same class distribution as the training set.  Moreover, for MSR-VTT, FID and CLIPSIM are introduced.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
They employed annotators to make prompts and filtered out them correctly.  Evaluation is done about video quality and faithfulness.  For each comparison, 5 different annotators are employed.  They report FVD and IS on 16K samples and generate samples that follow the same class distribution as the training set.  Moreover, for MSR-VTT, FID and CLIPSIM are introduced.	Automatic Metrics.For UCF-101, we write one template sentence for each class (without generating any video) and fix it for evaluation. We report Frechet Video Distance (FVD) and Inception Score (IS) on 10K samples following (Ho et al., 2022). We generate samples that follow the same class distribution as the training set. For MSR-VTT, we report Frechet Inception Distance (FID) (Parmar et al., 2022) and CLIPSIM (average CLIP similarity between video frames and text) (Wu et al., 2021a), where all 59,794 captions from the test set are used, following (Wu et al., 2021b).Human Evaluation Set and Metrics.We collect an evaluation set from Amazon Mechanical Turk (AMT) that consists of 300 prompts. We asked annotators what they would be interested in generating if there were a T2V system. We filtered out prompts that were incomplete (e.g., “jump into water”), too abstract (e.g., “climate change”), or offensive. We then identified 5 categories (animals, fantasy, people, nature and scenes, food and beverage) and selected prompts for these categories. These prompts were selected without generating any videos for them, and were kept fixed. In addition, we also used the DrawBench prompts from Imagen (Saharia et al., 2022) for human evaluation.We evaluate video quality and text-video faithfulness. For video quality, we show two videos in random order and ask annotators which one is of higher quality. For faithfulness, we additionally show the text and ask annotators which video has a better correspondence with the text (we suggest them to ignore quality issues). In addition, we also conducted human evaluation to compare video motion realism of our interpolation model and FILM (Reda et al., 2022).For each comparison, we use the majority vote from 5 different annotators as the final result.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Make-A-Video adopt unsupervised learning method by leveraging joint text-image prior that it is not need paried text-video data.  But, for training of the prior \operatorname{P}, text input is required.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Make-A-Video adopt supervised learning method by leveraging joint text-image prior that it is not need paried text-video data.  But, for training of the prior \operatorname{P}, text input is required.	The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages (Schuhmann et al., 2022), enabling the recent breakthroughs in Text-to-Image (T2I) modeling. However, replicating this success for videos is limited since a similarly sized (text, video) dataset cannot be easily collected.It would be wasteful to train Text-to-Video (T2V) models from scratchwhen there already exist models that can generate images.Moreover, unsupervised learning enables networks to learn from orders of magnitude more data. This large quantity of data is important to learn representations of more subtle, less common concepts in the world. Unsupervised learning has long had great success in advancing the field of natural language processing (NLP) (Liu et al., 2019a; Brown et al., 2020). Models pre-trained this way yield considerably higher performance than when solely trained in a supervised manner.Inspired by these motivations, we propose Make-A-Video. Make-A-Video leverages T2I models to learn the correspondence between text and the visual world, and uses unsupervised learning on unlabeled (unpaired) video data, to learn realistic motion. Together, Make-A-Video generates videos from text without leveraging paired text-video data.Clearly, text describing images does not capture the entirety of phenomena observed in videos. That said, one can often infer actions and events from static images (e.g. a woman drinking coffee, or an elephant kicking a football) as done in image-based action recognition systems (Girish et al., 2020). Moreover, even without text descriptions, unsupervised videos are sufficient to learn how different entities in the world move and interact (e.g. the motion of waves at the beach, or of an elephant’s trunk). As a result, a model that has only seen text describing images is surprisingly effective at generating short videos, as demonstrated by our temporal diffusion-based method. Make-A-Video sets the new state-of-the-art in T2V generation.The different components of Make-A-Video described above are trained independently. The only component that receives text as input is the prior \operatorname{P}. We train it on paired text-image data and do not fine-tune it on videos. The decoder, prior, and two super-resolution components are first trained on images alone (no aligned text). Recall that the decoder receives CLIP image embedding as input, and the super-resolution components receive downsampled images as input during training.After training on images, we add and initialize the new temporal layers and fine-tune them over unlabeled video data.16 frames are sampled from the original video with random fps ranging from 1 to 30. We use the beta function for sampling and while training the decoder, start from higher FPS ranges (less motion) and then transition to lower FPS ranges (more motion).The masked-frame-interpolation component is fine-tuned from the temporal decoder.Human Evaluation.We compare to CogVideo (the only public zero-shot T2V generation model) on DrawBench and our test set. We also evaluate on the 28 videos shown on the webpage of VDM (Ho et al., 2022) (which may be biased towards showcasing the model’s strengths). Since this is a very small test set, we randomly generate 8 videos for each input and perform evaluation 8 times and report the average results.We generate videos at 76\times 256\times 256 resolution for human evaluation.The results are shown in Table 3. Make-A-Video achieves much better performance in both video quality and text-video faithfulness in all benchmarks and comparisons. For CogVideo, the results are similar on DrawBench and our evaluation set. For VDM, it is worth noting that we have achieved significantly better resultswithout any cherry-picking.We also evaluate our frame interpolation network in comparison to FILM (Reda et al., 2022). We first generate low frame rate videos (1 FPS) from text prompts in DrawBench and our evaluation set, then use each method to upsample to 4 FPS. Raters choose our method for more realistic motion 62% of the time on our evaluation set and 54% of the time on DrawBench. We observe that our method excels when there are large differences between frames where having real-world knowledge of how objects move is crucial.As a next step we plan to address several of the technical limitations. As discussed earlier, our approach can not learn associations between text and phenomenon that can only be inferred in videos. How to incorporate these (e.g., generating a video of a person waving their hand left-to-right or right-to-left), along with generating longer videos, with multiple scenes and events, depicting more detailed stories, is left for future work.Our main contributions are:•We present Make-A-Video – an effective method that extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model.•We leverage joint text-image priors to bypass the need for paired text-video data, which in turn allows us to potentially scale to larger quantities of video data.•We present super-resolution strategies in space and time that, for the first time, generate high-definition, high frame-rate videos given a user-provided textual input.•We evaluate Make-A-Video against existing T2V systems and present: (a) State-of-the-art results in quantitative as well as qualitative measures, and (b) A more thorough evaluation than existing literature in T2V. We also collect a test set of 300 prompts for zero-shot T2V human evaluation which we plan to release.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Video diffusion models present the first results on a large text-conditioned video generation tasks, and they achieve state-of-the-art results on popular video datasets.  They train the model with image-video jointly to improve sample quality.	To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Table 5 reports results that verify the effectiveness of classifier-free guidance (Ho and Salimans, 2021) on text-to-video generation. As expected, there is clearimprovement in the Inception Score-like metrics with higher guidance weight, while the FID-like metrics improve and then degrade with increasing guidance weight. Similar findings have been reported on text-to-image generation (Nichol et al., 2021).Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Video diffusion models present the first results on a large text-conditioned image generation tasks, and they achieve state-of-the-art results on popular video datasets.  They train the model with image-video jointly to improve sample quality.	To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Table 5 reports results that verify the effectiveness of classifier-free guidance (Ho and Salimans, 2021) on text-to-video generation. As expected, there is clearimprovement in the Inception Score-like metrics with higher guidance weight, while the FID-like metrics improve and then degrade with increasing guidance weight. Similar findings have been reported on text-to-image generation (Nichol et al., 2021).Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Video diffusion models modify little of the archicture to accommodate video data within the memory constraints of deep learning accelerators.  They approach with the standard diffusion modelformalism.  In their method, one of skill to make high resolution video is the spatial upsampling introduced by Menick and Kalchbrenner (2019). Also, reconstruction guidance is extended to constuct the high-resolution model.  When they have low resolution ground truth videos, it upsamples them into high resolution videos using an unconditional high resolution diffusion model.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in which the mean squared error loss is imposed on a downsampled version of the model prediction, and backpropagation is performed through this downsampling.In this setting, we have low resolution ground truth videos \mathbf{x}^{a} (e.g. at the 64x64 spatial resolution), which may be generated from a low resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128 spatial resolution) using an unconditional high resolution diffusion model \hat{\mathbf{x}}_{\theta}. To accomplish this, we adjust the high resolution model as follows:𝐱~θ(𝐳t)=𝐱^θ(𝐳t)−wr⁢αt2∇𝐳t∥𝐱a−𝐱^θa(𝐳t)∥22\displaystyle\tilde{\mathbf{x}}_{\theta}(\mathbf{z}_{t})=\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t})-\frac{w_{r}\alpha_{t}}{2}\nabla_{\mathbf{z}_{t}}\lVert\mathbf{x}^{a}-\hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t})\rVert_{2}^{2}over~ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ∇ start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(8)where \hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t}) is our model’s reconstruction of the low-resolution video from \mathbf{z}_{t}, which is obtained by downsampling the high resolution output of the model using a differentiable downsampling algorithm such as bilinear interpolation.Note that it is also possible to simultaneously condition on low resolution videos while autoregressively extending samples at the high resolution using the same reconstruction guidance method. In Fig. 2, we show samples of this approach for extending 16x64x64 low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128 diffusion model.Our approach to video generation using diffusion models is to use the standard diffusion model formalism described in Section 2 with a neural network architecture suitable for video data. Each of our models is trained to jointly model a fixed number of frames at a fixed spatial resolution. To extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our models with a conditioning technique described later in Section 3.1.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Video diffusion models modify little of the archicture to accommodate video data within the memory constraints of deep learning accelerators.  They approach with the standard diffusion modelformalism.  In their method, the only to make high resolution video is the spatial upsampling introduced by Menick and Kalchbrenner (2019). Also, reconstruction guidance is extended to constuct the high-resolution model.  When they have low resolution ground truth videos, it upsamples them into high resolution videos using an unconditional high resolution diffusion model.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in which the mean squared error loss is imposed on a downsampled version of the model prediction, and backpropagation is performed through this downsampling.In this setting, we have low resolution ground truth videos \mathbf{x}^{a} (e.g. at the 64x64 spatial resolution), which may be generated from a low resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128 spatial resolution) using an unconditional high resolution diffusion model \hat{\mathbf{x}}_{\theta}. To accomplish this, we adjust the high resolution model as follows:𝐱~θ(𝐳t)=𝐱^θ(𝐳t)−wr⁢αt2∇𝐳t∥𝐱a−𝐱^θa(𝐳t)∥22\displaystyle\tilde{\mathbf{x}}_{\theta}(\mathbf{z}_{t})=\hat{\mathbf{x}}_{\theta}(\mathbf{z}_{t})-\frac{w_{r}\alpha_{t}}{2}\nabla_{\mathbf{z}_{t}}\lVert\mathbf{x}^{a}-\hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t})\rVert_{2}^{2}over~ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - divide start_ARG italic_w start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ∇ start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ∥ bold_x start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT - over^ start_ARG bold_x end_ARG start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT(8)where \hat{\mathbf{x}}^{a}_{\theta}(\mathbf{z}_{t}) is our model’s reconstruction of the low-resolution video from \mathbf{z}_{t}, which is obtained by downsampling the high resolution output of the model using a differentiable downsampling algorithm such as bilinear interpolation.Note that it is also possible to simultaneously condition on low resolution videos while autoregressively extending samples at the high resolution using the same reconstruction guidance method. In Fig. 2, we show samples of this approach for extending 16x64x64 low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128 diffusion model.Our approach to video generation using diffusion models is to use the standard diffusion model formalism described in Section 2 with a neural network architecture suitable for video data. Each of our models is trained to jointly model a fixed number of frames at a fixed spatial resolution. To extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our models with a conditioning technique described later in Section 3.1.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The higher performance in Text-to-Video Generation requires not only excellent fidelity of video samples but also good handling of social bias in text-description given as a condition.	Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.Our goal with this work is to advance research on methods in generative modeling, and our methods have the potential to positively impact creative downstream applications. As with prior work in generative modeling, however, our methods have the potential for causing harmful impact and could enhance malicious or unethical uses of generative models, such as fake content generation, harassment, and misinformation spread, and thus we have decided not to release our models. Like all generative models, our models reflect the biases of their training datasets and thus may require curation to ensure fair results from sampling. In particular, our text-to-video models inherit the challenges faced by prior work on text-to-image models, and our future work will involve auditing for forms of social bias, similar to Buolamwini and Gebru (2018); Burns et al. (2018); Steed and Caliskan (2021); Cho et al. (2022) for image-to-text and image labeling models. We see our work as only a starting point for further investigation on video diffusion models and investigation into their societal implications, and we will aim to explore benchmark evaluations for social and cultural bias in the video generation setting and make the necessary research advances to address them.	Correct		Incorrect	Incorrect	Correct	Unrelated	Incorrect	Correct
The higher performance in Text-to-Video Generation requires only excellent fidelity of video samples.	Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.Our goal with this work is to advance research on methods in generative modeling, and our methods have the potential to positively impact creative downstream applications. As with prior work in generative modeling, however, our methods have the potential for causing harmful impact and could enhance malicious or unethical uses of generative models, such as fake content generation, harassment, and misinformation spread, and thus we have decided not to release our models. Like all generative models, our models reflect the biases of their training datasets and thus may require curation to ensure fair results from sampling. In particular, our text-to-video models inherit the challenges faced by prior work on text-to-image models, and our future work will involve auditing for forms of social bias, similar to Buolamwini and Gebru (2018); Burns et al. (2018); Steed and Caliskan (2021); Cho et al. (2022) for image-to-text and image labeling models. We see our work as only a starting point for further investigation on video diffusion models and investigation into their societal implications, and we will aim to explore benchmark evaluations for social and cultural bias in the video generation setting and make the necessary research advances to address them.	Incorrect	Change concept	Unrelated	Unrelated	Incorrect	Unrelated	Incorrect	Incorrect
Other diffusion models that generate images use a 2D U-Net, but they use a 3D U-Net to handle video. A 3D U-Net diffusion model is used to generate a fixed number of video frames. A 2D U-Net is modified into each 2D convolution into a space-only 3D convolution, and inserted a temporal attention block that performs attention over the first axis and treats the spatial axes as batch axes. Authors concatenate random independent image frames to the end of each videosampled from the dataset and they choose these random independent images from random videos within the same dataset.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.We propose to extend this image diffusion model architecture to video data, given by a block of a fixed number of frames, using a particular type of 3D U-Net (Çiçek et al., 2016) that is factorized over space and time. First, we modify the image model architecture by changing each 2D convolution into a space-only 3D convolution, for instance, we change each 3x3 convolution into a 1x3x3 convolution (the first axis indexes video frames, the second and third index the spatial height and width). The attention in each spatial attention block remains as attention over space; i.e., the first axis is treated as a batch axis. Second, after each spatial attention block, we insert a temporalattention block that performs attention over the first axis and treats the spatial axes as batch axes.We use relative position embeddings (Shaw et al., 2018) in each temporal attention block so that the network can distinguish ordering of frames in a way that does not require an absolute notion of video time. We visualize the model architecture in Fig. 1.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
A 3D U-Net diffusion model is used to generate a fixed number of video frames. A 2D U-Net is modified into each 2D convolution into a space-only 3D convolution, and inserted a temporal attention block that performs attention over the first axis and treats the spatial axes as batch axes. Authors concatenate ordered independent image frames to the end of each videosampled from the dataset and they choose these ordered independent images from indicated videos within the same dataset.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro et al. (2012) for unconditional modeling of video. The benchmark consists of short clips of people performing one of 101 activities, and was originally collected for the purpose of training action recognition models. We model short segments of 16 frames from this dataset, downsampled to a spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by our model, and we compare against methods from the literature, finding that our method strongly improves upon the previous state-of-the-art. We additionally evaluate video prediction performance on the Kinetics-600 benchmark (Kay et al., 2017; Carreira et al., 2018). Kinetics-600 contains approximately 400 thousand training videos depicting 600 different activities. We train unconditional models on this dataset at the 64\times 64 resolution and evaluate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate FVD and Inception Score using the I3D network (Carreira and Zisserman, 2017). See Table 3 for results. In our reported results we sample test videos without replacement, and we use the same randomly selected subsequences for generating model samples and for defining the ground truth, since this results in the lowest bias and variance in the reported FVD metric. However, from personal communication we learned that (Luc et al., 2020; Clark et al., 2019) instead sampled with replacement, and used a different random seed when sampling the ground truth data. We find that this way of evaluating raises the FVD obtained by our model slightly, from 16.2 to 16.9. Inception Score is unaffected.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.We propose to extend this image diffusion model architecture to video data, given by a block of a fixed number of frames, using a particular type of 3D U-Net (Çiçek et al., 2016) that is factorized over space and time. First, we modify the image model architecture by changing each 2D convolution into a space-only 3D convolution, for instance, we change each 3x3 convolution into a 1x3x3 convolution (the first axis indexes video frames, the second and third index the spatial height and width). The attention in each spatial attention block remains as attention over space; i.e., the first axis is treated as a batch axis. Second, after each spatial attention block, we insert a temporalattention block that performs attention over the first axis and treats the spatial axes as batch axes.We use relative position embeddings (Shaw et al., 2018) in each temporal attention block so that the network can distinguish ordering of frames in a way that does not require an absolute notion of video time. We visualize the model architecture in Fig. 1.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Video Diffusion Models demonstrates their inital attempt to generate text-to-image generation results in various settings such as classifier-free guidance, jointly training of video-image, and unconditional and conditional generation. They consider several additional image frames for joint training of video-image. Moreover, they adjust the weight of classifier-free guidance, and conditioning method with the newly proposed reconstruction guidance for autoregressive extension and simultaneous spatial and temporal super-resolution.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.We report our results on video diffusion models for unconditional video generation (Section 4.1), conditional video generation (video prediction) (Section 4.2), and text-conditioned video generation (Section 4.3). We evaluate our models using standard metrics such as FVD (Unterthiner et al., 2018), FID (Heusel et al., 2017), and IS (Salimans et al., 2016); details on evaluation are provided below alongside each benchmark.Samples and additional results are provided at https://video-diffusion.github.io/.Architecture hyperparameters, training details, and compute resources are listed in Appendix A.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Video Diffusion Models demonstrates their inital attempt to generate text-to-image generation results in various settings such as classifier-free guidance, jointly training of video-image, video-text, text-image and unconditional and conditional generation. They consider several additional image frames for joint training of video-image. Moreover, they adjust the weight of classifier-free guidance, and conditioning method with the newly proposed reconstruction guidance for autoregressive extension and simultaneous spatial and temporal super-resolution.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.We report our results on video diffusion models for unconditional video generation (Section 4.1), conditional video generation (video prediction) (Section 4.2), and text-conditioned video generation (Section 4.3). We evaluate our models using standard metrics such as FVD (Unterthiner et al., 2018), FID (Heusel et al., 2017), and IS (Salimans et al., 2016); details on evaluation are provided below alongside each benchmark.Samples and additional results are provided at https://video-diffusion.github.io/.Architecture hyperparameters, training details, and compute resources are listed in Appendix A.The remaining experiments reported are on text-conditioned video generation. In this text-conditioned video generation setting, we employ a dataset of 10 million captioned videos, and we condition the diffusion model on captions in the form of BERT-large embeddings (Devlin et al., 2019) processed using attention pooling. We consider two model sizes: a small model for the joint training ablation, and a large model for generating the remaining results (both architectures are described in detail in Appendix A), and we explore the effects of joint video-image training, classifier-free guidance, and our newly proposed reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal super-resolution. We report the following metrics in this section on 4096 samples: the video metric FVD, and the Inception-based image metrics FID and IS measured by averaging activations across frames (FID/IS-avg) and by measuring the first frame only (FID/IS-first). For FID and FVD, we report two numbers which are measured against the training and validation sets, respectively. For IS, we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.Figure 3 shows the effect of classifier-free guidance (Ho and Salimans, 2021) on a text-conditioned video model. Similar to what was observed in other work that used classifier-free guidance on text-conditioned image generation (Nichol et al., 2021) and class-conditioned image generation (Ho and Salimans, 2021; Dhariwal and Nichol, 2021), adding guidance increases the sample fidelity of each individual image and emphases the effect of the conditioning signal.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Due to memory constraints of deep learning accelerators, a fixed number of video frames should be used. If memory constraints are addressed, a larger number of frames can be used. To address this issue, they introduce joint training on video and image. They concatenate random independent image frames to the end of each video sampled from the dataset to consider more frames during training and implement a memory optimization to fit more independent examples in a batch.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Due to memory constraints of deep learning accelerators, a random number of video frames should be used. If memory constraints are addressed, a larger number of frames can be used. To address this issue, they introduce joint training on video and image. They concatenate random independent image frames to the end of each video sampled from the dataset to consider more frames during training and implement a memory optimization to fit more independent examples in a batch.	We show that high quality videos can be generated using essentially the standard formulation of the Gaussian diffusion model (Sohl-Dickstein et al., 2015), with little modification other than straightforward architectural changes to accommodate video data within the memory constraints of deep learning accelerators. We train models that generate a fixed number of video frames using a 3D U-Net diffusion model architecture, and we enable generating longer videos by applying this model autoregressively using a new method for conditional generation. We additionally show the benefits of joint training on video and image modeling objectives. We test our methods on video prediction and unconditional video generation, where we achieve state-of-the-art sample quality scores, and we also show promising first results on text-conditioned video generation.The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Due to memory limit, authors consider newly joint training method utilizing both image and video. As more independent image frames are added, we can see the reduced variane of the gradient at the expense of some bias for the video modeling. Table 4 shows that additional frames per video helps to improve in video and image sample quality metrics.	As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
Due to memory limit, authors consider newly joint training method utilizing text, image and video. As more independent image frames are added, we can see the reduced variane of the gradient at the expense of some bias for the video modeling. Table 4 shows that additional frames per video helps to improve in video and image sample quality metrics.	As described in Section 3, one of the main advantages of our video architecture is that it allows us to easily train the model jointly on video and image generative modeling objectives. To implement this joint training, we concatenate random independent image frames to the end of each video sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent mixing information across video frames and each individual image frame. We choose these random independent images from random videos within the same dataset; in future work we plan to explore the effect of choosing images from other larger image-only datasets.Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider training on an additional 0, 4, or 8 independent image frames per video.One can see clear improvements in video and image sample quality metrics as more independent image frames are added. Adding independent image frames has the effect of reducing variance of the gradient at the expense of some bias for the video modeling objective, and thus it can be seen as a memory optimization to fit more independent examples in a batch.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	Incorrect	Invent something didn't mentioned	Correct	Correct	Correct	Incorrect	Incorrect	Correct
It enables generating longer videos by applying this model autoregressively using a new method for a conditional generation.  The authors explain that the conditioning method helps the model outperform the existing method.  The samples from the reconstruction guidance method are temporally coherent over the course of the entire autoregressive generation process and we can infer that the quality is not affected by generated frames.	The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.Figure 4 shows the samples of our reconstruction guidance method for conditional sampling compared to the replacement method (Section 3.1) for the purposes of generating long samples in a block-autoregressive manner (Section 4.3.3). The samples from the replacement method clearly show a lack of temporal coherence, since frames from different blocks throughout the generated videos appear to be uncorrelated samples (conditioned on \mathbf{c}). The samples from the reconstruction guidance method, by contrast, are clearly temporally coherent over the course of the entire autoregressive generation process. Figure 2 additionally shows samples of using the reconstruction guidance method to simultaneously condition on low frequency, low resolution videos while autoregressively extending temporally at a high resolution.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
It enables generating shorter videos by applying this model autoregressively using a new method for a conditional generation.  The authors explain that the conditioning method helps the model outperform the existing method.  The samples from the reconstruction guidance method are temporally coherent over the course of the entire autoregressive generation process and we can infer that the quality is not affected by generated frames.	The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate of at least 24 frames per second. To manage the computational requirements of training our models, we only train on a small subset of say 16 frames at a time. However, at test time we can generate longer videos by extending our samples. For example, we could first generate a video \mathbf{x}^{\text{a}}\sim p_{\theta}(\mathbf{x}) consisting of 16 frames, and then extend it with a second sample \mathbf{x}^{\text{b}}\sim p_{\theta}(\mathbf{x}^{\text{b}}|\mathbf{x}^{\text{a}}). If \mathbf{x}^{\text{b}} consists of frames following \mathbf{x}^{\text{a}}, this allows us to autoregressively extend our sampled videos to arbitrary lengths, which we demonstrate in Section 4.3.3. Alternatively, we could choose \mathbf{x}^{\text{a}} to represent a video of lower frame rate, and then define \mathbf{x}^{\text{b}} to be those frames in between the frames of \mathbf{x}^{\text{a}}.This allows one to then to upsample a video temporally, similar to how Menick and Kalchbrenner (2019) generate high resolution images through spatial upsampling.In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from diffusion models, an improvement over the replacement method ofSong et al. (2021).In Table 6 we present results on generating longer videos using both techniques, and find that our proposed method indeed improves over the replacement method in terms of perceptual quality scores.Figure 4 shows the samples of our reconstruction guidance method for conditional sampling compared to the replacement method (Section 3.1) for the purposes of generating long samples in a block-autoregressive manner (Section 4.3.3). The samples from the replacement method clearly show a lack of temporal coherence, since frames from different blocks throughout the generated videos appear to be uncorrelated samples (conditioned on \mathbf{c}). The samples from the reconstruction guidance method, by contrast, are clearly temporally coherent over the course of the entire autoregressive generation process. Figure 2 additionally shows samples of using the reconstruction guidance method to simultaneously condition on low frequency, low resolution videos while autoregressively extending temporally at a high resolution.We have additionally demonstrated the benefits of joint image-video training and classifier-free guidance for video diffusion models on both video and image sample quality metrics, and we also introduced a new reconstruction-guided conditional sampling method that outperforms existing replacement or imputation methods for conditional sampling from unconditionally trained models. Our reconstruction guidance method can generate long sequences using either frame interpolation (or temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial super-resolution. We look forward to investigating this method in a wider variety of conditioning settings.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The T2V generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.  They use the pre-trained T2I model which is able to generate images that align well with the text, including the verb terms.	Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Formally, given a video \mathcal{V}=\left\{v_{i}|i\in[1,m]\right\} with m frames, accompanied with a corresponding textual description \mathcal{T}, our objective is to generate novel videos \hat{\mathcal{V}} driven by text prompts \hat{\mathcal{T}} using a pre-trained T2I model M, i.e., M(\hat{\mathcal{V}}|\mathcal{V},\mathcal{T},\hat{\mathcal{T}}), where \mathcal{T} and \hat{\mathcal{T}} share the same verbs.Examples of output variations driven by \hat{\mathcal{T}} include: changes of subject, background (e.g., the place where the subject is), attribute (e.g., color, age, etc.), and other semantic modifications (see Fig. 1 and Fig. 7).Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The T2V generator is expected to capture necessary emotion knowledge from the input video and synthesize novel videos guided by edited prompts.  They use the pre-trained T2I model which is able to generate images that align well with the text, including the adjective and verb terms.	Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Formally, given a video \mathcal{V}=\left\{v_{i}|i\in[1,m]\right\} with m frames, accompanied with a corresponding textual description \mathcal{T}, our objective is to generate novel videos \hat{\mathcal{V}} driven by text prompts \hat{\mathcal{T}} using a pre-trained T2I model M, i.e., M(\hat{\mathcal{V}}|\mathcal{V},\mathcal{T},\hat{\mathcal{T}}), where \mathcal{T} and \hat{\mathcal{T}} share the same verbs.Examples of output variations driven by \hat{\mathcal{T}} include: changes of subject, background (e.g., the place where the subject is), attribute (e.g., color, age, etc.), and other semantic modifications (see Fig. 1 and Fig. 7).Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
One-Shot Tuning acquires temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SCAttn) and temporal self-attention (Temp-Attn).  It captures spatial information and yields similar semantics as the training video to perform semantic mixing.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.One of the applications of our Tune-A-Video is to replace the subject in the training video. As shown in Fig. 7, Tune-A-Video is able to generate videos with customized subjects via changing the corresponding terms in text prompt, for example, replacing the polar bear with mammoth (the 2nd row of Fig. 7) in the “walking bear” example; replacing the man with King Kong or astronaut (the 7th and 8th row of Fig. 7) in the “running man” example.The generated videos are consistent in time and well-aligned with the modified text prompts.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.The models without One-Shot Tuning uses the weight of pre-trained T2I for inference. The tuning itself (the 2nd row of Fig. 10) captures spatial information and yields similar semantics as the training video (e.g., the panda skis like the man in training video). This implies that our one-shot tuning strategy is capable of performing semantic mixing as [24, 21], but with more flexibility that the object categories can be replaced. However, the temporal consistency cannot be maintained through individual frame attention (e.g., the pandas in the 2nd row of Fig. 10 are inconsistent). Although our Tune-A-Video w/o tuning (the 3nd row of Fig. 10) output consistent content across frames, it does not have a notion of motion since it is only trained on static images. With tuning, our full model (the 4nd row of Fig. 10) is able to generate the temporally-coherent video that incorporates the motion information (i.e., skiing) in the training video, demonstrating the effectiveness of our One-Shot Tuning.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
One-Shot Tuning doesn't aqcuire temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SCAttn) and temporal self-attention (Temp-Attn).  It captures spatial information and yields similar semantics as the training video to perform semantic mixing.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.One of the applications of our Tune-A-Video is to replace the subject in the training video. As shown in Fig. 7, Tune-A-Video is able to generate videos with customized subjects via changing the corresponding terms in text prompt, for example, replacing the polar bear with mammoth (the 2nd row of Fig. 7) in the “walking bear” example; replacing the man with King Kong or astronaut (the 7th and 8th row of Fig. 7) in the “running man” example.The generated videos are consistent in time and well-aligned with the modified text prompts.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.The models without One-Shot Tuning uses the weight of pre-trained T2I for inference. The tuning itself (the 2nd row of Fig. 10) captures spatial information and yields similar semantics as the training video (e.g., the panda skis like the man in training video). This implies that our one-shot tuning strategy is capable of performing semantic mixing as [24, 21], but with more flexibility that the object categories can be replaced. However, the temporal consistency cannot be maintained through individual frame attention (e.g., the pandas in the 2nd row of Fig. 10 are inconsistent). Although our Tune-A-Video w/o tuning (the 3nd row of Fig. 10) output consistent content across frames, it does not have a notion of motion since it is only trained on static images. With tuning, our full model (the 4nd row of Fig. 10) is able to generate the temporally-coherent video that incorporates the motion information (i.e., skiing) in the training video, demonstrating the effectiveness of our One-Shot Tuning.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Tune-A-Video is based on a pre-trained T2I diffusion model and only updates the projection matrices in attention blocks, with the rest of parameters being frozen.  Moreover, SC-Attn reduce the computational complexity compared to CogView2.	We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.We provide quantitative and qualitative comparisons with CogVideo [19] (the only public333https://github.com/THUDM/CogVideo T2V generation model). CogVideo is based on a pre-trained T2I model CogView2 [4] and consists of 9.4 billion parameters (around 6\times larger than our Tune-A-Video). It is extensively trained on a large-scale dataset of 5.4 million captioned videos.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Tune-A-Video is based on a pre-trained T2I diffusion model and only updates the projection matrices in attention blocks without freezing other parameters.  Moreover, SC-Attn reduce the computational complexity compared to CogView2.	We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.We provide quantitative and qualitative comparisons with CogVideo [19] (the only public333https://github.com/THUDM/CogVideo T2V generation model). CogVideo is based on a pre-trained T2I model CogView2 [4] and consists of 9.4 billion parameters (around 6\times larger than our Tune-A-Video). It is extensively trained on a large-scale dataset of 5.4 million captioned videos.As shown in Fig. 9, the resultant video of “a woman is running on the lawn” generated by our Tune-A-Video trained on one text-video pair of “a man running on the beach”, is comparable to the one generated by CogVideo [19] trained on large-scale datasets. When we challenge the models using a more complex prompt, e.g., “a cat is running on the single-plank bridge, comic style”, our Tune-A-Video generates continuous video frames that are better aligned with the target prompt. This demonstrates that our model integrates the motion guidance from training video with the rich semantics of pre-trained T2I models for high temporal consistency and deep language understanding. See Fig. 15 for more qualitative comparisons with CogVideo.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
To imitate human's ability to adapt new visual concets, One-Shot Video Generation task is proposed.  Tune-A-Video generates videos with novel visual concepts (e. , subjects, backgrounds, attributes, styles, etc. ) guided by the text prompt.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We provide several applications of our Tune-A-Video in generating videos with novel visual concepts (e.g., subjects, backgrounds, attributes, styles, etc.) guided by the text prompt. Here, we showcase two training examples: “a polar bear is walking on ice” (“walking bear”, the 1st row of Fig. 7), “a young man is running on the beach” (“running man”, the 5th row of Fig. 7), and their variations with edited prompts. More examples can be found in Fig. 12-14.Our Tune-A-Video also enables changing the video background (i.e., place where the subject is), while keeping the motions of the subject and the temporal information consistent. For example, we can “send” a polar bear walking on the ice to Time Square (the 3rd of Fig. 7), and a man running on the beach to mountain (the 6th row of Fig. 7). We observe that some background semantic is tied to the subject, e.g., in the case of “walking bear”, although the background is replaced with Time Square, the color of the ground still remains similar to ice. We conjecture this is due to the strong regularities between “polar bear” and “ice”.We examine the ability of our Tune-A-Video in modifying the attributes such as color (e.g., a man wearing yellow to be in red, the 2nd row of Fig. 1) and age (e.g., a young man to become old, the 6th row of Fig. 7). We found that our Tune-A-Video even enables modifying the attribute that is not specified in the training prompt (e.g., we do not tell the model the skiing man in the training video wears yellow clothes). Notably, other properties in training video could be well preserved (e.g., the black backpack wore by the skiing man, and the blue shirts and black shorts wore by the running man).We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
To imitate human's ability to adapt new visual concets, One-Shot Video Generation task is proposed.  Tune-A-Video generates videos with novel visual concepts (e. , subjects, backgrounds, attributes, styles, etc. ) guided by the given images.	The large-scale multimodal dataset [35] with billions of text-image pairs crawled from Internet has enabled breakthrough in open-domain Text-to-Image (T2I) generation [25, 29, 4, 6, 34]. To replicate this success in Text-to-Video (T2V) generation, recent works [36, 14, 17, 48] extend the spatial-only T2I generation models to the spatio-temporal domain.These models generally adopt the standard paradigm to finetune T2I models on large-scale text-video datasets (e.g., WebVid-10M [2]).Although this paradigm produces decent results for T2V generation, it requires extensive training, which is expensive and not affordable to everyone.Human is capable of one-shot learning [22, 23, 42]. For instance, given a video paired with a textual description of “a man is skiing on snow” as a hint, we might hallucinate how a panda skis on snow if we could picture what a panda looks like.Since T2I models pre-trained with large-scale image-text data already capture the knowledge of open-domain concepts,an intuitive question arises:can it infer other novel videos from a single video example like human beings?A new T2V generation problem is therefore introduced, namely, One-Shot Video Generation, where only a single text-video pair is presented for training an open-domain T2V generator.The generator is expected to capture necessary motion knowledge from the input video and synthesize novel videos guided by edited prompts.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We provide several applications of our Tune-A-Video in generating videos with novel visual concepts (e.g., subjects, backgrounds, attributes, styles, etc.) guided by the text prompt. Here, we showcase two training examples: “a polar bear is walking on ice” (“walking bear”, the 1st row of Fig. 7), “a young man is running on the beach” (“running man”, the 5th row of Fig. 7), and their variations with edited prompts. More examples can be found in Fig. 12-14.Our Tune-A-Video also enables changing the video background (i.e., place where the subject is), while keeping the motions of the subject and the temporal information consistent. For example, we can “send” a polar bear walking on the ice to Time Square (the 3rd of Fig. 7), and a man running on the beach to mountain (the 6th row of Fig. 7). We observe that some background semantic is tied to the subject, e.g., in the case of “walking bear”, although the background is replaced with Time Square, the color of the ground still remains similar to ice. We conjecture this is due to the strong regularities between “polar bear” and “ice”.We examine the ability of our Tune-A-Video in modifying the attributes such as color (e.g., a man wearing yellow to be in red, the 2nd row of Fig. 1) and age (e.g., a young man to become old, the 6th row of Fig. 7). We found that our Tune-A-Video even enables modifying the attribute that is not specified in the training prompt (e.g., we do not tell the model the skiing man in the training video wears yellow clothes). Notably, other properties in training video could be well preserved (e.g., the black backpack wore by the skiing man, and the blue shirts and black shorts wore by the running man).We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
Factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation.  The self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.  Using full attention in space-time leads to quadratic growth in computation.  It is thus infeasible for generating long-form videos with increasing frames.	We first follow VDM baselines [17, 14] to use inflated 1\times 3\times 3 convolution kernels for video inputs and append temporal self-attention with causal mask for temporal modeling. However, we empirically observe that factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation (see Sec. 4.3). We further inflate the spatial self-attention to cross-frame attention as follows.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.There are alternative options for cross-frame attention in Fig. 6, including full attention and causal attention which also capture spatio-temporal consistency.However, such straightforward choices are actually not feasible in generating videos with continuous frames due to their high computational complexity.Specifically, given m frames and N sequences for each frame, the complexity for full attention is \mathcal{O}((mN)^{2}).It is not affordable if we need to generate long-form videos with a large value of m.Though causal attention only visits the previous frames, it actually shows the same complexity as full attention since the attention mask needs to accommodate the longest sequence, i.e., mN.We fine-tune the inflated T2V models for One-Shot Video Generation. The objective of one-shot tuning is to acquire temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SC-Attn) and temporal self-attention (Temp-Attn). The SC-Attn models the one-way mapping from frame \mathbf{v}_{i} to its previous frames (i.e., \mathbf{v}_{1} and \mathbf{v}_{i-1}), and due to the causality, key and value features derived from previous frames are independent to the output of \mathbf{v}_{i}.Therefore, we propose to fix W^{K} and W^{V}, and only update W^{Q} in SC-Attn layers.On the other hand, we fine-tune the entire Temp-Attn layers, including W^{Q}, W^{K}, W^{V}, as they are newly added and randomly initialized.Moreover, we update the query projection in cross-attention (Cross-Attn) for better video-text alignment.Fine-tuning the attention blocks is computationally efficient, and keeps the property of diffusion-based T2I models unchanged.As shown in our experiments, this is sufficient to produce temporally-coherent videos with novel text prompts.Fig. 5 highlights the training pipeline and trainable parameters during the one-shot tuning process.As mentioned in Sec. 3.3, the VDM baselines [17, 14] factorize space and time by appending an additional temporal attention after each spatial attention block in T2I diffusion models. Specifically, the original 2D spatial blocks are kept in space only, and additional temporal convolution/attention blocks are added after the spatial layers to capture time-related information. For a fair comparison, we adopt the same training pipeline in Fig. 5 to fine-tune the VDM baselines for One-Shot Video Generation. As shown in Fig. 8, the VDM baselines with factorized space-time attention fail to generate consistent content (compare the appearance of the subjects across frames), whereas our Tune-A-Video with spatio-temporal cross-frame attention maintains better temporal consistency.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation.  The self-attention layers in T2I models are driven by spatial similarities and pixel positions.  Using full attention in space-time leads to quadratic growth in computation.  It is thus infeasible for generating long-form videos with increasing frames.	We first follow VDM baselines [17, 14] to use inflated 1\times 3\times 3 convolution kernels for video inputs and append temporal self-attention with causal mask for temporal modeling. However, we empirically observe that factorized space-time attention in VDM baselines is insufficient to generate consistent content in the task of One-Shot Video Generation (see Sec. 4.3). We further inflate the spatial self-attention to cross-frame attention as follows.Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.There are alternative options for cross-frame attention in Fig. 6, including full attention and causal attention which also capture spatio-temporal consistency.However, such straightforward choices are actually not feasible in generating videos with continuous frames due to their high computational complexity.Specifically, given m frames and N sequences for each frame, the complexity for full attention is \mathcal{O}((mN)^{2}).It is not affordable if we need to generate long-form videos with a large value of m.Though causal attention only visits the previous frames, it actually shows the same complexity as full attention since the attention mask needs to accommodate the longest sequence, i.e., mN.We fine-tune the inflated T2V models for One-Shot Video Generation. The objective of one-shot tuning is to acquire temporal knowledge from one training video, which is enabled by Sparse-Causal Attention (SC-Attn) and temporal self-attention (Temp-Attn). The SC-Attn models the one-way mapping from frame \mathbf{v}_{i} to its previous frames (i.e., \mathbf{v}_{1} and \mathbf{v}_{i-1}), and due to the causality, key and value features derived from previous frames are independent to the output of \mathbf{v}_{i}.Therefore, we propose to fix W^{K} and W^{V}, and only update W^{Q} in SC-Attn layers.On the other hand, we fine-tune the entire Temp-Attn layers, including W^{Q}, W^{K}, W^{V}, as they are newly added and randomly initialized.Moreover, we update the query projection in cross-attention (Cross-Attn) for better video-text alignment.Fine-tuning the attention blocks is computationally efficient, and keeps the property of diffusion-based T2I models unchanged.As shown in our experiments, this is sufficient to produce temporally-coherent videos with novel text prompts.Fig. 5 highlights the training pipeline and trainable parameters during the one-shot tuning process.As mentioned in Sec. 3.3, the VDM baselines [17, 14] factorize space and time by appending an additional temporal attention after each spatial attention block in T2I diffusion models. Specifically, the original 2D spatial blocks are kept in space only, and additional temporal convolution/attention blocks are added after the spatial layers to capture time-related information. For a fair comparison, we adopt the same training pipeline in Fig. 5 to fine-tune the VDM baselines for One-Shot Video Generation. As shown in Fig. 8, the VDM baselines with factorized space-time attention fail to generate consistent content (compare the appearance of the subjects across frames), whereas our Tune-A-Video with spatio-temporal cross-frame attention maintains better temporal consistency.However, using full attention in space-time leads to quadratic growth in computation. It is thus infeasible for generating long-form videos with increasing frames.To reduce the computational complexity, we propose a novel variant of causal attention [19, 8], termed as Sparse-Causal Attention (SC-Attn). SC-Attn can be interpreted as a diluted version of causal attention which only visits the first video frame and the former video frame.SC-Attn is proven to be capable of properly capturing the motion in the training video.By autoregressively generating video frames, Tune-A-Video is able to produce videos of arbitrary lengths.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Correct
The authors extend the spatial self-attention in the T2I model from one image to multiple images to maintain content consistency across frames.  It is useful in spatiotemporal domain like generating videos.	Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.More recently, Imagen Video [14] improves VDM with cascaded diffusion models and v-prediction parameterization to generate high definition videos. Phenaki [41] is the first work to generate videos from time variable prompts. To achieve this, they compresses videos to small representations of discrete tokens with causal attention in time, and thus can handle variable-length videos. To address the lack of video-text pair data, they joint train on a large scale of image-text pairs and a smaller number of video-text pairs, achieving better generalization results than available video datasets. Make-A-Video [36] shares similar motivation and aims to transfer the significant progress in T2I generation to T2V generation. They combine the appearance-text information from text-image data together with the world movements from unsupervised video footage, and achieve the state-of-the-art in T2V generation.We follow [36] to use pre-trained T2I diffusion models and propose Tune-A-Video for one-shot T2V generation. Differently, Tune-A-Video explores a more efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion changes.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors extend the spatial self-attention in the T2I model from one image and its description text to multiple images and their description texts to maintain content consistency across frames.  It is useful in spatiotemporal domain like generating videos.	Intuitively, the key to video generation is to keep the continuous motion of consistent objects.So we make the following observations on state-of-the-art T2I diffusion models [31] that motivate our method accordingly.(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), but not continuously (the first row of Fig. 2).This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation.(2) Regarding consistent objects: Simply extending the self-attention in the T2I model from one image to multiple images maintains content consistency across frames.Taking the same example, when we generate consecutive frames in parallel with extended cross-frame attention to the 1st frame, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.We implement our findings into a simple yet effective method, dubbed as Tune-A-Video, for the task of One-Shot Video Generation.Tune-A-Video is based on a simple inflation of pre-trained T2I diffusion models [31] over time dimension, by extending the 3\times 3 convolution to 1\times 3\times 3, and spatial self-attention to spatio-temporal cross-frame attention.We propose an efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion dynamics from the one-shot video, with the rest of parameters being frozen.To imitate how humans adapt new visual concepts from a single exemplar, we introduce a new T2V generation task called One-Shot Video Generation, where only a single text-video pair is provided for training an open-domain T2V generator.To this end, we propose Tune-A-Video, a simple yet effective solution based on pretrained T2I diffusion models.Our method exploits the properties of pretrained T2I models by extending spatial self-attention to spatio-temporal domain with a tailored Sparse-Causal Attention.To capture necessary motion knowledge, we update the projection matrices in attention block on one training sample. Given this, our Tune-A-Video supports several T2V applications, including subject replacement, background change, attribute modification, style transfer, and etc.More recently, Imagen Video [14] improves VDM with cascaded diffusion models and v-prediction parameterization to generate high definition videos. Phenaki [41] is the first work to generate videos from time variable prompts. To achieve this, they compresses videos to small representations of discrete tokens with causal attention in time, and thus can handle variable-length videos. To address the lack of video-text pair data, they joint train on a large scale of image-text pairs and a smaller number of video-text pairs, achieving better generalization results than available video datasets. Make-A-Video [36] shares similar motivation and aims to transfer the significant progress in T2I generation to T2V generation. They combine the appearance-text information from text-image data together with the world movements from unsupervised video footage, and achieve the state-of-the-art in T2V generation.We follow [36] to use pre-trained T2I diffusion models and propose Tune-A-Video for one-shot T2V generation. Differently, Tune-A-Video explores a more efficient tuning strategy that only updates the projection matrices in attention blocks to capture continuous motion changes.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
RoBERTa performs at about 2. 6 BPC on the MLM task with the Wiki-40B dataset.  RoBERTa performs better than BERT.	We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text.We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus.	Correct		Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
RoBERTa performs at about 2.2 BPC on the MLM task with the Wiki-40B dataset.  RoBERTa performs better than BERT.	We first measure the perplexity of English pretrained MLMs in other languages. We use Wiki-40B, a multilingual language modeling dataset that covers 41 languages Guo et al. (2020). Following the Wiki-40B paper, we report bits per character (BPC) to allow comparison between models with different tokenizations of the text.We find that both BERT models perform notably worse on modeling other languages; however, RoBERTa, reduces the gap with the multilingual models from 2.51 BPC to 0.87 BPC (Figure 1(a)). This finding is consistent with Tran (2020), who also found RoBERTa transfers well cross-lingually.We first consider the performance of the encoders when probed for POS knowledge (Figure 1(b)).444For T5, this means that we evaluate the output of the encoder and discard the decoder. Unsurprisingly, on average all of the English models underperform the multilingual models. Similar to MLM, we find that RoBERTa performs better than BERT when probed for POS features on other languages; surprisingly, it also strongly outperforms T5, despite C4 containing more absolute non-English data than the RoBERTa corpus.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The role of the non-English data during the pre-training is that they enhance cross-lingual transfer and generalization.	In this paper, we demonstrate that English pretrained models are exposed to a considerable amount of non-English data during pretraining, particularly in the case of more recent models that are trained on larger corpora derived from web crawls. We also find that this non-English text acts as a significant source of signal for cross-lingual transfer.However, the presence of foreign language data in pretraining corpora is not inherently problematic. Models trained on these datasets perform exceedingly well on their target languages and generalize to other languages much better than expected. Rather, it is important to remember that these models are not performing zero-shot transfer when used in other languages, given the scale and data with which they were pretrained.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The role of the non-English data during the pre-training is bad because they are considered as noises.	In this paper, we demonstrate that English pretrained models are exposed to a considerable amount of non-English data during pretraining, particularly in the case of more recent models that are trained on larger corpora derived from web crawls. We also find that this non-English text acts as a significant source of signal for cross-lingual transfer.However, the presence of foreign language data in pretraining corpora is not inherently problematic. Models trained on these datasets perform exceedingly well on their target languages and generalize to other languages much better than expected. Rather, it is important to remember that these models are not performing zero-shot transfer when used in other languages, given the scale and data with which they were pretrained.	Incorrect	Change concept	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect	Incorrect
According to the authors, about 300k to 406M non-English tokens were found in the investigated English corpus.	A summary of the language identification experiments is presented in Figure 1.111Full results of this evaluation are detailed in Appendix C. We see that every corpus contains notable quantities of non-English data, with our estimates ranging between 300k to 406M tokens. An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
According to the authors, about 30k to 40M non-English tokens were found in the investigated English corpus.	A summary of the language identification experiments is presented in Figure 1.111Full results of this evaluation are detailed in Appendix C. We see that every corpus contains notable quantities of non-English data, with our estimates ranging between 300k to 406M tokens. An obvious factor that affects the amount of non-English data in each corpus is the overall size of the dataset; however, even when controlling for size by looking at the percentage of non-English data, we still see that the smaller corpora (Wikipedia, BookCorpus, and Stories) have relatively less non-English data.	Incorrect	Change number	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors indicate that six categories are used in non-English text-classifier	We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors indicate that three categories are used in non-English text-classifier	We also perform a closer analysis on a random subset (200 per corpus) of non-English lines predicted by the language classifier (Table 1). Each example is manually coded into one of six categories. The first set covers various kinds of foreign language data: NE, where the line contains only non-English language text; BiL, or bilingual, where the line contains both English and non-English text; Trans., in which the English and non-English data that are translations of each other; and Ent., where the line is primarily English but contains non-English entities. The last two codes pertain to errors made by the language classifier: En., where the line only contains English text, and XX, which refers to lines that contain no natural language.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors used English Wikipedia, BookCorpus, Stories, OpenWebText, CC-NEWS, and C4. En datasets in pretraining.	We consider the following pretraining datasets: English Wikipedia(11.8GB); BookCorpus (Zhu et al. 2015, 4.2GB); Stories (Trinh and Le 2018, 31GB); OpenWebText (Gokaslan and Cohen 2019, 38GB), which is an open-source version of WebText Radford et al. (2019); CC-NEWS (Liu et al. 2019, 76 GB); and C4.En (Raffel et al. 2020, 305GB), as provided by Dodge et al. (2021). We use the versions of Wikipedia, BookCorpus, and CC-NEWS used to pretrain RoBERTa.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The authors used English Wikipedia, BookCorpus, IMDB, ImageNet, Stories, OpenWebText, CC-NEWS, and C4. En datasets in pretraining.	We consider the following pretraining datasets: English Wikipedia(11.8GB); BookCorpus (Zhu et al. 2015, 4.2GB); Stories (Trinh and Le 2018, 31GB); OpenWebText (Gokaslan and Cohen 2019, 38GB), which is an open-source version of WebText Radford et al. (2019); CC-NEWS (Liu et al. 2019, 76 GB); and C4.En (Raffel et al. 2020, 305GB), as provided by Dodge et al. (2021). We use the versions of Wikipedia, BookCorpus, and CC-NEWS used to pretrain RoBERTa.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Lambada can be adopted for other NLP tasks as Lambada is an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the BC algorithm for high-level reasoning.  Lambada achieves significant improvements over existing approaches such as Chain-of-Thought and Selection-Inference in terms of prediction accuracy and proof accuracy.	We developed Lambada, an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the backward chaining (BC) algorithm for high-level reasoning. We showed that Lambada achieves significant improvements over competitive existing approaches such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy. Furthermore, we demonstrated how Lambada efficiently searches the entire proof space to accurately conclude that a statement can neither be proved nor disproved based on the theory.Although we only do experiments on formal reasoning problems and datasets, we believe our key insight on the efficacy of goal-directed reasoning with LMs is widely applicable and can be adapted to other NLP tasks where multi-step inference may be required. Going beyond the specific design of Lambada and its specialized modules, it would be useful to find other BC-inspired methods that might even incorporate BC into the LM directly e.g. a BC version of Chain-of-Thought.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Lambada can be adopted for other NLP tasks as Lambada is an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the BC algorithm for high-level reasoning.  However, Lambada doesn't achieve significant improvements over existing approaches such as Chain-of-Thought and Selection-Inference in terms of prediction accuracy and proof accuracy.	We developed Lambada, an algorithm for text-based deductive logical reasoning that combines the capacity of LMs to handle naturalistic text input with the backward chaining (BC) algorithm for high-level reasoning. We showed that Lambada achieves significant improvements over competitive existing approaches such as Chain-of-Thought and Selection-Inference both in terms of prediction accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy. Furthermore, we demonstrated how Lambada efficiently searches the entire proof space to accurately conclude that a statement can neither be proved nor disproved based on the theory.Although we only do experiments on formal reasoning problems and datasets, we believe our key insight on the efficacy of goal-directed reasoning with LMs is widely applicable and can be adapted to other NLP tasks where multi-step inference may be required. Going beyond the specific design of Lambada and its specialized modules, it would be useful to find other BC-inspired methods that might even incorporate BC into the LM directly e.g. a BC version of Chain-of-Thought.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The fast gradient sign method is very quick but may lead to sub-optimal perturbations thus damaging the overall robustness estimation, and fine-tuning with such adversarial samples may sometimes result in a drop in the overall performance of the model.  On the other hand, DeepFool creates adversarial perturbations that are closer to the absolute minimum compared to others thus giving us a more reliable tool in terms of robustness estimation and fine-tuning.	We compare the proposed DeepFool approach to state-of-the-art techniques to compute adversarial perturbations in [18] and [4]. The method in [18] solves a series of penalized optimization problems to find the minimal perturbation, whereas [4] estimates the minimal perturbation by taking the sign of the gradient\displaystyle\hat{\bm{r}}(\bm{x})=\epsilon\,\text{sign}\left(\nabla_{\bm{x}}J(\bm{\theta},\bm{x},y)\right),with J the cost used to train the neural network, \bm{\theta} is the model parameters, and y is the label of \bm{x}. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter \epsilon, we chose the smallest \epsilon such that 90\% of the data are misclassified after perturbation.555Using this method, we observed empirically that one cannot reach 100\% misclassification rate on some datasets. In fact, even by increasing \epsilon to be very large, this method can fail in misclassifying all samples.It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.We illustrate in Figure 1 perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.In this section, we fine-tune the networks of Table 1 on adversarial examples to build more robust classifiers for the MNIST and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool’s adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples.We fine-tune the networks by performing 5 additional epochs, with a 50\% decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5 extra epochs. For the sake of completeness, we also performed 5 extra epochs on the original data. The evolution of \hat{\rho}_{\text{adv}} for the different fine-tuning strategies is shown in Figures 5(a) to 5(d), where the robustness \hat{\rho}_{\text{adv}} is estimated using DeepFool, since this is the most accurate method, as shown in Table 1. Observe that fine-tuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN’s robustness is increased by about 40%. On the other hand, quite surprisingly, the method in [4] can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial perturbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations.To verify this hypothesis, we compare in Figure 7 the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by \alpha=1,2,3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see Figure 8). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations.Table 3 lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on generalization.666While the authors of [4] reported an increased generalization performance on the MNIST task (from 0.94\% to 0.84\%) using adversarial regularization, it should be noted that the their experimental setup is significantly different as [4] trained the network based on a modified cost function, while we performed straightforward fine-tuning.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The fast gradient sign method is very quick but may lead to sub-optimal perturbations thus damaging the overall robustness estimation, and fine-tuning with such adversarial samples may sometimes result in a drop in the overall performance of the model.  On the other hand, DeepFool creates adversarial perturbations that are closer to the absolute minimum compared to others thus giving us a less reliable tool in terms of robustness estimation and fine-tuning.	We compare the proposed DeepFool approach to state-of-the-art techniques to compute adversarial perturbations in [18] and [4]. The method in [18] solves a series of penalized optimization problems to find the minimal perturbation, whereas [4] estimates the minimal perturbation by taking the sign of the gradient\displaystyle\hat{\bm{r}}(\bm{x})=\epsilon\,\text{sign}\left(\nabla_{\bm{x}}J(\bm{\theta},\bm{x},y)\right),with J the cost used to train the neural network, \bm{\theta} is the model parameters, and y is the label of \bm{x}. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter \epsilon, we chose the smallest \epsilon such that 90\% of the data are misclassified after perturbation.555Using this method, we observed empirically that one cannot reach 100\% misclassification rate on some datasets. In fact, even by increasing \epsilon to be very large, this method can fail in misclassifying all samples.It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.We illustrate in Figure 1 perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.In this section, we fine-tune the networks of Table 1 on adversarial examples to build more robust classifiers for the MNIST and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool’s adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples.We fine-tune the networks by performing 5 additional epochs, with a 50\% decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5 extra epochs. For the sake of completeness, we also performed 5 extra epochs on the original data. The evolution of \hat{\rho}_{\text{adv}} for the different fine-tuning strategies is shown in Figures 5(a) to 5(d), where the robustness \hat{\rho}_{\text{adv}} is estimated using DeepFool, since this is the most accurate method, as shown in Table 1. Observe that fine-tuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN’s robustness is increased by about 40%. On the other hand, quite surprisingly, the method in [4] can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial perturbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations.To verify this hypothesis, we compare in Figure 7 the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by \alpha=1,2,3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see Figure 8). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations.Table 3 lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in [4] has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on generalization.666While the authors of [4] reported an increased generalization performance on the MNIST task (from 0.94\% to 0.84\%) using adversarial regularization, it should be noted that the their experimental setup is significantly different as [4] trained the network based on a modified cost function, while we performed straightforward fine-tuning.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks. Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Adversarial perturbation is a small and unnoticeable change to the data that fool the given model (i. e give a different class after applying the perturbation).  It allows an understanding limits of existing architectures and calculation of the robustness of the models.	Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics [1, 16], speech [12, 6], and computer vision [10, 8]. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data [18]. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure 1). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation \bm{r} that is sufficient to change the estimated label \hat{k}(\bm{x}):\displaystyle\Delta(\bm{x};\hat{k}):=\min_{\bm{r}}\|\bm{r}\|_{2}\text{ subject to }\hat{k}(\bm{x}+\bm{r})\neq\hat{k}(\bm{x}),(1)where \bm{x} is an image and \hat{k}(\bm{x}) is the estimated label. We call \Delta(\bm{x};\hat{k}) the robustness of \hat{k} at point \bm{x}. The robustness of classifier \hat{k} is then defined as\rho_{\text{adv}}(\hat{k})=\mathbb{E}_{\bm{x}}\frac{\Delta(\bm{x};\hat{k})}{\|\bm{x}\|_{2}},(2)where \mathbb{E}_{\bm{x}} is the expectation over the distribution of data.The study of adversarial perturbations helps us understand what features are used by a classifier.The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples.Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models [18]. This can actually become a real concern from a security point of view.An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Adversarial perturbation is a small but very noticeable change to the data that fool the given model (i. e give a different class after applying the perturbation).  It allows an understanding limits of existing architectures and calculation of the robustness of the models.	Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics [1, 16], speech [12, 6], and computer vision [10, 8]. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data [18]. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., Figure 1). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation \bm{r} that is sufficient to change the estimated label \hat{k}(\bm{x}):\displaystyle\Delta(\bm{x};\hat{k}):=\min_{\bm{r}}\|\bm{r}\|_{2}\text{ subject to }\hat{k}(\bm{x}+\bm{r})\neq\hat{k}(\bm{x}),(1)where \bm{x} is an image and \hat{k}(\bm{x}) is the estimated label. We call \Delta(\bm{x};\hat{k}) the robustness of \hat{k} at point \bm{x}. The robustness of classifier \hat{k} is then defined as\rho_{\text{adv}}(\hat{k})=\mathbb{E}_{\bm{x}}\frac{\Delta(\bm{x};\hat{k})}{\|\bm{x}\|_{2}},(2)where \mathbb{E}_{\bm{x}} is the expectation over the distribution of data.The study of adversarial perturbations helps us understand what features are used by a classifier.The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples.Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models [18]. This can actually become a real concern from a security point of view.An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-of-the-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Incorrect
The metrics that are used to compare different methods of finding adversarial perturbations are: the average robustness of the model estimated in some type of norm (2-norm or infinity-norm in the paper).  and the average running time needed to find the estimated minimal perturbation.	In order to evaluate the robustness to adversarial perturbations of a classifier f, we compute the average robustness \hat{\rho}_{\text{adv}}(f), defined by\hat{\rho}_{\text{adv}}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{2}}{\|\bm{x}\|_{2}},(15)where \hat{\bm{r}}(\bm{x}) is the estimated minimal perturbation obtained using DeepFool, and \mathscr{D} denotes the test set444For ILSVRC2012, we used the validation data..We report in Table 1 the accuracy and average robustness \hat{\rho}_{\text{adv}} of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Only one metric is used to compare different methods of finding adversarial perturbations: the average robustness of the model estimated in some type of norm (2-norm or infinity-norm in the paper)	In order to evaluate the robustness to adversarial perturbations of a classifier f, we compute the average robustness \hat{\rho}_{\text{adv}}(f), defined by\hat{\rho}_{\text{adv}}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{2}}{\|\bm{x}\|_{2}},(15)where \hat{\bm{r}}(\bm{x}) is the estimated minimal perturbation obtained using DeepFool, and \mathscr{D} denotes the test set444For ILSVRC2012, we used the validation data..We report in Table 1 the accuracy and average robustness \hat{\rho}_{\text{adv}} of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in Figure 9, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it heavily depends on existing optimization methods.  In the paper, its effectiveness is proven relative to other state-of-the-art methods.	It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.In practice, the above algorithm can often converge to a point on the zero level set \mathscr{F}. In order to reach the other side of the classification boundary, the final perturbation vector \hat{\bm{r}} is multiplied by a constant 1+\eta, with \eta\ll 1. In our experiments, we have used \eta=0.02. It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors only claim that the DeepFool can be used as a baseline for adversarial perturbation calculation and that it doesn't depend on existing optimization methods at all.  In the paper, its effectiveness is proven relative to other state-of-the-art methods.	It can be seen that DeepFool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with [4]. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in [18]. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers.On the complexity aspect, the proposed approach is substantially faster than the standard method proposed in [18].In fact, while the approach [18] involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks.It should be noted that, when perturbations are measured using the \ell_{\infty} norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. Table 2 reports the \ell_{\infty} robustness to adversarial perturbations measured by \hat{\rho}_{\text{adv}}^{\infty}(f)=\frac{1}{|\mathscr{D}|}\sum_{\bm{x}\in\mathscr{D}}\frac{\|\hat{\bm{r}}(\bm{x})\|_{\infty}}{\|\bm{x}\|_{\infty}}, where \hat{\bm{r}}(\bm{x}) is computed respectively using DeepFool (with p=\infty, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.Our main contributions are the following:•We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.•We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.•We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in [18]. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in [18] is time-consuming and therefore does not scale to large datasets. In [14], the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. [19] provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. [13] generated synthetic unrecognizable images, which are classified with high confidence. The authors of [3] also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of [4] introduced the “fast gradient sign” method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, [5] introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in [18] was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in [2] that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.In practice, the above algorithm can often converge to a point on the zero level set \mathscr{F}. In order to reach the other side of the classification boundary, the final perturbation vector \hat{\bm{r}} is multiplied by a constant 1+\eta, with \eta\ll 1. In our experiments, we have used \eta=0.02. It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Correct	Correct
The DeepFool method is designed iteratively starting from very simple binary classifiers to more general non-linear differentiable classifiers.  The effectiveness of the greedy algorithm is justified by previous work and the results show very small perturbations, thus the authors claim that it is a viable method.	We then approximate, at iteration i, the distance between xi and the complement of P , dist(xi, P c), by dist(xi,  ̃P c i ). Specifically, at each iteration of the algorithm, the perturbation vector that reaches the boundary of the polyhedron  ̃Pi is computed, and the current estimate updated. The method is given in Algorithm 2. It should be noted that the proposed algorithm operates in a greedy way and is not guaranteed to converge to the optimal perturbation in (1). However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation.It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The DeepFool method is designed iteratively starting from more general non-linear differentiable classifiers to very simple binary classifiers.  The effectiveness of the greedy algorithm is justified by previous work and the results show very small perturbations, thus the authors claim that it is a viable method.	We then approximate, at iteration i, the distance between xi and the complement of P , dist(xi, P c), by dist(xi,  ̃P c i ). Specifically, at each iteration of the algorithm, the perturbation vector that reaches the boundary of the polyhedron  ̃Pi is computed, and the current estimate updated. The method is given in Algorithm 2. It should be noted that the proposed algorithm operates in a greedy way and is not guaranteed to converge to the optimal perturbation in (1). However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation.It should be noted that the optimization strategy of DeepFool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton’s iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case [15]. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in [21]. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Although early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test.   An AI-based application might pave the way to facilitate early disease detection.  Disease detection using machine learning approaches is gaining popularity in health informatics.  Therefore, AI-based approach for fast detection is a good way to make people go for the early diagnosis of the hair diseases.	Some scalp infections may be treatable if diagnosed early. Some but not all diseases may go on their own. Only an expert physician can detect the illness by visual observation. In some cases, early disease detection is beneficial for dermatologists to initiate the treatment. An early scalp inspection includes a dermatoscopic examination of the scalp for inflammation, itching, localized lesion, dandruff, follicular flakes, louse eggs (nits), and a scalp biopsy. Besides visual observation, the patient can undergo blood and hormone tests to detect the exact disease. Unfortunately, most hair and scalp diseases are diagnosed in advanced stages, which complicate the treatment options. All these factors lengthen the diagnosis and treatment process. Therefore, researchers are putting more effort into developing different mechanisms for the early detection of hair and scalp diseases.Disease detection using machine learning approaches is gaining popularity in health informatics. Many skin and scalp-related diseases can be detected using images of infected regions within a few seconds. In one study by Choudhary et al. [18], a framework is developed to differentiate alopecia areata from healthy hair. They obtained 200 healthy hair images from the figaro1K dataset and 68 alopecia areata hair images from DermNet. After a series of enhancement and segmentation, three key features wereAlthough early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test. An AI-based application might pave the way to facilitate early disease detection. In this study, we developed a machine learning model to accurately predict three hair and scalp-related diseases: alopecia, folliculitis, and psoriasis by feeding 150 preprocessed image data into a 2-D convolutional neural network model. After using 70% of the data to train the model, we analyzed remaining 30% of images for testing our model. After subsequent training, the model gave an overall 96.2% training accuracy on the training data and 91.1% validation accuracy for the test data, with a high precision and recall scores for each disease type. We have also provided our dataset with this study. Our proposed system would assist dermatologists and patients with a better understanding of disease classification and initiating early treatment options for the three most frequently occurred hair and scalp diseases.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Although early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test.   An AI-based application might pave the way to facilitate early disease detection.  Disease detection using device learning approaches is gaining popularity in health informatics.  Therefore, AI-based approach for fast detection is a good way to make people go for the early diagnosis of the hair diseases.	Some scalp infections may be treatable if diagnosed early. Some but not all diseases may go on their own. Only an expert physician can detect the illness by visual observation. In some cases, early disease detection is beneficial for dermatologists to initiate the treatment. An early scalp inspection includes a dermatoscopic examination of the scalp for inflammation, itching, localized lesion, dandruff, follicular flakes, louse eggs (nits), and a scalp biopsy. Besides visual observation, the patient can undergo blood and hormone tests to detect the exact disease. Unfortunately, most hair and scalp diseases are diagnosed in advanced stages, which complicate the treatment options. All these factors lengthen the diagnosis and treatment process. Therefore, researchers are putting more effort into developing different mechanisms for the early detection of hair and scalp diseases.Disease detection using machine learning approaches is gaining popularity in health informatics. Many skin and scalp-related diseases can be detected using images of infected regions within a few seconds. In one study by Choudhary et al. [18], a framework is developed to differentiate alopecia areata from healthy hair. They obtained 200 healthy hair images from the figaro1K dataset and 68 alopecia areata hair images from DermNet. After a series of enhancement and segmentation, three key features wereAlthough early-stage detection of hair and scalp-related diseases is the key to the treatment process, hair loss and scalp diseases can often go undetected due to a lack of awareness and a lengthy diagnosis test. An AI-based application might pave the way to facilitate early disease detection. In this study, we developed a machine learning model to accurately predict three hair and scalp-related diseases: alopecia, folliculitis, and psoriasis by feeding 150 preprocessed image data into a 2-D convolutional neural network model. After using 70% of the data to train the model, we analyzed remaining 30% of images for testing our model. After subsequent training, the model gave an overall 96.2% training accuracy on the training data and 91.1% validation accuracy for the test data, with a high precision and recall scores for each disease type. We have also provided our dataset with this study. Our proposed system would assist dermatologists and patients with a better understanding of disease classification and initiating early treatment options for the three most frequently occurred hair and scalp diseases.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Input image gets high contrast when pass through HE and hence loose information by adding noise.  Compare to that CLAHE is a adaptive histogram equalization method in which the contrast amplification is limited, so as to reduce this problem of noise amplification.	extracted from the images: texture, shape, and color. The researchers divided the dataset into 70%-30% train-test-split and applied a support vector machine (SNM) and k-nearest neighbor (KNN) for the classification task. Overall, they achieved 91.4% and 88.9% accuracy using SVM and KNN, respectively, with a 10-fold cross-validation approach. However, using other machine learning algorithms might increase in the accuracy rate, which should have been discussed. Besides, the application of Histogram Equalization (HE) for image enhancement complicated the process of getting accurate texture features from distorted images, as HE itself adds noise to the output image, distorting the signals. Moreover, this study only shed light on alopecia areata disease, ignoring the inter-class differences between other similar type diseases, which increased the likelihood of inaccurate prediction of other diseases as alopecia areata, thereby making this framework less reliable.Often the captured image doesn’t reflect the natural view and needs contrast enhancement to meet the level of realistic view [24]. Especially images with high color depth and after denoising effects need normalization for a better realistic view [25]. First, we applied histogram equalization (HE). However, the HE increases the contrast of the background when used in images with low color depth, and information is lost as the histogram is not confined to the local region. To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a histogram for each region. After clipping the histogram, we distributed the clipped value over the histogram equalization, which gives us control of the over-amplification of the contrast and generates the resultant image shown in Fig. 4.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
Information picture gets high contrast when pass through HE and hence loose information by adding noise.  Compare to that CLAHE is a adaptive histogram equalization method in which the contrast amplification is limited, so as to reduce this problem of noise amplification.	extracted from the images: texture, shape, and color. The researchers divided the dataset into 70%-30% train-test-split and applied a support vector machine (SNM) and k-nearest neighbor (KNN) for the classification task. Overall, they achieved 91.4% and 88.9% accuracy using SVM and KNN, respectively, with a 10-fold cross-validation approach. However, using other machine learning algorithms might increase in the accuracy rate, which should have been discussed. Besides, the application of Histogram Equalization (HE) for image enhancement complicated the process of getting accurate texture features from distorted images, as HE itself adds noise to the output image, distorting the signals. Moreover, this study only shed light on alopecia areata disease, ignoring the inter-class differences between other similar type diseases, which increased the likelihood of inaccurate prediction of other diseases as alopecia areata, thereby making this framework less reliable.Often the captured image doesn’t reflect the natural view and needs contrast enhancement to meet the level of realistic view [24]. Especially images with high color depth and after denoising effects need normalization for a better realistic view [25]. First, we applied histogram equalization (HE). However, the HE increases the contrast of the background when used in images with low color depth, and information is lost as the histogram is not confined to the local region. To overcome the problem, we applied CLAHE (Contrast Limited Adaptive Histogram Equalization) by dividing an image into equal size non-overlapping areas and computing a histogram for each region. After clipping the histogram, we distributed the clipped value over the histogram equalization, which gives us control of the over-amplification of the contrast and generates the resultant image shown in Fig. 4.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
Autokeras is best way to find model parameter.  It automatically tries different combination (in this case is 25) and find size of the model network.  In this case the best size is 3 hidden layer with 1 input and 1 output.	In this study, CNN is utilized for classification because it takes image’s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model’s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Autokeras is best way to find model parameter.  It automatically tries different combination (in this case is 25) and find size of the model network.  In this case the best size is 3 buried layer with 1 input and 1 output.	In this study, CNN is utilized for classification because it takes image’s raw pixel data, trains a model, and extracts the features automatically for better detection. We used autokeras to find the best model for this problem. After trying 25 different combinations, we selected 3 hidden layers with 1 input and 1 output layer as our final model which is shown in Fig. 5. For training the model, we used batch_size = 16 with 50 epochs for each batch. The preprocessed data is divided into 70-30 train-test-split for training and validation purpose. Our model consists of 256 inputs, 3 x 3 square kernel, 3 output units and a softmax output. We used ReLU as our activation function to prevent the exponential growth of required computation and to explore the non-linear relationship between input and output variables. After each convolutional layer, input goes through the pooling layer having 2 x 2 kernel size to reduce the dimensions of the features map. Pooling layer summarizes the presented features in a region and helps to prevent the over-fitting problem by down sampling. We also used dropout layer after each pooling layer to prevent neurons in a layer from synchronously optimizing their weights and converging to the same goal. Our model’s dropout rate is 0.3, which means 30% of the neurons of this layer will be randomly dropped in each epoch.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
K-Modes clustering is more accurate than using K-means clustering as the K-mode algorithm uses categorical data to form clusters.  Since the data in this paper is categorical, K-Modes is used.  This question is found directly in the paper.	The personality prediction is supported in this work by a questionnaire-based investigation. Openness to criticism, flexibility, team spirit, aspirations, and work ethics are among the traits that personality interview questions reveal. This aids to figure out how well a candidate may collaborate and work with team members. The responses to these queries give insight into the qualifications for the position The K- Modes clustering method is used in this survey-based investigation. The technique, which is simple to use and effective with vast amounts of data, is used to group categorical data. Based on the number of comparable categories between data points, clusters are defined. The k- modes clustering algorithm is an advancement over the k- means clustering method. K-means is the most widely used centre-based partitional clustering technique. Huang extends the k-means clustering method to the k-modes clustering algorithm to organize the categorical data:KModes clustering is one of the unsupervised Machine Learning algorithms that is used to cluster categorical variables. It is easy to implement and efficiently handles a large amount of data. A Kmodes technique uses a randomly selected starting cluster centre (modes) as a seed, whichCategorical data cannot be clustered using the K-means clustering method due to the different metrics it uses. The K- mode cluster algorithms are based on the K-mean pattern but eliminate the restriction on numerical data while maintaining their efficacy. By removing the restriction imposed by Kmeans after modification, this K-mode technique extends the K-mean pattern to cluster categorical data:The distance cannot be calculated for categorical data points, though [46]. KModes algorithm is what we choose. It makes use of the differences between the data points (total mismatches). Our data points are more comparable overall, the smaller the differences. Rather than using means, it employs modes [36].K-Modes clustering is more accurate than using K-means clustering as the K-mode algorithm uses categorical data to form clusters.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
K-Modes clustering is more accurate than using K-implies bunching as the K-mode algorithm uses categorical data to form clusters.  Since the data in this paper is categorical, K-Modes is used.  This question is found directly in the paper.	The personality prediction is supported in this work by a questionnaire-based investigation. Openness to criticism, flexibility, team spirit, aspirations, and work ethics are among the traits that personality interview questions reveal. This aids to figure out how well a candidate may collaborate and work with team members. The responses to these queries give insight into the qualifications for the position The K- Modes clustering method is used in this survey-based investigation. The technique, which is simple to use and effective with vast amounts of data, is used to group categorical data. Based on the number of comparable categories between data points, clusters are defined. The k- modes clustering algorithm is an advancement over the k- means clustering method. K-means is the most widely used centre-based partitional clustering technique. Huang extends the k-means clustering method to the k-modes clustering algorithm to organize the categorical data:KModes clustering is one of the unsupervised Machine Learning algorithms that is used to cluster categorical variables. It is easy to implement and efficiently handles a large amount of data. A Kmodes technique uses a randomly selected starting cluster centre (modes) as a seed, whichCategorical data cannot be clustered using the K-means clustering method due to the different metrics it uses. The K- mode cluster algorithms are based on the K-mean pattern but eliminate the restriction on numerical data while maintaining their efficacy. By removing the restriction imposed by Kmeans after modification, this K-mode technique extends the K-mean pattern to cluster categorical data:The distance cannot be calculated for categorical data points, though [46]. KModes algorithm is what we choose. It makes use of the differences between the data points (total mismatches). Our data points are more comparable overall, the smaller the differences. Rather than using means, it employs modes [36].K-Modes clustering is more accurate than using K-means clustering as the K-mode algorithm uses categorical data to form clusters.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
The work uses gradient descent to transform the white noise image to match the stylistic and content representations of an artwork and a photograph respectively.  However, they do not discuss the reasons behind choosing gradient descent over other methods and do not provide alternatives.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The work uses Mini-Batch gradient descent and Stochastic gradient descent to transform the white noise image to match the stylistic and content representations of an artwork and a photograph respectively.  However, they do not discuss the reasons behind choosing gradient descent over other methods and do not provide alternatives.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The authors use the white noise image as a starting point for the loss function to turn it into a combination of given images.  And the results suggest that it works well.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	Correct		Incorrect	Correct	Correct	Incorrect	Correct	Correct
The authors use the Gaussian noise image as a starting point for the loss function to turn it into a combination of given images.  And the results suggest that it works well.	creases with the position of the layer in the network. Hence a given input image (cid:126)x is encoded in each layer of the CNN by the ﬁlter responses to that image. A layer with N l distinct ﬁlters has N l feature maps each of size M l , where M l is the height times the width of the feature map. So the responses in a layer l can be stored in a matrix F l ∈ R N l × M l where F lij is the activation of the i th ﬁlter at position j in layer l . To visualise the image information that is encoded at different layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent on a white noise image to ﬁnd another image that matches the feature responses of the original image. So let (cid:126)p and (cid:126)x be the original image and the image that is generated and P l and F l their respective feature representation in layer l . We then deﬁne the squared-error loss between the two feature representationsTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to ﬁnd another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let (cid:126)a and (cid:126)x be the original image and the image that is generated and A l and G l their respective style representations in layer l . The contribution of that layer to the total loss is thenThe gradients of E l with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e). To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation 11	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors calculate the difference between the stylistic representations of two images as the weighted average of the mean-squared distance of respective Gram matrices at each layer.  Specifically, the distance between two Gram matrices of certain layer l is calculated as E_l = \frac{1}{4*N_l^2*M_l^2} * (G_l_i_j^2 * A_l_i_j^2).  In other words, it is the mean of squared distance in Euclidean space.	On top of the CNN responses in each layer of the network we built a style representation	Correct		Unrelated	Unrelated	Correct	Unrelated	Correct	Incorrect
The authors calculate the difference between the stylistic representations of two images as the weighted average of the mean-squared distance of respective Gram matrices at each layer.  Specifically, the distance between two Gram matrices of certain layer l is calculated as E_l = \frac{1}{4*N_l^2*M_l^2} * (G_l_i_j^2 * A_l_i_j^2).  In other words, it is the mean of squared distance in Manhattan space.	On top of the CNN responses in each layer of the network we built a style representation	Incorrect	Change concept	Unrelated	Unrelated	Correct	Unrelated	Incorrect	Incorrect
Since it is difficult to satisfy both content and stylistic constraints on the resulting image, the α and β weights in the loss function are used to manipulate the emphases on the content and stylistic representations respectively.  Several different ratios of α/β (10^-5, 10^-4, 10^-3, 10^-2) are explored to demonstrate the differences between synthesized images.  In general, it allowed smooth and continuous regulation of two separate terms of the loss function, thus producing more visually pleasing images.	including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.To generate the images that mix the content of a photograph with the style of a painting	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Since it is difficult to satisfy both content and stylistic constraints on the resulting image, the α and β weights in the loss function are used to manipulate the emphases on the content and stylistic representations respectively.  Several different ratios of α/β (10^-5, 10^-4, 10^-3, 10^-2) are explored to demonstrate the differences between synthesized images and source images.  In general, it allowed smooth and continuous regulation of two separate terms of the loss function, thus producing more visually pleasing images.	including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.To generate the images that mix the content of a photograph with the style of a painting	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The work only claims that the idea of separating the sources of variation in visual perception might be useful for a range of experiments from psychophysics to electrophysiological neural recordings.  It does not go into detail about examples of such experiments.	In general, our method of synthesising images that mix content and style from different	Correct		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Incorrect
The work not only claims that the idea of separating the sources of variation in visual perception might be useful for a range of experiments from psychophysics to electrophysiological neural recordings, but also go into detail about examples of such experiments.	In general, our method of synthesising images that mix content and style from different	Incorrect	Change concept	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Incorrect
The content representation of the photograph resembles the pixel-wise image more in the lower layers, but encodes the more high-level contents in the higher layers.	The class of Deep Neural Networks that are most powerful in image processing tasks are called Convolutional Neural Networks. Convolutional Neural Networks consist of layers of small computational units that process visual information hierarchically in a feed-forward man- ner (Fig 1). Each layer of units can be understood as a collection of image ﬁlters, each of which extracts a certain feature from the input image. Thus, the output of a given layer consists of so-called feature maps: differently ﬁltered versions of the input image. When Convolutional Neural Networks are trained on object recognition, they develop a representation of the image that makes object information increasingly explicit along the pro- cessing hierarchy. 8 Therefore, along the processing hierarchy of the network, the input image is transformed into representations that increasingly care about the actual content of the im- age compared to its detailed pixel values. We can directly visualise the information each layer contains about the input image by reconstructing the image only from the feature maps in that layer 9 (Fig 1, content reconstructions, see Methods for details on how to reconstruct the im- age). Higher layers in the network capture the high-level content in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruc- tion. (Fig 1, content reconstructions d,e). In contrast, reconstructions from the lower layers simply reproduce the exact pixel values of the original image (Fig 1, content reconstructions a,b,c). We therefore refer to the feature responses in higher layers of the network as the content representation . To obtain a representation of the style of an input image, we use a feature space originally designed to capture texture information. 8 This feature space is built on top of the ﬁlter responses in each layer of the network. It consists of the correlations between the different ﬁlter responsesthe images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ ( w l = 1 / 5 in those layers, w l = 0 in all other layers) . The ratio α/β was either 1 × 10 − 3 (Fig 2 B,C,D) or 1 × 10 − 4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor w l was always equal to one divided by the number of active layers with a non-zero loss-weight w l .including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The content representation of the photograph resembles the pixel-wise image more in the higher layers, but encodes the more high-level contents in the lower layers.	The class of Deep Neural Networks that are most powerful in image processing tasks are called Convolutional Neural Networks. Convolutional Neural Networks consist of layers of small computational units that process visual information hierarchically in a feed-forward man- ner (Fig 1). Each layer of units can be understood as a collection of image ﬁlters, each of which extracts a certain feature from the input image. Thus, the output of a given layer consists of so-called feature maps: differently ﬁltered versions of the input image. When Convolutional Neural Networks are trained on object recognition, they develop a representation of the image that makes object information increasingly explicit along the pro- cessing hierarchy. 8 Therefore, along the processing hierarchy of the network, the input image is transformed into representations that increasingly care about the actual content of the im- age compared to its detailed pixel values. We can directly visualise the information each layer contains about the input image by reconstructing the image only from the feature maps in that layer 9 (Fig 1, content reconstructions, see Methods for details on how to reconstruct the im- age). Higher layers in the network capture the high-level content in terms of objects and their arrangement in the input image but do not constrain the exact pixel values of the reconstruc- tion. (Fig 1, content reconstructions d,e). In contrast, reconstructions from the lower layers simply reproduce the exact pixel values of the original image (Fig 1, content reconstructions a,b,c). We therefore refer to the feature responses in higher layers of the network as the content representation . To obtain a representation of the style of an input image, we use a feature space originally designed to capture texture information. 8 This feature space is built on top of the ﬁlter responses in each layer of the network. It consists of the correlations between the different ﬁlter responsesthe images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ ( w l = 1 / 5 in those layers, w l = 0 in all other layers) . The ratio α/β was either 1 × 10 − 3 (Fig 2 B,C,D) or 1 × 10 − 4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor w l was always equal to one divided by the number of active layers with a non-zero loss-weight w l .including only a smaller number of lower layers, leading to different visual experiences (Fig 3, along the rows). When matching the style representations up to higher layers in the network, local images structures are matched on an increasingly large scale, leading to a smoother and more continuous visual experience. Thus, the visually most appealing images are usually cre- ated by matching the style representation up to the highest layers in the network (Fig 3, last row). Of course, image content and style cannot be completely disentangled. When synthesising an image that combines the content of one image with the style of another, there usually does not exist an image that perfectly matches both constraints at the same time. However, the loss function we minimise during image synthesis contains two terms for content and style respectively, that are well separated (see Methods). We can therefore smoothly regulate the emphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong emphasis on style will result in images that match the appearance of the artwork, effectively giving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst column). When placing strong emphasis on content, one can clearly identify the photograph, but the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of source images one can adjust the trade-off between content and style to create visually appealing images. Here we present an artiﬁcial neural system that achieves a separation of image content from style, thus allowing to recast the content of one image in the style of any other image. We demonstrate this by creating new, artistic images that combine the style of several well-known paintings with the content of an arbitrarily chosen photograph. In particular, we derive the neural representations for the content and style of an image from the feature responses of high- performing Deep Neural Networks trained on object recognition. To our knowledge this is the ﬁrst demonstration of image features separating content from style in whole natural images.	Incorrect	Change concept	Correct	Correct	Correct	Incorrect	Correct	Incorrect
The weights w_l can manipulate the emphases between stylistic representations obtained from different layers.  To make the roles of each stylistic representation of each layer equal, the w_l is always kept at one divided by the number of active layers.  They are constants in the loss function that are set before starting to optimize the loss function.	On top of the CNN responses in each layer of the network we built a style representation that computes the correlations between the different filter responses, where the expectation is taken over the spatial extend of the input image. These feature correlations are given by the Gram matrix Gl ∈ RNl×Nl , where Gl ij is the inner product between the vectorised feature map and j in layer l: Gl ij = ∑ k F l ikF l jk. (3) To generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to find another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let ~a and ~x be the original image and the image that is generated and Al and Gl their respective style representations in layer l. The contribution of that layer to the total loss is then El = 1 4N 2 l M 2 l ∑ i,j (Gl ij − Al ij )2 (4) and the total loss is Lstyle(~a, ~x) = L∑ l=0 wlEl (5) where wl are weighting factors of the contribution of each layer to the total loss (see below for specific values of wl in our results). The derivative of El with respect to the activations in layer l can be computed analytically: ∂El ∂F l ij = { 1 N 2 l M 2 l ((F l)T (Gl − Al)) ji if F l ij > 0 0 if F l ij < 0 . (6) The gradients of El with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The five style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e).To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation of the photograph in one layer of the network and the style representation of the painting in a number of layers of the CNN. So let ~p be the photograph and ~a be the artwork. The loss function we minimise is Ltotal(~p, ~a, ~x) = αLcontent(~p, ~x) + βLstyle(~a, ~x) (7) where α and β are the weighting factors for content and style reconstruction respectively. For the images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (wl = 1/5 in those layers, wl = 0 in all other layers) . The ratio α/β was either 1×10−3 (Fig 2 B,C,D) or 1 × 10−4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor wl was always equal to one divided by the number of active layers with a non-zero loss-weight wl.	Correct		Incorrect	Correct	Correct	Incorrect	Correct	Correct
The weights w_l can manipulate the emphases between stylistic representations obtained from different layers.  To make the roles of each stylistic representation of each layer equal, the w_l is always kept at one divided by the number of active layers.  They are constants in the loss function that are set before starting to optimize the misfortune work.	On top of the CNN responses in each layer of the network we built a style representation that computes the correlations between the different filter responses, where the expectation is taken over the spatial extend of the input image. These feature correlations are given by the Gram matrix Gl ∈ RNl×Nl , where Gl ij is the inner product between the vectorised feature map and j in layer l: Gl ij = ∑ k F l ikF l jk. (3) To generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to find another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated. So let ~a and ~x be the original image and the image that is generated and Al and Gl their respective style representations in layer l. The contribution of that layer to the total loss is then El = 1 4N 2 l M 2 l ∑ i,j (Gl ij − Al ij )2 (4) and the total loss is Lstyle(~a, ~x) = L∑ l=0 wlEl (5) where wl are weighting factors of the contribution of each layer to the total loss (see below for specific values of wl in our results). The derivative of El with respect to the activations in layer l can be computed analytically: ∂El ∂F l ij = { 1 N 2 l M 2 l ((F l)T (Gl − Al)) ji if F l ij > 0 0 if F l ij < 0 . (6) The gradients of El with respect to the activations in lower layers of the network can be readily computed using standard error back-propagation. The five style reconstructions in Fig 1 were generated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e).To generate the images that mix the content of a photograph with the style of a painting (Fig 2) we jointly minimise the distance of a white noise image from the content representation of the photograph in one layer of the network and the style representation of the painting in a number of layers of the CNN. So let ~p be the photograph and ~a be the artwork. The loss function we minimise is Ltotal(~p, ~a, ~x) = αLcontent(~p, ~x) + βLstyle(~a, ~x) (7) where α and β are the weighting factors for content and style reconstruction respectively. For the images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the style representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (wl = 1/5 in those layers, wl = 0 in all other layers) . The ratio α/β was either 1×10−3 (Fig 2 B,C,D) or 1 × 10−4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and style reconstruction loss (along the columns) and for matching the style representations only on layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (E). The factor wl was always equal to one divided by the number of active layers with a non-zero loss-weight wl.	Incorrect	Tortured phrases	Incorrect	Correct	Correct	Incorrect	Correct	Incorrect
SBM-Transformer allows more flexible attention mask structures between linear to full attention with respective computational costs, while Reformer can only use block-diagonal masks that cannot model hierarchical contexts.	Table 8 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking first in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all five tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we find that flexibility of SBM is indeed beneficial, as Reformer struggles in ListOps, most likely due to the inability of block-diagonal masks to model hierarchical contexts.To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [2], each	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
SBM-Transformer allows more flexible attention mask structures between linear to full attention with respective computational costs, while Reformer can only use block-diagonal masks that can only model hierarchical contexts.	Table 8 shows the test accuracies of each method. Our SBM-Transformer achieves the best overall performance, ranking first in two tasks, and second in one other. SBM-Transformer also outperforms full attention in all five tasks while computing 30% or less attention scores on average, which supports our claim that masked attention with partial attention score computations can be preferred over full attention depending on the task. With respect to the attention mask structure, we find that flexibility of SBM is indeed beneficial, as Reformer struggles in ListOps, most likely due to the inability of block-diagonal masks to model hierarchical contexts.To contribute to the efficient Transformers lineage, we propose SBM-Transformer, capable of adjusting its attention sparsity data-adaptively based without fully computing the attention score matrix (Figure 1). Leveraging a mixed-membership Stochastic Block Model (SBM) [2], each	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The "beginning" of the decoder input is a sum of the latent variable z and the key signature token c, which is concatenated over the sequence dimension.  The concatenated embeddings are not added to any embedding such as that for the <bos> token. 	The encoder used in VTHarm is identical to the encoder used in STHarm, except that the conditional token c is con- catenated at the beginning of the note-based melody embed-where V denotes VTHarm, Concat d denotes the concatena- tion over the feature dimension, Pool denotes the average pooling over time, and self-AttBlock denotes only one loop of the self-attention block. The context encoder maps the chord input y 1 : O into the embedding e (V) O . Then, c is concatenated at the beginning of e (V) O over the sequence dimension before the multihead self-attention blocks. The self-attention output contains the harmonic context according to the key informa- tion. It is mean-aggregated over time so that it represents the global information of the chords [26]. The encoder output E ( c , x 1 : T ) is also mean aggregated over time to represent the global attribute of a melody. These two aggregated vectors are concatenated over the feature dimension and pass through the bottleneck, resulting in two parameters, µ , and σ . The latent code z is inferred from µ and σ through the reparam- eterization trick, and its prior is assumed to be the normal distribution [19].The right-shifted chord input is ﬁrst encoded with the same lookup table from the context encoder. The latent variable z and the key signature token c are added to the beginning, which corresponds to the ‘‘start-of-sequence’’ part of the chord embedding. The following attention network transfers the aggregated information from z and c to all frames of the embedding. The rest of the Transformer decoder reconstructs the target chords.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The "beginning" of the decoder input is a concatenation of the latent variable z and the key signature token c, which is summed over the sequence dimension. The concatenated embeddings are also added to the embedding for the <bos> token.	The encoder used in VTHarm is identical to the encoder used in STHarm, except that the conditional token c is con- catenated at the beginning of the note-based melody embed-where V denotes VTHarm, Concat d denotes the concatena- tion over the feature dimension, Pool denotes the average pooling over time, and self-AttBlock denotes only one loop of the self-attention block. The context encoder maps the chord input y 1 : O into the embedding e (V) O . Then, c is concatenated at the beginning of e (V) O over the sequence dimension before the multihead self-attention blocks. The self-attention output contains the harmonic context according to the key informa- tion. It is mean-aggregated over time so that it represents the global information of the chords [26]. The encoder output E ( c , x 1 : T ) is also mean aggregated over time to represent the global attribute of a melody. These two aggregated vectors are concatenated over the feature dimension and pass through the bottleneck, resulting in two parameters, µ , and σ . The latent code z is inferred from µ and σ through the reparam- eterization trick, and its prior is assumed to be the normal distribution [19].The right-shifted chord input is ﬁrst encoded with the same lookup table from the context encoder. The latent variable z and the key signature token c are added to the beginning, which corresponds to the ‘‘start-of-sequence’’ part of the chord embedding. The following attention network transfers the aggregated information from z and c to all frames of the embedding. The rest of the Transformer decoder reconstructs the target chords.	Incorrect	Opposite	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The method for exploring the effect of melody awareness can be one that deeply investigates how the awareness of the melody can affect the unexpectedness of the controlled chords and how this unexpectedness affects the perceived complexity and preference of the chords.	When the melody is unaware, BLSTM and rVTHarm obtain signiﬁcantly lower Preference scores than when the melody is aware ( p < 0 . 001). We further compute Pearson’s correlation coefﬁcient of U with C or P scores, as shown in Table 6. As a result, rVTHarm reveals the most nega- tive correlation of U with both C and P scores when the melody is aware. This indicates that 1) controlled chords are more unexpected and unpleasant with a familiar melody, and 2) some factors other than complexity seem to cause an increased unexpectedness in rVTHarm. However, the mean preference score of rVTHarm signiﬁcantly increases with melody awareness. This implies that the familiarity of the melody may strongly compensate for the high unexpect- edness of rVTHarm. This tendency needs further investi- gation to improve the robustness of controllable melody harmonization.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The method for exploring the effect of melody awareness can be one that deeply investigates how the awareness of the melody can affect the unexpectedness of the controlled chords and how this unexpectedness affects the perceived complexity and preference of the musical rhythm.	When the melody is unaware, BLSTM and rVTHarm obtain signiﬁcantly lower Preference scores than when the melody is aware ( p < 0 . 001). We further compute Pearson’s correlation coefﬁcient of U with C or P scores, as shown in Table 6. As a result, rVTHarm reveals the most nega- tive correlation of U with both C and P scores when the melody is aware. This indicates that 1) controlled chords are more unexpected and unpleasant with a familiar melody, and 2) some factors other than complexity seem to cause an increased unexpectedness in rVTHarm. However, the mean preference score of rVTHarm signiﬁcantly increases with melody awareness. This implies that the familiarity of the melody may strongly compensate for the high unexpect- edness of rVTHarm. This tendency needs further investi- gation to improve the robustness of controllable melody harmonization.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
An example of "explicit planning" would be the plan or strategy of abruptly increasing dynamics for performing a climax within the music to highlight a certain emotion such as anger. 	However, these studies have constrained musical creativity. Maezawa et al. controlled musical expression only through quantized features from the musical scores. Tan et al. did not consider controlling tempo or timing with a latent representation. These methods may have restricted any potential for rendering piano performances with flexible musical expression. Musical creativity can be expanded not only by composers but also by performers who can elastically choose various strategies to highlight multiple nuances or emotions [13, 14, 15]. Moreover, the music generation field can be also broadened if static music created by automatic composition systems can be easily colored with realistic and elastic expression [16].	Correct		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
An example of "explicit planning" would be the plan or strategy of abruptly increasing tempo for performing a climax within the music to highlight a certain emotion such as anger.	However, these studies have constrained musical creativity. Maezawa et al. controlled musical expression only through quantized features from the musical scores. Tan et al. did not consider controlling tempo or timing with a latent representation. These methods may have restricted any potential for rendering piano performances with flexible musical expression. Musical creativity can be expanded not only by composers but also by performers who can elastically choose various strategies to highlight multiple nuances or emotions [13, 14, 15]. Moreover, the music generation field can be also broadened if static music created by automatic composition systems can be easily colored with realistic and elastic expression [16].	Incorrect	Change concept	Unrelated	Unrelated	Correct	Unrelated	Incorrect	Incorrect
IsTopVoice is different from PositionInChord in that an index 1 of IsTopVoice represents the uppermost voice while that of PositionInChord represents the lowermost voice.  They are also different that IsTopVoice is binary while PositionInChord is multi-class.	Score Features. The features for a musical score represent eight categorical attributes for how the notes are composed:Pitch is a MIDI index number that ranges from 21 to 108.RelDuration and RelIOI are 11-class attributes of a quantized duration and IOI between a note onset and a previous chord, respectively. They range from 1 to 11, and each class represents a multiple of a 16th note’s length with respect to a given tempo [30, 31].IsTopVoice is a binary attribute of whether the note is the uppermost voice. It is heuristically computed regarding pitches and durations of surrounding notes.PositionInChord and NumInChord are 11-class attributes of a positional index of a note within its chord and the total number of notes in that chord, respectively, that range from 1 to 11. An index 1 for PositionInChord denotes the most bottom position.Staff is a binary attribute of the staff of a note, either of the G clef or F clef.IsDownbeat is a binary attribute of whether a note is at a downbeat or not.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
PositionInChord is different from IsTopVoice in that an index 1 of PositionInChord represents the uppermost voice while that of IsTopVoice represents the lowermost voice.  They are also different that PositionInChord is binary while IsTopVoice is multi-class.	Score Features. The features for a musical score represent eight categorical attributes for how the notes are composed:Pitch is a MIDI index number that ranges from 21 to 108.RelDuration and RelIOI are 11-class attributes of a quantized duration and IOI between a note onset and a previous chord, respectively. They range from 1 to 11, and each class represents a multiple of a 16th note’s length with respect to a given tempo [30, 31].IsTopVoice is a binary attribute of whether the note is the uppermost voice. It is heuristically computed regarding pitches and durations of surrounding notes.PositionInChord and NumInChord are 11-class attributes of a positional index of a note within its chord and the total number of notes in that chord, respectively, that range from 1 to 11. An index 1 for PositionInChord denotes the most bottom position.Staff is a binary attribute of the staff of a note, either of the G clef or F clef.IsDownbeat is a binary attribute of whether a note is at a downbeat or not.	Incorrect	Opposite	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors intend a "chord" to represent simultaneous notes to intuitively models a polyphonic structure of piano performance that is defined by its temporal progression.  More fine-grained resolution than the beat-based resolution can reflect trivial changes in expression that varies by simultaneous note groups, such as a syncopation. 	We employ a self-supervised learning framework to force the latent representations to learn our target attributes [25, 26, 24].In addition, we facilitate independent control of the three expressive attributes–dynamics, articulation, and tempo–by utilizing an existing method that aligns the latent code with a target attribute [27, 28]. Finally, we design a novel mechanism that intuitively models a polyphonic structure of piano performance. In particular, we insert intermediate steps for chordwise encoding and decoding of the piano performance to our encoder-decoder architecture, where a chord denotes a group of simultaneous notes.Our approach has several contributions as follows:1) Our system aims to control musical expression while maintaining any characteristics induced by a given musical structure;2) We use self-supervised learning where new supervisory signals are involved in regularizing the latent representations effectively;3) Our system aims to control multiple expressive attributes independently of each other;4) Lastly, we leverage an intermediate step that projects a notewise representation into the chordwise in the middle of our system to intuitively model the polyphonic structure of piano performance.Inspired by previous studies [4, 8, 9, 32], we build a twostep encoder and decoder: An encoder models both notewise and chordwise dependencies of the inputs, and a decoder reconstructs the notewise dependency from the chordwise representation and the notewise condition. We denote a chord as a group of notes that are hit simultaneously, regardless of the staff, so that they sound together at an instant time [33]. Thus, learning the chordwise dependency is analogous to direct modeling of the temporal progression of the piano performance. Let M 2 RC N be a matrix that aligns serialized notes to their polyphonic structure, where C and N are the number of chords and the number of notes, respectively. Within the encoder, the	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors intend a "chord" to represent a more meaningful unit in music, such as a beat.	We employ a self-supervised learning framework to force the latent representations to learn our target attributes [25, 26, 24].In addition, we facilitate independent control of the three expressive attributes–dynamics, articulation, and tempo–by utilizing an existing method that aligns the latent code with a target attribute [27, 28]. Finally, we design a novel mechanism that intuitively models a polyphonic structure of piano performance. In particular, we insert intermediate steps for chordwise encoding and decoding of the piano performance to our encoder-decoder architecture, where a chord denotes a group of simultaneous notes.Our approach has several contributions as follows:1) Our system aims to control musical expression while maintaining any characteristics induced by a given musical structure;2) We use self-supervised learning where new supervisory signals are involved in regularizing the latent representations effectively;3) Our system aims to control multiple expressive attributes independently of each other;4) Lastly, we leverage an intermediate step that projects a notewise representation into the chordwise in the middle of our system to intuitively model the polyphonic structure of piano performance.Inspired by previous studies [4, 8, 9, 32], we build a twostep encoder and decoder: An encoder models both notewise and chordwise dependencies of the inputs, and a decoder reconstructs the notewise dependency from the chordwise representation and the notewise condition. We denote a chord as a group of notes that are hit simultaneously, regardless of the staff, so that they sound together at an instant time [33]. Thus, learning the chordwise dependency is analogous to direct modeling of the temporal progression of the piano performance. Let M 2 RC N be a matrix that aligns serialized notes to their polyphonic structure, where C and N are the number of chords and the number of notes, respectively. Within the encoder, the	Incorrect	Opposite	Incorrect	Correct	Unrelated	Unrelated	Incorrect	Incorrect
The latent variable for explicit planning has no temporal dependency.  The latent variable is derived from the standard normal distribution without the dependency on the score features.	Inference. A probabilistic encoder parameterized by \phi approximates the posterior distibutions of the latent representations z^{(\text{pln})} and z^{(\text{str})} from the performance input x and conditional score input y:\displaystyle q_{\phi}(z^{(\text{pln})},z^{(\text{str})}|x,y)=\displaystyle q_{\phi}(z^{(\text{pln})}|x^{(\text{chd})})(3)\displaystyle\prod_{c=1}^{C}q_{\phi}(z^{(\text{str})}_{c}|x^{(\text{chd})}_{\leq c},y^{(\text{chd})}_{\leq c})where x^{(\text{chd})}=\text{N2C}(e_{x}) is the chordwise embedding, and e_{x} is the notewise embedding for x. The posterior distributions of z^{(\text{pln})}_{c} and z^{(\text{str})}_{c} are approximated by distribution parameters encoded by f^{(\text{pln})}(x^{(\text{chd})}) and f^{(\text{str})}(x^{(\text{chd})},y^{(\text{chd})}), where f^{(\text{pln})} and f^{(\text{str})} are bidirectional and unidirectional recurrent neural networks, respectively.We note that z^{(\text{pln})} is independent of the score features y. This allows a flexible transfer of the explicit planning among other musical pieces. On the other hand, z^{(\text{str})} is constrained by y since the structural attributes are dependent on the note structure.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.where y(chd) = N2C(ey) is the chordwise embedding, and ey is the notewise embedding for y. We assume that the prior of z(pln) c is a standard normal distribution. In contrast, z(str) c is sampled from a sequential prior [24, 36, 37], conditioned on both previous latent variables and chordwise score features: z(str) c   N( (prior); diag( (prior)2), where [ (prior);  (prior)] = f(prior)(z(str)	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The latent variable for explicit planning has a strong temporal dependency. The latent variable is derived from the standard normal distribution with a direct dependency on the score features.	Inference. A probabilistic encoder parameterized by \phi approximates the posterior distibutions of the latent representations z^{(\text{pln})} and z^{(\text{str})} from the performance input x and conditional score input y:\displaystyle q_{\phi}(z^{(\text{pln})},z^{(\text{str})}|x,y)=\displaystyle q_{\phi}(z^{(\text{pln})}|x^{(\text{chd})})(3)\displaystyle\prod_{c=1}^{C}q_{\phi}(z^{(\text{str})}_{c}|x^{(\text{chd})}_{\leq c},y^{(\text{chd})}_{\leq c})where x^{(\text{chd})}=\text{N2C}(e_{x}) is the chordwise embedding, and e_{x} is the notewise embedding for x. The posterior distributions of z^{(\text{pln})}_{c} and z^{(\text{str})}_{c} are approximated by distribution parameters encoded by f^{(\text{pln})}(x^{(\text{chd})}) and f^{(\text{str})}(x^{(\text{chd})},y^{(\text{chd})}), where f^{(\text{pln})} and f^{(\text{str})} are bidirectional and unidirectional recurrent neural networks, respectively.We note that z^{(\text{pln})} is independent of the score features y. This allows a flexible transfer of the explicit planning among other musical pieces. On the other hand, z^{(\text{str})} is constrained by y since the structural attributes are dependent on the note structure.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.where y(chd) = N2C(ey) is the chordwise embedding, and ey is the notewise embedding for y. We assume that the prior of z(pln) c is a standard normal distribution. In contrast, z(str) c is sampled from a sequential prior [24, 36, 37], conditioned on both previous latent variables and chordwise score features: z(str) c   N( (prior); diag( (prior)2), where [ (prior);  (prior)] = f(prior)(z(str)	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors use only one composer, Chopin, rather than several composers together because Chopin's music has been one of the most common resources that are analyzed by literature to investigate the development in Western musical expression with respect to various musical structures.  In other words, modeling music only from Chopin is assumed to be enough for learning Western musical expression derived from various musical patterns.	We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [22, 41, 6, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers who represent various eras of Western music. For subjective evaluation, we collect 42 songs of non-Classical songs from online source222http://www.ambrosepianotabs.com/page/library which are less constrained to written expression than most Classical excerpts.	Correct		Incorrect	Correct	Incorrect	Incorrect	Incorrect	Incorrect
The authors avoid using only one composer like Chopin, and instead use several composers together because a single composer's work is too limited to be a common resource for analyzing the development in Western musical expression. In other words, modeling music only from Chopin is assumed to be insufficient for learning Western musical expression derived from various musical patterns.	We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [22, 41, 6, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers who represent various eras of Western music. For subjective evaluation, we collect 42 songs of non-Classical songs from online source222http://www.ambrosepianotabs.com/page/library which are less constrained to written expression than most Classical excerpts.	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors did not use the previous studies as the baseline models since the proposed work attempts a new approach that disregards a typical assumption from the previous studies.  There has been no identical assumption in the previous studies that musical expression can vary regardless of the written expression provided by the composers.	Therefore, we attempt a new approach that renders piano performances with flexible musical expressions. We disregard a typical assumption from previous studies that a performer must follow a composer’s intent [4, 17–19]. According to the literature, performers learn to identify or imitate "expressive models", or explicit planning, of existing piano performances [20]. We focus on this attribute, defining it as a higher-level sketch of the expressive attributes (i.e. dynamics, articulation, and tempo [21]) that the performer draws based on a personal interpretation of the musical piece [4, 11, 20]. We also assume that the remaining attribute represents common performing strategies that are connected to certain musical patterns, while these strategies slightly differ across performers [22, 23]. We call this attribute as a structural attribute that belongs to given note structures of a musical piece.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors did not use the previous studies as the baseline models since the proposed work attempts a new approach that disregards a typical assumption from the previous studies. There has been no identical assumption in the previous studies that musical expression can vary regardless of the performer's interpretation.	Therefore, we attempt a new approach that renders piano performances with flexible musical expressions. We disregard a typical assumption from previous studies that a performer must follow a composer’s intent [4, 17–19]. According to the literature, performers learn to identify or imitate "expressive models", or explicit planning, of existing piano performances [20]. We focus on this attribute, defining it as a higher-level sketch of the expressive attributes (i.e. dynamics, articulation, and tempo [21]) that the performer draws based on a personal interpretation of the musical piece [4, 11, 20]. We also assume that the remaining attribute represents common performing strategies that are connected to certain musical patterns, while these strategies slightly differ across performers [22, 23]. We call this attribute as a structural attribute that belongs to given note structures of a musical piece.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The reconstruction metric that measures the performance for predicting the structure attribute is calculated from zero explicit planning.  The reason is that using a flat expression derived by the zero explicit planning can let the generated structural attribute be solely exposed, not mixed with any musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The reconstruction metric that measures the performance for predicting the dynamics attribute is calculated from zero explicit planning. The reason is that using a flat expression derived by the zero explicit planning can let the generated dynamics attribute be solely exposed, not mixed with any musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors use the randomly sampled z(str) to measure explicit planning as they aim to disentangle explicit planning from any structural attribute.  They also use z(pln) from zero explicit planning to measure the structural attributes since a flat expression can expose any structural attribute that is not mixed with arbitrary musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors use the z(pln) from zero explicit planning to measure explicit planning as they aim to disentangle explicit planning from any structural attribute.  They also use randomly sampled z(str) to measure the structural attributes since a flat expression can expose any structural attribute that is not mixed with arbitrary musical expression.	We verify whether the latent representations are well-disentangled by appropriate information[24]. To this end, each model infers the latent representations z^{(\text{pln})} and z^{(\text{str})} from the test sets. Each model also randomly samples \tilde{z}^{(\text{str})} and infers z^{(\text{pln})}_{0}\sim q_{\phi}(z^{(\text{pln})}|x_{0}). We use z^{(\text{pln})}_{0} to measure the structural attribute, since z^{(\text{pln})}_{0} represents a flat expression where the structural attribute can be solely exposed. Each model generates new outputs as x^{(\text{pln})}\sim p_{\theta}(x^{(\text{pln})}|z^{(\text{pln})},\tilde{z}^{(\text{str})},y) and x^{(\text{str})}\sim p_{\theta}(x^{(\text{str})}|z^{(\text{pln})}_{0},z^{(\text{str})},y). Then, we compute a new signal \tilde{I}^{(\text{pln})} from x^{(\text{pln})} using the polynomial regression. The MSE values are calculated as \text{MSE}_{\text{p}}=\text{MSE}(\tilde{I}^{(\text{pln})},I^{(\text{pln})}) and \text{MSE}_{\text{s}}=\text{MSE}(x^{(\text{str})},k-I^{(\text{pln})}).	Incorrect	Opposite	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
The authors didn't try the listening test for the samples from non-zero, realistic explicit planning due to the following reason.  Such realistic explicit planning should be inserted by the user, or inferred from the posterior distribution with respect to the ground truth data, maybe Classical music with various musical expressions, but the existing expressions can be already constrained by the written guidelines.  The written expression can be a strong bias to the listeners so that the new expression against the original expression can be perceived as awkward regardless of how natural the expression itself is.	We conduct a listening test to compare the proposed model architecture to Notewise and CVAE. We qualitatively evaluate the base quality of the samples that have flat expressions, so that quality judgments are independent of any preference of arbitrary explicit planning. We generate each sample using z^{(\text{pln})}_{0}. A listening test is composed of 30 trials where each participant chooses a more "human-like" sample out of the generated sample and its plain MIDI [9]. Both samples have the same length which is a maximum of 15 seconds, rendered with TiMidity++333https://sourceforge.net/projects/timidity/ without any pedal effect. Human-likeness denotes how similar the sample is to an actual piano performance that commonly appears in popular music. A total of 28 participants are involved, and 6 participants are professionally trained in music.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [6, 22, 41, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors conducted a listening test for the samples from non-zero, realistic explicit planning due to the following reason. Such realistic explicit planning should be inserted by the user, or inferred from the posterior distribution with respect to the ground truth data, maybe Classical music with various musical expressions, but the existing expressions can be freely interpreted beyond the written guidelines. The written expression can be a weak suggestion to the listeners so that the new expression against the original expression can be perceived as natural and innovative if the expression itself is natural.	We conduct a listening test to compare the proposed model architecture to Notewise and CVAE. We qualitatively evaluate the base quality of the samples that have flat expressions, so that quality judgments are independent of any preference of arbitrary explicit planning. We generate each sample using z^{(\text{pln})}_{0}. A listening test is composed of 30 trials where each participant chooses a more "human-like" sample out of the generated sample and its plain MIDI [9]. Both samples have the same length which is a maximum of 15 seconds, rendered with TiMidity++333https://sourceforge.net/projects/timidity/ without any pedal effect. Human-likeness denotes how similar the sample is to an actual piano performance that commonly appears in popular music. A total of 28 participants are involved, and 6 participants are professionally trained in music.Our system can render new piano performances from the scratch given a musical score. It can directly generate expressive parameters from the randomly sampled \tilde{z}^{(\text{pln})}\sim p_{\theta}(z^{(\text{pln})}) and \tilde{z}^{(\text{str})}\sim p_{\theta}(z^{(\text{str})}). We note that \tilde{z}^{(\text{pln})} does not have temporal dependency: each \tilde{z}^{(\text{pln})}_{c} is sampled independently of \tilde{z}^{(\text{pln})}_{c-1}. Hence, we need to insert specific values \{\alpha^{(c)}\}_{c=1}^{C}, which we call as "smooth sketches", into the target dimensions of z^{(\text{pln})} if any temporal dependency of explicit planning is necessary. Figure 2 shows that the controlled parameters are greatly correlated with \alpha, while their local characteristics follow those of the ground truth. In addition, the black and orange lines together demonstrate granular variety in the parameters induced by different \tilde{z}^{(\text{str})} for the same musical structure. Moreover, Figure 3 shows that our system can estimate explicit planning from arbitrary human performances, indicating that our system can derive relevant information on explicit planning from the unseen data.We use Yamaha e-Competition Dataset [8] and Vienna 4x22 Piano Corpus [40]. From these datasets, we collect 356 performances of 34 pieces by Frédéric Chopin, which have been representative research subjects for analyzing the Western musical expression [6, 22, 41, 42]. We use 30 pieces (108,738 batches) for training and the rest for testing. To verify the generality of model performances, we also collect the external dataset from ASAP dataset [43]. We use 116 performances for 23 pieces by 10 composers	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
There were great succeed in computer vision and speech tasks.  However, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks,thanks to improvements in optimization technology, larger datasets and streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Correct		Incorrect	Correct	Correct	Correct	Correct	Correct
There were great succeed in laptop vision and speech tasks.  However, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Supervised end-to-end learning has been extremely successful in computer vision, speech, or machine translation tasks,thanks to improvements in optimization technology, larger datasets and streamlined designs of deep convolutional or recurrent architectures. Despite these successes, this learning setup does not cover many aspects where learning is nonetheless possible and desirable.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
The purpose of dataset is a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithms and create good benchmark.	This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70,000 grayscale images in the size of 28\times 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. (2013); Ciregan et al. (2012).	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The purpose of dataset is a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking AI calculation and create good benchmark.	This paper introduced Fashion-MNIST, a fashion product images dataset intended to be a drop-in replacement of MNIST and whilst providing a more challenging alternative for benchmarking machine learning algorithm. The images in Fashion-MNIST are converted to a format that matches that of the MNIST dataset, making it immediately compatible with any machine learning package capable of working with the original MNIST dataset.Our aim with this work is to create a good benchmark dataset which has all the accessibility of MNIST, namely its small size, straightforward encoding and permissive license. We took the approach of sticking to the 10 classes 70,000 grayscale images in the size of 28\times 28 as in the original MNIST. In fact, the only change one needs to use this dataset is to change the URL from where the MNIST dataset is fetched. Moreover, Fashion-MNIST poses a more challenging classification task than the simple MNIST digits data, whereas the latter has been trained to accuracies above 99.7% as reported in Wan et al. (2013); Ciregan et al. (2012).	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Instance segmentation is a new type of computer vision task that aims to solve the problem of how to represent all objects in an image.  It combines elements from the classical computer vision tasks of object detection and localizing each object using a bounding box, and semantic segmentation.	Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection. Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Instance segmentation is a new type of pc imaginative and presicent task that aims to solve the problem of how to represent all objects in an image.  It combines elements from the classical computer vision tasks of object detection and localizing each object using a bounding box, and semantic segmentation.	Instance segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.111Following common terminology, we use object detection to denote detection via bounding boxes, not masks, and semantic segmentation to denote per-pixel classification without differentiating instances. Yet we note that instance segmentation is both semantic and a form of detection. Given this, one might expect a complex method is required to achieve good results. However, we show that a surprisingly simple, flexible, and fast system can surpass prior state-of-the-art instance segmentation results.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
PromptSource and Super-NaturalInstructions (also called "super-NI" in short) are two existing public datasets that contain instructions for tuning large language models. 	The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource (Bach et al., 2022) and Super-NaturalInstructions (Wang et al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T0 Bach et al. (2022); Sanh et al. (2022) and Tk-Instruct Wang et al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
PromptSource and Super-NaturalInstructions (also called "super-NI" in short) are two existing public datasets that contain instructions for tuning enormous language models. 	The recent NLP literature has witnessed a tremendous amount of activity in building models that can follow natural language instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022; Chung et al., 2022, i.a.).These developments are powered by two key components: large pre-trained language models (LM) and human-written instruction data.PromptSource (Bach et al., 2022) and Super-NaturalInstructions (Wang et al., 2022) are two notable recent datasets that use extensive manual annotation for collecting instructions to construct T0 Bach et al. (2022); Sanh et al. (2022) and Tk-Instruct Wang et al. (2022).However, this process is costly and often suffers limited diversity given that most human generations tend to be popular NLP tasks, falling short of covering a true variety of tasks and different ways to describe them.Given these limitations, continuing to improve the quality of instruction-tuned modelsnecessitates the development of alternative approaches for supervising instruction-tuned models.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	Incorrect	Tortured phrases	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
This paper only mentions and discusses the SuperNaturalInstructions (or SuperNI) dataset, and does not explicitly discuss the NaturalInstructions dataset.	Despite the comprehensiveness of SuperNI in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. Table 4 presents a small portion of the 252 tasks. The whole test set will be available upon request.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	Correct		Incorrect	Unrelated	Correct	Unrelated	Incorrect	Incorrect
This paper mentions and discusses the SuperNaturalInstructions (or SuperNI) dataset, and also  explicitly discuss the NaturalInstructions dataset.	Despite the comprehensiveness of SuperNI in collecting existing NLP tasks, most of these NLP tasks were proposed for research purposes and skewed toward classification. To better access the practical value of instruction-following models, a subset of the authors curate a new set of instructions motivated by user-oriented applications.We first brainstorm different domains where large LMs may be useful (e.g., email writing, social media, productivity tools, entertainment, programming), then craft instructions related to each domain along with an input-output instance (again, input is optional). We aim to diversify the styles and formats of these tasks (e.g., instructions may be long or short; input/output may take the form of bullet points, tables, codes, equations, etc.). In total, we create 252 instructions with 1 instance per instruction. We believe it can serve as a testbed for evaluating how instruction-based models handle diverse and unfamiliar instructions. Table 4 presents a small portion of the 252 tasks. The whole test set will be available upon request.A series of works have found evidence that vanilla language models can be effective at following general language instructions if tuned with annotated “instructional” data – datasets containing language instructional commands and their desired outcome based on human judgement Weller et al. (2020); Mishra et al. (2022); Wang et al. (2022); Wei et al. (2022); Sanh et al. (2022); Ouyang et al. (2022); Parmar et al. (2022); Scialom et al. (2022); Chung et al. (2022); Luo et al. (2022); Puri et al. (2022); Yin et al. (2022); Chakrabarty et al. (2022); Lin et al. (2022); Gupta et al. (2022); Muennighoff et al. (2022).Additionally, they show a direct correlation between the size and diversity of the “instructional” data and the generalizability of resulting models to unseen tasks.Since these developments depend on human annotated “instructional” data, this poses a bottleneck for progress toward more generalizable models(for example see Fig. 5a in Wang et al., 2022).Our work aims to tackle this bottleneck by reducing the dependence on human annotators.	Incorrect	Change concept	Incorrect	Unrelated	Correct	Incorrect	Incorrect	Incorrect
The deep learning model does feature extraction by processing the image through many layers and giving an encoding of the face that can be used to solve different FR tasks.  The early layers of a deep learning model tend to represent simple textures that continuously evolve into facial structures in the later layers.	For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a “zero-shot” learning task. Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest. Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set. For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 10^{6}-10^{7} IDs [38, 20].But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning [22]. Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2. It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience. The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes. In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye. In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction. Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of “one-to-many augmentation” to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.	Correct		Incorrect	Unrelated	Correct	Incorrect	Incorrect	Correct
The profound learning model does feature extraction by processing the image through many layers and giving an encoding of the face that can be used to solve different FR tasks.  The early layers of a deep learning model tend to represent simple textures that continuously evolve into facial structures in the later layers.	For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a “zero-shot” learning task. Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest. Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set. For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 10^{6}-10^{7} IDs [38, 20].But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning [22]. Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation. They learn multiple levels of representations that correspond to different levels of abstraction. The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2. It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience. The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes. In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye. In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction. Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.3D FR has inherent advantages over 2D methods, but 3D deep FR is not well developed due to the lack of large annotated 3D data. To enlarge 3D training datasets, most works use the methods of “one-to-many augmentation” to synthesize 3D faces. However, the effective methods for extracting deep features of 3D faces remain to be explored. Kim et al. [204] fine-tuned a 2D CNN with a small amount of 3D scans for 3D FR. Zulqarnain et al. [259] used a three-channel (corresponding to depth, azimuth and elevation angles of the normal vector) image as input and minimized the average prediction log-loss. Zhang et al. [260] first selected 30 feature points from the Candide-3 face model to characterize faces, then conducted the unsupervised pretraining of face depth data, and finally performed the supervised fine-tuning.	Incorrect	Tortured phrases	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
Deep Neural Networks such as CNNs are much better option than the handcrafted features for computer vision problem such as segmentation.	Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Profound Neural Networks such as CNNs are much better option than the handcrafted features for computer vision problem such as segmentation.	Recent research in computer vision and pattern recognition has highlighted the capabilities of Convolutional Neural Networks (CNNs) to solve challenging tasks such as classification, segmentation and object detection, achieving state-of-the-art performances.This success has been attributed to the ability of CNNs to learn a hierarchical representation of raw input data, without relying on handcrafted features.As the inputs are processed through the network layers, the level of abstraction of the resulting features increases.Shallower layers grasp local information while deeper layers use filters whose receptive fields are much broader that therefore capture global information [19].CNNs have been recently used for medical image segmentation.Early approaches obtain anatomy delineation in images or volumes by performing patch-wise image classification. Such segmentations are obtained by only considering local context and therefore are prone to failure, especially in challenging modalities such as ultrasound, where a high number of mis-classified voxel are to be expected.Post-processing approaches such as connected components analysis normally yield no improvement and therefore, more recent works, propose to use the network predictions in combination with Markov random fields [6], voting strategies [9] or more traditional approaches such as level-sets [2].Patch-wise approaches also suffer from efficiency issues. When densely extracted patches are processed in a CNN, a high number of computations is redundant and therefore the total algorithm runtime is high. In this case, more efficient computational schemes can be adopted.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
There are no pixels without any content because for the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image.	One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
There are no pixels without any content because for the pixels in the border region of the image, the missing context is extrapolated by mirroring the info picture.	One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
Dot product attention is calculated using optimized matrix multiplication  operations whereas Additive attention is computed by compatibility function using a feed-forward network with a single hidden layer.  Multiplicative Attention is much faster and more space-efficient than the additive attention.	The two most commonly used attention functions are additive attention (bahdanau2014neural, ), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \frac{1}{\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of d_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_{k} (DBLP:journals/corr/BritzGLL17, ). We suspect that for large values of d_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q\cdot k=\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 0 and variance d_{k}.. To counteract this effect, we scale the dot products by \frac{1}{\sqrt{d_{k}}}.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
Dot product attention is calculated using optimized matrix multiplication  operations whereas Additive attention is computed by compatibility function using a feed-forward network with a single unseen layer.  Multiplicative Attention is much faster and more space-efficient than the additive attention.	The two most commonly used attention functions are additive attention (bahdanau2014neural, ), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \frac{1}{\sqrt{d_{k}}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.While for small values of d_{k} the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_{k} (DBLP:journals/corr/BritzGLL17, ). We suspect that for large values of d_{k}, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q\cdot k=\sum_{i=1}^{d_{k}}q_{i}k_{i}, has mean 0 and variance d_{k}.. To counteract this effect, we scale the dot products by \frac{1}{\sqrt{d_{k}}}.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
The authors cite the increases in the number of well-constructed large-scale datasets and the usage of CNNs as the main reasons for the progress in the field of image recognition.  Well-constructed large-scale datasets facilitate better learning, and the CNN architecture is appropriate for learning informative features from images.	Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
The authors cite the increases in the number of well-constructed large-scale datasets and the usage of CNNs as the main reasons for the progress in the field of photo reputation.  Well-constructed large-scale datasets facilitate better learning, and the CNN architecture is appropriate for learning informative features from images.	Tremendous progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet [1, 2]) and the recent revival of deep convolutional neural networks (CNN) [3, 4].For data-driven learning, large-scale well-annotated datasets with representative data distribution characteristics are crucial to learning more accurate or generalizable models [5, 4].Unlike previous image datasets used in computer vision, ImageNet [1] offers a very comprehensive database of more than 1.2 million categorized natural images of 1000+ classes.The CNN models trained upon this database serve as the backbone for significantly improving many object detection and image segmentation problems using other datasets [6, 7], e.g., PASCAL [8] and medical image categorization [9, 10, 11, 12].However, there exists no large-scale annotated medical image dataset comparable to ImageNet, as data acquisition is difficult, and quality annotation is costly.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
There are three main techniques that are used to apply CNNs to tasks involving medical images: 1) training from scratch, 2) using pre-skilled CNNs as feature extractors, then using those features with hand-crafted features, and 3) performing unsupervised pre-training then using CNN for fine-tuning.	There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the “CNN from scratch” [13, 14, 15, 16, 17]; 2) using “off-the-shelf CNN” features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for Chest X-rays [10] and CT lung nodule identification [9, 12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18, 19, 20, 21].A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the “curse-of-dimensionality” issue in [22], in order to acquire a sufficient number of training image samples.Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10, 9, 12].They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.Until the detection aggregation approach [22], [41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83% recall on 3FP per patient [22], using a “Cifar-10” CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
There are three main techniques that are used to apply CNNs to tasks involving medical images: 1) training from scratch, 2) using pre-skilled CNNs as feature extractors, then using those features with hand-crafted features, and 3) performing unsupervised pre-training then using CNN for fine-tuning.	There are currently three major techniques that successfully employ CNNs to medical image classification: 1) training the “CNN from scratch” [13, 14, 15, 16, 17]; 2) using “off-the-shelf CNN” features (without retraining the CNN) as complementary information channels to existing hand-crafted image features, for Chest X-rays [10] and CT lung nodule identification [9, 12]; and 3) performing unsupervised pre-training on natural or medical images and fine-tuning on medical target images using CNN or other types of deep learning models [18, 19, 20, 21].A decompositional 2.5D view resampling and an aggregation of random view classification scores are used to eliminate the “curse-of-dimensionality” issue in [22], in order to acquire a sufficient number of training image samples.Although natural images and medical images differ significantly, conventional image descriptors developed for object recognition in natural images, such as the scale-invariant feature transform (SIFT) [30] and the histogram of oriented gradients (HOG) [31], have been widely used for object detection and segmentation in medical image analysis. Recently, ImageNet pre-trained CNNs have been used for chest pathology identification and detection in X-ray and CT modalities [10, 9, 12].They have yielded the best performance results by integrating low-level image features (e.g., GIST [32], bag of visual words (BoVW) and bag-of-frequency [12]). However, the fine-tuning of an ImageNet pre-trained CNN model on medical image datasets has not yet been exploited.Until the detection aggregation approach [22], [41], thoracoabdominal lymph node (LN) detection via CADe mechanisms has yielded poor performance results. In [22], each 3D LN candidate produces up to 100 random 2.5D orthogonally sampled images or views which are then used to train an effective CNN model. The best performance on abdominal LN detection is achieved at 83% recall on 3FP per patient [22], using a “Cifar-10” CNN. Using the thoracoabdominal LN detection datasets [22], we aim to surpass this CADe performance level, by testing different CNN architectures, exploring various dataset re-sampling protocols, and applying transfer learning from ImageNet pre-trained CNN models.	Incorrect	Tortured phrases	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The goal behind transforming the axial, coronal and sagittal representations to RGB is to help the learning process of transfer learning models that were pre-trained on ImageNet.	Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products.Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
The goal behind transforming the axial, coronal and sagittal representations to RGB is to help the learning process of transfer learning models that were pre-educated on ImageNet.	Unlike the heart or the liver, lymph nodes have no pre-determined anatomic orientation. Hence, the purely random image resampling (with respect to scale, displacement and orientation) and reformatting (the axial, coronal, and sagittal views are in any system randomly resampled coordinates) is a natural choice, which also happens to yield high CNN performance. Although we integrate three channels of information from three orthogonal views for LN detection, the pixel-wise spatial correlations between or among channels are not necessary. The convolutional kernels in the lower level CNN architectures can learn the optimal weights to linearly combine the observations from the axial, coronal, and sagittal channels by computing their dot-products.Transforming axial, coronal, and sagittal representations to RGB also facilitates transfer learning from CNN models trained on ImageNet.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
A process consisting of deconvolution, back-engendering with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images.	The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figure 2 (b,c), are shown in Figure 13 (a,b). The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6x6) reception fields in AlexNet [4]. Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes. Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Figure 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figure 2 (b) and (c)) better than AlexNet-ImNet. The filters shown in Figure 13 that better localize regions on the input images (Figure 2 (b) and (c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
A process consisting of deconvolution, back-propagation with convolution, and un-pooling from the activation maps of the pooling units was used to reconstruct the original ILD images.	The last pooling layer (pool-5) activation maps of the ImageNet pre-trained AlexNet [4] (analogical to AlexNet-ImNet) and AlexNet-TL, obtained by processing two input images of Figure 2 (b,c), are shown in Figure 13 (a,b). The last pooling layer activation map summarizes the entire input image by highlighting which relative locations or neural reception fields relative to the image are activated. There are a total of 256 (6x6) reception fields in AlexNet [4]. Pooling units where the relative image location of the disease region is present in the image are highlighted with green boxes. Next, we reconstruct the original ILD images using the process of de-convolution, back-propagating with convolution and un-pooling from the activation maps of the chosen pooling units [72]. From the reconstructed images (Figure 13 bottom), we observe that with fine-tuning, AlexNet-TL detects and localizes objects of interest (ILD disease regions depicted in in Figure 2 (b) and (c)) better than AlexNet-ImNet. The filters shown in Figure 13 that better localize regions on the input images (Figure 2 (b) and (c)) respectively, produce relatively higher activations (in the top 5%) among all 512 reception field responses in the fine-tuned AlexNet-TL model. As observed in [73], the final CNN classification score can not be driven solely by a single strong activation in the receptions fields, but often by a sparse set of high activations (i.e., varying selective or sparse activations per input image).	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Correct	Correct
Single image super-resolution is the task of generating a high-resolution image from a low-resolution one.	We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Single image super-resolution is the task of generating a high-goal image from a low-resolution one.	We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
Backpropagation through time (BPTT) is used to train language models to enable gradient propagation for large input sequences.  This work proposes BPTT for text classification (BPT3C) where a document is first divided into fixed-length batches.  At the beginning of each batch, the model is initialized with the final state of the previous batch and gradients are back-propagated to the batches whose hidden states contributed to the final prediction.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Feedback Processing through time (BPTT) is used to train language models to enable gradient propagation for large input sequences.  This work proposes BPTT for text classification (BPT3C) where a document is first divided into fixed-length batches.  At the beginning of each batch, the model is initialized with the final state of the previous batch and gradients are back-propagated to the batches whose hidden states contributed to the final prediction.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
This work uses Backpropagation Through Time for Text Classification (BPT3C) where a document is divided into fixed-length batches and the gradients are back-propagated to batches.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	Correct		Correct	Correct	Correct	Correct	Correct	Correct
This work uses Back spread Through Time for Text Classification (BPT3C) where a document is divided into fixed-length batches and the gradients are back-propagated to batches.	Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences Merity et al. (2017a).	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM’s fixed and limited context size by taking into account the entire context of every token via a token-level attention mechanism.	The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document.We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models’ performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g. ‘killed’\rightarrow ‘deceased’, and co-reference or anaphora resolution, e.g. ‘ent119 was killed’ \rightarrow ‘he was identified.’ However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The main difference between the attention-based LSTM and the vanilla one is that the former addresses the limitation of vanilla LSTM’s fixed and limited context size by taking into account the entire context of every token via a token-level consideration system.	The Attentive Reader can be viewed as a generalisation of the application ofMemory Networks to question answering [3]. That model employsan attention mechanism at the sentence level where each sentence is representedby a bag of embeddings. The Attentive Reader employs a finer grained tokenlevel attention mechanism where the tokens are embedded given their entirefuture and past context in the input document.We can visualise the attention mechanism as a heatmap over a context document togain further insight into the models’ performance. The highlighted words showwhich tokens in the document were attended to by the model. In addition we mustalso take into account that the vectors at each token integrate long rangecontextual information via the bidirectional LSTM encoders.Figure 3 depicts heat maps for two queries that were correctlyanswered by the Attentive Reader.777Note that these examples were chosenas they were short, the average CNN validation document contained 763 tokens and27 entities, thus most instances were significantly harder to answer than theseexamples. In both cases confidently arriving at the correct answer requires themodel to perform both significant lexical generalsiation, e.g. ‘killed’\rightarrow ‘deceased’, and co-reference or anaphora resolution, e.g. ‘ent119 was killed’ \rightarrow ‘he was identified.’ However it is alsoclear that the model is able to integrate these signals with rough heuristicindicators such as the proximity of query words to the candidate answer.	Incorrect	Tortured phrases	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
The authors investigated 1) number of LSTM blocks per hidden layer 2) learning rate 3) momentum 4) standard deviation of Gaussian input noise with random searches with uniform sampling.	We performed 27 random searches (one for each combination of the nine variants and three datasets).Each random search encompasses 200 trials for a total of 5400 trials of randomly sampling the following hyperparameters:•number of LSTM blocks per hidden layer:log-uniform samples from [20,200];•learning rate:log-uniform samples from [10^{-6},10^{-2}];•momentum:1-\text{log-uniform samples from $[0.01,1.0]$};•standard deviation of Gaussian input noise:uniform samples from [0,1].	Correct		Correct	Correct	Correct	Correct	Incorrect	Incorrect
The authors investigated 1) number of LSTM blocks per hidden layer 2) learning rate 3) momentum 4) standard deviation of Gaussian input commotion with random searches with uniform sampling.	We performed 27 random searches (one for each combination of the nine variants and three datasets).Each random search encompasses 200 trials for a total of 5400 trials of randomly sampling the following hyperparameters:•number of LSTM blocks per hidden layer:log-uniform samples from [20,200];•learning rate:log-uniform samples from [10^{-6},10^{-2}];•momentum:1-\text{log-uniform samples from $[0.01,1.0]$};•standard deviation of Gaussian input noise:uniform samples from [0,1].	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Incorrect
To ensure the fair comparison that needs to be similar for each variant, the authors tuned the hyperparameters individually for each variant, and use random search to 1) obtain good hyperparameters and 2) collect enough amount of samples for analyzing the general effect of each variant.	For fair comparison, the setup needs to be similar for each variant.Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant.For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant.Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters [18] for every combination of variant and dataset.Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section V-B).	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
To ensure the fair comparison that needs to be similar for each variant, the authors didn't tune the hyperparameters individually for each variant, but used random search to 1) obtain good hyperparameters and 2) collect enough amount of samples for analyzing the general effect of each variant.	For fair comparison, the setup needs to be similar for each variant.Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant.For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant.Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters [18] for every combination of variant and dataset.Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section V-B).	Incorrect	Opposite	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The authors use different ratios of test-train-validation split for each dataset.  Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the three datasets (TIMIT Speech corpus, IAM Online Handwriting Database, and JSB Chorales dataset) used in the experiment.  Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset.  (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192).	The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696, 400, and 192 sequences, having 304 frames on average.The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.2 From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.The IAM Online Handwriting Database [38] consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see Figure 2(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5355, 2956 and 3859 sequences respectively.JSB Chorales: JSB Chorales is a collection of 382 four part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger, Lewandowski et al. [41]. 5 These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The authors use different ratios of test-train-validation split for each dataset.  Speficially, the authors did not use the predefined ratio value when splitting the data into train-validation-test sets for the five datasets (TIMIT Speech corpus, IAM Online Handwriting Database, IMDB movie-review dataset, MNLI dataset and JSB Chorales dataset) used in the experiment.  Instead, they used the predefined data split for IAM Online Handwriting Database and JSB Chorales dataset.  (5355:3859:2956 and 229:77:76) They also followed Halberstadt [37] in splitting the TIMIT dataset (3696:400:192).	The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt [37] into 3696, 400, and 192 sequences, having 304 frames on average.The TIMIT Speech corpus [26] is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones.2 From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) [35] + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. [20, 36]). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.The IAM Online Handwriting Database [38] consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see Figure 2(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5355, 2956 and 3859 sequences respectively.JSB Chorales: JSB Chorales is a collection of 382 four part harmonized chorales by J. S. Bach [40], consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger, Lewandowski et al. [41]. 5 These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61.	Incorrect	Invent something didn't mentioned	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
fANOVA marginalize over hyperparameter dimensions using regression trees to predict the marginal error for single parameter while averaging over all other parameters.	The fANOVA framework for assessing hyperparameter importance by Hutter et al. [19] is based on the observation that marginalizing over dimensions can be done efficiently in regression trees.This allows predicting the marginal error for one hyperparameter while averaging over all the others.Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
fANOVA marginalize over hyperparameter dimensions using regression trees to predict the marginal error for multi parameters while averaging over all other parameters.	The fANOVA framework for assessing hyperparameter importance by Hutter et al. [19] is based on the observation that marginalizing over dimensions can be done efficiently in regression trees.This allows predicting the marginal error for one hyperparameter while averaging over all the others.Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
The authors define an “ensemble of models” as a set of separate models with the same architecture and training procedure, but different randomly initialized parameters whose predictions are then averaged to increase performance.	We trained 10 separate models to predict P(h_{t}|\mathbf{s}_{t};\boldsymbol{\theta}), using exactly the same architecture and trainingprocedure as the baseline. The models are randomly initialized with different initial parameter values and we find thatthis creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble tosignificantly outperform the individual models. We have explored adding diversity to the models by varying the sets ofdata that each model sees, but we found this to not significantly change our results, so we opted for the simplerapproach. For the distillation we tried temperatures of [1,{\bf 2},5,10] and used a relative weight of 0.5 on thecross-entropy for the hard targets, where bold font indicates the best value that was used fortable 1 .	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
The authors define an “ensemble of models” as a set of separate models with different architecture and training procedure, and different randomly initialized parameters whose predictions are then averaged to increase performance.	We trained 10 separate models to predict P(h_{t}|\mathbf{s}_{t};\boldsymbol{\theta}), using exactly the same architecture and trainingprocedure as the baseline. The models are randomly initialized with different initial parameter values and we find thatthis creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble tosignificantly outperform the individual models. We have explored adding diversity to the models by varying the sets ofdata that each model sees, but we found this to not significantly change our results, so we opted for the simplerapproach. For the distillation we tried temperatures of [1,{\bf 2},5,10] and used a relative weight of 0.5 on thecross-entropy for the hard targets, where bold font indicates the best value that was used fortable 1 .	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Correct	Incorrect
Temperature is a value used in the softmax output layer.  The softmax layer converts the logit computed for each class into a probability by comparing with other logits and increasing the temperature produces a softer probability distribution over classes.  Specifically, in distillation, a high temperature is used in the cumbersome model to produce a soft target distribution for each case in the transfer set.	In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer setand using a soft target distribution for each case in the transfer set that is produced by using the cumbersome modelwith a high temperature in its softmax. The same high temperature is used when training the distilled model, but afterit has been trained it uses a temperature of 1.Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi , computed for each class into a probability, qi , by comparing zi with the other logits. qi = exp(zi/T ) P j exp(zj/T ) (1) 2 where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Temperature is a value used in the softmax yield layer.  The softmax layer converts the logit computed for each class into a probability by comparing with other logits and increasing the temperature produces a softer probability distribution over classes.  Specifically, in distillation, a high temperature is used in the cumbersome model to produce a soft target distribution for each case in the transfer set.	In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer setand using a soft target distribution for each case in the transfer set that is produced by using the cumbersome modelwith a high temperature in its softmax. The same high temperature is used when training the distilled model, but afterit has been trained it uses a temperature of 1.Neural networks typically produce class probabilities by using a “softmax” output layer that converts the logit, zi , computed for each class into a probability, qi , by comparing zi with the other logits. qi = exp(zi/T ) P j exp(zj/T ) (1) 2 where T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over classes.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
For speech recognition, the architecture was based on the acoustic model used by Android voice search.  For MNIST, the architecture was strongly regularized using dropout and weight constraints as described in prior work.	To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations.We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
For speech acknowledgement, the architecture was based on the acoustic model used by Android voice search.  For MNIST, the architecture was strongly regularized using dropout and weight constraints as described in prior work.	To see how well distillation works, we trained a single large neuralnet with two hidden layers of 1200 rectified linear hidden units onall 60,000 training cases. The net was strongly regularized usingdropout and weight-constraints as described in [5]. Dropoutcan be viewed as a way of training an exponentially large ensemble ofmodels that share weights. In addition, the input images were jitteredby up to two pixels in any direction. This net achieved 67 testerrors whereas a smaller net with two hidden layers of 800 rectifiedlinear hidden units and no regularization achieved 146 errors. But ifthe smaller net was regularized solely by adding the additional taskof matching the soft targets produced by the large net at atemperature of 20, it achieved 74 test errors. This shows that softtargets can transfer a great deal of knowledge to the distilled model,including the knowledge about how to generalize that is learned fromtranslated training data even though the transfer set does not containany translations.We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	Incorrect	Tortured phrases	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
P0: According to the authors, they used about 2000 hours of spoken English data which yielded about 700M training examples.  Approximately, the length of the average training sample would be 0. 01 seconds.	We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
P0: According to the authors, they used about 2000 hours of spoken English data which yielded about 700M training examples.  Approximately, the length of the average training sample would be 0. 01 seconds.	We use an architecture with 8 hidden layers each containing 2560 rectified linear units and a final softmax layer with14,000 labels (HMM targets h_{t}). The input is 26 frames of 40 Mel-scaled filterbank coefficients with a 10ms advanceper frame and we predict the HMM state of 21{}^{st} frame. The total number of parameters is about 85M. This is aslightly outdated version of the acoustic model used by Android voice search, and should be considered as a very strongbaseline. To train the DNN acoustic model we use about 2000 hours of spoken English data, which yields about 700Mtraining examples. This system achieves a frame accuracy of 58.9%, and a Word Error Rate (WER) of 10.9% on our development set.	Incorrect	Change number	Correct	Correct	Correct	Incorrect	Incorrect	Correct
In training of Google’s baseline model of JFT, asynchronous stochastic gradient descent (SGD) involved running replicas of the neural net different sets of cores to compute gradients on given mini-batches, which are then sent to a shared parameter server which returns new values for the parameters.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
In training of Google’s baseline model of JFT, asynchronous stochastic slope plummet involved running replicas of the neural net different sets of cores to compute gradients on given mini-batches, which are then sent to a shared parameter server which returns new values for the parameters.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.	Incorrect	Tortured phrases	Correct	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The K-means algorithm clusters the set of classes that the models often predict together.  In this work, this clustering approach did not require true labels.  However, the models themselves were trained using examples from a dataset, JFT, which contains labeled images.  Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes S^{m} that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results.To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The K-means algorithm clusters the set of classes that the models often predict together.  In this work, this clustering approach did not require true labels.  However, the models themselves were trained using examples from a dataset, JFT, which contains only synthesized images.  Thus, although the K-means algorithm does not require a labeled dataset, the models whose predictions are used in the algorithm required a labeled dataset.	JFT is an internal Google dataset that has 100 million labeled imageswith 15,000 labels. When we did this work, Google’s baseline model forJFT was a deep convolutional neural network [7] that had been trained forabout six months using asynchronous stochastic gradient descent on alarge number of cores. This training used two types ofparallelism [2]. First, there were many replicas of the neural net runningon different sets of cores and processing different mini-batches fromthe training set. Each replica computes the average gradient on itscurrent mini-batch and sends this gradient to a sharded parameter server whichsends back new values for the parameters. These new values reflect allof the gradients received by the parameter server since the last timeit sent parameters to the replica. Second, each replica is spread overmultiple cores by putting different subsets of the neurons on eachcore. Ensemble training is yet a third type of parallelism that can bewrapped around the other two types, but only if a lot more cores areavailable. Waiting for several years to train an ensemble of models wasnot an option, so we needed a much faster way to improve the baselinemodel.In order to derive groupings of object categories for the specialists, we decided to focus on categories that our fullnetwork often confuses. Even though we could have computed the confusion matrix and used it as a way to find suchclusters, we opted for a simpler approach that does not require the true labels to construct the clusters.In particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so thata set of classes S^{m} that are often predicted together will be used as targets for one of our specialist models, m. We applied an on-lineversion of the K-means algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown inTable 2). We tried several clustering algorithms which produced similar results.To reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. These weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. After training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled.	Incorrect	Change concept	Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
ARSG is the sequence generator based on the RNN network, which utilizes the attention mechanism.	An attention-based recurrent sequence generator (ARSG) is a recurrent neuralnetwork that stochastically generates an output sequence (y_{1},\dots,y_{T})from an input x. In practice, x is often processed by an encoderwhich outputs a sequential input representation h=(h_{1},\ldots,h_{L}) moresuitable for the attention mechanism to work with.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
ARSG is the sequence generator based on the RNN network, which utilizes the attention mechanism.	An attention-based recurrent sequence generator (ARSG) is a recurrent neuralnetwork that stochastically generates an output sequence (y_{1},\dots,y_{T})from an input x. In practice, x is often processed by an encoderwhich outputs a sequential input representation h=(h_{1},\ldots,h_{L}) moresuitable for the attention mechanism to work with.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
BERTSum is a specialized variant of BERT on the task of extractive summarization, picking out the sentences from a text to constitute its summary.	We adopt the popular BERTSum (Liu and Lapata, 2019) for extractive summarization. With the contextualized representation from BERT, the model uses a binary classifier to predict whether each sentence belongs in the summary. We train the model on the CNN/DM dataset (See et al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric.Extractive summarization is the taskof picking the most representative sentences as the summary for the given document(s). Current state-of-the-art models, which are mostly based on large-scale pretrained language models Liu and Lapata (2019); Zhong et al. (2020); Jia et al. (2020); Ruan et al. (2022), can deliver good performance, but why and how such models work so well still remain an open question. In our case study, we adoptthe popular BERTSum(Liu and Lapata, 2019).	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
BERTSum is a specialized variant of GPT on the task of extractive summarization, picking out the sentences from a text to constitute its summary.	We adopt the popular BERTSum (Liu and Lapata, 2019) for extractive summarization. With the contextualized representation from BERT, the model uses a binary classifier to predict whether each sentence belongs in the summary. We train the model on the CNN/DM dataset (See et al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric.Extractive summarization is the taskof picking the most representative sentences as the summary for the given document(s). Current state-of-the-art models, which are mostly based on large-scale pretrained language models Liu and Lapata (2019); Zhong et al. (2020); Jia et al. (2020); Ruan et al. (2022), can deliver good performance, but why and how such models work so well still remain an open question. In our case study, we adoptthe popular BERTSum(Liu and Lapata, 2019).	Incorrect	Change concept	Correct	Incorrect	Correct	Correct	Correct	Incorrect
Interpretability is measured with the PDR framework.  Summarization performance measured in ROUGE is 15% better.  Topic segmentation performance measured in F1 is 12% better.	In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019).	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Interpretability is measured with the PDR framework.  Summarization performance measured in ROUGE is 20% better.  Topic segmentation performance measured in F1 is 22% better.	In both scenarios, we argue the interpretability of the resulting model is improved. We provide a justification of our claim based on the Predictive, Descriptive, and Relevant (PDR) framework proposed by Murdoch et al. (2019). Specifically, by injecting human-interpretable patterns into the model, we increase the model’s descriptive accuracy by explicitly encoding useful relationships between input tokens in the attention weights while simultaneously improving the predictive accuracy in task performance. Further, the patterns are relevant for the problem since they are discovered in the human-in-the-loop process and are verified to be important for the task.As shown in Table 1, our pattern-infused models outperform the plain transformer models for both the CNN/DM and NYT-50 datasets under all three settings(6 Layer 8 Heads, 6 Layer 12 Heads, and 6 Layer 12 Heads with BERT embeddings).Similarly for topic segmentation,results also show that the pattern-injection approach substantially outperforms the vanilla transformer across all metrics.It is worthemphasizing that the performance gain is slightly higher for summarization models. When normalized by the ROUGE scores of extractive oracle summaries555As reported by Liu and Lapata (2019), the ROUGE scores (R-1/R-2/R-L) of the oracle upper bound for CNN/DM and NYT-50 are respectively, 52.59/31.24/48.87 and 49.18/33.24/46.02., the pattern-infused summarization models achieve an average 15\% improvement over the baselines, while the topic-segmentation models achieve a 12\% improvement over the baselines.In-line with prior work (McCoy et al., 2020), we also find that the performance is consistent across random seeds, where we report an extremely low standard deviation of0.03 (ROUGE) and 0.002 (F1) for extractive summarization and topic segmentation, respectively. Overall, the results from our experiments convincingly demonstrates the benefits of our approach and the generalizability of the patterns discovered by our pipeline.In summary, the key aim of our experiments was to verify consistent improvements over our own baselines under the same settings in order to probethe benefits (effectiveness and efficiency) of the discovered patterns for the task. Therefore, we do not perform extensive tuning to achieve the same results reported by Liu and Lapata (2019).	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Author claims that Transformer only using absolute positional encoding often generates dissimilar representations for nodes with similar local structures.   It shows the relaxed structural inductive bias.  The reason is that structural similarity between nodes is not contained  in absolute positional encoding.	We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:	Correct		Correct	Correct	Correct	Correct	Correct	Correct
Author claims that Transformer only using absolute positional encoding often generates similar representations for nodes with similar local structures.   It shows the relaxed structural inductive bias.  The reason is that structural similarity between nodes is not contained  in absolute positional encoding.	We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem:	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
There are two most widely adopted limitations of GNNs : over-smoothing and over-squashing. Over-smoothing is a phenomenon that indicates representations of GNNs get similar to each others as the number of layers increases. Over-squashing is a difficulty of node representations to contain messages that come from distant neighbors.	While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs (Xu et al., 2019; Morris et al., 2019), as well as known problems such as over-smoothing (Li et al., 2018, 2019; Chen et al., 2020; Oono & Suzuki, 2020) and over-squashing (Alon & Yahav, 2021).Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain “bottlenecks” in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN (Kipf & Welling, 2017), which was based on performing convolutions on the graph. Gilmer et al. (2017) reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples (Hamilton et al., 2017; Xu et al., 2019; Corso et al., 2020; Hu et al., 2020b; Veličković et al., 2018; Li et al., 2020a; Yang et al., 2022). However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.	Correct		Correct	Correct	Correct	Incorrect	Correct	Correct
There are three most widely adopted limitations of GNNs : over-smoothing, over-fitting and over-squashing. Over-smoothing is a phenomenon that indicates representations of GNNs get similar to each others as the number of layers increases. Over-squashing is a difficulty of node representations to contain messages that come from distant neighbors.	While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs (Xu et al., 2019; Morris et al., 2019), as well as known problems such as over-smoothing (Li et al., 2018, 2019; Chen et al., 2020; Oono & Suzuki, 2020) and over-squashing (Alon & Yahav, 2021).Over-smoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain “bottlenecks” in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.Message passing graph neural networks have recently been one of the leading methods for graph representation learning. An early seminal example is the GCN (Kipf & Welling, 2017), which was based on performing convolutions on the graph. Gilmer et al. (2017) reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples (Hamilton et al., 2017; Xu et al., 2019; Corso et al., 2020; Hu et al., 2020b; Veličković et al., 2018; Li et al., 2020a; Yang et al., 2022). However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.	Incorrect	Invent something didn't mentioned	Correct	Correct	Correct	Incorrect	Incorrect	Correct
The advantage of relative encoding compared to absolute encoding is the flexibility of using representations of position or distances into the self-attention mechanism directly. The reason is that self-attentions using absolute encoding only use node features, but self-attention mechanisms with relative encoding are able to utilize representations.	While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer (Dwivedi & Bresson, 2021) provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN (Kreuzer et al., 2021) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding (Shaw et al., 2018) in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. Mialon et al. (2021) propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations (Ying et al., 2021) or by using GNNs to integrate the graph structure (Rong et al., 2020; Jain et al., 2021; Mialon et al., 2021; Shi et al., 2021).	Correct		Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
The advantage of relative encoding compared to absolute encoding is the flexibility of using representations of position or distances into the self-attention mechanism directly. The reason is that self-considerations using absolute encoding only use node features, but self-attention mechanisms with relative encoding are able to utilize representations.	While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) Transformer architecture. Graph Transformer (Dwivedi & Bresson, 2021) provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN (Kreuzer et al., 2021) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph Transformer methods also use a relative encoding (Shaw et al., 2018) in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. Mialon et al. (2021) propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into Transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph Transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations (Ying et al., 2021) or by using GNNs to integrate the graph structure (Rong et al., 2020; Jain et al., 2021; Mialon et al., 2021; Shi et al., 2021).	Incorrect	Tortured phrases	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Correct
SAT predicts class of nodes and graphs better than other SOTA models.  Also, SAT is more explainable compared to other transformer-based models. The reason is that performance comparision shows SATs performs better than others,  and we can also explain the best range of substructure to consider with minimal hyperparameter tuning.	We show the performance of SATs compared to other GNNs and Transformers in Table 1 and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard.We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
SOTA predicts class of nodes and graphs better than other SAT models.  Also, SOTA is more explainable compared to other transformer-based models. The reason is that performance comparision shows SOTAs performs better than others,  and we can also explain the best range of substructure to consider with minimal hyperparameter tuning.	We show the performance of SATs compared to other GNNs and Transformers in Table 1 and 2. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and Transformers. In particular, for the CODE2 dataset, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place on the OGB leaderboard.We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance with minimal hyperparameter tuning, SAT also provides better interpretability than the Transformer.	Incorrect	Change concept	Incorrect	Incorrect	Correct	Correct	Incorrect	Incorrect
Author chooses RWPE for absolute positional encoding to show the outperformance of SAT is due to its structure-awareness. The reason is that SAT is equivalent to a vanilla Transformer using RWPE that isn't structure-aware if k=0. Hence, the performance improvement with the k growth shows the effectiveness of structure-aware encoding.	While the self-attention in Eq. (5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE (Dwivedi et al., 2022), though any other absolute positional representations, including learnable ones, can also be used.The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Author chooses RWPE for absolute positional encoding to show the outperformance of SAT is due to its structure-awareness. The reason is that SAT is equivalent to a fine-tuned Transformer using RWPE that isn't structure-aware if k=0. Hence, the performance improvement with the k growth shows the effectiveness of structure-aware encoding.	While the self-attention in Eq. (5) is structure-aware, most absolute encoding techniques are only position-aware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE (Dwivedi et al., 2022), though any other absolute positional representations, including learnable ones, can also be used.The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure 2(a) shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k=0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k=3 for both k-subtree and k-subgraph extractors. As k increases beyond k=4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks (Kipf & Welling, 2017). We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.	Incorrect	Change concept	Correct	Correct	Correct	Correct	Incorrect	Correct
Unseen data indicates node that is not contained in training data.  The reason is that the purpose of this paper is to generate embeddings quickly for the systems which constantly encounter entirely new nodes and graphs. Also, authors train algorithms with 2000-2004 data, while test is conducted on 2005 data.	However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model.Our first two experiments are on classifying nodes in evolving information graphs, a task that is especially relevant to high-throughput production systems, which constantly encounter unseen data.Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30].	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
Unseen data indicates node that is not contained in preparing information.  The reason is that the purpose of this paper is to generate embeddings quickly for the systems which constantly encounter entirely new nodes and graphs. Also, authors train algorithms with 2000-2004 data, while test is conducted on 2005 data.	However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.This inductive capability is essential for high-throughput, production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes (e.g., posts on Reddit, users and videos on Youtube).An inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features:for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model.Our first two experiments are on classifying nodes in evolving information graphs, a task that is especially relevant to high-throughput production systems, which constantly encounter unseen data.Citation data.Our first task is predicting paper subject categories on a large citation dataset.We use an undirected citation graph dataset derived from the Thomson Reuters Web of Science Core Collection, corresponding to all papers in six biology-related fields for the years 2000-2005.The node labels for this dataset correspond to the six different field labels.In total, this is dataset contains 302,424 nodes with an average degree of 9.15.We train all the algorithms on the 2000-2004 data and use the 2005 data for testing (with 30% used for validation).For features, we used node degrees and processed the paper abstracts according Arora et al.’s [2] sentence embedding approach, with 300-dimensional word vectors trained using the GenSim word2vec implementation [30].	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
ERNIE use multi-level masking to incorporate knowledge into language model, which includes entity-level masking and phrase-level masking. The reason is that to learn enhanced language representation by entity-level masking andphrase-level masking is a main purpose of ERNIE.	In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.Paragraph 10 : We present a novel language representation	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
ERNIE use multi-level masking to incorporate knowledge into language model, which includes only phrase-level masking. The reason is that to learn enhanced language representation by phrase-level masking is a main purpose of ERNIE.	In this paper, we propose a model called ERNIE (enhanced representation through knowledge integration) by using knowledge masking strategies. In addition to basic masking strategy, we use two kinds of knowledge strategies: phrase-level strategy and entity-level strategy. We take a phrase or a entity as one unit, which is usually composed of several words. All of the words in the same unit are masked during word representation training, instead of only one word or character being masked. In this way, the prior knowledge of phrases and entities are implicitly learned during the training procedure.Instead of adding the knowledge embedding directly, ERNIE implicitly learned the information about knowledge and longer semantic dependency, such as the relationship between entities, the property of a entity and the type of a event, to guide word embedding learning. This can make the model have better generalization and adaptability.Paragraph 10 : We present a novel language representation	Incorrect	Change concept	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
[Paired training data consists of training examples ({xi, yi}^N i=1), where the correspondence between xiand yi exists.  An instance of 'paired examples' is labels↔photos from the CMP Facade Database.	This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined.Figure 8 shows some example results on other paired datasets used in “pix2pix” [22], such as architectural labels\leftrightarrowphotos from the CMP Facade Database [40], and edges\leftrightarrowshoes from the UT Zappos50Kdataset [60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision.	Correct		Incorrect	Correct	Correct	Correct	Incorrect	Correct
[Paired preparation information consists of training examples ({xi, yi}^N i=1), where the correspondence between xiand yi exists.  An instance of 'paired examples' is labels↔photos from the CMP Facade Database.	This problem can be more broadly described as image-to-image translation [22], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale to color, image to semantic labels, edge-map to photograph.Years of research in computer vision, image processing, computational photography, and graphics have produced powerful translation systems in the supervised setting, where example image pairs \{x_{i},y_{i}\}_{i=1}^{N} are available (Figure 2, left), e.g.,[11, 19, 22, 23, 28, 33, 45, 56, 58, 62].However, obtaining paired training data can be difficult and expensive. For example, only a couple of datasets exist for tasks like semantic segmentation (e.g., [4]), and they are relatively small.Obtaining input-output pairs for graphics tasks like artistic stylization can be even more difficult since the desired output is highly complex, typically requiring artistic authoring. For many tasks, like object transfiguration (e.g., zebra\leftrightarrowhorse,  Figure 1 top-middle), the desired output is not even well-defined.Figure 8 shows some example results on other paired datasets used in “pix2pix” [22], such as architectural labels\leftrightarrowphotos from the CMP Facade Database [40], and edges\leftrightarrowshoes from the UT Zappos50Kdataset [60]. The image quality of our results is close to those produced by the fully supervised pix2pix while our method learns the mapping without paired supervision.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
[If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class.  This has been proposed in previous works which require a separate partitioning phase that is decoupled from the weight updates, while the paper’s approach is simple to learn with ordinary gradient descent methods.	A natural question is whether it makes sense to use multiple prototypes per class instead of just one.If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in Mensink et al. (2013) and Rippel et al. (2016); however both methods require a separate partitioning phase that is decoupled from the weight updates, while our approach is simple to learn with ordinary gradient descent methods.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
[If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class.  This has been proposed in previous works which require a separate partitioning phase that is decoupled from the weight updates, while the paper’s approach is simple to learn with ordinary gradient plunge methods.	A natural question is whether it makes sense to use multiple prototypes per class instead of just one.If the number of prototypes per class is fixed and greater than 1, then this would require a partitioning scheme to further cluster the support points within a class. This has been proposed in Mensink et al. (2013) and Rippel et al. (2016); however both methods require a separate partitioning phase that is decoupled from the weight updates, while our approach is simple to learn with ordinary gradient descent methods.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
[PointNet lacks the ability to capture local context at different scales.  The paper introduces a hierarchical feature learning framework to resolve this limitation.  The idea of hierarchical feature learning has been very successful and convolutional neural network is one of the most prominent examples.  The paper's approach introduces hierarchical feature learning and captures geometry features at different scales.  This is very important for understanding scenes at multiple levels and labeling objects with various sizes.	PointNet achieved impressive performance on a few benchmarks. However, it lacks the ability to capture local context at different scales. We will introduce a hierarchical feature learning framework in the next section to resolve the limitation.Our approach outperforms all the baseline methods by a large margin. In comparison with [5], which learns on voxelized scans, we directly learn on point clouds to avoid additional quantization error, and conduct data dependent sampling to allow more effective learning. Compared with [20], our approach introduces hierarchical feature learning and captures geometry features at different scales. This is very important for understanding scenes at multiple levels and labeling objects with various sizes.We visualize example scene labeling results in Fig. 6.The idea of hierarchical feature learning has been very successful. Among all the learning models, convolutional neural network [10, 25, 8] is one of the most prominent ones.However, convolution does not apply to unordered point sets with distance metrics, which is the focus of our work.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
[PointNet lacks the ability to capture local context at different scales.  The paper introduces a hierarchical feature learning framework to resolve this limitation.  The idea of hierarchical feature learning has been very successful and convolutional neural organization is one of the most prominent examples.  The paper's approach introduces hierarchical feature learning and captures geometry features at different scales.  This is very important for understanding scenes at multiple levels and labeling objects with various sizes.	PointNet achieved impressive performance on a few benchmarks. However, it lacks the ability to capture local context at different scales. We will introduce a hierarchical feature learning framework in the next section to resolve the limitation.Our approach outperforms all the baseline methods by a large margin. In comparison with [5], which learns on voxelized scans, we directly learn on point clouds to avoid additional quantization error, and conduct data dependent sampling to allow more effective learning. Compared with [20], our approach introduces hierarchical feature learning and captures geometry features at different scales. This is very important for understanding scenes at multiple levels and labeling objects with various sizes.We visualize example scene labeling results in Fig. 6.The idea of hierarchical feature learning has been very successful. Among all the learning models, convolutional neural network [10, 25, 8] is one of the most prominent ones.However, convolution does not apply to unordered point sets with distance metrics, which is the focus of our work.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
[An SVM or multi-layer perceptron classifier can be trained on the shape global features for classification.  However, point segmentation requires a combination of local and global knowledge.  After computing the global point cloud feature vector, the paper feeds it back to per point features by concatenating the global feature with each of the point features.  Then the paper extracts new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	The output from the above section forms a vector [f_{1},\dots,f_{K}], which is a global signature of the input set. We can easily train a SVM or multi-layer perceptron classifier on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. We can achieve this by a simple yet highly effective manner. Our solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating the global feature with each of the point features. Then we extract new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	Correct		Correct	Correct	Correct	Correct	Correct	Correct
[An help vector machine or multi-layer perceptron classifier can be trained on the shape global features for classification.  However, point segmentation requires a combination of local and global knowledge.  After computing the global point cloud feature vector, the paper feeds it back to per point features by concatenating the global feature with each of the point features.  Then the paper extracts new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	The output from the above section forms a vector [f_{1},\dots,f_{K}], which is a global signature of the input set. We can easily train a SVM or multi-layer perceptron classifier on the shape global features for classification. However, point segmentation requires a combination of local and global knowledge. We can achieve this by a simple yet highly effective manner. Our solution can be seen in Fig 2 (Segmentation Network). After computing the global point cloud feature vector, we feed it back to per point features by concatenating the global feature with each of the point features. Then we extract new per point features based on the combined point features - this time the per point feature is aware of both the local and global information.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Correct	Correct
The adversary (attacking part) must at least have some partial knowledge of the input (e. , images, text) and expected output (e. , classification) in order to select the architecture of the attacking system.  The adversary selects an appropriate architecture adapted to the input-output relation.  For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice.  The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc. , have relatively little impact on the success of the attack, so they do not determine the architecture.	Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section 6 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success.Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}.Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.Goodfellow’s algorithm: Recall from Equation 5 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \varepsilon added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \varepsilon onadversarial sample transferability.In Figure 8, architecture A outperformsall others: it is a copy of the oracle’s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24\%and 80.21\%, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \varepsilon above0.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The adversary (attacking part) must at least have some partial knowledge of the input (e. , images, text) and expected output (e. , classification) in order to select the architecture of the attacking system.  The adversary selects an appropriate architecture adapted to the input-output relation.  For instance, if the task is image classification or machine visioon, a convolutional neural network is the best choice.  The parameters of the system (Deep Neural Network), like training epochs, number of layers , nodes etc. , have relatively little impact on the success of the attack, so they do not determine the architecture.	Substitute Architecture: This factor is not the mostlimiting as the adversary must at least have some partial knowledge of theoracle input (e.g., images, text) andexpected output (e.g., classification). The adversary can thus usean architecture adapted to the input-output relation. For instance, aconvolutional neural network is suitable for image classification. Furthermore,we show in Section 6 that the type, number, and size of layers usedin the substitute DNN have relatively littleimpact on the success of the attack. Adversaries can also consider performingan architecture exploration and train several substitute models beforeselecting the one yielding the highest attack success.Substitute DNN Training Algorithm: We now describe the five-step training procedure outlined in Algorithm 1:•Initial Collection (1): The adversary collects a very small set S_{0} of inputs representative of the input domain. For instance, if the targeted oracle O classifies handwritten digits, the adversary collects 10 images of each digit 0 through 9. We show in Section 5 that this set does not necessarily have to come from the distribution from which the targeted oracle was trained.•Architecture Selection (2): The adversary selects an architecture to be trained as the substitute F. Again, this can be done using high-level knowledge of the classification task performed by the oracle (e.g., convolutional networks are appropriate for vision)•Substitute Training: The adversary iteratively trains moreaccurate substitute DNNs F_{\rho} by repeating the following for ρ∈0..ρm⁢a⁢x\rho\in 0..\rho_{max}italic_ρ ∈ 0 . . italic_ρ start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT:–Labeling (3): By querying for the labels \tilde{O}(\vec{x}) output by oracle O, the adversary labels each sample \vec{x}\in S_{\rho} in its initial substitute training set S_{\rho}.–Training (4): The adversary trains the architecture chosen at step (2) using substitute training set S_{\rho} in conjunction with classical training techniques.–Augmentation (5): The adversary applies our augmentation technique on the initial substitute training set S_{\rho} to produce a larger substitute training set S_{\rho+1} with more synthetic training points. This new training set better represents the model’s decision boundaries. The adversary repeats steps (3) and (4) with the augmented set S_{\rho+1}.Step (3) is repeated several times to increase the substitute DNN’s accuracy and the similarity of its decision boundaries with the oracle. We introduce the term substitute training epoch, indexed with \rho, to refer to each iteration performed. This leads to this formalization of the Jacobian-based Dataset Augmentation performed at step (5) of our substitute training algorithm to find more synthetic training points:S_{\rho+1}=\{\vec{x}+\lambda\cdot\operatorname{sgn}(J_{F}[\tilde{O}(\vec{x})]):\vec{x}\in S_{\rho}\}\cup S_{\rho}(4)where \lambda is a parameter of the augmentation: it defines the size of the step taken in the sensitive direction identified by the Jacobian matrix to augment the set S_{\rho} into S_{\rho+1}.Choosing an Architecture: We train substitute DNNs A and F to M (cf.Table 13) using 150 samples from the MNIST test set as the substitute training set. During eachof the 6 substitute training epochs, the DNN is trained for 5 epochsfrom scratch. Between epochs, synthetic data is added to the training set using Jacobian-baseddataset augmentations with step \lambda=0.1. The substitutearchitectures differ from the oracle’s by the type, number, and sizeof layers. In Table 1,we report the accuracy of each architecture after 2 and 6 substitute training epochs, as well as the adversarialsample transferability after 6 epochs. Adversarial samples are crafted using the Goodfellow algorithm with an inputvariation of \varepsilon=0.4 (which we justify later). The last column ofTable 1shows that the choice of architecture has a limitedimpact on adversarial sample transferability, and therefore on the attacksuccess. The most important transferability drop follows from removing allconvolutional layers. Changing the hidden layer activation function fromrectified linear to a sigmoid does not impact transferability significantly.Goodfellow’s algorithm: Recall from Equation 5 the perturbation computed in the Goodfellow attack.Its only parameter is the variation \varepsilon added inthe direction of the gradient sign. We use the same architecture set asbefore to quantify the impact of \varepsilon onadversarial sample transferability.In Figure 8, architecture A outperformsall others: it is a copy of the oracle’s and acts as a baseline. Otherarchitectures have asymptotic transferability rates ranging between 72.24\%and 80.21\%, confirming that the substitute architecture choice hasa limited impact on transferability. Increasing the value of \varepsilon above0.4 yields little improvement in transferability and should be avoidedto guarantee indistinguishability of adversarial samples to humans.	Incorrect	Tortured phrases	Incorrect	Correct	Correct	Incorrect	Incorrect	Correct
The reason between different scale of availability between classification and detection datasets is due to the  fact that labelling images for detection is far more expensive than labelling for classification or tagging.  For example common object detection datasets contain only 10 to 100 thousands images with dozen to hundred tags whereas image classification datasets  have million of images with thousands of classes.  Object detection methods like YOLO can utilize the large amount of classification data to help the detection task.	Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2].We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future.We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together.This approach presents a few challenges. Detection datasets have only common objects and general labels, like “dog” or “boat”. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”. If we want to train on both datasets we need a coherent way to merge these labels.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
The reason between different scale of availability between classification and detection datasets is due to the  fact that labelling images for detection is far more expensive than labelling for classification or tagging.  For example common object detection datasets contain only 10 to 100 thousands images with dozen to hundred tags whereas image classification datasets  have million of images with thousands of classes.  Object detection methods like Just Look Once can utilize the large amount of classification data to help the detection task.	Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2].We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future.We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together.This approach presents a few challenges. Detection datasets have only common objects and general labels, like “dog” or “boat”. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”. If we want to train on both datasets we need a coherent way to merge these labels.	Incorrect	Tortured phrases	Correct	Correct	Correct	Correct	Incorrect	Correct
Proximal Policy Optimization is an optimization algorithm used to train the controller RNN.  It is done by employing a global work queue system for generating a pool of child networks controlled by the RNN.	In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task [31]. The controller RNN was trained using Proximal Policy Optimization (PPO) [51] by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Proximal Policy Optimization is an optimization algorithm used to train the controller recurrent neural community.  It is done by employing a global work queue system for generating a pool of child networks controlled by the recurrent neural community.	In this section, we describe our experiments with the method described above to learn convolutional cells. In summary, all architecture searches are performed using the CIFAR-10 classification task [31]. The controller RNN was trained using Proximal Policy Optimization (PPO) [51] by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Correct	Correct
From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 2. 40% which is better than the previous record.	For the task of image classification with CIFAR-10, we set N=4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table 1 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
From the above evidential paragraph, we can see that the cutout data augmentation achieves a state-of-the-art error rate of 1.9% which is better than the previous record.	For the task of image classification with CIFAR-10, we set N=4 or 6 (Figure 2). The test accuracies of the best architectures are reported in Table 1 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. The best single run from our model achieves 2.19% error rate.	Incorrect	Change number	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Top-1 and Top-5 error rate are the evaluation metrics used to compare the performace of various models.	First we observe the top-1 and top-5 validation-error evolution of thefour variants during training. After the experiment was conducted, we havefound that our continuous evaluation was conducted on a subset of thevalidation set which omitted about 1700 blacklisted entities due to poorbounding boxes. It turned out that the omission should have been only performedfor the CLSLOC benchmark, but yields somewhat incomparable (more optimistic)numbers when compared to other reports including some earlier reports by ourteam. The difference is about 0.3% for top-1 error and about 0.15% forthe top-5 error. However, since the differences are consistent, we thinkthe comparison between the curves is a fair one.	Correct		Incorrect	Correct	Correct	Incorrect	Incorrect	Incorrect
Top-1 and Top-5 blunder rate are the evaluation metrics used to compare the performace of various models.	First we observe the top-1 and top-5 validation-error evolution of thefour variants during training. After the experiment was conducted, we havefound that our continuous evaluation was conducted on a subset of thevalidation set which omitted about 1700 blacklisted entities due to poorbounding boxes. It turned out that the omission should have been only performedfor the CLSLOC benchmark, but yields somewhat incomparable (more optimistic)numbers when compared to other reports including some earlier reports by ourteam. The difference is about 0.3% for top-1 error and about 0.15% forthe top-5 error. However, since the differences are consistent, we thinkthe comparison between the curves is a fair one.	Incorrect	Tortured phrases	Incorrect	Incorrect	Correct	Incorrect	Incorrect	Incorrect
Language drift occurs when a language model pre-trained on a large text corpus and fine-tuned for a specific task loses syntactic and semantic understanding as it improves learning the target task only.  Authors suggested that their novel autogenous class-specific prior-preserving loss solves this issue.	In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject.The phenomenon of language drift has been an observed problem in the language model literature Lee2019CounteringLD ; lu2020countering , where a language model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language as it learns to improve in the target task. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffusion models. Since our text prompt contains both the [identifier] and [class noun], when a diffusion model is fine-tuned on a small set of subject images, we observe that it slowly forgets how to generate subjects of the same class and progressively forgets the class-specific prior and can not generate different instances of the class in question. Figure 13 (middle) shows some sample generated images of “a dog” after fine-tuning the model on specific dog images. The results clearly show that the model loses the capability of generating generic dog images with naive fine-tuning.We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{"a [class noun]"})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4.	Correct		Correct	Correct	Correct	Correct	Incorrect	Correct
Language drift occurs when a language model pre-prepared on a large text corpus and fine-tuned for a specific task loses syntactic and semantic understanding as it improves learning the target task only.  Authors suggested that their novel autogenous class-specific prior-preserving loss solves this issue.	In order to accomplish this, our first task is to implant the subject instance into the output domain of the model and to bind the subject with a unique identifier. We present our method to design the identifier below as well as a new approach to supervise a fine-tuning process of the model such that it re-uses its prior for our subject instance. A key problem is that fine-tuning on a small set of images showing our subject is prone to overfitting on the given images. In addition, language drift Lee2019CounteringLD ; lu2020countering  is a common problem in language models, and manifests itself in text-to-image diffusion models as well: the model can forget how to generate other subjects of the same class, and lose the embedded knowledge on the diversity and natural variations of instances belonging to that class. For this, we present an autogenous class-specific prior preservation loss, where we alleviate overfitting and prevent language drift by encouraging the diffusion model to keep generating diverse instances of the same class as our subject.The phenomenon of language drift has been an observed problem in the language model literature Lee2019CounteringLD ; lu2020countering , where a language model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language as it learns to improve in the target task. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffusion models. Since our text prompt contains both the [identifier] and [class noun], when a diffusion model is fine-tuned on a small set of subject images, we observe that it slowly forgets how to generate subjects of the same class and progressively forgets the class-specific prior and can not generate different instances of the class in question. Figure 13 (middle) shows some sample generated images of “a dog” after fine-tuning the model on specific dog images. The results clearly show that the model loses the capability of generating generic dog images with naive fine-tuning.We propose an autogenous class-specific prior-preserving loss to counter both the overfitting and language drift issues. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. Specifically, we generate data \mathbf{x}_{\text{pr}}=\hat{\mathbf{x}}(\mathbf{z}_{t_{1}},\mathbf{c}_{\text{pr}}) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise \mathbf{z}_{t_{1}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}) and conditioning vector \mathbf{c}_{\text{pr}}\coloneqq\Gamma(f(\text{"a [class noun]"})). The loss becomes:\mathbb{E}_{\mathbf{x},\mathbf{c},{\bm{\epsilon}},{\bm{\epsilon}}^{\prime},t}\!\left[w_{t}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t}\mathbf{x}+\sigma_{t}{\bm{\epsilon}},\mathbf{c})-\mathbf{x}\|^{2}_{2}+\lambda w_{t^{\prime}}\|\hat{\mathbf{x}}_{\theta}(\alpha_{t^{\prime}}\mathbf{x}_{\text{pr}}+\sigma_{t^{\prime}}{\bm{\epsilon}}^{\prime},\mathbf{c}_{\text{pr}})-\mathbf{x}_{\text{pr}}\|^{2}_{2}\right],(2)where \lambda controls for the relative weight of the prior-preservation term. Figure 4 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss.Despite being simple, we find this prior-preservation loss is effective in overcoming the overfitting and language-drift issues.We find that \sim 200 epochs at learning rate 10^{-5} with \lambda=1 is enough to achieve good results. During this process, \sim 200\times N “a [class noun]” samples are generated, with N being the size of the subject dataset, usually ranging from 3-5 images. The training process takes about 15 minutes on one TPUv4.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.	In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.We explore the following architectural changes:For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.	Correct		Correct	Correct	Correct	Incorrect	Incorrect	Correct
They use variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and down-examining, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.	In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models.We explore the following architectural changes:For all comparisons in this section, we train models on ImageNet 128\times128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, so we opt not to use this change in further experiments.In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks.The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples Szegedy et al. (2013).Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-the-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256\times256 and 512\times512.	Incorrect	Tortured phrases	Correct	Correct	Correct	Incorrect	Incorrect	Correct
Inbreeding is known to negatively affects the reproductive performance of male animals [ 21 ], and when expressed at high levels can cause poor semen quality [ 22 , 23 , 24 , 25 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
SCFA is found to be lower in autistic patients who are correlated with presence of  Faecalibacterium, Ruminococcus , and  Bifidobacterium  species. Clostridia  species are found in abundance in ASD patients which is responsible for the production of propionate  50 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Whereas,  Alistipes, Bilophila, Veillonella ,Faecalibacterium, Ruminococcus, Bifidobacterium  and Genera  Prevotella, Coprococcus,  unclassified  Veillonellaceae, Prevotella copri, Faecalibacterium prausnitzii,  and  Haemophilus parainfluenzae  were detected in lower abundance in ASD patients  [50] .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Overall, glycolysis was found to be crucial in the early stages of Leishmania spp. infection in macrophages and neutrophils, whereas enhanced mitochondrial metabolism was revealed to be important in the late stages of infection ( 87 ,  88 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The peptide isolated from  Streptomyces  sp. LK3 extract manifested activity against the malaria parasite Plasmodium  133 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Correct	Correct	Correct	Unrelated	Incorrect	Correct
Pro-inflammatory cytokines, including IL-1β, IL-2, IL-6, IL-12p40, interferon (IFN)-γ, and tumor necrosis factor (TNF)-α, have been observed in sera from ETEC-infected pigs 11 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The bridging position of the Talin-1 molecule leads to its unfolding and stretching due to the actomyosin contraction, and these conformation changes affect the affinity and interaction with its binding partners ( 57 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The fact is that the suitable condition of the interaction between the spike protein and ACE2 receptor occurs in the case of increasing the β-sheets and α-helices, and decreasing the coils, bends, and turns [ 42 , 43 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In another study, Hu et al. confirmed that miR-93-5p could enhance the drug resistance of prolactinoma cells by regulation of TGF-β1/Smad3-dependent fibrosis [ 76 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Among the four dominant compounds, patchouli alcohol had the highest concentration and produced a distinctive aroma that lasted longer [ 55 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The interaction of aromatic compounds and metal complexes is an important subject because of its importance in the fields of nucleic acids, 83,84  proteins, 85,86  crystal engineering, 87  material science 88  and drug design.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The other MDR strain, PA_65, clustered into group 2 with the well-known virulent strain UCBPP-PA14 [ 15 ] and MDR Indian ocular isolate VRFPA04 [ 16 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Therefore, the use of the first-order shear deformation theory (e.g., Timoshenko theory) and higher-order shear deformation theories is recommended [ [49] ,  [50] ,  [51] ,  [52] ,  [53] ,  [54] ].	Firearm violence remains a significant problem in the US (with 2787 adolescents killed in 2015). However, the research on school firearm violence prevention practices and policies is scant. Parents are major stakeholders in relation to firearm violence by youths and school safety in general. The purpose of this study was to examine what parents thought schools should be doing to reduce the risk of firearm violence in schools. A valid and Related questionnaire was mailed to a national random sample of 600 parents who had at least one child enrolled in a public secondary school (response rate = 47%). Parents perceived inadequate parental monitoring/rearing practices (73%), peer harassment and/or bullying (58%), inadequate mental health care services for youth (54%), and easy access to guns (51%) as major causes of firearm violence in schools. The school policies perceived to be most effective in reducing firearm violence were installing an alert system in schools (70%), working with law enforcement to design an emergency response plan (70%), creating a comprehensive security plan (68%), requiring criminal background checks for all school personnel prior to hiring (67%), and implementing an anonymous system for students to report peer concerns regarding potential violence (67%). Parents seem to have a limited grasp of potentially effective interventions to reduce firearm violence.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The complexity of the legal boundaries surrounding cannabis has been recognised as a major factor that has hindered the development of effective CB research [ 146 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Recently, various types of surfactants have begun to be used as stabilizing agents in the synthesis of AgNPs to increase their stability, such as sugars [ 25 ], trisodium citrate [ 26 ], lignosulfates [ 27 ], ascorbic acid [ 28 ], plant extracts [ 29 ] etc.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The size of the studied area is an important factor in the measurement of bone density, and any errors in the measurement of area interfere with the measurement of bone mineral density (BMD).[ 5 ] A decreased T-score is considered to be a criterion for diagnosing osteoporotic disorders and is related to the so-called fracture threshold.[ 6 ] Eighty percent of fractures have a close relationship with BMD reduction.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Poor sanitation and hygiene was reported in different studies after slaughtering food-animals and butchering poultry [ 43 , 44 , 45 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The ZnO has been biosynthesized from various plants like  Abutilon indicum  [ 21 ],  Maringa oleifera  [ 22 ],  Cinnamomum verum  [ 23 ],  Ixora coccinea  [ 24 ], and  Punica granatum  [ 25 ], which have shown excellent photocatalytic activities [ 26 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Calpain is an intracellular calciumdependent cysteine protease  Khorchid and Ikura, 2002   Vickers, 2017 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
NASH is one of the most common forms of chronic liver disease in developed countries, leading to a significant burden on healthcare systems worldwide. The global burden of NAFLD, and NASH in 2018 was approximately 25 and 35, respectively  4 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
CR is distinct from the risk of loss due to default or the inability of customers to meet shortterm obligations on lending, trading, hedging settlements, and other financial agreements. The ratio of loan loss provisions to total gross loans is used to assess CR  112 ,  113 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The strict implementation of non-therapeutic measures [ 5 , 6 ] together with the quarantine and isolation of infected and suspected cases led to a decline in quality of life and aggravated psychological problems across different cohorts [ 7 , 8 , 9 , 10 , 11 , 12 , 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Therefore, the development of artificial APCs was suggested to address these challenges. aAPCs are synthetic APCs composed of a cognate antigenic peptide presented by MHC molecules for binding to TCR and co-stimulatory molecules for binding to related receptors at the immunological synapse, thereby activating T cells [ 93 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Scopolamine, a physiological modulator of neurotransmitter acetylcholine, impairs learning and memory in laboratory rats [ 58 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These composites are widely used in these applications due to their lower density, corrosion and wear resistance, competitive prices, and high flexibility. On the other hand, the majority of polymer foams are considered as electrical insulators  7 , 8 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
However, numerous antioxidant substances that have been employed to reduce hepatic oxidative stress have been studied within the scope of earlier investigations (Eşrefogˇlu et al.  2007 ; Layachi and Kechrid  2012 ; Vickers  2017 ; Zafeer et al.  2012 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The action of chitosan could be in the form of pathogen cell disturbance or as an inducer for defense responses in the host plant via inducing and inhibiting different biochemical activities during the plant–pathogen interaction [ 23 , 24 , 25 , 26 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Studies in India after receiving the AZD1222 (ChAdOx1S) or BBV152 vaccine reported a breakthrough infection rate of just over 13% [ 18 ,  19 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is native to southern areas of Europe, Northern Africa, South and North America, and Australia, as well as in some parts of Asia where it is used to cure liver diseases and is beneficial for lactating mothers [ 1 , 2 , 3 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Supercapacitors (SCs) are an intermediate point between capacitors and conventional batteries. 5,6  The energy densities of Li-ion batteries (LIBs) reached 150 to 200 W h kg −1 , 7,8  which are significantly higher compared to other battery types, such as Ni–Cd 9,10  Ni–MH, 9  and lead batteries. 9  Applications for electric vehicles are now feasible thanks to SCs with high power densities, longer life cycles, and fast charging. 11  SCs are cheaper than batteries and provide good power with a moderate energy density. 12–14  The SCs are considered a main electrical energy storage EES because of their good energy and power densities. 10,11,15–18  According to their storing method, SCs are further divided into PCs (pseudo capacitors) and EDLCs (electric double-layer capacitors).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The magnetic resonance imaging (MRI) pictures of a healthy and tumorous brain are shown in Figures  1(a)  and  1(b) , respectively.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It has been noted that the administration form of Res may affect its elimination, which can be delayed when micronized Res is used [ 92 , 93 , 94 , 95 , 96 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The oxygen ion absorption is also enhanced using CaO, which is crucial for the catalysis process. CaO in bulk has a large  E g  of 7.1 eV ref.  17  and  18  and a high    of 11.8 dielectric constant.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The results were also explained by Fick’s second rule of diffusion, which states that after a specific time, there will be a final equilibrium between the solute concentration in the solid matrix (plant sample) and the bulk solution (extraction solvent) [ 103 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, coronavirus affects the water sources since severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) has been diagnosed in untreated wastewater (Vickers,  2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Should the ureteral outlet stricture not be addressed in a timely manner, complicated upper urinary tract infections, stones, renal insufficiency, and other serious complications may develop [ 6 , 7 , 8 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The photocatalytic efficiency of Ln (La, Nd, or Sm)-doped zinc oxide nanoparticles in the decomposition of p-nitrophenol was investigated by Khatamian et al. [ 19 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Heme iron has prooxidant properties that may encourage oxidative injury and inflammation in several organs  39 . Numerous health consequences, including diabetes, CVD, and cancer, have been linked to heme iron  40 , 41 , 42 , 43 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The depersonalization component represents the interpersonal relationships that lead to a negative interaction. The sense of low personal accomplishment refers to feelings of incompetence  4 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For instance, in ophthalmology, considerable variations in retinal images across different institutions can be attributable to factors such as the use of distinct imaging devices ( 34 ), heterogeneous patient populations ( 35 ), and inconsistencies in image acquisition protocols ( 36 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Though, it is notable that the ACE2 receptor is expressed in numerous tissues and organs such as the heart, intestine, brain, adipose tissue, pancreas, kidney, vasculature, and liver, making them potential targets for COVID-19, which may clarify the destruction reported in multi-organ systems in patients with COVID-19 [ 56 , 58 , 59 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Though, it is notable that viral binding to the ACE2 declines ACE2 availability and stability, leading to the manifestation of harmful biological effects (fibrotic, vasoconstrictive, proliferative, and pro-hypertrophic, induction of oxidative stress, and inflammatory effects) and an imbalance in the RAAS in various body parts [ 29 , 58 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Several findings have confirmed the valuable activities of metformin in patients infected with COVID-19, although some findings have revealed that Metformin-treatment can increase disease severity (but not mortality) and risk of acidosis in patients with COVID-19 [ 58 , 82 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It has been demonstrated that hPMSCs secrete a variety of cell factors, including IL-6, IL-8, and IL-10, granulocyte colony-stimulating factor, and chemokine (C-C motif) ligand 5 [ 178 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The management available for TBI in the current scenario is the intracranial pressure control, use of hypertonic solutions, surgical and seizure interventions [ 37 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
CVD related deaths in Sub-Saharan Africa (SSA) have increased by >50% in the past three decades. 66  In SSA, CVDs are responsible for approximately 37% of all chronic disease-related deaths 67  with the most common all-cause-related CVD death being ischemic heart disease, stroke, and hypertensive heart disease. 66  , 68  Notably, the World Health Organization (WHO) estimates the prevalence of hypertension as the highest in the African region, with about 46% of adults aged 25 years and older being hypertensive compared to the rest of the world. 69  Unfortunately, levels of hypertension access to screening, appropriate diagnosis and treatment in this region are low, with an estimated 40% of diabetes patients unaware of their diagnosis. 67  A review by Yuyun et al., on CVDs in SSA from 1990 to 2019 estimates the prevalence rates of CVD risk factors as follows: 1) Smoking at 10%; 2) hypertension at 30% (40% in urban and 20% in rural areas); 3) diabetes at 7%; 4) dyslipidemia at 25%; 5) physical inactivity at 22%; and 6) obesity being higher among women (2–40%) compared to men (1–15%). The true burden of these risk factors and their complications in SSA remain uncertain as most of these countries are either void of data or have deficient data recording systems hence not sufficiently dependable to support mounting of a commensurate response 70  and/or policy development. Another major concern in SSA is the tendency of CVDs to occur at younger ages, approximately two decades earlier compared to high-income countries. 71  This poses an additional risk to family and community well-being (economic impact for loss of productive years of life), 72  regional socioeconomic development and health systems sustainability. 67  Therefore, interventions targeting the younger population, especially in healthy living and behavioral changes, are crucial in primary prevention and should be a key strategy to reduce morbidity and mortality from CVDs in SSA.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The authors of ( Vickers, 2017 ) employed the CNN framework for classification tasks and the VGG network for illness localization.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Importantly, the capability of MSCs of differentiation into myofibroblasts with the development of fibrous tissue was already well-demonstrated in previous experimental studies ( 74 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Therefore, pregnant women and newborn babies should be prioritized when adopting measures/ interventions to prevent and manage COVID-19 [ 9 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Precisely, 0.0 < TM-score < 0.30 indicates random structural similarity, whereas 0.50 < TM-score < 1.00 implies that both structures are within the same fold 55 , 56 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Despite the evidence on the safety and effectiveness of vaccines for mothers, there are still challenges in achieving high vaccination coverage during pregnancy worldwide [ 22 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This study showed that ANC follow-up had a higher chance of having women’s protected from tetanus compared to mothers with no ANC follow-up; which is also consistent with the previous study done in different countries worldwide [ 22 ,  25 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Moreover, BR are involved in controlling abiotic stress responses 9  via: (1) increasing activities of antioxidative enzymes 10 ; hence, lessening the production rate of superoxide anion 11 , (2) reducing abscisic acid (ABA) accumulation 12  in spite of that these hormones increase stomatal closure under drought conditions 10 , 13  and (3) increasing the osmotic permeability of root cells to take up more water from soil.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As a result, drug loading, micelle size, and uptake/release kinetics are all critical factors to consider when it comes to nanoparticle drug delivery [ 139 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Protein-protein interaction (PPI) analysis is important to understand diverse biological processes including cell proliferation ( Nooren and Thornton, 2003 ), signal transduction ( Pawson and Nash, 2000 ), DNA transcription, replication ( Zhang et al., 2012 ;  Vickers, 2017 ), hormone regulation ( Zhao, 2015 ), cycle control ( Kulminskaya and Oberer, 2020 ), and neuro-transmission ( Südhof, 1995 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
T cell activity is associated with less disease severity in SARS-CoV-2 infection, indicating that T cells play an essential role in controlling and treating primary SARS-CoV-2 infection [ 32 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
On the other hand, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), with the infection being commonly referred to as COVID-19, rapidly spread globally, producing a pandemic with approximately 497 million cases and 6.1 million deaths at the time of writing ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Close contact with domestic animals has been reported to be one of the key risk factors related to the transmission of  C. burnetii  to humans [ 45 , 46 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Telomere length has an impact on total life expectancy, and telomere shortening is a sign of molecular aging ( Blackburn, 2009 ;  Beery et al., 2012 ;  Karthik et al., 2014 ;  Alda et al., 2016 ;  Thimmapuram et al., 2017 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In leukocytes, psychological stress has been linked to rapid telomere shortening, whereas meditation has been linked to increased telomere length ( Blackburn, 2009 ;  Beery et al., 2012 ;  Karthik et al., 2014 ;  Thimmapuram et al., 2017 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
In various studies, CONUT score was shown to have a strong association with patient prognosis and the presence of a cardiovascular disease, including coronary heart disease and congestive heart failure [ 8 , 9 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In silico studies offer a pathway to assess crucial ADMET parameters, including absorption, distribution, metabolism, excretion, and toxicity ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, ion-exchange nanofibrous membranes are frequently regarded as a means to enhance the flow and permeability of membranes that selectively filter monovalent and divalent ions [ 78 , 79 , 80 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
To generate these stochastically-interpolated time series data, multi-point fractional Brownian bridges [ 5 ] were used. An in-depth discussion and corresponding applications of regular fractional Brownian bridges are provided in [ 12 , 13 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Plastics are divided into different sizes nano, micro, meso, and macro with methodological limitations that determine the borderlines  13 . Plastics 5mm are usually called macroplastics  14 , 15 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Microwave irradiation (MI, V Instek Analytical, Gujarat, India) takes less time than conventional heating (CH), according to an experiment by Jafarirad et al. [ 78 ], and this is due to the higher level of heating provided by MI and a consequently faster response rate.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The reactions of the soil microbial community to agricultural management are extremely complicated. Environmental factors such as moisture and soil temperature, which are often variably influenced by organic amendments, can regulate the diversity and community composition of soil organisms ( Carey et al., 2015 ;  Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Almost all plants of this family contain all the representative phytochemical groups, such as saponins [ 7 ], flavonoids [ 8 ], aldehydes [ 9 ], hemiterpene glycosides, triterpenes [ 10 ], etc.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Biological oxygen demand (BOD) is a traditional factor that effect on the operational and functional of wastewater treatment process and, effectiveness of WWTPs 6 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Patients with EPTB have enlarged lymph nodes and musculoskeletal pain and they also complain of other types of non-specific symptoms [ 6 – 8 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Organic trace contaminants can be degraded either by primary metabolism, where microorganisms utilize them as sources of carbon and possibly also nitrogen, phosphorus, or sulphur ( Alexander, 1999 ), or by co-metabolism where contaminants are transformed by enzymes without being used as an energy source ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Polysaccharides used as coatings may have limited applications due to their low drug loading capacity, which can be a problem when administering a less potent drug at a high dose [ 32 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The common symptoms of COVID-19 are fever, fatigue, shortness of breath, dry cough, dyspnea, and decreased white blood cells [ 5 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The postmortem period is an essential task for forensic science and environmental concerns 1 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For rGO, the absorption spectrum shows a red-shift of the 235 nm peak to 290 nm because of the oxygen functional group removal and conjugate structure restoration. 33  Similarly, for MWCNTs, the absorption spectrum shows a significance peak from 210 to 295 nm.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The attractive properties of NE enabled their use as a vehicle for the distribution of essential oils, nucleic acid, drugs antimicrobial agents, repellents, and as an imaging agent [ 71 , 73 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Hydrophilic phytochemicals, such as polyphenols and flavonoids, have lower absorption in the body due to their high molecular size, which made absorption across biological membranes difficult. These constraints have been overcome thanks to phytosome  71 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Antimicrobial resistance (AMR) is a phenomenon that happens when bacteria evolve techniques to resist antibiotics intended to kill them, resulting in infections that are difficult to cure and an increased risk of disease transmission [ 9 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Life-threatening conditions can arise due to VMs because of their unpredictable clinical evolution and manifestations [ 19 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The precursors that are utilized for the synthesis of TiO2 NPs are Ti(OBu)4 [ 55 ], Ti[OCH(CH3)2]4 (TTIP) [ 56 ], TiCl3 [ 57 ], and TiCl4 [ 58 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Sub-Saharan Africa, has a number of HIV-infected individuals of up to 5% who are lost both before treatment 1  and after initiating treatment. 2 , 3 , 4  Our observation of discrepant relationship between rates of viral suppression for the Day Hospital which was 89% for all available results at the end March 2019 and the 12-months retention rate which was was 67% prompted us to plan a review to understand if patients who had defaulted care were really not on care and hence carry out this study through implementation of a patient tracking exercise for patients not in care as per hospital records from October 2018 to March 2019 dubbed “Return to Care Campaign.’	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There is also a great need to set up data linkages through which information of patients that are transferred to other facilities could be shared. This is in line with another study. 12 , 13	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As a result, developing safe, clean, cost-effective, and biocompatible nanomaterials with environmentally friendly methods is critical. These biological methods are safe and environmentally friendly, and they do not necessitate any preparatory conditions or processes [ 10 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The synthesis of Ag-NPs was achieved by changing the concentration of plant extract in the solution of distilled water and plant extract as reported by similar routes [ 10 , 11 , 12 , 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
With in exploration of the dynamics of epidemiological models, fractional order differential equations introduce a new dimension. For a consequence, the fractional form of various epidemical models has been studied, as shown in  38  40 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Even high accumulation of B atoms inside the cells have been shown by these nanoparticles but they have failure in the therapeutic window for BNCT. More improvements are still needed to design a  10 Benriched BNNPs source  103 , 104 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The passivation film resistance formation of WE54 is not ideal. 82  EW62 (Mg–6%Nd–2%Y–0.5%Zr) magnesium alloy has improved corrosion resistance and mechanical properties compared to its conventional counterpart (CC), and rapid solidification properties also reduce its cytotoxicity. 83  Increased Nd solubility in passivated substrates and outer layers helps reduce hydrogen formation and hydrogen embrittlement. 84  EW10X04 (1.16 wt% Nd, 0.48 wt% Y, 0.48 wt% Zr, 0.43 wt% Ca) developed by adding a small amount of Ca to EW10 (1.15 wt% Nd, 0.43 wt% Y, 0.46 wt% Zr), the corrosion test in 0.9% NaCl solution found that the corrosion rate of EW10X04 (about 0.5 mm year −1 ) was significantly slower than that of EW10 (more than 1 mm year −1 ), however, the mechanical properties of EW10X04 decreased. 85	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Each of its elements,  C ij , indicates the average number of contacts made by a person in age group  i  with persons in age group  j  on a daily basis. As a result, we use the countryspecific contact matrices supplied in Reference  Karthik etal., 2014 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Similarly, males outperformed females in PIU severity in various cultures, including Indian [ 96 ], Vietnamese [ 97 ], British [ 98 ], Romanian [ 99 ] and Bengali [ 100 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Several animal studies have investigated the effect of carnosine supplementation on lipid parameters and have shown dyslipidemia improvement and reduction of oxidation and glycation of LDL-C following carnosine treatment in both diabetic and non-diabetic rodents [ 16 , 56 , 57 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This is the primary condition for distinguishing target auditory data from non-target auditory data and also for normal auditory function [ 17 , 18 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The advancement of endoscopic techniques in the past decade has improved the surgical management of cerebellopontine angle (CPA) tumors.[ 2 , 11 , 23 , 45 ] Endoscope-assisted microsurgery improves the ability to evaluate the extent of resection, achieve safe tumor resection, and reduces the risk of surgery-related morbidity.[ 2 ] Endoscope-assisted microsurgery has been well studied and applied for CPA tumors, microvascular decompression (MVD), and aneurysm surgery.[ 2 , 7 , 18 , 21 , 26 , 33 , 36 , 45 , 47 ] The endoscope, used as a tool for better surgical visualization.[ 2 , 10 , 44 , 45 ]	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Motaghi et al. investigated the adsorption performance of Co (II) in water using the bio-nanocomposite MOF MCS/AC@UiO-66 and achieved a remarkable adsorption efficiency of 44.5 % within 15 min.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Virus adsorption onto the large suspended solids in sewage, which is accompanied by gravitational sedimentation, is considered as the main and first mechanism in the treatment phase for the virus removal [ 93 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The recruitment and nuclear translocation of β-arrestin, which in turn works as an epigenetic regulator of multiple angiogenic/metastatic genes, including β-catenin, is facilitated by GRK5/6-mediated phosphorylation of the receptor [ 49 , 190 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In organotins however,  In-vitro  studies have shown increased expression of IL-1β, tumor necrosis factor (TNF-α), IL-6, and nitric oxide synthase (iNOS) in the cultured astrocytes and microglia ( 245 ,  246 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
When children with allergic airway diseases were compared to children from similar surrounding environments, such as both from urban areas, a mild reduction in microbiota diversity was observed, and microorganisms from the phylum Firmicutes were significantly less expressed than in healthy children [ 108 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In the United States (US), annual losses of USD 3 billion were estimated to occur due to loss of wages, treatment costs, and production losses associated with livestock [ 21 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Concerns over nitrate intake originate from the fact that they are associated with the genesis of some forms of cancer and methemoglobinemia throughout the world 4 , 5 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Biogenically synthesized silver nanoparticles have been potent therapeutic agents with prominent antimicrobial properties [ 51 – 53 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Water quality indices are pivotal in assessing contamination levels and potential risks linked to heavy metals within water sources 11 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Fourteen articles stated that experienced drivers perceived hazards faster than less experienced ones [ 17 ,  29 ,  35 ,  37 ,  41 ,  45 ,  60 ,  66 ,  71 ,  72 ,  74 ,  84 ,  85 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In surface water where bromide and iodide ions are not present, chlorinated byproducts become prevalent during disinfection using free chlorine [ [13] ,  [14] ,  [15] ,  [16] ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Total antioxidant power is reduced. The composition of procyanidins and flavan-3-ols is altered. The amount of anthocyanin is reduced. Aremu et al. (1995) Aremu et al. (1995) ( Aremu et al., 1995 ) ( Aremu et al., 1995 ) ( Aremu et al., 1995 ) ( McShea et al., 2008 ) McShea et al. (2008) (Vicker., 2017) ( Moreno et al., 2012 ), ( Djikeng et al., 2018 ) ( Moreno et al., 2012 ), ( Djikeng et al., 2018 ) Conching Solid particles are coated with fat and volatile acids are evaporated. Desirable color is developed. Proper viscosity is achieved. In the Short time, conching process higher amount of monomer is produced. In Long term, conching process polyphenols undergo thermal alterations that are followed by a condensation reaction.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This condition exaggerates the loss of total polyphenol content and flavonoids  Krysiak, 2011 . The monomeric flavonols are lost during normal roasting temperature  Vicker., 2017   Ioannone et al., 2015 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
At 125C the number of TRAP increases by 7. At the end of roasting, at 145C, temperature TRAP decreases by about 20  Vickers, 2017 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Anthocyanin is an essential antioxidant component present in raw beans, but this content is reduced in fermented cocoa beans due to polymerization and hydrolysis of condensed tannin  Djikeng et al., 2018   Moreno et al., 2012 . 2040 reduction of total antioxidant capacity was observed during fermentation. Antioxidant capacity was measured by DPPH, ABTS, and FRAP  Vickers, 2017   Mattia et al., 2013 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Sometimes, several substances present in the flavonols form hydrogen peroxide ( Vickers, 2017 ), which negatively affects the growth of gram-positive bacteria.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Some metabolites generated from the gut microbiome, including butyrate, propionate, and SCFAs such as acetate, have shown critical roles in human health and are considerably associated with kidney diseases, hypertension, and inflammatory bowel diseases [ 13 , 14 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As shown in  Figure 6 c, the C 0  curve of NCMO is closer to a straight line, and the value of C 0  does not change significantly with increasing frequency, indicating that eddy current loss is the dominant mechanism of magnetic loss [ 38 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This is a collaborative decisionmaking technique that allows stakeholders and experts to vote on priorities and rank desired items in order of priority from highest to lowest. This study also points out that building sustainability assessment tools can vary by country and geographic area  65 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Polymeric NPs are organic NPs, which are either matrix particles—that are generally solid which can adhere to molecules to be transported—or are encapsulated within the particle [ 24 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Oleic acid promotes an amorphous form in the solid lipid matrix, which lowers particle crystallinity and results in a high encapsulation efficiency [ 237 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
After measuring the initial saturated weights of the specimens, they were then kept in solutions containing 5% sulfuric acid, 5% magnesium sulfate, and 3.5% sodium chloride for 28 days [ 15 ,  33 ,  36 ,  42 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Studies on sea urchin have highlighted their role in spine and tube foot regeneration, indicating multipotent progenitor cells’ involvement ( Juliano et al., 2006 ;  Karthik et al., 2014 ;  Bodnar and Coffman, 2016 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Across diverse organisms, Piwi influences regeneration, with examples in Botrylloides species and cnidarians like jellyfish ( Seipel et al., 2003 ;  Juliano et al., 2006 ;  Frank et al., 2009 ;  Alié et al., 2011 ;  Brown and Swalla, 2012 ;  Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
AD, the most common neurodegenerative disease, comprises several pathogenic mechanisms that contribute to its development ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Vickers (2017)  and  Lai et al. (2021)  found that tourists’ memories of past pleasant travel experiences can enhance their emotions toward a destination.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, The vibrations detected at 443 cm −1  and 535 cm −1  represent Cu–O stretching and vibration modes specific to the monoclinic phase of CuO 36 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The results demonstrate that the particulate concentration factor has a dual influence on velocity distribution. Some recent studies are found in  14 , 15 , 16 , 17 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, the authors used their temporal features to test the multivariate long short-term memory fully convolutional network method  [206] .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There is evidence showing thrombotic abnormalities, in addition to abnormalities in the function of various organs in patients with COVID-19 [ 15 ], which lead to higher mortality.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is important to note that fractional order derivatives and integrals play an important role in epidemiological modelling and other real-world problems  [40] ,  [41] ,  [42] ,  [43] ,  [44] ,  [45] ,  [46] ,  [47] ,  [48] ,  [49] ,  [50] ,  [51]  because they capture the memory effect and other nonlocal properties.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Environmentally, fossil-based products influence the health of living species and contribute to air, soil, and water pollution [ 9 , 11 , 12 , 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Out of the proteins of COVID-19, the spike protein made of S-Glycoprotein has binding capability with human angiotensin converting enzyme 2 (ACE 2 ) [ [20] ,  [21] ] opening the development of biosensor using ACE 2  for efficient and rapid virus detection.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
ICP-MS [ 9 ], GC [ 10 ], HPLC [ 11 , 12 ], and CE [ 13 ], coprecipitation [ 14 ], and solvent extraction techniques (liquid-liquid extraction and dispersive liquid-solid phase sorbent-based extraction).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
However, recent studies have revealed that methylated arsenic species can induce DNA damage, chromosomal aberrations, and tumor promotion in rodents [ 13 , 14 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The high adhesive probability predicted for NP would mean it has a high level of adhesion, which has been reported to play a vital role in enabling virus entry and adherence to the host cell 101 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Despite this, the usage of ChatGPT in the medical industry has a lot of potential. Future research should thoroughly document its functionality, distribution strategies, and theoretical underpinnings (Payton et al.,  2017 ).	Firearm violence remains a significant problem in the US (with 2787 adolescents killed in 2015). However, the research on school firearm violence prevention practices and policies is scant. Parents are major stakeholders in relation to firearm violence by youths and school safety in general. The purpose of this study was to examine what parents thought schools should be doing to reduce the risk of firearm violence in schools. A valid and Related questionnaire was mailed to a national random sample of 600 parents who had at least one child enrolled in a public secondary school (response rate = 47%). Parents perceived inadequate parental monitoring/rearing practices (73%), peer harassment and/or bullying (58%), inadequate mental health care services for youth (54%), and easy access to guns (51%) as major causes of firearm violence in schools. The school policies perceived to be most effective in reducing firearm violence were installing an alert system in schools (70%), working with law enforcement to design an emergency response plan (70%), creating a comprehensive security plan (68%), requiring criminal background checks for all school personnel prior to hiring (67%), and implementing an anonymous system for students to report peer concerns regarding potential violence (67%). Parents seem to have a limited grasp of potentially effective interventions to reduce firearm violence.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
Therefore, secondary (bioreactor, activated sludge, and aerated biological processes) and tertiary (chlorination, performic acid, ultraviolet radiations, ozonation, and nanomaterials) treatments should be employed where necessary ( Vickers, 2017 ;  Gerba and Pepper, 2019 ;  Randazzo et al., 2020a ,  2020b ;  Teymoorian et al., 2021 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This homeostasis is crucial not only for the expression of metallozymes ( Hood and Skaar, 2012 ) and other proteins related to bacterial metabolism but also for the adequate expression of virulent factors to cause infection ( Waldron and Robinson, 2009 ;  Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
High levels of iron in the lung are linked to a high risk of pulmonary injury  55 . Both acute and chronic lung injury disrupts the iron regulatory state in the lungs  56 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Hyperferritinemia syndromes characterize many autoimmune diseases  61  and because of its immunomodulatory effects possibly play a pathogenic role. High circulating ferritin displays an acute phase response and is crucial to inflammation  56 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
By applying a foliar spray of JA, the oxidative stress was alleviated and other harmful impacts were also increased because, due to the application of JA, the root growth was stunted and leaf and chlorophyll synthesis was also decreased in some plants [ 58 , 97 , 98 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The activation of the sympathoadrenal (SPA) and hypothalamic–pituitary–adrenal (HPA) axes under preslaughter stress releases catecholamines (noradrenaline/norepinephrine) secretion, which leads to various physiological and behavioral responses to increased energy demand needed for “flight or fight responses” ( 7 ,  8 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As for dye analysis, UV-Vis spectrophotometry is among the most commonly used analytical techniques, as it is relatively robust, rapid, cheap, accurate, and precise compared to the other mentioned techniques [ 18 , 19 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For instance, Starr et al. recommended 6 months of age [ 5 ] and several other studies suggested the age of 3 to 11 months for the TFTC surgery [ 6 ,  7 ,  6 ,  8 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
One way cancer cells impact immune suppression is by increasing the number of immunosuppressive tumor-infiltrating lymphocytes in the TIME ( 215 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The fastest growth is always in the direction parallel to the thermal gradient at the solidification front, which is normal to the rear melt-pool surface [ 27 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Antimicrobial resistance (AMR) is a phenomenon whereby microorganisms develop a variety of strategies to combat medications designed to kill them, resulting in microbes that are resistant to treatment protocols [ 6 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
By giving this result, SHCC is the solving long awaited solution to reinforced concrete construction and this is because of its high tensile ductility and compact crack width [ 5 , 6 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
On the other hand, for all the three developed models, water to binder ratio, flyash percentage, and superplasticizer percentage are the least contributing factor. This also seems correct in view of material engineering and in line with the previous work  4 ,  5 ,  6 , 92 , 93 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Furthermore, there have been instances where  C. coli  was the dominant or only species identified (Silva et al.,  2011 ; Vickers,  2017 ; Wei et al.,  2014 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In some countries, slaughter and butcheries waste is released into the environment [ 109 , 110 , 111 ], untreated animal waste serves as fertilizer [ 102 ], and surface water is a shared resource between humans and animals [ 112 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Hexadecanoic acid, a saturated fatty acid, is found in plants, animals, and microbes  33 . Germacrene D is a sesquiterpene pioneer of cadences and selinenes  34 ,  35 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In particular, autophagy is activated during differentiation to perform necessary cellular remodeling, including protein turnover, lysosomal degradation of organelles, and to ensure the quality of intracellular proteins and organelles [ 10 , 15 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Table 1 Oligonucleotide primer sequences used in this study Target genes Nucleotide sequence (5′ → 3′) Amplicon size (bp) Annealing temperature ( ◦ C) References Bacterial confrimatin and typing   Kmt1 F: ATCCGCTATTTACCCAGTGG 460 55 Townsend et al. ( 1998 ) R: GCTGTAAACGAACTCGCCAC   hyaD-hyaC (Serogroup B) F: TGCCAAAATCGCAGTGAG 1044 55 Townsend et al. ( 2001 ) R: TTGCCATCATTGTCAGTG   BcbD (Serogroup C) F: CATTTATCCAAGCTCCACC 760 55 Townsend et al. ( 2001 ) R: GCCCGAGAGTTTCAATCC   DcbF (Serogroup D) F: TACAAAAGAAAGACTAGGAGCCC 657 55 Townsend et al. ( 2001 ) R: CATCTACCCACTCAACCATATCAG   EcbJ (Serogroup E) F: TCCGCAGAAAATTATTGACTC 511 55 Townsend et al. ( 2001 ) R: GCTTGCTGCTTGATTTTGTC   FcbD (Serogroup F) F: AATCGGAGAACGCAGAAATCAG 851 55 Townsend et al. ( 2001 ) R: TTCCGCCGTCAATTACTCTG Virulence genes   pfh A F: TTCAGAGGGATCAATCTTCG 286 55 Tang et al. ( 2009 ) R: AACTCCAGT TGGTTTGTCG   ptf A F: TGTGGAATTCAGCATTTTAGTGTGTC 468 55 Tang et al. ( 2009 ) R: TCATGAATTCTTATGCGCAAAATCCTGCTGG fim A F: CCATCGGATCTAAACGACCTA 866 55 Tang et al. ( 2009 ) R: AGTATTAGTTCCTGCGGGTG   exb B F: TTGGCTTGTGATTGAACGC 291 55 Tang et al. ( 2009 ) R: TGCAGGAATGGCGACTAA A   pm HAS F: TCAATGTTTGCGATAGTCCGTTAG 430 60 Tang et al. ( 2009 ) R: TGGCGAATGATCGGTGATAGA   tox A F: CTTAGATGAGCGACAAGG 864 55 Liu et al. ( 2017 ) R: GAATGCCACACCTCTATAG   omp A F: CGCATAGCACTCAAGTTTCTCC 201 60 Tang et al. ( 2009 ) R: CATAAACAGATTGACCGAAACG   omp H F: CGCGTATGAAGGTTTAGGT 438 55 Tang et al. ( 2009 ) R: TTTAGATTGTGCGTAGTCAAC   sod A F: TACCAGAATTAGGCTACGC 361 60 Vickers ( 2017 ) R: GAAACGGGTTGCTGCCGCT′   sod C F: AGTTAGTAGCGGGGTTGGCA 253 60 Vickers ( 2017 ) R: TGGTGCTGGGTGATCATCATG   nan H F: CACTGCCTTATAGCCGTATTCC 964 60 Vickers ( 2017 ) R: AGCACTGTTACCCGAACCC   hgb A F: TGGCGGATAGTCATCAAG 420 60 Vickers ( 2017 ) R: CCAAAGAACCACTACCCA   oma 87 F: ATGAAAAAACTTTTAATTGCGAGC 984 60 Vickers ( 2017 ) R: TGACTTGCGCAGTTGCATAAC Resistance genes   erm X F: TCCTTACCAGTGCCCTTATCC 390 65 Rosato et al. ( 2001 ) R: GAGTTCCAGCGCATCACC   dfr A1 F: CTCACGATAAACAAAGAGTCA 201 50 Abdolmaleki et al. ( 2019 ) R: CAATCATTGCTTCGTATAACG   mcr 1 F: CGGTCAGTCCGTTTGTTC 305 60 Zou et al. ( 2017 ) R: CTTGGTCGGTCTGTAGGG   sul 1 F: CGG CGT GGG CTA CCT GAA CG 433 50 Heuer and Smalla ( 2007 ) R: GCC GAT CGC GTG AAG TTC CG   bla ROB-1 F: AATAACCCTTGCCCCAATTC 685 60 Klima et al. ( 2014 ) R: TCGCTTATCAGGTGTGCTTG   tet H F: ATACTGCTGATCACCGT 1076 60 Klima et al. ( 2014 ) R: TCCCAATAAGCGACGCT	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As the smallest EVs, exosomes possess unique characteristics over other nanocarriers, including immunomodulatory effects, biodegradability, longer circulatory half-life, and permeability across the blood-brain barrier (BBB) [ 27 ,  31 ,  32 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These findings are consistent with all those outlined in the literature for Bac. 58  FT-IR spectra of plain F4 and F4 were similarly representative of no interaction that took place between Bac and excipients. 59	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These data confirm the concept that the NISNV formula can reduce the crystallinity of pure Bac while maintaining its amorphous character. 60  As an overall conclusion, F4 showed more diffused peaks indicative of the amorphous form of the entrapped Bac in vesicles. 59 , 61  Previously, similar outcomes had been stated for different drugs loaded into NISNV. 62	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Tick infestation can be effectively controlled by preventing tick adhesion to hosts through the use of repellent compounds [ 34 , 67 , 68 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
However, in some studies, this correlation has been negated by providing the reason that, due to heavy rainfall, mosquito breeding sites are being removed [ 50 , 51 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Antioxidant enzymes such as GSH-Px, CAT, and SOD are considered main defense lines against the generation of toxic ROS leading to direct detoxification [ 83 , 84 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Recently, molecular docking was employed by many researchers as an effective tool to stimulate the potential interactions between two entities, thus guiding the design of effective therapeutic derivatives through the evaluation of its binding affinities with various enzymes [ 44 , 45 , 46 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Compound 1  22  was stirred in a solution of hydrogen chloride in ethyl acetate and methanol for 1h at 25C.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Besides, systemic delivery of DC-derived EVs could mediate CD4 +  T cell activation and improve cardiac function post-myocardial infarction in mice [ 140 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Root exudates contain a wide range of organic compounds including organic acids, sugars, and many other metabolites, which are secreted by plants into the rhizosphere ( Baetz and Martinoia, 2014 ;  Preece and Peñuelas, 2020 ;  Vives-Peris et al., 2020 )  via  diffusion, ion channel pumping, and vesicle transport ( Vickers, 2017 ;  Demidchik, 2018 ;  He et al., 2021 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The rapid electron transfers between the transducer and analyte molecules are considered “electronic wires” and “electrocatalysts” given the nanoscale and structure of metal nanoparticles, [ 99 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Here, the DNA and protein interactions (sequence-specific and sequence non-specific binding) are involved in many biological processes including regulation of transcription, DNA repair, DNA modification, etc. [ 54 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, this design illustrated a MoS2-based PENG device attached to a person’s body for the purpose of harvesting a variety of body motion energy, showcasing its strong potential for use in wearable technology [ 93 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Self−powered wearable sensors based on PENG: ( a ) The design principal of self−power sensor based on flexible piezoelectric nanogenerator [ 93 ]. ( b ) Structure of biomedical sensor based on ultraflexible piezoelectric energy harvesting and sensing device [ 94 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
FMD is not an endemic disease; it is occasionally seen in the Eastern region and spreads to other parts of the country that are FMD-free ( 55 ,  56 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is possible for filamentous fungi to produce a wide range of pigment colors, including yellow, orange, red, brown, chestnut, and bronze [ 5 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These peptides are generated by sequences of signal peptides containing residues of nonpolar peptides such as prenylates ( Vickers, 2017 ), pepducins ( Covic et al., 2002 ), and staples ( Lau et al., 2015 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Vaccine hesitancy is not a new phenomenon limited to COVID-19 vaccines; studies from Africa have noted a worrying trend in vaccine hesitancy in recent years [ 10 , 42 , 43 , 44 , 45 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is becoming more and more crucial to regularly check the levels of patulin present in colored fruits, such as hawthorns, red grapes, plums, sour cherries, as well as various types of berries like strawberries, raspberries, blueberries, and blackberries ( Vaclavikova et al., 2015 ;  Vickers, 2017 ;  Iqbal et al., 2018 ;  Wu et al., 2019 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
According to Vickers study on apple juice, the LODs in this test was found to be 1.54 gL. It should be noted that patulin was not detected prior to the sampling process  Vickers, 2017 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In Japan, a study showed that non-permanent workers were much more likely to be bullied [ 42 ], and in the Finnish study, non-permanent work was a risk factor for depressive disorder and delayed return to work [ 43 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
When an investor holds a large volume of capitalized stocks, indicates the economys unfavorable state may be low, unstable, and depressed. Furthermore,  56  suggested that small firms have a likelihood of failing than big firms because large firms have better market experience than small firms, with limited resources and finance. Previous researchers  37 ,  57  used a log of the total asset to measure bank size.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Financially distressed organizations frequently suffer from vast debt burdens characterized by highinterest payments. While  25 ,  38 ,  57 ,  65 ,  66  rivaled that leverage has negative relation with financial distress.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The random effect model regression coefficient of liquidity positive and significant relation to financial distress; the result of the study is consistent [ 39 ,  57 ,  63 ,  65 ,  66 ], indicate that liquidity has a positive link with financial distress.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Small firms need better market experience, connections, and financial resources. On the other hand, a study conducted by  38 ,  57  concluded that firm size does not determine financial distress.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There are almost 250,000 metabolites in plants the concentration and total number are considerably higher in stressed than nonstressed environments  60 . The detection of valid metabolomic markers will enhance stress tolerance in plants  59 , 61 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Recently some studies have found abnormal macrophage polarization and dysregulation of immune cum inflammatory response which causes delayed wound healing in burn wounds [ 13 , 14 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The decellularized matrix was lyophilized followed by ultrasonic cavitation as per the method of Badylak et al. [ 13 ] ( Fig. 1 ,  Fig. 2 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Subsequently, the cells were examined under a fluorescent microscope to determine the percentage of live and dead cells [ 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In addition to this Glycosaminoglycan (GAG) levels in the fabricated graft of the present study were found to be above the threshold level,  GAG  provides an essential micro-environmental condition conducive for cell-to-cell contact, migration of cells, and transformation of cells which helps in providing micro-environmental conditions that mimic actual subcellular conditions [ [12] ,  [13] ,  [14] ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
A previous study has reported that lutein nanocrystals (nanosuspension) display remarkably greater saturation solubility compared to the lutein microcrystals (coarse powder and the nanocrystals had improved skin penetration ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Various organs such as the heart, musculoskeletal system, lung, GI tract, kidneys, and brain may be responsible for the long-term manifestations ( Vickers, 2017 ;  Divani et al., 2020 ;  Andalib et al., 2021 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Furthermore, it has been demonstrated that polyamines are attributed to being involved in maintaining membranes shielding from damage under stressful environments [ 114 ] and controlling the formation of nucleic acid as well as enzyme activity [ 115 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The stimulation of macrophages with LPS causes secretion of EVs enriched in cytokines and miRNA, which promote inflammation through activation of NF-κB pathways in naive immune cells ( 97 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This reduces mitochondrial membrane potential (ΔΨm) and the release of cytochrome C from mitochondria to the cytosol [ 51 , 52 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
COG seems to have antitumor effects and antioxidant, and could be employed as an antiosteoporotic herbal agent, according to latest research ( Ramchandani et al., 2014 ;  Vickers, 2017 ;  Hejazi et al., 2018 ;  Cui et al., 2019 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The MAPK pathway or its downstream effectors contribute to carcinogenesis and proliferation in many types of malignancies. They can be activated in juvenile gliomas as a result of NF1 and BRAF gene alterations  57 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These pharmaceutical-activated compounds sustain harmful impacts on marine life as they are resistant to microbial degradation [ 112 , 113 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Sphingosine-1-phosphate (S1P), a signaling lipid, has significant regulatory roles in the body, such as proliferation, survival and migration of cells, inflammation, vascular permeability, and immune response through five subfamilies of the receptors ( 5 6 7 8 9 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In these studies, mostly the jute, banana fiber, curaua fibers, and sisal were mixed with different weight ratios, and the epoxy resin with a different kind of hardener was used as the matrix. They could improve the impact resistance of the structures and reduce the material density by around 25.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
While salt content affects the energy requirements for membrane processes, salt concentration has no impact on the energy needs for thermal desalination systems [ 7 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Compared to ceramic membranes, membranes which are made of polymers show a lower heat conductivity (0.1–0.5 Wm1 K1) [ 7 , 98 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In addition, the odds of virological unsuppressed was significantly higher among adults living with HIVTB coinfection when compared with adults living with HIV only. This finding is in line with studies conducted in Uganda and South Africa  44 ,  45 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The p53 targets are sestrins that promote the peroxiredoxins activity, upregulating the cellular levels of antioxidants [ 98 , 99 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Some plants, such as garlic, inhibit the enzyme caspase 3 and cytochrome P450 2E1 (CYP2E1), which have a toxic effect on the testes, reducing testicular function and improving spermatogenesis by reducing these two enzymes ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This finding is similar to the study done in Southwest Nigeria, Egypt, and Ethiopia that stated a significant association between years of study and self-medication ( Osemene and Lamikanra 2012 ;  Karthik et al., 2014 ;  Helal and Abou-ElWafa 2017 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
However, a previous study was done in Serbia reported that the high level of parents’ education was independently associated with self-medication practice ( Karthik et al., 2014 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Incorrect	Unrelated	Unrelated	Incorrect	Unrelated
So far, various colorimetric chemosensors have been developed for MOR detection and, among them, chiral colloidal CdSe quantum dots (CdSe-QDs) functionalized with l- and d-cysteine [ 11 ], antimorphine-functionalized graphene quantum dots (GQDs) [ 12 ], CdS quantum dots functionalized by antimorphine antibody [ 13 ], citrate-capped gold nanoparticles (AuNPs) [ 14 ], and melamine modified gold nanoparticles (MA-AuNPs) [ 15 ] have been reported.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Although its advantageous effect, there are many reports on gonadotoxicity [ 3 ,  9 ] cytotoxicity [ 10 ], and genotoxicity impact of HDU [ 11 ,  12 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In addition, the mean density % of cresyl violet stain [ 80 ] and the area percentage of both the GFAP and CD68 immune expression were evaluated [ 81 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The biosafety of aluminum content presented in AZxx and AMxx alloys is arguable as aluminum adversely affects osteoblasts and is reported to be neurotoxic [ 18 – 20 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
From this perspective e, the perception that the climate is changing can be seen as a prerequisite for adopting farming adaptation procedures [ 51 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The existence of viable form of this virus in water and wastewater may be associated with issues for providing public health and difficulties in implementation of pandemic control strategies (Vickers  2017 ); this situation can be exacerbated in developing countries that do not have adequate access to sanitation and safe water.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Similarly, other studies ( Vickers, 2017 ;  Wu et al., 2019b ) have indicated that additional barcode primers, including matK, rbcL, and trnL-trnF, have demonstrated successful amplification within coffee species.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Therefore, the plant extract may have a high antioxidant ability that may support the protection of beta cells from harmful oxidative stress and other damaging factors [ 47 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Although nano emulsions are created by a uniform thin wrapper enclosing a hydrocarbon chamber, nanomaterials are a robust elastomeric matrix created using micro-emulsion polymer ( Figure 5 ) ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The inclusion of hydrophobic toxic compounds is this programs key benefit. The polymeric nanoparticles have sizes ranging between 10 and 100 nanometric scale  Vickers, 2017 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Vickers [ 45 ] also reported the retention of Cd on the surface of organic waste and demonstrated that this sorption is not an instantaneously reversible process.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Different mental, environmental, and genetic factors are involved in the development of the disease  131 , 132 . In China, the prevalence rate of migraine was about 9.3 in 2012  132 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The limitations of employing plants for AuNP formation is that it is difficult to identify reactive components since plant biomass contains a wide range of organic components [ 84 , 85 , 86 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
As a result of p70’s activation of ribosome biosynthesis by phosphorylating the ribosomal S6 protein, mRNA translation is increased [ 73 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Similarly, Graven and Grant reported that self-confidence, which is significantly diminished in depression, does not only function as an independent but also a mediator factor for self-care in HF patients and that its reinforcement promotes self-care commitment behaviors while helping to treat depression [ 33 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The activation of CAT on the first day of exposure might be due to the conversion of superoxide anions to H 2 O 2  and play a role in the detoxification of reactive oxygen species (ROS) in  L. japonica  under amantadine exposure ( Karthik et al., 2014 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
The types of fat consumption have been investigated and discussed as a factor in cardiovascular ( 62 ), as part of nutritional planning, and for the increase of insulin sensitivity ( 63 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Previous investigations proved that AgNPs doesn’t only accumulate in the brain but also induce particular degree of tissue damage  [38] .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These strains are sporeforming bacteria that are resistant to high temperatures and acidity  24 . B. coagulans  can survive in gastrointestinal tract conditions better than other probiotic microorganisms  25 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
B cells are found at lower levels in GC than in gastritis, indicating an immune response suppression mechanism that may be facilitated by the microbiome [ 104 , 105 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Moreover, multiple myths and conspiracy theories about vaccines and COVID-19 [ 5 ,  31 ,  32 ] would also potentially affect the COVID-19 vaccine acceptance.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Amlodipine treatment could change the levels of MDA and TAC to the extent of clinical significance defined for this trial in the context of offlabel use of the drug. In the current study, no adverse side effects were detected in parallel to other studies.  26  ,   27	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Concerns regarding the safety of SARS-CoV-2 vaccines have arisen due to reported incidents of thromboembolic events, hypersensitivity reactions, and tachycardia post-vaccination [ 43 ,  59 ,  60 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This is in line with a study conducted in Italy  5 , USA  63 , and Poland  64 . Another study reported that poor eating, inactivity, and binge eating raise BMI  65  68 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Dendritic cells (DCs) are known as the most potent antigen-presenting cells (APCs) in the body (Vickers  2017 ), which play an essential role in the acquisition, processing, transportation, and presentation of various antigens.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Viruslike particles are multiprotein structures that cannot replicate due to their lack of a viral genome  273 ,  274 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Moreover, plant nutrients may also be entrapped within superabsorbent matrices for their controlled release, hindering the water losses due to evaporation, and reducing the irrigation [ 379 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Likewise, ExCell miRDB is a specialized database containing curated information related to DEmiRs in biofluids  31 . Similarly, miRandola gives detailed information regarding several circulating extracellular miRNAs  32 ,  33 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Honey’s antiviral activity is frequently linked to anti-oxidant, anti-inflammatory, anti-resistance, and anti-apoptotic activities through altering cellular signaling pathways such as MAPK, NF-kB, Nrf2, and others ( Docherty et al., 2004 ;  Shakibaei et al., 2011a ;  Shakibaei et al., 2011b ;  Vickers, 2017 ;  Buhrmann et al., 2019 ;  Buhrmann et al., 2020 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The level of agreement was classified as follows: fair agreement [0.40–0.59], good agreement [0.60–0.74], and excellent agreement [> 0.75] ( Sim and Wright, 2005 ;  Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This finding indicates that this process takes place independently of the hypoxiainducible factor HIF  8 ,  39 . A growing body of studies links the ERK12 signaling pathway to angiogenesis  45 ,  64 ,  75 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Blood collection in rats is a common procedure in many research studies that require blood samples for analysis (e.g., cytokines and growth factors) or for blood concentrate production [ 40 , 41 , 42 , 43 , 44 , 45 , 46 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Studying the LST of an area could supply useful information on the human survival of such an area 15 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Doping of many elements also changed the phase transition path of NiTi alloy; for example, the doping of the Fe element caused the appearance of the R phase [ 38 ], and the doping of the Cu element caused the appearance of the B19 phase (martensite phase, rhomboidal structure) [ 39 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Among recurrent stroke patients, the mortality rate is approximately 56%, which is much higher than for initial stroke patients. Prior studies have indicated that up to 43% of initial stroke victims are at risk of stroke recurrence within five years. Consequently, preventing recurrence is an essential strategy for diminishing the mortality rate of this severe illness. 2 ,  3 ,  4  Thirteen systematic reviews indicated that self-management interventions (e.g., telephone calls, behavior therapy, and dissemination of informational materials relating to adherence) among chronic disease patients could significantly improve their survival rate, level of independence, and death rate. 5 , 6  However, there are few studies of self-management interventions for stroke patients. In our literature review, we found that among five meta-analyses and systematic reviews that included 217 innovative studies, there were no reported studies in which stroke patients were participants. 7 ,  8 ,  9 ,  10  Though there is one systematic review investigating the effects of self-management interventions in people with stroke, 11  the target population of which were exclusively those living in the community, and focus mainly on the effects on this population's quality of life.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
This goal can be met by using diamond as a high-performance substrate for the culture of non-differentiated cells, such as neural stem cells (NSCs) [ 5 , 6 ], human-induced pluripotent stem cells (IPS) and IPS-derived neuronal progenitors [ 7 ], various types of neurons [ 8 , 9 , 10 ], cells from the connective tissue such as fibroblasts [ 11 , 12 , 13 , 14 ] and osteoblasts [ 15 , 16 , 17 ], epithelial cells [ 18 ], among others [ 1 ]; and by using it to create biointerfaces [ 19 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Diamond has been used for other biomedical applications such as biosensors [ 4 , 7 , 42 , 43 ] and biointerfaces, also because of its mechanical strength and wear resistance.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
A global survey among healthcare workers and the public showed a great sense of loss of control of becoming infected and dying, more so infecting their loved ones with COVID-19 [ 16 ], although fear can also be attributed to decreased physical and environmental well-being [ 17 ,  18 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Moreover, many more factors like post-natal vitamin A supplementation 24 , acute infections like diarrhea and malaria 17 , 25 – 27 , child spacing 10 , maternal knowledge and awareness about infant and young child feeding 14  iodized salt use 26 , water sources 10 , low birth weight 21 , birth order 28 , bottle feeding 29 , inappropriate initiation time of complementary feeding 15 , 20 , age of the child 27 , 30 , and low (< 45 kg) maternal weight 21  are also behind the occurrence of acute malnutrition.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Diarrhea was determined when the child had three or more loose or watery stool in a 24-h’s period within the 2 weeks prior to the survey and this was assessed by asking the mothers or care takers 30 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The odds of being severe acute malnutrition were higher among younger children aged 611 and 1217months compared to older children 1823 and23months. This finding is in line with studies done in Ethiopia 30 , Niger 39 , and Ghana 28 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The finding of our study is also in agreement with studies done in Ethiopia 30 – 32  which indicates larger family size (≥ 4 or 5) was determining factor for wasting among under-five children.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Animal studies have shown that sugar could increase depression incidence by activating the hypothalamic–pituitary–adrenal (HPA) axis and inducing elevation in glucocorticoids [ 60 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Studies have reported significant changes in the thickness of the maxillary alveolar bone following alterations in the inclination of the maxillary incisors [ 27 , 28 , 29 , 30 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Janurio et al.  38  reported about 1.6 and 3 mm, which are also close to the findings of our study. However, other studies have reported a high variation in CEJbone crest 0.8 to 7.2 mm  28 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
A study conducted in Dessie and Kombolcha revealed that the majority of the drivers were using cloth face masks, followed by N95, 37  while in the current study surgical mask was the commonest type of mask used. Similarly, the majority of the respondents were wearing medical face masks in Malaysia, 43 and Hong Kong. 44 This could be attributed to the knowledge differences among drivers in the 2 study areas and/or affordability issues because there are price differences on face masks in different areas in the country.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Silencing of  BgFtz-f1  prevents normal molting and development of  B. germanica  [ 17 ], and  Ftz-f1  is involved in the regulation of  Leptinotarsa decemlineata ’s pupation by regulating 20E and JH titers [ 18 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Modified mRNAs encoding IGF-1 [ 115 ], VEGF-A [ 116 ], and mutant FSTL1 [ 117 ] have undergone preclinical studies, and there are two ongoing clinical trials for VEGF-A (NCT03370887, NCT02935712).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Current evidence suggests that microbes-derived metabolites are capable of regulating enteric neuron functions, such as the excitability of enteric nerve endings, affecting the endocrine as well as immune pathways in an indirect manner and consequently interacting with the CNS ( Vickers, 2017 ;  Agirman and Hsiao, 2021 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
With efficient mechanical and thermal resistance, CA is utilized in diverse manufacturing practices, including syringe filters, photographic films, and cigarette filters (CF) [ 6 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Other factors, such as IL-6, can participate in the regulation of cellular and humoral immune responses [ 35 , [37] ,  [38] ,  [39] ,  [40] ,  [41] ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Recently, considerable computational efforts have been implicated in appraising the functional and structural outcomes resulted from novel sequence variations. A growing number of in silico tools appeared beneficial, identifying deleterious mutations related to diseases 23 , 24 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Thus elevated 5-HT interacts with the postsynaptic serotonin receptors for synthesizing BDNF by various cellular pathways [ 85 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Nevertheless, screen time and its effects on children’s mental health are contentious, primarily due to conflicting research findings and methodological limitations.[ 19 20 21 ] In the ongoing discourse, there exists a divide among experts regarding the advisability of limiting screen exposure in young children.[ 22 23 ] Conversely, an opposing viewpoint contends that restricted screen time and other activities like studying and physical exercise can yield certain advantages.[ 24 25 26 ] Conversely, children’s use of social networks has been associated with increased introversion and behavioral problems, including higher-than-normal levels of depression, attention deficits, and anxiety.[ 27 ]	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There is a significant amount of evidence that suggests Ca 2+  mediated signaling and tune CAMTAs, which are involved in the transmission of stress cues, such as light [ 40 , 41 ], temperature [ 42 ], salt [ 43 ], cold [ 44 ], oxidative signals [ 45 ], reactive oxygen species (ROS) [ 46 ], ET as a hormone signals [ 47 ], abscisic acid (ABA) [ 48 ], gibberellins (GA) [ 49 ], and auxins (IAA) [ 50 ], play a crucial part in plants under cold/low temperatures ( Table 1 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
While CNV can explain many observed changes in CRG expression, it is not the only factor involved in expression level regulation  12 . Other factors that can influence gene expression include DNA methylation and transcription factors  13 ,  14 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This potential endophenotype for SCH is visual backward masking (VBM) ( 16 ) particularly the shine-through approach, and has a far greater sensitivity than most of the other cognitive and perceptual processes ( 17 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Microstate B was related to the visual network, according to previous studies ( 37 ,  61 ,  67 ), and imagination associated with the awareness of situational personal memory, that is, the mental visualization of the situation ( 68 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally,  B. pseudocatenulatum  is commonly found in human faecal samples throughout their lifetime, where some of these strains have shown beneficial properties, such as the production of enterolignan, urolithin, and conjugated linoleic acid ( Yang et al., 2017b ;  Vickers, 2017 ;  Gaya et al., 2018 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Common clinical workflows consist of preoperative computed tomography (CT) imaging for stereotactic guidance as well as postoperative CT-imaging for rule out of bleeding and determination of the electrode orientation while the visualization of tiny orientation markers of the electrode is a domain of intraoperative X-ray imaging (Schmidt et al.  2022 ; Vickers  2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Honey and its main components could combat against Herpes zoster, 38  rubella, 39  influenza, 40  herpes disease, 41  respiratory syncytial virus, 42  AIDS, 43  immunodeficiency virus, 44  viral hepatitis, 45  gingivostomatitis, 46  rabies, 47  rhinoconjunctivitis 48  and COVID-19. 49  The mechanisms of anti-viral properties of honey and its main components is very vast and unknown. 42  The anti-viral activity of honey and its main components is usually associated, similar to other natural products like resveratrol, calebin A or curcumin with anti-oxidant, anti-inflammatory, anti-resistance and anti-apoptotic effects by modulating cellular signaling pathways such as MAPK, NF-κB, Nrf2, etc. 50 - 58  In addition, these agents also have direct effect on the structure of the virus, such as the interaction of honey and its major components with structural and/or non-structural proteins in the virus or binding to target receptors on the virus. 58	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Healthcare providers need to know how to work cancer screening into policies and procedures. Otherwise, it is not easy, and they will need more time for shared decisionmaking processes  70 , 75 .	Firearm violence remains a significant problem in the US (with 2787 adolescents killed in 2015). However, the research on school firearm violence prevention practices and policies is scant. Parents are major stakeholders in relation to firearm violence by youths and school safety in general. The purpose of this study was to examine what parents thought schools should be doing to reduce the risk of firearm violence in schools. A valid and Related questionnaire was mailed to a national random sample of 600 parents who had at least one child enrolled in a public secondary school (response rate = 47%). Parents perceived inadequate parental monitoring/rearing practices (73%), peer harassment and/or bullying (58%), inadequate mental health care services for youth (54%), and easy access to guns (51%) as major causes of firearm violence in schools. The school policies perceived to be most effective in reducing firearm violence were installing an alert system in schools (70%), working with law enforcement to design an emergency response plan (70%), creating a comprehensive security plan (68%), requiring criminal background checks for all school personnel prior to hiring (67%), and implementing an anonymous system for students to report peer concerns regarding potential violence (67%). Parents seem to have a limited grasp of potentially effective interventions to reduce firearm violence.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Incorrect
In this regard, in addition to the economic effects caused by the tense labor market, the health measures required to deal with the COVID-19 disease have led to significant changes in the environments in which children grow and develop ( 4 ,  7 – 9 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The major stimulant is to relying on passive uptake through the enhanced permeability and retention EPR effect  63 . This has been attained, for example, with antibodies  64 , peptides  65 , folates  66 , and aptamers  67 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These limitations suggest a pathway for the development of biosensors for cancer biomarker detection which are capable of showing a selective and sensitive immunological response. 7,8  This is beneficial as it reduces the cost of diagnosis and helps in fundamental understanding of cancer at an early stage. 9  In this context, extensive research has been carried out in recent years on point-of-care (POC) devices for the detection of oral cancer biomarkers. 8  Researchers in these fields tend to develop biosensing devices which combine an antibody as a recognition element with a transducer that converts an electrical signal into a measurable signal generated due to the interaction between the antigen and the antibody. 10,11	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Lopinavir LPV is usually combined with ritonavir RTV to increase and Polygonum and it is also a virucidal agent  105 . Emodin could significantly block the interaction between the S protein of SARSCoV and ACE2  106 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It has been recommended to breathe H 2  gas (66.6% H 2 ) mixed with O 2  (33.3% O 2 ), since H 2  plays a vital role in preventing lung function loss and emphysema and other lung conditions, according to a publication issued about the prevention of COVID-19 by the Health Commission of China of Clinical Guidance for Pneumonia Treatment [ 7 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
According to compiled research, 10% of children aged 2 to 11 have one or more COVID-19-related persistent symptoms, and this number rises to 13% for adolescents aged 12 to 16 years old [ 7 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Cell apoptosis and autophagy were also considerably increased in the cell lines of H1975 and A549 of lung cancer cured by using various doses of H 2  gas [ 82 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Image-activated cell sorting (IACS) appears to become the most advanced technology for measuring the visual, electrical, and mechanical characteristics of cells [ 132 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Cell membrane mucin-1 (MUC1), a product of aberrant glycoform expression, is one of the big-sized proteins capable of transferring O-glycan proteins that are over-expressed by a large number of adenocarcinomas ( 70 ,  71 ). mAb (5E5)-based CARs can select MUC1 glycopeptide epitope as a target and potentially kill pancreatic tumors (NCT02587689) ( 72 ,  73 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Moreover, pulmonary edema prevents gas exchange and reduces carbon dioxide removal resulting in hypoxemia and acute respiratory failure  164 . Approximately 20 of the infected patients developed hypoxia and ARDS leading to multiple organ failures  165 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Furthermore, virus can interact with kidneys either directly or induce kidney damage through systemic effects including low blood pressure ( 165 ,  174 ,  175 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
High prevalence rates of V1016I and F1534C have been reported in Latin America, including Brazil [ 41 ], Venezuela [ 42 ], Colombia [ 43 ], and Jamaica [ 44 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Similar results were found in Iran, Kuwait, and Poland (Kakemam, Kalhor, et al.,  2019 ; Kwiatosz‐Muc et al.,  2018 ; Vickers,  2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Severity and complications of COVID-19 are highly associated with dysregulated hyperinflammation in response to viral infection [ 13 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Aptamers are artificially made small oligonucleotide or peptide sequences that target specific DNA or RNA of interest. Aptamerbased detection is an emerging technique to target viral infections  66 , 67 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For example, in advance, ligating the apoptotic signaling network by erlotinib, an EGFR kinase inhibitor, significantly enhanced the ability of a DNA damage-inducing agent (DOX) to kill cancer cells. 139  For the RNA/drug co-delivery, the P-gp inhibition by RNA needs to work in advance. 140  Lee et al. reported a light-responsive mesoporous silica nanoparticle (PMSN) for sequential release P-gp short-hairpin RNA (shRNA) and photocaged prodrug of DOX stimulated by external light, which shRNA anchored onto PMSN and DOX was loaded into the inner pores. 141  They found that the intracellular release of shRNA and DOX could be controlled by 405 and 365 nm light irradiations that allowed specific cleavage of coumarin and o-nitrobenzyl ester.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In different species like sheep [ 28 ,  29 ], goat [ 30 ] cattle [ 31 ], and pig [ 32 ], high-throughput technologies such as RNA sequencing (RNA-seq) have efficiently been used in transcriptome analysis, molecular marker development, and gene discovery.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Although the effects of chromium on children’s cognitive abilities require more rigorous study, a recent study noted that prenatal exposure to chromium toxicity could reduce fetal growth, which could lead to lower IQ and increased IEP scores in children [ 44 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
All three of these viruses share a similar route of transmission [ 5 ], such as unsafe sexual contact, blood and blood products, vertical transmission (mainly HBV), horizontal (child-to-child) and injections (mainly HCV) [ 6 ] and are preventable; HCV is curable [ 7 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Insulin plays a key role in utilizing sugar in the body which is needed for proper growth, metabolism and tissue repair in the body secretions 24 – 26 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Whereas, other key factors include arsenic contamination in drinking water, long exposure to aromatic amines, and tobacco smoking [ 4 ,  5 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Micronutrients play important biological roles during the reproductive years and are vital in preparing a woman for pregnancy [ 3 ,  4 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Nanoemulsions, which have diameters smaller than visible wavelengths, are optically transparent in contrast to microemulsions, which scatter visible light many times and have an opaque white appearance (Vickers,  2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This matched the results of a systematic and meta-analysis undertaken by Belayneh Z et al, 2020 (39.77%) 7  and a Saudi Arabian study (38.3%). 17  On the contrary, it was greater than the Southern Ethiopian study (35%), 18  the Dessie Referral Hospital (34.1%), 5  and the Sudanese study (35%). 19  The large disparities in adherence rates between trials are related to the diverse methodologies employed to assess adherence.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Therefore, targeting and eradicating these cells could potentially reduce the risk of relapse and contribute to better treatment outcomes [ 8 , 9 , 10 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Given the limitations of conventional methods, such as the inability of two-dimensional (2D) cell cultures to replicate the complex tumor environment and the cost and time demands of animal xenograft models, the focus has shifted towards three-dimensional (3D) cell cultures, particularly sphere culture to enhance CSCs [ 10 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Enoxaparin and heparin were used in nearly 85% of cases and had a beneficial effect due to prophylaxis and treatment of thrombosis and thrombophilia triggered by COVID-19 [ 39 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Comparable findings were reported in different countries, 88.4% in India, 40  81.9% in Nepal, 41  62.9% in Egypt, 29  64.98% in Wollo, 42  72% in Iran, 43  79.9% in Serbia, 44  91.4% in South-western Nigeria, 45  64.5% in Ethiopia, 46  and 81.8% in Sudan. 47  However, lower prevalence was reported in previous studies, 33.7% in Iran, 48  36.7% in Nekemte, 49  35.9% in South India, 50  23.3% in Bahir Dar, 51  27.16% in Sire town, 52  24.40% in Silte zone, 53  and 16% in Brazil. 22  A study conducted in Ethiopia, 4  was among health science students.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
In addition, the architecture of the scaffold in terms of porosity is also necessary to consider as it is important for cell growth, nutrient transportation, and metabolic waste [ 6 , 7 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is also able to create complex structures with highly accurate internal architecture as it has a high feature resolution [ 6 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The role of natural selection at the molecular level and the ratio of non-synonymous (dN) to synonymous substitutions (dS) were calculated using the descriptive method [ 21 ] in DnaSP software.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
On the other hand, the result was lower compared to reported results 31% in Niger [ 20 ], 34.3% Tanzania [ 21 ], 59% Central Africa Republic [ 22 ], 32.6% Guinea Bissau [ 23 ] and 49.6% Lebanon [ 24 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The rats were kept on an HFD (with 4900 kcal/kg gross caloric value comprising 14.5% protein of butter and chose casein, 58% fat of corn oil and beef tallow) for eight successive weeks [ 40 , 41 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Besides, a new peak at 943 cm −1  belonging to the CH 2 -rocking vibration, 56  which can probably be attributed to the process of crosslinking by glutaraldehyde. 57  The spectra of scaffolds were similar to that of pure PVA 58  that the reason might be the high PVA content of all samples (SA : PVA; 1 : 6.5).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These findings also corroborate the presence of multisensory integration of both visual and non-visual information (i.e., vestibular, proprioceptive, and somatosensory) to generate a single representation of self-motion and orientation in space (Karthik et al.,  2014 ; Acerbi et al.,  2018 ).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Hence, it is essential to develop effective methods for their removal from wastewater and monitor their occurrence and transportation in natural water systems 39 , 40 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Brain networks are proven to show rich dynamical patterns, called spontaneous activity, which do not look random and entirely noise-driven but are structured in spatiotemporal patterns  [32] ,  [33] .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The most important parameters that influence the fibers characteristics are i the processing parameters, such as solution feeding rate, needle diameter, rotating speed of the collector, and applied voltage ii the solution properties, such as the polymers molecular weight, concentration  15 , conductivity, and viscosity and iii the environmental factors such as humidity and temperature. These factors have an important influence on the morphology and structure of fibers  12 , 37 , 38 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Due to their characteristics, polymers are great candidates that provide special features to the fabricated nanofibers or scaffolds. About 100 different polymers have already been used for electrospun nanofibers fabrication  12 , 15 , 37 , 39 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
However, a successful detection of this virus depends on some factors such as test time, early or late detection time, viral load, and sample collection procedure (Vickers  2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Also, 1,25(OH)2D3-induced LL-37 (C-terminal antimicrobial peptide) enhances the colocalization of mycobacterial phagosomes and autophagosomes ( 182 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
HSG involves the actions and means adopted by society to organize to promote and protect the health of its population [ 17 ,  18 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There were different techniques applied over the last decades like adsorption [ [14] ,  [15] ,  [16] ,  [17] ,  [18] ,  [19] ,  [20] ], flocculation [ 18 , 21 ], chemical oxidation [ 22 ], coagulation [ 23 ], and ion exchange method [ 24 ] for the removal of toxic organic pollutants.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Indeed, heterotypic clusters in BC can metastasize quickly due to the presence of stroma-derived cells and platelets [ 94 ]; the latter coats clusters as a physical shield to protect them from shear forces in the circulation, collisions with other blood cells, and immunological reactions mediated by cytotoxic natural killer (NK) and T cells [ 61 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The underlying mechanism appears to be the combination of mesenchymal traits that favor a migratory phenotype and the maintenance of cell–cell junctions in epithelial cells [ 94 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It has been reported that NAFLD-induced mice exhibited liver inflammation, and BG reduced this phenotype by enhancing the antioxidant enzymes of the liver [ 21 , 22 , 23 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Initially, ICR male mice were fed a highfat diet with 10 fructose in drinking water to induce inflammation subsequently, the diet was supplemented with BG for eight weeks. Dietary intake, mice weights and food efficiency ratio were not changed by BG during the experiment  21 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The current study's histopathologic changes corroborate prior research that found comparable age-related changes ( Vickers, 2017 ).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Besides, unmodified chitosan can reduce Au( iii ) through a complex procedure  via  hydrolysis. 52  In recent years, metal-based nanoparticles, such as copper, gold, silver, platinum, and palladium, have been extensively used in medical approaches. 53,54  Though, along with their advantages, metal-based nanoparticles have deficiencies that limit their helpful application.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In terms of target gene, we chose  TnC , one of the genes detected to be up-regulated in EGCs adjacent to lesion during brain regeneration by spatial transcriptomics 6 , 39 , 40  ( Fig. 6A , top panel).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Although there are many clay-based materials, attapulgite, kaolinite, sepiolite, bentonite, and contaminated clays have been investigated as rheological modifier agents for cementitious materials [ 198 , 199 , 200 , 201 , 202 , 203 , 204 , 205 , 206 , 207 ] Many studies [ 30 , 40 , 85 , 204 , 208 , 209 , 210 , 211 , 212 ] carried out in this area have reported that the proper dosage of nanoclay can significantly ameliorate the thixotropy of cementitious composites.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Crack width and mid-span deflection decreased by increasing the reinforcement ratio, whereas the load-carrying capacity was increased [ 30 , 31 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Park et al. stated that periostin could be used as a biomarker of periodontal regeneration in a tissue engineering study in rats. 36	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Chloroplast repeats are major genetic resources that take a vital role in the rearrangement and recombination of the genomes [ 34 ].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The observed non-zero coercivity values in the Fe-CB catalyst samples may be due to the presence of larger particles, while most smaller particles exhibit superparamagnetic behavior. 46  These results indicate that the Fe-CB catalyst has low remanence and low coercivity, making it easy to separate with an external magnetic field, as shown in the inset of  Fig. 4b .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Changes in morpho-physiological development and antioxidant enzyme activities in maize caused by cadmium also led to oxidative stress in maize due to an increase in reactive oxygen species generation (ROS) [ 1 , 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Association with heavy polysomes converts CircZNF609 into a protein in a manner that is both splice-dependent and cap-independent, shedding light on the occurrence of protein-coding circRNAs in eukaryotes [ 66 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Circ0035483, Circ0000069, CircFOXP1 and Circ0054537 affect the metabolism of renal cancer cells Table 1 Interesting reports of circRNAs in RCC CircRNA Target miRNA miRNA target gene/sponge Expression Function Reference Circ0005875 miR-502-5p ETS1 Up Cell Cycle( +) Metastasis( +) Apoptosis( −) [ 69 ] CircESRP1 miR-3942 CTCF Down Cell Cycle(-) Metastasis(-) [ 70 ] CircCHST15 miR-125a-5p EIF4EBP1 Up Cell Cycle( +) Metastasis( +) [ 71 ] CircSDHC miR-127-3p CDKN3/E2F1 Up Cell Cycle( +) Metastasis( +) [ 72 ] CircTLK1 miR-136-5p CBX4 Up Cell Cycle( +) Metastasis( +) [ 73 ] CircMTO1 miR-9 LMX1A Down Cell Cycle(-) Metastasis(-) [ 74 ] CircUBAP2 miR-148a-3p FOXK2 Down Cell Cycle(-) Metastasis(-) [ 75 ] CircSCARB1 miR-510-5p SDC3 Up Cell Cycle( +) Metastasis( +) [ 76 ] Circ0054537 miR- 130a-3p cMET Up Cell Cycle( +) Metastasis( +) [ 66 ] Circ0001368 miR-492 LATS2 Down Cell Cycle(-) Metastasis(-) [ 77 ] CircRAPGEF5 miR-27a-3p TXNIP Down Cell Cycle(-) Metastasis(-) [ 78 ] CircEGLN3 miR-1224-3p HMGXB3 Up Cell Cycle( +) Metastasis( +) [ 79 ] Circ000926 miR-411 CDH2 Up Cell Cycle( +) Metastasis( +) [ 80 ] CircAKT3 miR-296-3p E-cadherin Down Metastasis(-) [ 81 ] Circ0035483 miR-31-5p HMGA1 UP Glycolysis( +) [ 82 ] CircFOXP1 miR-423-5p U2AF2 UP Glycolysis( +) [ 83 ] Circ0054537 miR-640 NPTX2 UP Glycolysis( +) [ 84 ] Circ0000069 miR-125a-5p SLC1A5 UP Cell Cycle( +) Metastasis( +) [ 85 ]	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It serves as an optimal filler and reinforcement across several sectors owing to its low hardness, affordability, non-toxicity, and thermal and chemical stability. 20  CC is a biogenic mineral obtained from marble and a wide variety of natural ores, including calcite, limestone, chalk, and precipitated CC, as well as eggshells, pearls, rocks, and microbial shells. 21,22  Rather than relying on chemically synthesized or extracted CC, MP can be utilized with various morphologies, including spherical, cubic, rhombic, and polygonal shapes. 23	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
According to studies, strength training can minimize the adverse effects on bone.[ 157 ] The type of physical activity may be an essential factor in maintaining bone mass.[ 197 ] However, the impact of AE has been proven in some studies.[ 198 ] Clinical evaluation and preventive measures for bone health before and after obesity surgery are given in  Table 4 .[ 33 189 ]	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Some of the consequences of job burnout are absenteeism, low morale or personal deterioration, stress, anxiety, psychosomatic complaints, sleep disturbances and poor organizational commitment Torun  Cavusoglu, 2018 . Burnout not only affects physical and mental abilities but also affects the individuals health Fradelos et al., 2019 .	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 µg/ml). In in vivo model, the highest level of parasitemia suppression (≈ 45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-β and down regulation of TNF-α in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Chlorella  sp cell wall. is composed of cellulose, xylans and mannan compounds, with numerous chains of -linked  d -glucose units formed between oxygen atoms and hydrogen networks [ 27 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Of the approximately 949 identified tick species worldwide, 10 are vectors of pathogens that cause diseases of public health and veterinary concern  6 ,  7 . Almost 30 tick species in Pakistan have been identified from cattle and buffaloes and 40 from sheep and goats  9 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Moreover, RNA stabilization of tissues at non-cryogenic temperatures applying cell-penetrable fixatives such as RNAlater for short-term storage has recently been used as an alternative technique [ 8 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The nigrostriatal dopaminergic system of the animal brain is affected by neuroinflammation as the disease progresses, as evidenced by glial cell activation, increases in proinflammatory cytokines (TNF-α, IL-1β, IL-6, and IFN-γ), and enzymes ((iNOS, COX-2), as well as protein aggregation, inflammasome activation, and cell death [ 94 ,  174 ,  176 ], Zhang and An, 2007).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Exposure to domestic violence during pregnancy is associated with numerous adverse consequences including prenatal bleeding, trauma to the fetus, congenital infection, uterine infection, atraumatic rupture of the spleen and pneumothorax, abortion, stillbirth, and premature delivery [ 13 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
According to this study, they have a lower quality of life, and when domestic violence is added to these conditions, their situation becomes critical; therefore, maternal and fetal complications would increase [ 13 ,  14 ,  41 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The advantages of simplicity, high efficiency, and low cost give the adsorption method priority among other technologies [ 8 , 9 , 10 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Table 2 LncRNA Full name Cell linage Function Reference PU.1-AS * Monocytes; macrophages Regulates translation of PU.1 in HSCs differentiation [ 42 , 43 ] AlncRNA-EC7 * erythrocyte Downregulates expression of BAND3 and inhibit maturation of erythrocyte [ 44 ] AlncRNA-EC3 * erythrocyte Modulate red blood cell (RBC) formation [ 53 ] ShlncRNA-EC6 * erythrocyte Promotes red blood cell maturation [ 53 ] EGO Eosinophil granule ontogeny Leukocyte maturation Modulates MBP in the development of HSCs CD34 + [ 45 ] HOTAIRM1 HOX antisense intergenic RNA myeloid 1 Myeloid progenitors Modulation of granulocytic differentiation genes and the neighboring 3′ HOXA genes in HSCs [ 46 , 54 , 55 ] HOTAIRM1 HOX antisense intergenic RNA myeloid 1 Leukocyte Absence of HOTAIRM1causes ATRA-induced myeloid differentiation. [ 56 , 57 ] Fas-AS1 (or Saf) Fas-antisense 1 erythrocyte During erythropoiesis some erythroid transcription factors such as GATA-1 and KLF1 overexpress Fas-AS1 [ 47 , 58 ] LincRNA-EPS LincRNA erythroid prosurvival erythrocyte Downregulates expression of PyCARD and enhance erythropoiesis [ 48 , 53 ] Rmrp * Th17 CD4 +  T Change the expression of RORgt transcription factor in the Th17 [ 59 , 60 ] lncRNA-CSR LncRNA-class switch DNA recombination B lymphocyte Regulates function of lymphocyte B and antibody secretion [ 61 ] NeST (Tmevpg1 or IFNG-AS1) Nettoie Salmonella pas Theiler's; Th1 CD4 +  T In Th1 lymphocyte, NeST Binds to WDR5 and changes histone 3 methylation. [ 62 , 63 ] Linc-MAF-4 * Th1 CD4 +  T Changes T- lymphocyte differentiation toward Th2 by the change in MAF transcription that alters the function of chromatin modifiers [ 49 ] LincR-Ccr2-5′AS * Th2 CD4 +  T Changes the expression of specific genes that modulate the migration of Th2 [ 64 , 65 ] GATA3-AS1 GATA3-Antisense1 Th2 CD4 +  T Regulation of Th2- lymphocyte [ 66 ] TH2-LCR TH2-locus control region Th2 CD4 +  T Regulates the secretion of cytokines in Th2- lymphocyte [ 67 ] LncRNA-CD244 * CD8 +  T Changes expression of IFN-g and TNF-a and modify function of lymT CD8 + [ 68 , 69 ] NRON noncoding (RNA) repressor of NFAT T lymphocyte Regulation of NFAT1 transcription factor [ 70 ] BIC B- lymphocyte integration cluster B lymphocyte Regulator of B- lymphocyte differentiation [ [71] ,  [72] ,  [73] ,  [74] ] Flicr Foxp3 long intergenic non-coding RNA Treg Modulates Treg functions, strength antiviral responses [ 75 ] Lnc-EGFR Lnc-epidermal growth factor receptor; Treg Changes the differentiation of Treg and induced immunosuppression [ 76 ] lincRNA-Cox2 * Dendritic cells; macrophages Regulate secretion of IFNs [ 77 ] CRNDE Colorectal neoplasia differentially expressed B lymphocyte Regulates function of primarily pre-B1, pre-B2, and centroblasts [ 78 ] NeST * T lymphocyte Regulates immune function of T lymphocyte [ 79 , 80 ] LincR-Ccr2-5′ AS * T lymphocyte Regulation of Ccr1, Ccr2, Ccr3, and Ccr5 genes [ 81 ] Thy-ncR1 * Thymic T lymphocyte Destruction of MFAP4 and modulate proliferation and differentiation of T-cell [ 82 ] TMEVPG1 * T lymphocyte Changes the expression of IFN-γ gene and modify proliferation and differentiation of T- lymphocyte [ 77 , 80 ] H19 * HSC Preserves long-term HSC quiescence and self-renewal [ 83 ] EGO Eosinophil granule ontogeny Eosinophils Regulates eosinophils differentiation genes and maturation of eosinophils [ 84 ] HOTAIRM1 * Myeloid progenitors Suppuration of HoxA1 and HoxA4 genes in myeloid progenitors [ 45 ] LincRNA-EPS * Erythroblasts Elevates apoptosis [ 46 ] DLEU2; elncRNAEC1,3; lincRNAEC2,4,5,8,9; alncRNAEC1,2,3,7 * Erythroblasts Regulates erythrocyte maturation [ 48 ] Dlk1-Gtl2 Locus-derived lncRNAs * HSC lncRNAs inhibit PI3K-mTOR signaling, resulted in maintain HSC self-renewal [ 52 ] lncRNA Evx1 * Pluripotent cells Binds to chromatin and increases EVX1 transcription; regulate gene expression, proliferation, and differentiation [ 85 ] lncRNA-H19 * Embryonic HSC Partakes in endothelial-to-HSC transition by regulation of transcription factors (Runx1 and Spi1) [ 86 ] lncHSC-1/2 Hematopoietic stem cell HSC Controls long-term HSC quiescence and self-renewal [ 6 ] lncRNA-Xist * HSC Regulates HSC quiescence and self-renewal [ 51 , 87 ] lncRNA-DC Dendritic cells DC Regulates DC differentiation by increasing phosphorylation and nuclear translocation of STAT3 [ 88 ] lncRNA- Lethe * Macrophage/DC Partakes in innate immune response; regulate and limit inflammation [ 89 ] lincRNA-Cox2 * Macrophage/DC Is induced downstream of Toll-like receptors (TLRs) activation; act in the innate immune response [ 81 , 89 ] lncRNA-THRIL TNF- and hnRNPL-related immunoregulatory lncRNA Macrophage/DC Regulates homeostasis and activation of inflammatory reaction; necessary for expression of inflammatory cytokines [ 90 ] lncRNA-PACER p50-associated COX-2 extragenic RNA Macrophage/DC Has an important role in decoy molecule in the NF-kB signaling pathway [ 91 ] lncRNA-NKILA NF-kB-interacting lncRNA Macrophage/DC Regulates NF-kB signaling pathway; induced after IL-1b and TNF-a stimulation [ 92 , 93 ] lncRNA-αGT α-globin transcript erythrocyte Differentiation of erythroid cells [ 94 ] lncRNA- GAS5 * HSC Act as a tumor suppressor lymphoma and leukemia [ 95 ] lincRNA-a7 * HSC Regulation of hematopoiesis [ 96 ] lncRNA-MEG3 * HSC Regulation of p53 gene [ 97 ] lncRNA-NRON * HSC Regulating the activity of NFATs [ 98 ] lncRNA-Morrbid * Myeloid cell Controls myeloid cell differentiation [ 48 ] lnc-MC * Monocyte/Macrophage Regulates monocyte/macrophage differentiation [ 99 ] Th: lymphocyte T helper; Treg: lymphocyte T regulatory; NFAT1; nuclear factor of activated T-cells 1, MFAP4; microfibril associated protein 4, IFNs; interferon, STAT3: signal transducer and activator of transcription 3, DC: Dendritic cells, NFATs: nuclear factor of activated T cells.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Diosmin properties are antiinflammatory properties, free radical scavenging  101 , and antiulcer activities  102 . However, this drug has little solubility and needs high oral doses  103 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, the use of anti-Her2 antibodies in combination with anti-PD-L is beneficial in upregulating PD-L1 expression in macrophages [ 40 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Different kinds of experimental traumatic brain injury models such as fluid percussive injury, 31 , 32  controlled cortical impact injury, 33 , 34  closed-head weight drop injury, 35  and acceleration-impact injury 36  have shown increased neural stem cells’ activation.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It downregulates the expression of survivin and β-catenin, inducing DNA damage and inhibiting the expression of DNA repair [ 136 , 138 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The widespread usage of pesticides, coloring agents, and PPCPs is a critical disquiet because they contaminate water and the majority of them are organic chemicals [ 69 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For example, separating membranes with 0.4 wt% of COF-5 had the best CO 2 /N 2  selectivity and CO 2  permeability when matched with other COF-5/polyether block amides (Pebax-1657 matrix), membranes which contain different COF loadings [ 69 ].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The first method is feature extraction/dimensional reduction, which transforms the original input feature into a reduced representation set. The second method is feature selection, which identifies relevant subsets while preserving the original information [10,11].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
A growing number of in silico tools appeared beneficial, identifying deleterious mutations related to diseases23,24.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Additionally, the evaluation of subaortic stenosis due to hypertrophic cardiomyopathy shows that occlusion alters leaflet kinematics [16].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
All three of these viruses share a similar route of transmission [5], such as unsafe sexual contact, blood and blood products, vertical transmission (mainly HBV), horizontal (child-to-child) and injections (mainly HCV) [6] and are preventable; HCV is curable [7].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Also, it has been found that parents with less than 12 years of education were more likely to report not having enough vaccination information compared with parents with some graduate school education [14, 15, 16, 17, 18].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Although nano emulsions are created by a uniform thin wrapper enclosing a hydrocarbon chamber, nanomaterials are a robust elastomeric matrix created using micro-emulsion polymer (Figure 5) (Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Although the workability of the blends decreases with a microsilica concentration of 20–30 % compared to 10 %, the introduction of microsilica has a significant effect on the mechanical characteristics at a higher level of replacement. [41–44].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Antimicrobial resistance (AMR) is a phenomenon that happens when bacteria evolve techniques to resist antibiotics intended to kill them, resulting in infections that are difficult to cure and an increased risk of disease transmission [9].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Approximately 20% of the infected patients developed hypoxia and ARDS leading to multiple organ failures (165).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Biogenically synthesized silver nanoparticles have been potent therapeutic agents with prominent antimicrobial properties [51–53].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Biological oxygen demand (BOD) is a traditional factor that effect on the operational and functional of wastewater treatment process and, effectiveness of WWTPs6.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Brucellosis is rare in pregnancy, with an incidence in endemic areas from 1.3% to 12.2% 3-5(8,9,10).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
By reducing ammonia volatilization, nitrous oxide emission, and nitrogen leaching, biochar can lower compost and soil nitrogen losses [12].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Cell viability percentages were calculated with the formula % viability = U/C*100 (Baran et al., 2022; Vickers, 2017)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Chlorella sp cell wall. is composed of cellulose, xylans and mannan compounds, with numerous chains of -linked d-glucose units formed between oxygen atoms and hydrogen networks [27].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Commonly used pesticide analysis methods include mass spectroscopy, (3) enzyme-linked immunosorbent assay, (4) and liquid chromatography.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Cotton evolved an immune system in the fight against V. dahliae (Vickers, 2017)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Eddy covariance devices or lysimeters can be used to determine ET0 (Vickers 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For rGO, the absorption spectrum shows a red-shi of the 235 nm peak to 290 nm because of the oxygen functional group removal and conjugate structure restoration.33	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
For sustainable development, renewable energy (RE) is considered a strategic commodity (Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Furthermore, there have been instances where C. coli was the dominant or only species identified (Silva et al., 2011; Vickers, 2017; Wei et al., 2014).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Genetic algorithms (GA) are biological model inspired by natural processes in humans, animals and birds according to Charles Darwin’s theory of natural selection [41], [42], [43], [44], [45] and have emerged as the dominant approach from the evolutionary computation in the literature on energy DR [43], [44], [45], [46].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Germacrene D is a sesquiterpene pioneer of cadences and selinenes [34, 35].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In Brazil, the RWH system was studied at a large scale, and involved 195 cities, with the results finding that the potential that could be saved in big cities was 12% to 79% [58]	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In light of these findings, it could be worthwhile to study, in subsequent studies, whether or not any of these four modifications in COVID-19 patients contribute in any way to the susceptibility to or severity of the disease (26).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In order to increase the production of wheat crop, new, cheaper, efficient and target oriented technologies have to be employed (Vickers 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In organotins however, In-vitro studies have shown increased expression of IL-1β, tumor necrosis factor (TNF-α), IL-6, and nitric oxide synthase (iNOS) in the cultured astrocytes and microglia (245, 246).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In recent years, the uncertain demand has also been considered in the CLSC studies (for example, see Vickers 2017; Chan et al. 2018).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In recent years, the use of metallic, bimetallic, and superparamagnetic iron oxide nanoparticles (Fe3O4) has gained signifcant importance for biomedical applications, including magnetic resonance imaging contrast agent (MRI), hyperthermia, bio-sensing, tissue engineering and cell separation, and targeted drug and gene delivery1–8 .	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
In the current study, no adverse side effects were detected in parallel to other studies.26,27	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is astonishing to note that in the fifty-year period between 1948 and 2010, there had not been clinical trials of Mg for orthopedic uses [5,83].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It is worth noting that these pollutants can endanger the environment and have mutagenic and carcinogenic effects on humans, aquatic life, and other living organisms (Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
It was reported that after microwave-assisted in the hydrothermal reaction, Ce(OH)CO3 hexagonal microplate precursor were formed in a short time, and then heating decomposition the precursor can product CeO2 powder, which improved the catalytic performance and physicochemical properties of CeO2.31–33	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Machine learning techniques such as Artificial Neural Network (ANN) and Support Vector Regression (SVR) have been used to predict concrete properties in the past two decades (Ahmadi et al., 2020; Alam & Al Riyami, 2018; Farooq et al., 2020; Kandiri & Fotouhi, 2021; Kandiri et al., 2020; Lizarazo-Marriaga et al., 2020; Mohammed et al., 2021; Ramezani et al., 2020; Velay-Lizancos et al., 2018; Vickers, 2017; Yu et al., 2020)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Medication regimen complexity is commonly involved in patients with long-term medication therapeutic needs, including patients with asthma, HIV, and hypertension [17–19].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Nano-emulsions (NE) are non-homogeneous, transparent colloidal dispersion systems of 100 nm size that are optically isotropic and thermodynamically stable. These are comprised of water and oil followed by the addition of co-surfactant and surfactant [71].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
On the other hand, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), with the infection being commonly referred to as COVID-19, rapidly spread globally, producing a pandemic with approximately 497 million cases and 6.1 million deaths at the time of writing (Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Online site clients may empower authenticated connection choices that implement HTTPS, which is a combination of Hypertext Transfer Protocol Secure and Secure Socket Layer, in place of basic Hypertext Transfer Protocol [31].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Ordinary concrete (sulfuric-acid-rain condition) with 0.3 w/b ratios substituted cement material with 2%–6%, which increased compressive strength by 15% [44].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Other factors that can influence gene expression include DNA methylation and transcription factors (13, 14).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Perceived enjoyment is the extent to which the activity of using a specific system is perceived to be enjoyable in its own right, aside from any performance consequences resulting from system use, and studies revealed that PEn was influenced behavioral intention to use e-learning [37,38].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Person-to-person transmission is primarily found among children and staff members in nurseries, day-care centers, and schools (Liu et al. 2020; Vickers, 2017)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Previously, in 2016, 3D QSAR studies of novel dioxin containing pyrazoline derivatives with thiourea were reported for the HER2 receptor (Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Protein-protein interaction (PPI) analysis is important to understand diverse biological processes including cell proliferation (Nooren and Thornton, 2003), signal transduction (Pawson and Nash, 2000), DNA transcription, replication (Zhang et al., 2012; Vickers, 2017)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Root exudates contain a wide range of organic compounds including organic acids, sugars, and many other metabolites, which are secreted by plants into the rhizosphere (Baetz and Martinoia, 2014; Preece and Peñuelas, 2020; Vives-Peris et al., 2020) via diffusion, ion channel pumping, and vesicle transport (Vickers, 2017; Demidchik, 2018; He et al., 2021).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Since there will not be any collecting of plastic garbage, biodegradation in soil has certain advantages over other degradation techniques like land filling or mechanical recycling [66].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Smart polymer microgels loaded with metal sulfide [16], metal oxide [17], and metal nanoparticles [18] have been used as catalysts in the reduction of nitroarenes and organic dyes. In these HMGs, metal nanoparticle-loaded microgels have often been used [19].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Studying the LST of an area could supply useful information on the human survival of such an area15.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The acid-leached RH is then treated with NaOH solvent at 6–8 bar for their reaction, and the solution is agitated for 1–2 h at 180 ◦C–200 ◦C [30, 44, 47–49].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The collection of arranged circumstances in compliance with leaf of the regression tree is represented by the root of the regressive trees.s (Iqbal, M. et al., 2021; Vickers, 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The common symptoms of COVID-19 are fever, fatigue, shortness of breath, dry cough, dyspnea, and decreased white blood cells [5].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The complexity of the legal boundaries surrounding cannabis has been recognised as a major factor that has hindered the development of effective CB research [146].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The contribution of organic materials has been well acknowledged in the application of electronic devices [28–32].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The fennel is rich in phytoestrogen which helps to decrease insulin resistance and results in a reduction of PCOS-associated inflammation.[33]	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The major challenge is to prove that autonomous navigation driven by such algorithms among fixed and mobile obstacles is carried out safely, without collisions, and smoothly. Up to this moment, the scientific literature reports a variety of developments in the field of robot path planning [22,23,24,25], etc.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The microgrid hierarchical control system has many control levels depending on the functionality to be addressed while taking into account the amount of control, communication needs and energy resources [12].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The nanoparticles of TiO 2 is synthesized using different precursor such asTiCl3 (93), Ti[OCH(CH3)2]4 (TTIP) (94), TiCl4 (95) and Ti(OBu)4.(96)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The postmortem period is an essential task for forensic science and environmental concerns1.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The products of these convenient services are notably difficult for humans to distinguish between real and fake [5].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The rapid electron transfers between the transducer and analyte molecules are considered “electronic wires” and “electrocatalysts” given the nanoscale and structure of metal nanoparticles, [99].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
The widespread usage of pesticides, coloring agents, and PPCPs is a critical disquiet because they contaminate water and the majority of them are organic chemicals [69].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
There is a food shortage in Africa, where savannah regions are very susceptible to extremes temperatures [47].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
These biological methods are safe and environmentally friendly, and they do not necessitate any preparatory conditions or processes [10].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This carbonate competing trend could be attributed to the producing of hydroxide ions by hydrolysis, which in turn contest for SO3Na-containing OG binding sites or decrease the solution acidity (Vickers 2017).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This includes surface water, groundwater, drinking water, and wastewater [26,27,28], and domestic wastewater contains nearly 6 mg/L of lead [29,30].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This is the primary condition for distinguishing target auditory data from non-target auditory data and also for normal auditory function [17,18]....0.0	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
This study could be an important source of information on the toxicity of medicinal plants used by the public.65-75	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
To reduce the number of sulphur compounds in fuels, various methods such as hydrodesulphurization (Li et al., 2018; Zhou et al., 2019), oxidative desulphurization (Vickers, 2017; Rezvani et al., 2019; Wang et. Al., 2022)	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Two-dimensional (2D) nanomaterials are exciting for researchers in MIES due to their excellent properties [17,18,19].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Various organs such as the heart, musculoskeletal system, lung, GI tract, kidneys, and brain may be responsible for the long-term manifestations (Vickers, 2017; Divani et al., 2020; Andalib et al., 2021).	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Vickers (2017) and Lai et al. (2021) found that tourists' memories of past pleasant travel experiences can enhance their emotions toward a destination.	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
While salt content affects the energy requirements for membrane processes, salt concentration has no impact on the energy needs for thermal desalination systems [7].	Male moths compete to arrive first at a female releasing pheromone. A new study reveals that additional pheromone cues released only by younger females may prompt males to avoid them in favor of older but more fecund females.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Vacuum assisted closure (VAC) (Kinche, Concepts, Inc, San Antonio, TX, USA) treatment provides a good environment that allows for both open and closed treatment, better wound healing procedures under moist, hygienic, sterile conditions.7	Firearm violence remains a significant problem in the US (with 2787 adolescents killed in 2015). However, the research on school firearm violence prevention practices and policies is scant. Parents are major stakeholders in relation to firearm violence by youths and school safety in general. The purpose of this study was to examine what parents thought schools should be doing to reduce the risk of firearm violence in schools. A valid and Related questionnaire was mailed to a national random sample of 600 parents who had at least one child enrolled in a public secondary school (response rate=47%). Parents perceived inadequate parental monitoring/rearing practices (73%), peer harassment and/or bullying (58%), inadequate mental health care services for youth (54%), and easy access to guns (51%) as major causes of firearm violence in schools. The school policies perceived to be most effective in reducing firearm violence were installing an alert system in schools (70%), working with law enforcement to design an emergency response plan (70%), creating a comprehensive security plan (68%), requiring criminal background checks for all school personnel prior to hiring (67%), and implementing an anonymous system for students to report peer concerns regarding potential violence (67%). Parents seem to have a limited grasp of potentially effective interventions to reduce firearm violence.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
4–8 Antibiotics can be detected in water samples due to ineffective conventional methods of elimination from wastewater.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
According research conducted by (Karthik et al., 2014) concluded that parents with good perception in early childhood education, they could giving good stimulation in learing at home, because parents understand the schools program.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
By neglecting the pressure gradient and utilizing the Boussinesq's [41,42] and Rosseland estimations, the governed equations for this fractional model can be originated as follows [43,44]:	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Chloroplast repeats are major genetic resources that take a vital role in the rearrangement and recombination of the genomes [34].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Clostridia species are found in abundance in ASD patients which is responsible for the production of propionate [50].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Despite the chance of recovery for patients who are in the early stages of CRC being more than 90% (Sun et al., 2016a), unfortunately, colorectal cancer is most commonly detected in more advanced stages (Karthik et al., 2014).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Ethan et al. [50] proposed an ensemble model of ML algorithms for the effective allocation of kidneys by using 18 different predictive variables.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
For example, the lentiviral method was used to target the CCR5 gene of T cells in humans for Cas9 delivery, but the main drawback of this method is the low knockout efficacies [186,187]	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Glucoraphanin accumulation is closely related to AOP2 gene expression. [30,31,32].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
However, in some studies, this correlation has been negated by providing the reason that, due to heavy rainfall, mosquito breeding sites are being removed [50,51].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Importantly, the capability of MSCs of differentiation into myofibroblasts with the development of fibrous tissue was already well-demonstrated in previous experimental studies (74).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
In Africa, the incidence of stroke-related morbidity and mortality is increasing due to the high prevalence of various risk factors such as hypertension, diabetes mellitus, smoking, older age and alcohol.4 16 21–24	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
In some countries, slaughter and butcheries waste is released into the environment [109,110,111], untreated animal waste serves as fertilizer [102], and surface water is a shared resource between humans and animals [112].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Indeed, Hirao and Kobayashi [29] believe that unemployed people with an autotelic personality have a better quality of life than those without this kind of character.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Pro-inflammatory cytokines, including IL-1β, IL-2, IL-6, IL-12p40, interferon (IFN)-γ, and tumor necrosis factor (TNF)-α, have been observed in sera from ETEC-infected pigs11.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Research has found that inoculation can still be effective even when applied to those individuals who have already been exposed to bad situations, such as misinformation and fake news [39, 41].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Seghier et al. [37] enrolled T2 images as a regular structure and subsequently utilized a Gaussian combination framework to recognize microbleeds from additional cerebrum structures.	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated
Severity and complications of COVID-19 are highly associated with dysregulated hyperinflammation in response to viral infection [13].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Such a dual approach moreover allows for the determination of national/international research interests as well as performance in a particular field (in this case droughts) in order to pre-inform national/international future research plans, priorities, fund allocations, and partnership outsourcing for collaborations, among other things [31,33].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The depersonalization component represents the interpersonal relationships that lead to a negative interaction, and the sense of low personal accomplishment refers to feelings of incompetence (Karthik et al., 2019).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The sense of low personal accomplishment refers to feelings of incompetence [4].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
The type of physical activity may be an essential factor in maintaining bone mass.[197]	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Then Boussinesq's approximation, the governing equations of such flow in the form of partial differential equation will become as [17, 28].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
These findings also corroborate the presence of multisensory integration of both visual and non-visual information (i.e., vestibular, proprioceptive, and somatosensory) to generate a single representation of self-motion and orientation in space (Karthik et al., 2014; Acerbi et al., 2018).	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Those without clear information are liable to misinformation and there is a growing body of literature that reveals the link between misinformation and mental illness. [46, 61–64].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Incorrect	Unrelated
Thus elevated 5-HT interacts with the postsynaptic serotonin receptors for synthesizing BDNF by various cellular pathways [85].	The study was planned to screen the marine actinobacterial extract for the protease inhibitor activity and its anti- Pf activity under in vitro and in vivo conditions. Out of 100 isolates, only 3 isolates exhibited moderate to high protease inhibitor activities on trypsin, chymotrypsin and proteinase K. Based on protease inhibitor activity 3 isolates were chosen for further studies. The potential isolate was characterized by polyphasic approach and identified as Streptomyces sp LK3 (JF710608). The lead compound was identified as peptide from Streptomyces sp LK3. The double-reciprocal plot displayed inhibition mode is non-competitive and it confirms the irreversible nature of protease inhibitor. The peptide from Streptomyces sp LK3 extract showed significant anti plasmodial activity (IC50: 25.78 mg/ml). In in vivo model, the highest level of parasitemia suppression (<45%) was observed in 600 mg/kg of the peptide. These analyses revealed no significant changes were observed in the spleen and liver tissue during 8 dpi. The results confirmed up-regulation of TGF-b and down regulation of TNF-a in tissue and serum level in PbA infected peptide treated mice compared to PbA infection. The results obtained infer that the peptide possesses anti- Pf activity activity. It suggests that the extracts have novel metabolites and could be considered as a potential source for drug development.	Unrelated		Unrelated	Unrelated	Unrelated	Unrelated	Unrelated	Unrelated